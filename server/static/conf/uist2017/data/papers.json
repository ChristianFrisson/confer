entities={
  "TOCHI 0014.R2": {
    "abstract": "Abstract: Emoji, a set of pictographic Unicode characters, have seen strong uptake over the last couple of years. All common mobile platforms and many desktop systems now support emoji entry and users have embraced their use. Yet, we currently know very little about what makes for good emoji entry. While soft keyboards for text entry are well optimized, based on language and touch models, no such information exists to guide the design of emoji keyboards. In this article, we investigate of the problem of emoji entry, starting with a study of the current state of the emoji keyboard implementation in Android. To enable moving forward to novel emoji keyboard designs, we then explore a model for emoji similarity that is able to inform such designs. This semantic model is based on data from 21 million collected tweets containing emoji. We compare this model against a solely description-based model of emoji in a crowdsourced study. Our model shows good performance in capturing detailed relationships between emoji.\n", 
    "authors": [
      {
        "name": "Henning Pohl"
      }, 
      {
        "name": "Christian Domin"
      }, 
      {
        "name": "Micheal Leibniz Rohs"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "TOCHI 0014.R2 Beyond Just Text: Semantic Emoji Similarity Modeling to Support Expressive Communication", 
    "type": "paper"
  }, 
  "uistf1006": {
    "abstract": "Low-fidelity prototyping at the early stages of user interface (UI) design can help designers and system builders quickly explore their ideas.  However, interactive behaviors in such prototypes are often replaced by textual descriptions because it usually takes even professionals hours or days to create animated interactive elements due to the complexity of creating them.  In this paper, we introduce SketchExpress, a crowd-powered prototyping tool that enables crowd workers to create reusable interactive behaviors easily and accurately. With the system, a requester\u00e2\u0080\u0094designers or end-users\u00e2\u0080\u0094describes aloud how an interface should behave and crowd workers make the sketched prototype interactive within minutes using a demonstrate-remix-replay approach.  These behaviors are manually demonstrated, refined using remix functions, and then can be replayed later. The recorded behaviors persist for future reuse to help users communicate with the animated prototype. We conducted a study with crowd workers recruited from Mechanical Turk, which demonstrated that workers could create animations using SketchExpress in 2.9 minutes on average with 27% gain in the quality of animations compared to the baseline condition of manual demonstration.", 
    "authors": [
      {
        "name": "Sang Won Lee"
      }, 
      {
        "name": "Yujin Zhang"
      }, 
      {
        "name": "Isabelle Wong"
      }, 
      {
        "name": "Yiwei Yang"
      }, 
      {
        "name": "Stephanie D O'Keefe"
      }, 
      {
        "name": "Walter S Lasecki"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "SketchExpress: Remixing Animations for More Effective Crowd-Powered Prototyping of Interactive Interfaces", 
    "type": "paper"
  }, 
  "uistf1143": {
    "abstract": "Live animation of 2D characters is a new form of storytelling that has started to appear on streaming platforms and broadcast TV. Unlike traditional animation, human performers control characters in real time so that they can respond and improvise to live events. Current live animation systems provide a range of animation controls, such as camera input to drive head movements, audio for lip sync, and keyboard shortcuts to trigger discrete pose changes via artwork swaps. However, managing all of these controls during a live performance is challenging. In this work, we present a new interactive system that specifically addresses the problem of triggering artwork swaps in live settings. Our key contributions are the design of a multi-touch triggering interface that overlays visual triggers around a live preview of the character, and a predictive triggering model that leverages practice performances to suggest pose transitions during live performances. We evaluate our system with quantitative experiments, a user study with novice participants, and interviews with professional animators.", 
    "authors": [
      {
        "name": "Nora S Willett"
      }, 
      {
        "name": "Wilmot Li"
      }, 
      {
        "name": "Jovan Popovic"
      }, 
      {
        "name": "Adam Finkelstein"
      }
    ], 
    "award": true, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Triggering Artwork Swaps for Live Animation", 
    "type": "paper"
  }, 
  "uistf1172": {
    "abstract": "FlexStylus, a flexible stylus, detects deformation of the barrel as a vector with both a rotational and an absolute value, providing two degrees of freedom with the goal of improving the expressivity of digital art using a stylus device. We outline the construction of the prototype and the principles behind the sensing method, which uses a cluster of four fibre-optic based deformation sensors. We propose interaction techniques using the FlexStylus to improve menu navigation and tool selection. Finally, we describe a study comparing users\u00e2\u0080\u0099 ability to match a changing target value using a commercial pressure stylus and the FlexStylus\u00e2\u0080\u0099 absolute deformation. When using the FlexStylus, users had a significantly higher accuracy overall. This suggests that deformation may be a useful input method for future work considering stylus augmentation.", 
    "authors": [
      {
        "name": "Nicholas Fellion"
      }, 
      {
        "name": "Thomas Pietrzak"
      }, 
      {
        "name": "Audrey Girouard"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "FlexStylus: Leveraging Bend Input for Pen Interaction", 
    "type": "paper"
  }, 
  "uistf1201": {
    "abstract": "Squeezing sensations are one of the most common and intimate forms of human contact. In this paper, we investigate HapticClench, a device that generates squeezing sensations using shape memory alloys. We define squeezing feedback in terms of it perceptual properties and conduct a psychophysical evaluation of HapticClench. HapticClench is capable of generating up to four levels of distinguishable load and works well in distracted scenarios. HapticClench has a high spatial acuity and can generate spatial patterns on the wrist that the user can accurately recognize. We also demonstrate the use of HapticClench for communicating gradual progress of an activity, and for generating squeezing sensations using rings and loose bracelets.", 
    "authors": [
      {
        "name": "Aakar Gupta"
      }, 
      {
        "name": "Antony Albert Raj Irudayaraj"
      }, 
      {
        "name": "Ravin Balakrishnan"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "HapticClench: Investigating Squeeze Sensations using Memory Alloys", 
    "type": "paper"
  }, 
  "uistf1335": {
    "abstract": "Ungrounded haptic devices for virtual reality (VR) applications lack the ability to convincingly render the sensations of a grasped virtual object's rigidity and weight. We present Grabity, a wearable haptic device designed to simulate kinesthetic pad opposition grip forces and weight for grasping virtual objects in VR. The device is mounted on the index finger and thumb and enables precision grasps with a wide range of motion. A unidirectional brake creates rigid grasping force feedback. Two voice coil actuators create virtual force tangential to each finger pad through asymmetric skin deformation. These forces can be perceived as gravitational and inertial forces of virtual objects. The rotational orientation of the voice coil actuators is passively aligned with the real direction of gravity through a revolute joint, causing the virtual forces to always point downward. This paper evaluates the performance of Grabity through two user studies, finding promising ability to simulate different levels of weight with convincing object rigidity. The first user study shows that Grabity can convey various magnitudes of weight and force sensations to users by manipulating the amplitude of the asymmetric vibration. The second user study shows that users can differentiate different weights in a virtual environment using Grabity.", 
    "authors": [
      {
        "name": "Inrak Choi"
      }, 
      {
        "name": "Heather Culbertson"
      }, 
      {
        "name": "Mark Roman Miller"
      }, 
      {
        "name": "Alex Olwal"
      }, 
      {
        "name": "Sean Follmer"
      }
    ], 
    "award": true, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Grabity: A Wearable Haptic Interface for Simulating Weight and Grasping in Virtual Reality", 
    "type": "paper"
  }, 
  "uistf1353": {
    "abstract": "This paper introduces a novel method for designing personalized orthopedic casts which are aware of thermal-comfort while satisfying mechanical requirements. Our pipeline starts from thermal images taken by an infrared camera, by which the distribution of thermal-comfort sensitivity is generated on the surface of a 3D scanned model. We formulate a hollowed Voronoi tessellation pattern to represent the covered region for a web-like cast design. The pattern is further optimized according to the thermal-comfort sensitivity calculated from thermal images. Working together with a thickness variation method, we generate a solid model for a personalized cast maximizing both thermal comfort and mechanical stiffness. To demonstrate the effectiveness of our approach, 3D printed models of personalized casts are tested on body parts of different individuals.", 
    "authors": [
      {
        "name": "Xiaoting Zhang"
      }, 
      {
        "name": "Guoxin Fang"
      }, 
      {
        "name": "Chengkai Dai"
      }, 
      {
        "name": "Jouke Verlinden"
      }, 
      {
        "name": "Jun Wu"
      }, 
      {
        "name": "Emily Whiting"
      }, 
      {
        "name": "Charlie C. L. Wang"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Thermal-Comfort Design of Personalized Casts", 
    "type": "paper"
  }, 
  "uistf1384": {
    "abstract": "Due to the development of 3D sensing and modeling techniques, the state-of-the-art mixed reality devices such as Microsoft Hololens have the ability of digitalizing the physical world. \\ This unique feature bridges the gap between virtuality and reality and largely elevates the user experience. \\ Unfortunately, the current solution only performs well if the virtual contents complement the real scene. \\ It can easily cause visual artifacts when the reality needs to be modified due to the virtuality (e.g., remove real objects to offer more space for virtual objects), a common scenario in mixed reality applications such as room redecoration and environment design. \\ We present a novel system, called \\\\emph{SceneCtrl}, that allows the user to interactively edit the real scene sensed by Hololens, such that the reality can be adapted to suit virtuality. \\ Our proof-of-concept prototype employs scene reconstruction and understanding to enable efficient editing such as deleting, moving, and copying real objects in the scene. \\ We also demonstrate \\\\emph{SceneCtrl} on a number of example scenarios in mixed reality, verifying the enhanced experience by resolving conflicts between virtuality and reality.", 
    "authors": [
      {
        "name": "Ya-Ting Yue"
      }, 
      {
        "name": "Yongliang Yang"
      }, 
      {
        "name": "Gang Ren"
      }, 
      {
        "name": "Wenping Wang"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "SceneCtrl: Mixed Reality Enhancement via Efficient Scene Editing", 
    "type": "paper"
  }, 
  "uistf1491": {
    "abstract": "This paper describes a simple 3D display that can be built from a tablet computer and a plastic sheet folded into a cone. This display allows naturally viewing a three-dimensional object from any direction over a 360-degree path of travel without the use of a head mount or special glasses. Inspired by the classic Pepper's Ghost illusion, our approach uses a curved transparent surface to reflect the image displayed on a 2D display. By properly pre-distorting the displayed image our system can produce a perspective-correct image to the viewer that appears to be suspended inside the reflector. We use the gyroscope integrated into modern tablet computers to adjust the rendered image based on the relative orientation of the viewer. The end result is a natural and intuitive interface for inspecting a 3D object.  Our choice of a cone reflector is obtained by analyzing optical performance and stereo-compatibility over  rotationally-symmetric conic reflector shapes. \\ We also present the prototypes we built and measure the performance of our display through side-by-side comparisons with reference images.", 
    "authors": [
      {
        "name": "Xuan Luo"
      }, 
      {
        "name": "Jason Lawrence"
      }, 
      {
        "name": "Steven M. Seitz"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Pepper's Cone: An Inexpensive Do-It-Yourself 3D Display", 
    "type": "paper"
  }, 
  "uistf1526": {
    "abstract": "Email is more than just a communication medium. Email serves as an external memory for people---it contains our reservation numbers, meeting details, phone numbers, and more. Often, people need access to this information while on the go, which is cumbersome from mobile devices with limited I/O bandwidth. In this paper, we introduce WearMail, a conversational interface to retrieve specific information in email. WearMail is mostly automated but is made robust to information extraction tasks via a novel privacy-preserving human computation workflow. In WearMail, crowdworkers never have direct access to emails, but rather (i) generate an email filter to help the system find messages that may contain the desired information, and (ii) generate examples of the requested information that are then used to create custom, low-level information extractors that run automatically within the set of filtered emails. We explore the impact of varying levels of obfuscation on result quality, demonstrating that workers are able to deal with highly-obfuscated information nearly as well as with the original. WearMail introduces general mechanisms that let the crowd search and select private data without having direct access to the data itself. ", 
    "authors": [
      {
        "name": "Saiganesh Swaminathan"
      }, 
      {
        "name": "Raymond Fok"
      }, 
      {
        "name": "Fanglin Chen"
      }, 
      {
        "name": "Ting-Hao Huang"
      }, 
      {
        "name": "Irene Lin"
      }, 
      {
        "name": "Rohan Jadvani"
      }, 
      {
        "name": "Walter Lasecki"
      }, 
      {
        "name": "Jeffrey Bigham"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "WearMail: On-the-Go Access to Information in Your Email with a Privacy-Preserving Human Computation Workflow", 
    "type": "paper"
  }, 
  "uistf1660": {
    "abstract": "Gestures have become an important tool for natural interaction with computers and thus several wearables have been developed to detect hand gestures. However, many existing solutions are unsuitable for practical use due to low accuracy, high cost or poor ergonomics. We present SensIR, a bracelet that uses near-infrared sensing to infer hand gestures. The bracelet is composed of pairs of infrared emitters and receivers that are used to measure both the transmission and reflection of light through/off the wrist. SensIR improves the accuracy of existing infrared gesture sensing systems through the key idea of taking measurements with all possible combinations of emitters and receivers. Our study shows that SensIR is capable of detecting 12 discrete gestures with 93.3% accuracy. SensIR has several advantages compared to other systems such as high accuracy, low cost, robustness against bad skin coupling and thin form-factor.", 
    "authors": [
      {
        "name": "Jess McIntosh"
      }, 
      {
        "name": "Asier Marzo"
      }, 
      {
        "name": "Mike Fraser"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "SensIR: Detecting Hand Gestures with a Wearable Bracelet using Infrared Transmission and Reflection", 
    "type": "paper"
  }, 
  "uistf1663": {
    "abstract": "Interaction in VR involves large body movements, easily inducing fatigue and discomfort. We propose Erg-O, a manipulation technique that leverages visual dominance to maintain the visual location of the elements in VR, while making them accessible from more comfortable locations. Our solution works in an open-ended fashion (no prior knowledge of the object the user wants to touch), can be used with multiple objects, and still allows interaction with any other point within user\u00e2\u0080\u0099s reach. We use optimization approaches to compute the best physical location to interact with each visual element, and space partitioning techniques to distort the visual and physical spaces based on those mappings and allow multi-object retargeting. In this paper we describe the Erg-O technique, propose two retargeting strategies and report the results from a user study on 3D selection under different conditions, elaborating on their potential and application to specific usage scenarios.", 
    "authors": [
      {
        "name": "Roberto Antonio Montano Murillo"
      }, 
      {
        "name": "Sriram Subramanian"
      }, 
      {
        "name": "Diego Martinez Plasencia"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Erg-O: Ergonomic Optimization of Immersive Virtual Environments", 
    "type": "paper"
  }, 
  "uistf1706": {
    "abstract": "We present a low cost method to measure and characterize the end-to-end latency when using a touch system (tap latency) or an input device equipped with a physical button. Our method relies on a vibration sensor attached to a finger and a photo-diode to detect the screen response. Both are connected to a micro-controller connected to a host computer using a low-latency USB communication protocol in order to combine software and hardware probes to help determine where the latency comes from. We present the operating principle of our method before investigating the main sources of latency in several systems. We show that most of the latency originates from the display side. Our method can help application designers characterize and troubleshoot latency on a wide range of interactive systems.", 
    "authors": [
      {
        "name": "G\u00c3\u00a9ry Casiez"
      }, 
      {
        "name": "Thomas Pietrzak"
      }, 
      {
        "name": "Damien Marchal"
      }, 
      {
        "name": "S\u00c3\u00a9bastien Poulmane"
      }, 
      {
        "name": "Matthieu Falce"
      }, 
      {
        "name": "Nicolas Roussel"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Characterizing Latency in Touch and Button-Equipped Interactive Systems", 
    "type": "paper"
  }, 
  "uistf1723": {
    "abstract": "We present a sphere-shaped interactive display system, named Qoom, as a new input and output device. Unlike existing sphere-shaped displays, Qoom is a perfectly spherical ball that can be rotated, thrown, or even kicked.  \\ First, we discuss how spherical displays can be used in daily life and describe how users interact with spheres.  \\ Then, we show how we developed the Qoom prototype that uses touch and rotation detection, real-time object tracking, and spherical projection mapping.  \\ We implemented actions including touching, rotating, bouncing and throwing as controls.  \\ We also developed applications for Qoom that utilize the unique advantages of ball displays.", 
    "authors": [
      {
        "name": "Shio Miyafuji"
      }, 
      {
        "name": "Zhengqing Li"
      }, 
      {
        "name": "Toshiki Sato"
      }, 
      {
        "name": "Hideki Koike"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Qoom: An Interactive Omnidirectional Ball Display", 
    "type": "paper"
  }, 
  "uistf1785": {
    "abstract": "We propose an immersive telepresence system for puppetry that transmits a human performer's body and facial movements into a puppet with audiovisual feedback to the performer. The cameras carried in place of puppet's eyes stream live video to the HMD worn by the performer, so that performers can see the images from the puppet's eyes with their own eyes and have a visual understanding of the puppet's ambience. In conventional methods to manipulate a puppet (a hand-puppet, a string-puppet, and a rod-puppet), there is a need to practice manipulating puppets, and there is difficulty carrying out interactions with the audience. Moreover, puppeteers must be positioned exactly where the puppet is. The proposed system addresses these issues by enabling a human performer to manipulate the puppet remotely using his or her body and facial movements. We conducted several user studies with both beginners and professional puppeteers. The results show that, unlike the conventional method, the proposed system facilitates the manipulation of puppets especially for beginners. Moreover, this system allows performers to enjoy puppetry and fascinate audiences.", 
    "authors": [
      {
        "name": "Mose Sakashita"
      }, 
      {
        "name": "Tatsuya Minagawa"
      }, 
      {
        "name": "Amy Koike"
      }, 
      {
        "name": "Ippei Suzuki"
      }, 
      {
        "name": "Keisuke Kawahara"
      }, 
      {
        "name": "Yoichi Ochiai"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "You as a Puppet: Evaluation of Telepresence User Interface for Puppetry", 
    "type": "paper"
  }, 
  "uistf1814": {
    "abstract": "We present MagTics, a novel flexible and wearable haptic interface based on magnetically actuated bidirectional tactile pixels (taxels). \\ MagTics' thin form factor and flexibility allows for rich haptic feedback in mobile settings. \\ We propose a novel actuation mechanism based on bistable electromagnetic latching that combines high frame rate and holding force with low energy consumption and a soft and flexible form factor. \\ We overcome limitations of traditional soft actuators by placing several hard actuation cells, driven by flexible printed electronics, in a soft 3D printed case. A novel EM-shielding prevents magnet-magnet interactions and allows for high actuator densities. \\ A prototypical implementation comprising of 4 actuated pins on a 1.7 cm pitch, with 2 mm travel, and generating 160 mN to 200 mN  of latching force is used to implement a number of compelling application scenarios including adding haptic and tactile display capabilities to wearable devices, to existing input devices and to provide localized haptic feedback in virtual reality. \\ Finally, we report results of a psychophysical study, conducted to inform future developments and to identify possible application domains.", 
    "authors": [
      {
        "name": "Fabrizio Pece"
      }, 
      {
        "name": "Juan Jose Zarate"
      }, 
      {
        "name": "Velko Vechev"
      }, 
      {
        "name": "Nadine Besse"
      }, 
      {
        "name": "Olexandr Gudozhnik"
      }, 
      {
        "name": "Herbert Shea"
      }, 
      {
        "name": "Otmar Hilliges"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "MagTics: Flexible and Thin Form Factor Magnetic Actuators for Dynamic and Wearable Haptic Feedback", 
    "type": "paper"
  }, 
  "uistf1910": {
    "abstract": "The small screen size of a smartwatch limits user experience when watching or interacting with media. We propose a supplementary tactile feedback system to enhance the user experience with a method unique to the smartwatch form factor. Our system has a deformable surface on the back of the watch face, allowing the visual scene on screen to extend into 2.5D physical space. This allows the user to watch and feel virtual objects, such as experiencing a ball bouncing against the wrist. We devised two controlled experiments to analyze the influence of tactile display resolution on the illusion of virtual object presence. Our first study revealed that on average, a taxel can render virtual objects between 70% and 138% of its own size without shattering the illusion. From the second study, we found visual and haptic feedback can be separated by 4.5mm to 16.2mm for the tested taxels. Based on the results, we developed a prototype (called RetroShape) with 4\u00c3\u00974 10mm taxels using micro servo motors, and demonstrated its unique capability through a set of tactile-enhanced games and videos. A preliminary user evaluation showed that participants welcome RetroShape as a useful addition to existing smartwatch output. ", 
    "authors": [
      {
        "name": "Da-Yuan Huang"
      }, 
      {
        "name": "Ruizhen Guo"
      }, 
      {
        "name": "Jun Gong"
      }, 
      {
        "name": "Jingxian Wang"
      }, 
      {
        "name": "John M Graham"
      }, 
      {
        "name": "De-Nian Yang"
      }, 
      {
        "name": "Xing-Dong Yang"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "RetroShape: Leveraging Rear-Surface Shape Displays for 2.5D Interaction on Smartwatches", 
    "type": "paper"
  }, 
  "uistf1973": {
    "abstract": "The creation of 3D contents still remains one of the most crucial problems for the emerging applications such as 3D printing and Augmented Reality. In Augmented Reality, how to create virtual contents that seamlessly overlay with the real environment is a key problem for human-computer interaction and many subsequent applications. In this paper, we present a sketch-based interactive tool, which we term \\\\emph{SweepCanvas}, for rapid exploratory 3D modeling on top of an RGB-D image. Our aim is to offer end-users a simple yet efficient way to quickly create 3D models on an image. We develop a novel sketch-based modeling interface, which takes a pair of user strokes as input and instantly generates a curved 3D surface by sweeping one stroke along the other. A key enabler of our system is an optimization procedure that extracts pairs of spatial planes from the context to position and sweep the strokes. We demonstrate the effectiveness and power of our modeling system on various RGB-D data sets and validate the use cases via a pilot study.", 
    "authors": [
      {
        "name": "Yuwei Li"
      }, 
      {
        "name": "Xi Luo"
      }, 
      {
        "name": "Youyi Zheng"
      }, 
      {
        "name": "Pengfei Xu"
      }, 
      {
        "name": "Hongbo Fu"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "SweepCanvas: Sketch-based 3D Prototyping on an RGB-D Image", 
    "type": "paper"
  }, 
  "uistf2014": {
    "abstract": "We present SoundCraft, a smartwatch prototype embedded with a microphone array, that localizes angularly, in azimuth and elevation, acoustic signatures: non-vocal acoustics that are produced using our hands. Acoustic signatures are common in our daily lives, such as when snapping or rubbing our fingers, tapping on objects or even when using an auxiliary object to generate the sound. We demonstrate that we can capture and leverage the spatial location of such naturally occurring acoustics using our prototype. We describe our algorithm, which we adopt from the MUltiple SIgnal Classification (MUSIC) technique [31], that enables robust localization and classification of the acoustics when the microphones are required to be placed at close proximity. SoundCraft enables a rich set of spatial interaction techniques, including quick access to smartwatch content, rapid command invocation, in-situ sketching, and also multi-user around device interaction. Via a series of user studies, we validate SoundCraft\u00e2\u0080\u0099s localization and classification capabilities in non-noisy and noisy environments.", 
    "authors": [
      {
        "name": "Teng Han"
      }, 
      {
        "name": "Khalad Hasan"
      }, 
      {
        "name": "Keisuke Nakamura"
      }, 
      {
        "name": "Randy Gomez"
      }, 
      {
        "name": "Pourang Irani"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "SoundCraft: Enabling Spatial Interactions on Smartwatches using Hand Generated Acoustics", 
    "type": "paper"
  }, 
  "uistf2024": {
    "abstract": "We introduce ImAxes immersive system for exploring multivariate data using fluid, modeless interaction. \\ The basic interface element is an embodied data axis.  The user can manipulate these axes like physical objects in the immersive environment and combine them into sophisticated visualisations.  The type of visualisation that appears depends on the proximity and relative orientation of the axes with respect to one another, which we describe with a formal grammar.  This straight-forward composability leads to a number of emergent visualisations and interactions, which we review, and then demonstrate with a detailed multivariate data analysis use case.", 
    "authors": [
      {
        "name": "Maxime Cordeil"
      }, 
      {
        "name": "Andrew Cunningham"
      }, 
      {
        "name": "Tim Dwyer"
      }, 
      {
        "name": "Bruce H Thomas"
      }, 
      {
        "name": "Kim Marriott"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "ImAxes: Immersive Axes as Embodied Affordances for Interactive Multivariate Data Visualisation", 
    "type": "paper"
  }, 
  "uistf2052": {
    "abstract": "Eye contact is an important non-verbal cue in social signal processing and promising as a measure of overt attention in human-object interactions and attentive user interfaces. However, robust detection of eye contact across different users, gaze targets, camera positions, and illumination conditions is notoriously challenging. We present a novel method for eye contact detection that combines a state-of-the-art appearance-based gaze estimator with a novel approach for unsupervised gaze target discovery, i.e. without the need for tedious and time-consuming manual data annotation. We evaluate our method in two real-world scenarios: detecting eye contact at the workplace, including on the main work display, from cameras mounted to target objects, as well as during everyday social interactions with the wearer of a head-mounted egocentric camera. We empirically evaluate the performance of our method in both scenarios and demonstrate its effectiveness for detecting eye contact independent of target object type and size, camera position, and user and recording environment.", 
    "authors": [
      {
        "name": "Xucong Zhang"
      }, 
      {
        "name": "Yusuke Sugano"
      }, 
      {
        "name": "Andreas Bulling"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "Everyday Eye Contact Detection Using Unsupervised Gaze Target Discovery", 
    "type": "paper"
  }, 
  "uistf2235": {
    "abstract": "We present Pyro, a micro thumb-tip gesture recognition technique based on thermal infrared signals radiating from the fingers. Pyro uses a compact, low-power passive sensor, making it suitable for wearable and mobile applications. To demonstrate the feasibility of Pyro, we developed a self-contained prototype consisting of the infrared pyroelectric sensor, a custom sensing circuit, and software for signal processing and machine learning. A ten-participant user study yielded a 93.9% cross-validation accuracy and 84.9% leave-one-session-out accuracy on six thumb-tip gestures. Subsequent lab studies demonstrated Pyro\u00e2\u0080\u0099s robustness to varying light conditions, hand temperatures, and background motion. We conclude by discussing the insights we gained from this work and future research questions.", 
    "authors": [
      {
        "name": "Jun Gong"
      }, 
      {
        "name": "Yang Zhang"
      }, 
      {
        "name": "Xia Zhou"
      }, 
      {
        "name": "Xing-Dong Yang"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Pyro: Thumb-Tip Gesture Recognition Using Pyroelectric Infrared Sensing", 
    "type": "paper"
  }, 
  "uistf2241": {
    "abstract": "SmoothMoves is an interaction technique for augmented reality (AR) based on smooth pursuits head movements. It works by computing correlations between the movements of on-screen targets and the user\u00e2\u0080\u0099s head while tracking those targets. The paper presents three studies. The first suggests that head based input can act as an easier and more affordable surrogate for eye-based input in many smooth pursuits interface designs. A follow-up study grounds the technique in the domain of augmented reality, and captures the error rates and acquisition times on different types of AR devices: head-mounted (2.6%, 1965ms) and hand-held (4.9%, 2089ms). Finally, the paper presents an interactive lighting system prototype that demonstrates the benefits of using smooth pursuits head movements in interaction with AR interfaces. A final qualitative study reports on positive feedback regarding the technique\u00e2\u0080\u0099s suitability for this scenario. Together, these results indicate show SmoothMoves is viable, efficient and immediately available for a wide range of wearable devices that feature embedded motion sensing.", 
    "authors": [
      {
        "name": "Augusto Esteves"
      }, 
      {
        "name": "David Verweij"
      }, 
      {
        "name": "Liza Jahan Suraiya"
      }, 
      {
        "name": "MD. Rasel Islam"
      }, 
      {
        "name": "Youryang Lee"
      }, 
      {
        "name": "Ian Oakley"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "SmoothMoves: Smooth Pursuits Head Movements for Augmented Reality", 
    "type": "paper"
  }, 
  "uistf2291": {
    "abstract": "Two recent innovations in immersive media include the ability to capture very high resolution panoramic imagery, and the rise of consumer level heads-up displays for virtual reality. Unfortunately, zooming to examine the high resolution in VR breaks the basic contract with the user, that the FOV of the visual field matches the FOV of the imagery. In this paper, we study methods to overcome this restriction to allow high resolution panoramic imagery to be able to be explored in VR. \\  \\ We introduce and test new interface modalities for exploring high resolution panoramic imagery in VR. In particular, we demonstrate that limiting the visual FOV of the zoomed in imagery to the central portion of the visual field, and modulating the transparency or zoom level of the imagery during rapid panning, reduce simulator sickness and help with targeting tasks. ", 
    "authors": [
      {
        "name": "Huiwen Chang"
      }, 
      {
        "name": "Michael Cohen"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Panning and Zooming High-Resolution Panoramas in Virtual Reality Devices", 
    "type": "paper"
  }, 
  "uistf2357": {
    "abstract": "Analog circuit design is a complex, error-prone task in which the processes of gathering observations, formulating reasonable hypotheses, and manually adjusting the circuit raise significant barriers to an iterative workflow. We present Scanalog, a tool built on programmable analog hardware that enables users to rapidly explore different circuit designs using direct manipulation, and receive immediate feedback on the resulting behaviors without manual assembly, calculation, or probing. Users can interactively tune modular signal transformations on hardware with real inputs, while observing real-time changes at all points in the circuit. They can create custom unit tests and assertions to detect potential issues. We describe three interactive applications demonstrating the expressive potential of Scanalog. In an informal evaluation, users successfully conditioned analog sensors and described Scanalog as both enjoyable and easy to use. ", 
    "authors": [
      {
        "name": "Evan N Strasnick"
      }, 
      {
        "name": "Maneesh Agrawala"
      }, 
      {
        "name": "Sean Follmer"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "Scanalog: Interactive Design and Debugging of Analog Circuits with Programmable Hardware", 
    "type": "paper"
  }, 
  "uistf2440": {
    "abstract": "HCI researchers lack low latency and robust systems to support the design and development of interaction techniques using finger identification. We developed a low cost prototype using piezo based vibration sensors attached to each finger. By combining the events from an input device with the information from the vibration sensors we demonstrate how to achieve low latency and robust finger identification. Our prototype was evaluated in a controlled experiment, using two keyboards and a touchpad, showing recognition rates of 98.2% for the keyboard and, for the touchpad, 99.7% for single touch and 94.7% for two simultaneous touches. These results were confirmed in an additional laboratory style experiment with ecologically valid tasks. Last we present new interactions techniques made possible using this technology.", 
    "authors": [
      {
        "name": "Damien Masson"
      }, 
      {
        "name": "Alix Goguey"
      }, 
      {
        "name": "Sylvain Malacria"
      }, 
      {
        "name": "G\u00c3\u00a9ry Casiez"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "WhichFingers: Identifying Fingers on Touch Surfaces and Keyboards Using Vibration Sensors", 
    "type": "paper"
  }, 
  "uistf2447": {
    "abstract": "This paper enables data storage and interaction with smart fabric, without the need for onboard electronics or batteries. To do this, we present the first smart fabric design that harnesses the ferromagnetic properties of conductive thread. Specifically, we manipulate the polarity of magnetized fabric and encode different forms of data including 2D images and bit strings. These bits can be read by swiping a commodity smartphone across the fabric, using its inbuilt magnetometer. Our results show that magnetized fabric retains its data even after washing, drying and ironing. Using a glove made of magnetized fabric, we can also perform six gestures in front of a smartphone, with a classification accuracy of 90.1%. Finally, using magnetized thread, we create fashion accessories like necklaces, ties, wristbands and belts with data storage capabilities as well as enable authentication applications. \\ ", 
    "authors": [
      {
        "name": "Justin Chan"
      }, 
      {
        "name": "Shyamnath Gollakota"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Data Storage and Interaction using Magnetized Fabric", 
    "type": "paper"
  }, 
  "uistf2588": {
    "abstract": "We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.", 
    "authors": [
      {
        "name": "Andreas Fender"
      }, 
      {
        "name": "David Lindlbauer"
      }, 
      {
        "name": "Philipp Herholz"
      }, 
      {
        "name": "Marc Alexa"
      }, 
      {
        "name": "Joerg Mueller"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior", 
    "type": "paper"
  }, 
  "uistf2681": {
    "abstract": "Smart rings have a unique form factor suitable for many applications, however, they offer little opportunity to provide the user with natural output. We propose passive kinesthetic force feedback as a novel output method for rotational input on smart rings. With this new output channel, friction force profiles can be designed, programmed, and felt by a user when they rotate the ring. This modality enables new interactions for ring form factors. We demonstrate the potential of this new haptic output method though Frictio, a prototype smart ring. In a controlled experiment, we determined the recognizability of six force profiles, including Hard Stop, Ramp-Up, Ramp-Down, Resistant Force, Bump, and No Force. The results showed that participants could distinguish between the force profiles with 94% accuracy. We conclude by presenting a set of novel interaction techniques that Frictio enables, and discuss insights and directions for future research.", 
    "authors": [
      {
        "name": "Teng Han"
      }, 
      {
        "name": "Qian Han"
      }, 
      {
        "name": "Michelle Annett"
      }, 
      {
        "name": "Fraser Anderson"
      }, 
      {
        "name": "Da-Yuan Huang"
      }, 
      {
        "name": "Xing-Dong Yang"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Frictio: Passive Kinesthetic Force Feedback for Smart Ring Output", 
    "type": "paper"
  }, 
  "uistf2786": {
    "abstract": "Standard vehicle infotainment systems often include touch screens that allow the driver to control their mobile phone, navigation, audio, and vehicle configurations. For the driver\u00e2\u0080\u0099s safety, these interfaces are often disabled or simplified while the car is in motion. Although this reduced functionality aids in reducing distraction for the driver, it also disrupts the usability of infotainment systems for passengers. Current infotainment systems are unaware of the seating position of their user and hence, cannot adapt. We present Carpacio, a system that takes advantage of the capacitive coupling created between the touchscreen and the electrode present in the seat when the user touches the capacitive screen. Using this capacitive coupling phenomenon, a car infotainment system can intelligently distinguish who is interacting with the screen seamlessly, and adjust its user interface accordingly. Manufacturers can easily incorporate Carpacio into vehicles since the included seat occupancy detection sensor or seat heating coils can be used as the seat electrode. We evaluated Carpacio in eight different cars and five mobile devices and found that it correctly detected over 2600 touches with an accuracy of 99.4%.", 
    "authors": [
      {
        "name": "Edward Jay Wang"
      }, 
      {
        "name": "Jake Garrison"
      }, 
      {
        "name": "Eric Whitmire"
      }, 
      {
        "name": "Mayank Goel"
      }, 
      {
        "name": "Shwetak N Patel"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Carpacio: Repurposing Capacitive Sensors to Distinguish Driver and Passenger Touches on In-Vehicle Screens", 
    "type": "paper"
  }, 
  "uistf2860": {
    "abstract": "This study proposes BlowFab, a prototyping method used to create a 2.5-dimensional prototype in a short time by combining laser cutting and blow molding techniques. The user creates adhesive areas and inflatable areas by engraving and cutting multilayered plastic sheets using a laser cutter. These adhesive areas are fused automatically by overlapping two crafted sheets and softening them with a heater. The user can then create hard prototypes by injecting air into the sheets. \\  \\ Objects can be bent in any direction by cutting incisions or engraving a resistant resin. The user can create uneven textures by engraving a pattern with a heat-resistant film. These techniques can be used for prototyping various strong inflatable objects. The finished prototype is strong and can be collapsed readily for storage when not required.  \\  \\ In this study, the design process is described using the proposed method.  \\ The study also evaluates possible bending mechanisms and texture expression methods along with various usage scenarios and discusses the resolution, strength, and reusability of the prototype developed.", 
    "authors": [
      {
        "name": "Junichi Yamaoka"
      }, 
      {
        "name": "Ryuma Niiyama"
      }, 
      {
        "name": "Yasuaki Kakehi"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "BlowFab: Rapid Prototyping for Rigid and Reusable Objects using Inflation of Laser-cut Surfaces", 
    "type": "paper"
  }, 
  "uistf2895": {
    "abstract": "Touch sensing with multiple electrodes allows expressive touch interactions. \\ The adaptability and flexibility of the sensor are important in efficiently prototyping touch based systems. The proposed technique uses capacitive touch sensing and simplifies the connections as the electrodes are connected in series via capacitors and the interface circuit is connected to the electrode array by just two wires. The touched electrode is recognized by measuring the capacitance changes while switching the polarity of the signal. We show that the technique is capable of detecting different touches through simulations and actual measurements. User tests show that ten electrodes are successfully recognized after user calibration. They also show the proposal's other novel capabilities of multi-touch (2-touch) and `capacitor-free' design. Various forms of electrodes and applications are examined to elucidate the application range.", 
    "authors": [
      {
        "name": "Hiroyuki Manabe"
      }, 
      {
        "name": "Wataru Yamada"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "A Capacitive Touch Sensing Technique with Series-connected Sensing Electrodes", 
    "type": "paper"
  }, 
  "uistf2925": {
    "abstract": "Pointing is a fundamental interaction technique where user movement is translated to spatial input on a display. Conventionally, this is based on a rigid configuration of a display coupled with a pointing device that determines the types of movement that can be sensed, and the specific ways users can affect pointer input. Spontaneous spatial coupling is a novel input technique that instead allows any body movement, or movement of tangible objects, to be appropriated for touchless pointing on an ad hoc basis. Pointer acquisition is facilitated by the display presenting graphical objects in motion, to which users can synchronise to define a temporary spatial coupling with the body part or tangible object they used in the process. The technique can be deployed using minimal hardware, as demonstrated by MatchPoint, a generic computer vision-based implementation of the technique that requires only a webcam. We explore the design space of spontaneous spatial coupling, demonstrate the versatility of the technique with application examples, and evaluate MatchPoint performance using a multi-directional pointing task.", 
    "authors": [
      {
        "name": "Christopher Clarke"
      }, 
      {
        "name": "Hans Gellersen"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "MatchPoint: Spontaneous Spatial Coupling of Body Movement for Touchless Pointing", 
    "type": "paper"
  }, 
  "uistf2935": {
    "abstract": "We present Dwell+, a method that boosts the effectiveness of typical dwell selection by augmenting the passive dwell duration with active haptic ticks which promptly drives rapid switches of modes forward through the user's skin sensations. In this way, Dwell+ enables multi-level dwell selection using rapid haptic ticks. To select a mode from a button, users dwell-touch the button until the mode of selection is haptically prompted. \\  \\ Our haptic stimulation design consists of a short 10ms vibrotacile feedback that indicates a mode arriving and a break that separates consecutive modes. We first tested the effectiveness of 170ms, 150ms, 130ms, and 110ms intervals between modes for a 10-level selection. The results reveal that 3-beats-per-chunk rhythm design, e.g., displaying longer 25ms vibrations initially for all three modes, could potentially achieve higher accuracy. The second study reveals significant improvement wherein a 94.5% accuracy was achieved for a 10-level Dwell+ selection using the 170ms interval with 3-beats-per-chunk design, and a 93.82% rate of accuracy using the more frequent 150ms interval with similar chunks for 5-level selection. The performance of conducting touch and receiving vibration from disparate hands was investigated for our final study to provide a wider range of usage. Our applications demonstrated implementing Dwell+ across interfaces, such as text input on a smartwatch, enhancing touch space for HMDs, boosting modalities of stylus-based tool selection, and extending the input vocabulary of physical interfaces.", 
    "authors": [
      {
        "name": "Yi-Chi Liao"
      }, 
      {
        "name": "Yen-Chiu Chen"
      }, 
      {
        "name": "Liwei Chan"
      }, 
      {
        "name": "Bing-Yu Chen"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Dwell+: Multi-Level Mode Selection Using Vibrotactile Cues", 
    "type": "paper"
  }, 
  "uistf3019": {
    "abstract": "Tutorials are vital for helping people perform complex software-based tasks in domains such as programming, data science, system administration, and computational research. However, it is tedious to create detailed step-by-step tutorials for tasks that span multiple interrelated GUI and command-line applications. To address this challenge, we created Torta, an end-to-end system that automatically generates step-by-step GUI and command-line app tutorials by demonstration, provides an editor to trim, organize, and add validation criteria to these tutorials, and provides a web-based viewer that can validate step-level progress and automatically run certain steps. The core technical insight that underpins Torta is that combining operating-system-wide activity tracing and screencast recording makes it easier to generate mixed-media (text+video) tutorials that span multiple GUI and command-line apps. An exploratory study on 10 computer science teaching assistants (TAs) found that they all preferred the experience and results of using Torta to record programming and sysadmin tutorials relevant to classes they teach rather than manually writing tutorials. A follow-up study on 6 students found that they all preferred following the Torta tutorials created by those TAs over the manually-written versions.", 
    "authors": [
      {
        "name": "Alok Mysore"
      }, 
      {
        "name": "Philip J Guo"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Torta: Generating Mixed-Media GUI and Command-Line App Tutorials Using Operating-System-Wide Activity Tracing", 
    "type": "paper"
  }, 
  "uistf3196": {
    "abstract": "We propose a novel interactive system to simplify the process of indoor 3D CAD room modeling. Traditional room modeling methods require users to measure room and furniture dimensions, and manually select models that match the scene from large catalogs. Users then employ a mouse and keyboard interface to construct walls and place the objects in their appropriate locations. In contrast, our system leverages the sensing capabilities of a 3D aware mobile device, recent advances in object recognition, and a novel augmented reality user interface, to capture indoor 3D room models in-situ. With a few taps, a user can mark the surface of an object, take a photo, and the system retrieves and places a matching 3D model into the scene, from a large online database. User studies indicate that this modality is significantly quicker, more accurate, and requires less effort than traditional desktop tools.", 
    "authors": [
      {
        "name": "Aditya Sankar"
      }, 
      {
        "name": "Steve Seitz"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Interactive Room Capture on 3D-Aware Mobile Devices", 
    "type": "paper"
  }, 
  "uistf3285": {
    "abstract": "While gaze holds a lot of promise for hands-free interaction with public displays, remote eye trackers with their confined tracking box restrict users to a single stationary position in front of the display. We present EyeScout, an active eye tracking system that combines an eye tracker mounted on a rail system with a computational method to automatically detect and align the tracker with the user\u00e2\u0080\u0099s lateral movement. EyeScout addresses key limitations of current gaze-enabled large public displays by offering two novel gaze-interaction modes for a single user: In \"Walk then Interact\" the user can walk up to an arbitrary position in front of the display and interact, while in \"Walk and Interact\" the user can interact even while on the move. We report on a user study that shows that EyeScout is well perceived by users, extends a public display's sweet spot into a sweet line, and reduces gaze interaction kick-off time to 3.5 seconds \u00e2\u0080\u0093 a 62% improvement over state of the art solutions. We discuss sample applications that demonstrate how EyeScout can enable position and movement-independent gaze interaction with large public displays.", 
    "authors": [
      {
        "name": "Mohamed Khamis"
      }, 
      {
        "name": "Axel Hoesl"
      }, 
      {
        "name": "Alexander Klimczak"
      }, 
      {
        "name": "Martin Reiss"
      }, 
      {
        "name": "Florian Alt"
      }, 
      {
        "name": "Andreas Bulling"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "EyeScout: Active Eye Tracking for Position and Movement Independent Gaze Interaction with Large Public Displays", 
    "type": "paper"
  }, 
  "uistf3335": {
    "abstract": "We present iSphere, a flying spherical display that can display high resolution and bright images in all directions from anywhere in 3D space. Our goal is to build a new platform which can physically and directly emerge arbitrary bodies in the real world. iSphere flies by itself using a built-in drone and creates a spherical display by rotating arcuate multi light-emitting diode (LED) tapes around the drone. As a result of the persistence of human vision, we see it as a spherical display flying in the sky.  The proposed method yields large display surfaces, high resolution, drone mobility, high visibility and 360\u00c2\u00b0 field of view. Previous approaches fail to match these characteristics, because of problems with aerodynamics and payload. We construct a prototype and validate the proposed method. The unique characteristics and benefits of flying spherical display surfaces are discussed and we describe application scenarios based on iSphere such as guidance, signage and telepresence.", 
    "authors": [
      {
        "name": "Wataru Yamada"
      }, 
      {
        "name": "Kazuhiro Yamada"
      }, 
      {
        "name": "Hiroyuki Manabe"
      }, 
      {
        "name": "Daizo Ikeda"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "iSphere: Self-Luminous Spherical Drone Display", 
    "type": "paper"
  }, 
  "uistf3396": {
    "abstract": "Visualizations of run-time program state help novices form proper mental models and debug their code. We push this technique to the extreme by posing the following question: What if a live programming environment for an imperative language always displays the entire history of all run-time values for all program variables all the time? To explore this question, we built a prototype live IDE called Omnicode (\"Omniscient Code\") that continually runs the user's Python code and uses a scatterplot matrix to visualize the entire history of all of its numerical values, along with meaningful numbers derived from other data types. To filter the visualizations and hone in on specific points of interest, the user can brush and link over the scatterplots or select portions of code. They can also zoom in to view detailed stack and heap visualizations at each execution step. An exploratory study on 10 novice programmers discovered that they found Omnicode to be useful for debugging, forming mental models, explaining their code to others, and discovering moments of serendipity that would not have been likely within an ordinary IDE.", 
    "authors": [
      {
        "name": "Hyeonsu Kang"
      }, 
      {
        "name": "Philip J Guo"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Omnicode: A Novice-Oriented Live Programming Environment with Always-On Run-Time Value Visualizations", 
    "type": "paper"
  }, 
  "uistf3412": {
    "abstract": "We motivate, design, and prototype a modular smartphone designed to make temporary device lending trustworthy and convenient. The concept is that the phone can be separated into pieces, so a child, friend, or even stranger can begin an access-controlled interaction with one piece, while the own-er retains another piece to continue their tasks and monitor activity. This is grounded in a survey capturing attitudes towards device lending, and an exploratory study probing how people might lend pieces of different kinds of modular smartphones. Design considerations are generated for a hardware form factor and software interface to support different lending scenarios. A functional prototype combining three smartphones into a single modular device is described and used to demonstrate a lending interaction design. A usability test validates the concept using the prototype.", 
    "authors": [
      {
        "name": "Teddy Seyed"
      }, 
      {
        "name": "Xing-Dong Yang"
      }, 
      {
        "name": "Daniel Vogel"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "A Modular Smartphone for Lending", 
    "type": "paper"
  }, 
  "uistf3450": {
    "abstract": "The rise of Maker communities and open-source electronic prototyping platforms have made electronic circuit projects increasingly popular around the world.  \\ Although there are software tools that support the debugging and sharing of circuits, they require users to manually create the virtual circuits in software, which can be time-consuming and error-prone. \\ We present CircuitSense, a system that automatically recognizes the wires and electronic components placed on breadboards. It uses a combination of passive sensing and active probing to detect and generate the corresponding circuit representation in software in real-time. CircuitSense bridges the gap between the physical and virtual representations of circuits. It enables users to interactively construct and experiment with physical circuits while gaining the benefits of using software tools. It also dramatically simplifies the sharing of circuit designs with online communities.", 
    "authors": [
      {
        "name": "Te-Yen Wu"
      }, 
      {
        "name": "Bryan Wang"
      }, 
      {
        "name": "Jiun-Yu Lee"
      }, 
      {
        "name": "Hao-Ping Shen"
      }, 
      {
        "name": "Yu-Chian Wu"
      }, 
      {
        "name": "Yu-An Chen"
      }, 
      {
        "name": "Pin-sung Ku"
      }, 
      {
        "name": "MING-WEI HSU"
      }, 
      {
        "name": "Yu-Chih Lin"
      }, 
      {
        "name": "Mike Y. Chen"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "CircuitSense: Automatic Sensing of Physical Circuits and Generation of Virtual Circuits to Support Software Tools.", 
    "type": "paper"
  }, 
  "uistf3497": {
    "abstract": "We present AirCode, a technique that allows the user to tag physically fabricated objects with given information. An AirCode tag consists of a group of carefully designed air pockets placed beneath the object surface. These air pockets are easily produced during the fabrication process of the object, without any additional material or postprocessing. Meanwhile, the air pockets affect only the scattering light transport under the surface, and thus are hard to notice to our naked eyes. But, by using a computational imaging method, the tags become detectable. We present a tool that automates the design of air pockets for the user to encode information. AirCode system also allows the user to retrieve the information from captured images via a robust decoding algorithm. We demonstrate our tagging technique with applications for metadata embedding, robotic grasping, as well as conveying object affordances. \\ ", 
    "authors": [
      {
        "name": "Dingzeyu Li"
      }, 
      {
        "name": "Avinash S Nair"
      }, 
      {
        "name": "Shree K Nayar"
      }, 
      {
        "name": "Changxi Zheng"
      }
    ], 
    "award": true, 
    "hm": false, 
    "subtype": "paper", 
    "title": "AirCode: Unobtrusive Physical Tags for Digital Fabrication", 
    "type": "paper"
  }, 
  "uistf3506": {
    "abstract": "Virtual reality filmmakers creating 360-degree video currently rely on \\ cinematography techniques that were developed for traditional narrow \\ field of view film. They typically edit together a sequence of shots \\ so that they appear at a fixed-orientation irrespective of the viewer's \\ field of view. But because viewers set their own camera orientation \\ they may miss important story content while looking in the wrong \\ direction. \\ We present new interactive shot orientation techniques that are \\ designed to help viewers see all of the important content in \\ 360-degree video stories.  Our viewpoint-oriented technique \\ reorients the shot at each cut so that the most \\ important content lies in the the viewer's current field of view. Our \\ active reorientation technique, lets the viewer press a button to \\ immediately reorient the shot so that important content lies in their \\ field of view. \\ We present a 360-degree video player which implements these techniques \\ and conduct a user study which finds that users spend \\ 5.2-9.5% more time viewing the important points (manually labelled) of the scene with our techniques \\ compared to the traditional fixed-orientation cuts.  \\ In practice, 360-degree video creators may label important content, but we also provide an automatic method for determining important content in existing 360-degree videos.", 
    "authors": [
      {
        "name": "Amy Pavel"
      }, 
      {
        "name": "Bjoern Hartmann"
      }, 
      {
        "name": "Maneesh Agrawala"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Shot Orientation Controls for Interactive Cinematography with 360 Video", 
    "type": "paper"
  }, 
  "uistf3567": {
    "abstract": "The dramatic decrease in price and increase in availability of hobbyist electronics has led to a wide array of embedded and interactive devices. While electronics have become more widespread, developing and prototyping the required circuitry for these devices is still difficult, requiring knowledge of electronics, components, and programming. In this paper, we present Trigger-Action-Circuits (TAC), an interactive system that leverages generative design to produce circuitry, firmware, and assembly instructions, based on high-level, behavioural descriptions. TAC is able to generate multiple candidate circuits from a behavioural description, giving the user a number of alternative circuits that may be best suited to their use case (e.g., based on cost, component availability or ease of assembly). The generated circuitry uses off-the-shelf, commodity electronics, not specialized hardware components, enabling scalability and extensibility. TAC supports a range of common components and behaviors that are frequently required for prototyping electronic circuits. A user study demonstrated that TAC helps users avoid problems encountered during circuit design and assembly, with users completing their circuits significantly faster than with traditional methods.", 
    "authors": [
      {
        "name": "Fraser Anderson"
      }, 
      {
        "name": "Tovi Grossman"
      }, 
      {
        "name": "George Fitzmaurice"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Trigger-Action-Circuits: Leveraging Generative Design to Enable Novices to Design and Build Circuitry", 
    "type": "paper"
  }, 
  "uistf3584": {
    "abstract": "Most of our daily activities take place in the physical world, which inherently imposes physical constraints. In contrast, the digital world is very flexible, but usually isolated from its physical counterpart. To combine these two realms, many Mixed Reality (MR) techniques have been explored, at different levels in the continuum. In this work we present an integrated Mixed Reality ecosystem that allows users to incrementally transition from pure physical to pure virtual experiences in a unique reality. This system stands on a conceptual framework composed of 6 levels. This paper presents these levels as well as the related interaction techniques.", 
    "authors": [
      {
        "name": "Joan Sol Roo"
      }, 
      {
        "name": "Martin Hachet"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "One Reality: Augmenting How the Physical World is Experienced by combining Multiple Mixed Reality Modalities", 
    "type": "paper"
  }, 
  "uistf3598": {
    "abstract": "CommandBoard offers a simple, efficient and incrementally learnable technique for issuing gesture commands from a soft keyboard. We transform the area above the keyboard into a command-gesture input space that lets users draw unique command gestures or type command names followed by execute. Novices who pause see an in-context dynamic guide, whereas experts simply draw. Our studies show that CommandBoard\u00e2\u0080\u0099s inline gesture shortcuts are significantly faster (almost double) than markdown symbols and significantly preferred by users. We demonstrate additional techniques for more complex commands, and discuss trade-offs with respect to the user\u00e2\u0080\u0099s knowledge and motor skills, as well as the size and structure of the command space.", 
    "authors": [
      {
        "name": "Jessalyn Alvina"
      }, 
      {
        "name": "Carla Florencia Griggio"
      }, 
      {
        "name": "Xiaojun Bi"
      }, 
      {
        "name": "Wendy E. Mackay"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "CommandBoard: Creating a General-Purpose Command Gesture Input Space for Soft Keyboard", 
    "type": "paper"
  }, 
  "uistf3635": {
    "abstract": "Facial expressions transmit a variety of social, grammatical, and affective signals. For technology to leverage this rich source of communication, tools that better model the breadth of information they convey are required. MiFace is a novel framework for creating expression lexicons that map signal values to parameterized facial muscle movements. In traditional mapping paradigms using posed photographs, na\u00c3\u00afve judges select from predetermined label sets and movements are inferred by trained experts. The set of generally accepted expressions established in this way is limited to six basic displays of affect. In contrast, our approach generatively simulates muscle movements on a 3D avatar. By applying natural language processing techniques to crowdsourced free-response labels for the resulting images, we efficiently converge on an expression\u00e2\u0080\u0099s value across signal categories. Two studies returned 218 discriminable facial expressions with 51 unique labels. The six basic emotions are included, but we additionally define such nuanced expressions as embarrassed, curious, and hopeful. \\ ", 
    "authors": [
      {
        "name": "Crystal Butler"
      }, 
      {
        "name": "Stephanie Ann Michalowicz"
      }, 
      {
        "name": "Lakshmi Subramanian"
      }, 
      {
        "name": "Winslow Burleson"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "More than a Feeling: The MiFace Framework for Defining Facial Communication Mappings", 
    "type": "paper"
  }, 
  "uistf3763": {
    "abstract": "When bringing animated characters to life, artists often augment the primary motion of a figure by adding secondary animation -- subtle movement of parts like hair, foliage or cloth that complements and emphasizes the primary motion. Traditionally, artists add secondary motion to animated illustrations only through arduous manual effort, and often eschew it entirely. Emerging ``live'' performance applications allow both novices and experts to perform the primary motion of a character, but only a virtuoso performer could manage the degrees of freedom needed to specify both primary and secondary motion together. This paper introduces physically-inspired rigs that propagate the primary motion of layered, illustrated characters to produce plausible secondary motion. These composable elements are rigged and controlled via a small number of parameters to produce an expressive range of effects. Our approach supports a variety of the most common secondary effects, which we demonstrate with an assortment of characters of varying complexity.", 
    "authors": [
      {
        "name": "Nora S Willett"
      }, 
      {
        "name": "Wilmot Li"
      }, 
      {
        "name": "Jovan Popovic"
      }, 
      {
        "name": "Floraine Berthouzoz"
      }, 
      {
        "name": "Adam Finkelstein"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Secondary Motion for Performed 2D Animation", 
    "type": "paper"
  }, 
  "uistf3771": {
    "abstract": "We introduce Codestrates, a literate computing approach to developing interactive software. Codestrates blurs the distinction between the use and development of applications. It builds on the literate computing approach, commonly found in interactive notebooks such as Jupyter notebook. Literate computing weaves together prose and live computation in the same document. However, literate computing in interactive notebooks are limited to computation and it is challenging to extend their user interface, reprogram their functionality, or develop stand-alone applications. Codestrates builds literate computing capabilities on top of Webstrates and demonstrates how it can be used for (i) collaborative interactive notebooks, (ii) extending its functionality from within itself, and (iii) developing reprogrammable applications.", 
    "authors": [
      {
        "name": "Roman R\u00c3\u00a4dle"
      }, 
      {
        "name": "Midas Nouwens"
      }, 
      {
        "name": "Kristian B Antonsen"
      }, 
      {
        "name": "James R Eagan"
      }, 
      {
        "name": "Clemens N Klokmose"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Codestrates: Literate Computing with Webstrates", 
    "type": "paper"
  }, 
  "uistf3775": {
    "abstract": "We present StrutModeling, a computationally enhanced construction kit that enables users without a 3D modeling background to prototype 3D models by assembling struts and hub primitives in physical space. Physical 3D models are immediately captured in software and result in readily available models for 3D printing. Given the concrete physical format of StrutModels, modeled objects can be tested and fine tuned in the presence of existing objects and specific needs of users. StrutModeling avoids puzzling with pieces by contributing an adjustable strut and universal hub design. Struts can be adjusted in length and snap to magnetic hubs in any configuration. As such, arbitrarily complex models can be modeled, tested, and adjusted during the design phase. In addition, the embedded sensing capabilities allow struts to be used as measuring devices for lengths and angles, and tune physical mesh models according to existing physical objects.", 
    "authors": [
      {
        "name": "Danny Leen"
      }, 
      {
        "name": "Raf Ramakers"
      }, 
      {
        "name": "Kris Luyten"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "StrutModeling: A Low-Fidelity Construction Kit to Iteratively Model, Test, and Adapt 3D Objects", 
    "type": "paper"
  }, 
  "uistf3835": {
    "abstract": "The predominant interaction paradigm of current audio spatialization tools, which are primarily geared towards expert users, imposes a design process in which users are characterized as stationary, limiting the application domain of these tools. Navigable 3D sonic virtual realities, on the other hand, can support many applications ranging from soundscape prototyping to spatial data representation. Although modern game engines provide a limited set of audio features to create such sonic environments, the interaction methods are inherited from the graphical design features of such systems, and are not specific to the auditory modality. To address such limitations, we introduce INVISO, a novel web-based user interface for designing and experiencing rich and dynamic sonic virtual realities. Our interface enables both novice and expert users to construct complex immersive sonic environments with 3D dynamic sound components. INVISO is platform-independent and facilitates a variety of mixed reality applications, such as those where users can simultaneously experience and manipulate a virtual sonic environment. In this paper, we detail the interface design considerations for our audio-specific VR tool. To evaluate the usability of INVISO, we conduct two user studies: The first demonstrates that our visual interface effectively facilitates the generation of creative audio environments; the second demonstrates that both expert and non-expert users are able to use our software to accurately recreate complex 3D audio scenes. ", 
    "authors": [
      {
        "name": "Anil Camci"
      }, 
      {
        "name": "Kristine Lee"
      }, 
      {
        "name": "Cody J Roberts"
      }, 
      {
        "name": "Angus G Forbes"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "INVISO: A Cross-platform User Interface for Creating Virtual Sonic Environments", 
    "type": "paper"
  }, 
  "uistf3858": {
    "abstract": "Virtual Reality allow users to explore virtual environments naturally, by moving their head and body. However, the size of the environments they can explore is limited by real world constraints, such as the tracking technology or the physical space available. Existing techniques removing these limitations often break the metaphor of natural navigation in VR (e.g. steering techniques), involve control commands (e.g., teleporting) or hinder precise navigation (e.g., scaling user\u00e2\u0080\u0099s displacements). This paper proposes NaviFields, which quantify the requirements for precise navigation of each point of the environment, allowing natural navigation within relevant areas, while scaling users\u00e2\u0080\u0099 displacements when travelling across non-relevant spaces. This expands the size of the navigable space, retains the natural navigation metaphor and still allows for areas with precise control of the virtual head. We present a formal description of our NaviFields technique, which we compared against two alternative solutions (i.e., homogeneous scaling and natural navigation). Our results demonstrate our ability to cover larger spaces, introduce minimal disruption when travelling across bigger distances and improve very significantly the precise control of the viewpoint inside relevant areas.", 
    "authors": [
      {
        "name": "Roberto Antonio Montano Murillo"
      }, 
      {
        "name": "Elia Gatti"
      }, 
      {
        "name": "Miguel Oliver Segovia"
      }, 
      {
        "name": "Marianna Obrist"
      }, 
      {
        "name": "Jose Pascual Molina Masso"
      }, 
      {
        "name": "Diego Martinez Plasencia"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "NaviFields: Relevance fields for adaptive VR navigation", 
    "type": "paper"
  }, 
  "uistf3948": {
    "abstract": "Electric current and voltage are fundamental to learning, understanding, and debugging circuits. Although both can be measured using tools such as multimeters and oscilloscopes, electric current is much more difficult to measure because users have to unplug parts of a circuit and then insert the measuring tools in serial. Furthermore, users need to restore the circuits back to its original state after measurements have been taken. In practice, this cumbersome process poses a formidable barrier to knowing how current flows throughout a circuit. We present CurrentViz, a system that can sense and visualize the electric current flowing through a circuit, which helps users quickly understand otherwise invisible circuit behavior. It supports fully automatic, ubiquitous, and real-time collection of amperage information of breadboarded circuits. It also supports visualization of the amperage data on a circuit schematic to provide an intuitive view into the current state of a circuit. \\ ", 
    "authors": [
      {
        "name": "Te-Yen Wu"
      }, 
      {
        "name": "Hao-Ping Shen"
      }, 
      {
        "name": "Yu-Chian Wu"
      }, 
      {
        "name": "Yu-An Chen"
      }, 
      {
        "name": "Pin-sung Ku"
      }, 
      {
        "name": "MING-WEI HSU"
      }, 
      {
        "name": "Jun-You Liu"
      }, 
      {
        "name": "Yu-Chih Lin"
      }, 
      {
        "name": "Mike Y. Chen"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "CurrentViz: Sensing and Visualizing Electric Current of Breadboarded Circuits", 
    "type": "paper"
  }, 
  "uistf3971": {
    "abstract": "Mobile app designers often look at examples during the early stages of the design process. In this work, we present an approach for low-overhead collection of performance data for such mobile app designs examples found in the wild. It requires no development overhead since designers can directly work with the app they are interested in instead of building prototypes based on it. It uses anonymous, unsupervised, crowdworkers to use apps while collecting detailed data about the user interactions and the app UIs in the background. Performance measures of interests (such as time on task, completion rates etc.) are then computed from these interaction traces. We demonstrate the usefulness of this approach through case studies highlighting the differences in user behaviors between apps and present examples of usability issues that it can help discover.", 
    "authors": [
      {
        "name": "Biplab Deka"
      }, 
      {
        "name": "Zifeng Huang"
      }, 
      {
        "name": "Chad D Franzen"
      }, 
      {
        "name": "Jeffrey Nichols"
      }, 
      {
        "name": "Yang Li"
      }, 
      {
        "name": "Ranjitha Kumar"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "ZIPT: Zero-Integration Performance Testing of Mobile App Designs", 
    "type": "paper"
  }, 
  "uistf4070": {
    "abstract": "Research experiences today are limited to a privileged few at select universities. Providing open access to research experiences would enable global upward mobility and increased diversity in the scientific workforce. How can we coordinate a crowd of diverse volunteers on open-ended research? How could a PI have enough visibility into each person's contributions to recommend them for further study? We present Crowd Research, a crowdsourcing technique that coordinates open-ended research through an iterative cycle of open contribution, synchronous collaboration, and peer assessment. To aid upward mobility and recognize contributions in publications, we introduce a decentralized credit system: participants allocate credits to each other, which a graph centrality algorithm translates into a collectively-created author order. Over 1,500 people from 62 countries have participated, 74% from institutions with low access to research. Over two years and three projects, this crowd has produced articles at top-tier Computer Science venues, and participants have gone on to leading graduate programs.", 
    "authors": [
      {
        "name": "Rajan Vaish"
      }, 
      {
        "name": "Snehalkumar `Neil' S. Gaikwad"
      }, 
      {
        "name": "Geza Kovacs"
      }, 
      {
        "name": "Andreas Veit"
      }, 
      {
        "name": "Ranjay A Krishna"
      }, 
      {
        "name": "Imanol Arrieta Ibarra"
      }, 
      {
        "name": "Camelia Simoiu"
      }, 
      {
        "name": "Michael Wilber"
      }, 
      {
        "name": "Serge Belongie"
      }, 
      {
        "name": "Sharad C. Goel"
      }, 
      {
        "name": "James Davis"
      }, 
      {
        "name": "Michael S Bernstein"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "Crowd Research: Open and Scalable University Laboratories", 
    "type": "paper"
  }, 
  "uistf4170": {
    "abstract": "We present a jaw, face, or head movement (face-related movement) recognition system called CanalSense. It recognizes face-related movements using barometers embedded in earphones. We find that face-related movements change air pressure inside the ear canals, which shows characteristic changes depending on the type and degree of the movement. We also find that such characteristic changes can be used to recognize face-related movements. We conduct an experiment to measure the accuracy of recognition. As a result, random forest shows per-user recognition accuracies of 87.6% for eleven face-related movements and 87.5% for four OpenMouth levels.", 
    "authors": [
      {
        "name": "Toshiyuki Ando"
      }, 
      {
        "name": "Yuki Kubo"
      }, 
      {
        "name": "Buntarou Shizuki"
      }, 
      {
        "name": "Shin Takahashi"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "CanalSense: Face-Related Movement Recognition System based on Sensing Air Pressure in Ear Canals", 
    "type": "paper"
  }, 
  "uistf4174": {
    "abstract": "As three-dimensional printers become more available, 3D printed models can serve as important learning materials, especially for blind people who perceive the models tactilely. Such models can be much more powerful when augmented with audio annotations that describe the model and their elements. We present Markit and Talkit, a low-barrier toolkit for creating and interacting with 3D models with audio annotations. Makers (e.g., hobbyists, teachers, and friends of blind people) can use Markit to mark model elements and associate then with text annotations. A blind user can then print the augmented model, launch the Talkit application, and access the annotations by touching the model and following Talkit\u00e2\u0080\u0099s verbal cues. Talkit uses an RGB camera and a microphone to sense users\u00e2\u0080\u0099 inputs so it can run on a variety of devices. We evaluated Markit with eight sighted \u00e2\u0080\u009cmakers\u00e2\u0080\u009d and Talkit with eight blind people. On average, non-experts added two annotations to a model in 275 seconds (SD=70) with Markit. Meanwhile, with Talkit, blind people found a specified annotation on a model in an average of 7 seconds (SD=8).", 
    "authors": [
      {
        "name": "Lei Shi"
      }, 
      {
        "name": "Yuhang Zhao"
      }, 
      {
        "name": "Shiri Azenkot"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Markit and Talkit: A Low-Barrier Toolkit to Augment 3D Printed Models with Audio Annotations", 
    "type": "paper"
  }, 
  "uistf4234": {
    "abstract": "We present a new, publicly available dataset of mobile app designs with the goal of advancing the development of data-driven mobile app design tools. This dataset was mined from over 9.7K existing Android apps from 27 categories using a combination of human and automated crawling of apps. The dataset contains more than 72K unique UI screens captured as screenshots and view hierarchies, metadata from the Google Play Store (category, ratings, etc.), and the set of human interaction traces captured in the crawl. We sketch the diverse set of applications that could be enabled by this dataset and demonstrate that the dataset\u00e2\u0080\u0099s scale can support deep learning techniques by training and evaluating an autoencoder for UI layout similarity.", 
    "authors": [
      {
        "name": "Biplab Deka"
      }, 
      {
        "name": "Zifeng Huang"
      }, 
      {
        "name": "Chad D Franzen"
      }, 
      {
        "name": "Joshua Hibschman"
      }, 
      {
        "name": "Dan Afergan"
      }, 
      {
        "name": "Yang Li"
      }, 
      {
        "name": "Jeffrey Nichols"
      }, 
      {
        "name": "Ranjitha Kumar"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "RICO: A Mobile App Dataset for Building Data-Driven Design Applications", 
    "type": "paper"
  }, 
  "uistf4289": {
    "abstract": "Over the last decades, there have been numerous efforts in wearable computing research to enable interactive textiles. Most work focus, however, on integrating sensors for planar touch gestures, and thus do not fully take advantage of the flexible, deformable and tangible material properties of textile. In this work, we introduce SmartSleeve, a deformable textile sensor, which can sense both surface and deformation gestures in real-time. It expands the gesture vocabulary with a range of expressive interaction techniques, and we explore new opportunities using advanced deformation gestures, such as, Twirl, Twist, Fold, Push and Stretch. We describe our sensor design, hardware implementation and its novel non-rigid connector architecture. We provide a detailed description of our hybrid gesture detection pipeline that uses learning-based algorithms and heuristics to enable real-time gesture detection and tracking. Its modular architecture allows us to derive new gestures through the combination with continuous properties like pressure, location, and direction. Finally, we report on the promising results from our evaluations which demonstrate real-time classification. ", 
    "authors": [
      {
        "name": "Patrick Parzer"
      }, 
      {
        "name": "Adwait Sharma"
      }, 
      {
        "name": "Anita Vogl"
      }, 
      {
        "name": "J\u00c3\u00bcrgen Steimle"
      }, 
      {
        "name": "Alex Olwal"
      }, 
      {
        "name": "Michael Haller"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "SmartSleeve: Real-time Sensing of Surface and Deformation Gestures on Flexible, Interactive Textiles, using a Hybrid Gesture Detection Pipeline", 
    "type": "paper"
  }, 
  "uistf4298": {
    "abstract": "Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process. ", 
    "authors": [
      {
        "name": "Zoya Bylinskii"
      }, 
      {
        "name": "Nam Wook Kim"
      }, 
      {
        "name": "Peter O'Donovan"
      }, 
      {
        "name": "Sami Alsheikh"
      }, 
      {
        "name": "Spandan Madan"
      }, 
      {
        "name": "Hanspeter Pfister"
      }, 
      {
        "name": "Fredo Durand"
      }, 
      {
        "name": "Bryan Russell"
      }, 
      {
        "name": "Aaron Hertzmann"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "Learning Visual Importance for Graphic Designs and Data Visualizations", 
    "type": "paper"
  }, 
  "uistf4355": {
    "abstract": "We present iSoft, a single volume soft sensor capable of sensing real-time continuous contact and unidirectional stretching. We propose a low-cost and an easy way to fabricate such piezoresistive elastomer-based soft sensors for instant interactions. We employ an electrical impedance tomography (EIT) technique to estimate changes of resistance distribution on the sensor caused by fingertip contact. To compensate for the rebound elasticity of the elastomer and achieve real-time continuous contact sensing, we apply a dynamic baseline update for EIT. The baseline updates are triggered by fingertip contact and movement detections. Further, we support unidirectional stretching sensing using a model-based approach which works separately with continuous contact sensing. We also provide a software toolkit for users to design and deploy personalized interfaces with customized sensors. Through a series of experiments and evaluations, we validate the performance of contact and stretching sensing. Through example applications, we show the variety of examples enabled by iSoft.", 
    "authors": [
      {
        "name": "Sang Ho Yoon"
      }, 
      {
        "name": "Ke Huo"
      }, 
      {
        "name": "Yunbo Zhang"
      }, 
      {
        "name": "Guiming Chen"
      }, 
      {
        "name": "Luis Paredes"
      }, 
      {
        "name": "Subramanian Chidambaram"
      }, 
      {
        "name": "Karthik Ramani"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "iSoft: A Customizable Soft Sensor with Real-time Continuous Contact and Stretching Sensing", 
    "type": "paper"
  }, 
  "uistf4432": {
    "abstract": "In this paper, we introduce two different transforming steering wheel systems that can be utilized to augment user experience for future partially autonomous and fully autonomous vehicles. The first one is a robotic steering wheel that can mechanically transform by using its actuators to move the various components into different positions. The second system is a LED steering wheel that can visually transform by using LEDs embedded along the rim of wheel to change colors. Both steering wheel systems contain onboard microcontrollers developed to interface with our driving simulator. The main function of these two systems is to provide emergency warnings to drivers in a variety of safety critical scenarios, although the design space that we propose for these steering wheel systems also includes the use as interactive user interfaces.  \\  \\ To evaluate the effectiveness of the emergency alerts, we conducted a driving simulator study examining the performance of participants (N=56) after an abrupt loss of autonomous vehicle control. Drivers who experienced the robotic steering wheel performed significantly better than those who experienced the LED steering wheel. The results of this study suggest that alerts utilizing mechanical movement are more effective than purely visual warnings. \\ ", 
    "authors": [
      {
        "name": "Brian Mok"
      }, 
      {
        "name": "Mishel Johns"
      }, 
      {
        "name": "Stephen Yang"
      }, 
      {
        "name": "Wendy Ju"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Reinventing the Wheel: Transforming Steering Wheel Systems for Autonomous Vehicles", 
    "type": "paper"
  }, 
  "uistf4474": {
    "abstract": "360-degree video contains a full field of environmental content. However, browsing these videos, either on screens or through head-mounted displays (HMDs), users consume only a subset of the full field of view per a natural viewing experience. This causes a search problem when a region-of-interest (ROI) in a video is outside of the current field of view (FOV) on the screen, or users may search for non-existing ROIs.  \\  \\ We propose Outside-In, a visualization technique which re-introduces off-screen regions-of-interest (ROIs) into the main screen as spatial picture-in-picture (PIP) previews. The geometry of the preview windows further encodes a ROI's relative location vis-\u00c3\u00a0-vis the main screen view, allowing for effective navigation. In an 18-participant study, we compare Outside-In with traditional arrow-based guidance within three types of 360-degree video. Results show that Outside-In outperforms in regard to understanding spatial relationship, the storyline of the content and overall preference. Two applications are demonstrated for use with Outside-In in 360-degree video navigation with touchscreens, and live telepresence.", 
    "authors": [
      {
        "name": "Yung-Ta Lin"
      }, 
      {
        "name": "Yi-Chi Liao"
      }, 
      {
        "name": "Shan-Yuan Teng"
      }, 
      {
        "name": "Yi-Ju Chung"
      }, 
      {
        "name": "Liwei Chan"
      }, 
      {
        "name": "Bing-Yu Chen"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Outside-In: Visualizing Out-of-Sight Regions-of-Interest in a 360 Video Using Spatial Picture-in-Picture Previews", 
    "type": "paper"
  }, 
  "uistf4499": {
    "abstract": "Usability has a distinct subjective component, yet surprisingly little is known about its neural basis and relation to the neuroanatomy of aesthetics. To begin closing this gap, we conducted two functional magnetic resonance imaging studies in which participants were shown static webpages (in the first study) and videos of interaction with webpages (in the second study). The webpages were controlled so as to exhibit high and low levels of perceived usability and perceived aesthetics. Our results show unique links between perceived usability and brain areas involved in functions such as emotional processing (left fusiform gyrus, superior frontal gyrus), anticipation of physical interaction (precentral gyrus), task intention (anterior cingulate cortex), and linguistic processing (medial and bilateral superior frontal gyri). We use these findings to discuss the brain correlates of perceived usability and the use of fMRI for usability evaluation and for generating new user experiences. \\ ", 
    "authors": [
      {
        "name": "Chi Thanh Vi"
      }, 
      {
        "name": "Kasper Hornb\u00c3\u00a6k"
      }, 
      {
        "name": "Sriram Subramanian"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Neuroanatomical Correlates of Perceived Usability", 
    "type": "paper"
  }, 
  "uistf4553": {
    "abstract": "The Maker movement has encouraged more people to start working with electronics and embedded processors. A key challenge in developing and debugging custom embedded systems is understanding their behavior, particularly at the boundary between hardware and software. Existing tools such as step debuggers and logic analyzers only focus on software or hardware, respectively. This paper presents a new development environment designed to illuminate the boundary between embedded code and circuits. Bifr\u00c3\u00b6st automatically instruments and captures the progress of the user\u00e2\u0080\u0099s code, variable values, and the electrical and bus activity occurring at the interface between the processor and the circuit it operates in. This data is displayed in a linked visualization that allows navigation through time and program execution, enabling comparisons between variables in code and signals in circuits. Automatic checks can detect low-level hardware configuration and protocol issues, while user-authored checks can test particular application semantics. In an exploratory study with ten participants, we investigated how Bifr\u00c3\u00b6st influences debugging workflows.", 
    "authors": [
      {
        "name": "Will McGrath"
      }, 
      {
        "name": "Daniel Drew"
      }, 
      {
        "name": "Jeremy Warner"
      }, 
      {
        "name": "Majeed Kazemitabaar"
      }, 
      {
        "name": "Mitchell Karchemsky"
      }, 
      {
        "name": "David A Mellis"
      }, 
      {
        "name": "Bjoern Hartmann"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Bifr\u00c3\u00b6st : Visualizing and Checking Behavior of Embedded Systems across Hardware and Software", 
    "type": "paper"
  }, 
  "uistf4597": {
    "abstract": "Collaborative review and feedback is an important part of conventional filmmaking and now Virtual Reality (VR) video production as well. However, conventional collaborative review practices do not easily translate to VR video because VR video is normally viewed in a headset, which makes it difficult to align gaze, share context, and take notes. This paper presents CollaVR, an application that enables multiple users to review a VR video together while wearing headsets. We interviewed VR video professionals to distill key considerations in reviewing VR video. Based on these insights, we developed a set of networked tools that enable filmmakers to collaborate and review video in real-time. We conducted a preliminary expert study to solicit feedback from VR video professionals about our system and assess their usage of the system with and without collaboration features.", 
    "authors": [
      {
        "name": "Cuong Nguyen"
      }, 
      {
        "name": "Stephen DiVerdi"
      }, 
      {
        "name": "Aaron Hertzmann"
      }, 
      {
        "name": "Feng Liu"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "CollaVR: Collaborative In-Headset Review for VR Video", 
    "type": "paper"
  }, 
  "uistf4607": {
    "abstract": "The emergence of personal computing devices offers both a challenge and opportunity for displaying text: small screens can be hard to read, but also support higher resolution. To fit content on a small screen, text must be small. This small text size can make computing devices unusable, in particular to low-vision users, whose vision is not correctable with glasses. Usability is also decreased for sighted users straining to read the small letters, especially without glasses at hand. We propose animated scripts called livefonts for displaying English with improved legibility for all users. Because paper does not support animation, traditional text is static. However, modern screens support animation, and livefonts capitalize on this capability. We evaluate our livefont variations' legibility through a controlled lab study with low-vision and sighted participants, and find our animated scripts to be legible across vision types at approximately half the size (area) of traditional letters, while previous smartfonts (static alternate scripts) did not show a significant legibility advantage for low-vision users. We evaluate the learnability of our livefont with low-vision and sighted participants, and find it to be comparably learnable to static smartfonts after two thousand practice sentences.", 
    "authors": [
      {
        "name": "Danielle Bragg"
      }, 
      {
        "name": "Shiri Azenkot"
      }, 
      {
        "name": "Kevin Larson"
      }, 
      {
        "name": "Ann Bessemans"
      }, 
      {
        "name": "Adam Kalai"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Designing and Evaluating Livefonts", 
    "type": "paper"
  }, 
  "uistf4623": {
    "abstract": "Redubbing is an extensively used technique to correct errors in voiceover recordings. It involves re-recording a part of a voiceover, identifying the corresponding section of audio in the original recording that needs to be replaced, and using low level audio tools to replace the audio. Although this sequence of steps can be performed using traditional audio editing tools, the process can be tedious when dealing with long voiceover recordings and prohibitively difficult for users not familiar with such tools. To address this issue, we present AutoDub, a novel system for redubbing voiceover recordings. Using our system, a user simply needs to re-record the part of the voiceover that needs to be replaced. Our system automatically locates the corresponding part in the original recording and performs the low level audio processing to replace it. The system can be easily incorporated in any existing sophisticated audio editor or can be employed as a functionality in an audio-guided user interface. User studies involving participation from novice, knowledgeable and expert users indicate that our tool is preferred to a traditional audio editor based redubbing approach by all categories of users due to its faster and easier redubbing capabilities.", 
    "authors": [
      {
        "name": "Shrikant Venkataramani"
      }, 
      {
        "name": "Paris Smaragdis"
      }, 
      {
        "name": "Gautham Mysore"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "AutoDub: Automatic Redubbing for Voiceover Editing", 
    "type": "paper"
  }, 
  "uistf4637": {
    "abstract": "We present DreamSketch, a novel 3D design interface that combines the free-form and expressive qualities of sketching with the computational power of generative design algorithms. In DreamSketch, a user coarsely defines the problem by sketching the design context. Then, a generative design algorithm produces multiple solutions that are augmented as 3D objects in the sketched context. The user can interact with the scene to navigate through the generated solutions. The combination of sketching and generative algorithms enables designers to explore multiple ideas and make better informed design decisions during the early stages of design. Design study sessions with designers and mechanical engineers demonstrate the expressive nature and creative possibilities of DreamSketch.", 
    "authors": [
      {
        "name": "Rubaiat Habib Kazi"
      }, 
      {
        "name": "Tovi Grossman"
      }, 
      {
        "name": "Hyunmin Cheong"
      }, 
      {
        "name": "Ali B. Hashemi"
      }, 
      {
        "name": "George Fitzmaurice"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "DreamSketch: Early Stage 3D Design Explorations with Sketching and Generative Design", 
    "type": "paper"
  }, 
  "uistf4658": {
    "abstract": "Data science courses and tutorials have grown popular in recent years, yet they are still taught using production-grade programming tools (e.g., R, MATLAB, and Python IDEs) within desktop computing environments. Although powerful, these tools present high barriers to entry for novices, forcing them to grapple with the extrinsic complexities of software installation and configuration, data file management, data parsing, and Unix-like command-line interfaces. To lower the barrier for novices to get started with learning data science, we created DS.js, a bookmarklet that embeds a data science programming environment directly into any existing webpage. By transforming any webpage into an example-centric IDE, DS.js eliminates the aforementioned complexities of desktop-based environments and turns the entire web into a rich substrate for learning data science. DS.js automatically parses HTML tables and CSV/TSV data sets on the target webpage, attaches code editors to each data set, provides a data table manipulation and visualization API designed for novices, and gives instructional scaffolding in the form of bidirectional previews of how the user's code and data relate.", 
    "authors": [
      {
        "name": "Xiong Zhang"
      }, 
      {
        "name": "Philip J Guo"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "DS.js: Turn Any Webpage into an Example-Centric Live Programming Environment for Learning Data Science", 
    "type": "paper"
  }, 
  "uistf4706": {
    "abstract": "We propose a system for real-time six degrees of freedom (6DoF) tracking of a passive stylus that achieves sub-millimeter accuracy, which is suitable for writing or drawing in mixed reality applications.  Our system is particularly easy to implement, requiring only a monocular camera, a 3D printed dodecahedron, and hand-glued binary square markers. The accuracy and performance we achieve are due to model-based tracking using a calibrated model and a combination of sparse pose estimation and dense alignment.  We demonstrate the system performance in terms of speed and accuracy on a number of synthetic and real datasets, showing that it can be competitive with state-of-the-art multi-camera motion capture systems.  We also demonstrate several applications of the technology ranging from 2D and 3D drawing in VR to general object manipulation and board games.", 
    "authors": [
      {
        "name": "Po-Chen Wu"
      }, 
      {
        "name": "Robert Wang"
      }, 
      {
        "name": "Kenrick Kin"
      }, 
      {
        "name": "Christopher Twigg"
      }, 
      {
        "name": "Shangchen Han"
      }, 
      {
        "name": "Ming-Hsuan Yang"
      }, 
      {
        "name": "Shao-Yi Chien"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "DodecaPen: Accurate 6DoF Tracking of a Passive Stylus", 
    "type": "paper"
  }, 
  "uistf4922": {
    "abstract": "Reflector is a novel direct pointing method that utilizes hidden design space on reflective screens. By aligning a part of the user\u00e2\u0080\u0099s onscreen reflection with objects rendered on the screen, Reflector enables (1) distance-independent and (2) private pointing on commodity screens. Reflector can be implemented easily in both desktop and mobile conditions through a single camera installed at the edge of the screen. Reflector\u00e2\u0080\u0099s pointing performance was compared to today\u00e2\u0080\u0099s major direct input devices: eye trackers and touchscreens. We demonstrate that Reflector allows the user to point more reliably, regardless of distance from the screen, compared to an eye tracker. Further, due to the private nature of an onscreen reflection, Reflector shows a shoulder surfing success rate 20 times lower than that of touchscreens for the task of entering a 4-digit PIN.", 
    "authors": [
      {
        "name": "JONG-IN LEE"
      }, 
      {
        "name": "Sunjun Kim"
      }, 
      {
        "name": "Masaaki Fukumoto"
      }, 
      {
        "name": "Byungjoo Lee"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Reflector: Distance-Independent, Private Pointing on a Reflective Screen", 
    "type": "paper"
  }, 
  "uistf4937": {
    "abstract": "Here we report the new soft sensor \u00e2\u0080\u009cFoamSense\u00e2\u0080\u009d that can measure the deformation state of a volumetric soft object such as compressed, bent, twisted and sheared (Figure 1). This sensor is made by impregnating a porous soft object with conductive ink. The design process of FoamSense is explained. We then summarized the features and basic characteristics of some porous materials for designing these sensors appropriately. We also proposed the potential of using digital fabrication for controlling the carrier structure of FoamSense. Proposed porous structure showed an anisotropic sensor characteristic. We discussed the potential and limitation of this approach. Three possible applications are proposed by using FoamSense. FoamSense supports a richer interaction between the user and soft objects.", 
    "authors": [
      {
        "name": "Satoshi Nakamaru"
      }, 
      {
        "name": "Ryosuke Nakayama"
      }, 
      {
        "name": "Ryuma Niiyama"
      }, 
      {
        "name": "Yasuaki Kakehi"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "FoamSense: Design of three dimensional soft sensors with porous materials", 
    "type": "paper"
  }, 
  "uistf4994": {
    "abstract": "Human actuation is the idea of using people to provide large-scale force feedback to users. The Haptic Turk system, for example, used four human actuators to lift and push a virtual reality user; TurkDeck used ten human actuators to place and animate props for a single user. While the experience of human actuators was decent, it was still inferior to the experience these people could have had, had they participated as a user. In this paper, we address this issue by making everyone a user. We introduce mutual human actuation, a version of human actuation that works without dedicated human actuators. The key idea is to run pairs of users at the same time and have them provide human actuation to each other. Our system, Mutual Turk, achieves this by (1)\u00c2\u00a0offering shared props through which users can exchange forces while obscuring the fact that there is a human on the other side, and (2)\u00c2\u00a0synchronizing the two users\u00e2\u0080\u0099 timelines such that their way of manipulating the shared props is consistent across both virtual worlds. We demonstrate mutual human actuation with an example experience in which users pilot kites though storms, tug fish out of ponds, are pummeled by hail, battle monsters, hop across chasms, push loaded carts, and ride in moving vehicles.", 
    "authors": [
      {
        "name": "Lung-Pan Cheng"
      }, 
      {
        "name": "Sebastian Marwecki"
      }, 
      {
        "name": "Patrick Baudisch"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Mutual Human Actuation", 
    "type": "paper"
  }
}