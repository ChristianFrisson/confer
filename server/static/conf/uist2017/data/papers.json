entities={
  "TOCHI 0014.R2": {
    "abstract": "Abstract: Emoji, a set of pictographic Unicode characters, have seen strong uptake over the last couple of years. All common mobile platforms and many desktop systems now support emoji entry and users have embraced their use. Yet, we currently know very little about what makes for good emoji entry. While soft keyboards for text entry are well optimized, based on language and touch models, no such information exists to guide the design of emoji keyboards. In this article, we investigate of the problem of emoji entry, starting with a study of the current state of the emoji keyboard implementation in Android. To enable moving forward to novel emoji keyboard designs, we then explore a model for emoji similarity that is able to inform such designs. This semantic model is based on data from 21 million collected tweets containing emoji. We compare this model against a solely description-based model of emoji in a crowdsourced study. Our model shows good performance in capturing detailed relationships between emoji.\n", 
    "authors": [], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "TOCHI 0014.R2 Beyond Just Text: Semantic Emoji Similarity Modeling to Support Expressive Communication", 
    "type": "paper"
  }, 
  "uistde102": {
    "abstract": "The computer keyboard is a widely used input device to operate computers, such as text entry and command execution. Typically, keystrokes are detected as binary states (e.g. \u00c3\u00a2\u00e2\u0082\u00ac\u00c5\u0093pressed\u00c3\u00a2\u00e2\u0082\u00ac\u00c2\u009d vs. \u00c3\u00a2\u00e2\u0082\u00ac\u00c5\u0093not pressed\u00c3\u00a2\u00e2\u0082\u00ac\u00c2\u009d). Due to this, more complex input commands need multiple key presses that could go up to pressing four keys at the same time, such as pressing \u00c3\u00a2\u00e2\u0082\u00ac\u00c5\u0093Cmd + Shift + Opt + 4\u00c3\u00a2\u00e2\u0082\u00ac\u00c2\u009d to take a screenshot to the clipboard on macOS. We present GestAKey, a technique to enable multifunctional keystrokes on a single key, providing new interaction possibilities on the familiar keyboards. The system consists of touch sensitive keycaps and a software backend that recognizes micro-gestures performed on individual keys to perform system commands or input special characters. In this demo, attendees will get the chance to interact with several GestAKey-enabled proof-of-concept applications.", 
    "authors": [
      {
        "name": "Yilei  Shi, Singapore University of Technology and Design"
      }, 
      {
        "name": "Tom\u00c3\u00a1S  Vega G\u00c3\u00a1Lvez, Singapore University of Technology and Design"
      }, 
      {
        "name": "Haimo  Zhang, Singapore University of Technology and Design"
      }, 
      {
        "name": "Suranga  Nanayakkara, Singapore University of Technology and Design"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "GestAKey: Get More Done with Just-a-Key on a Keyboard", 
    "type": "paper"
  }, 
  "uistde106": {
    "abstract": "Human actuation is the idea of using people to provide large-scale force feedback to users. The Haptic Turk system, for example, used four human actuators to lift and push a virtual reality user; TurkDeck used ten human actuators to place and animate props for a single user. While the experience of human actuators was decent, it was still inferior to the experience these people could have had, had they participated as a user. In this paper, we address this issue by making everyone a user. We introduce mutual human actuation, a version of human actuation that works without dedicated human actuators. The key idea is to run pairs of users at the same time and have them provide human actuation to each other. Our system, Mutual Turk, achieves this by (1)\u00c3\u0082 offering shared props through which users can exchange forces while obscuring the fact that there is a human on the other side, and (2)\u00c3\u0082 synchronizing the two users\u00c3\u00a2\u00e2\u0082\u00ac\u00e2\u0084\u00a2 timelines such that their way of manipulating the shared props is consistent across both virtual worlds. We demonstrate mutual human actuation with an example experience in which users pilot kites though storms, tug fish out of ponds, are pummeled by hail, battle monsters, hop across chasms, push loaded carts, and ride in moving vehicles. ", 
    "authors": [
      {
        "name": "Lung-Pan  Cheng, Hasso Plattner Institute"
      }, 
      {
        "name": "Sebastian  Marwecki, Hasso Plattner Institute"
      }, 
      {
        "name": "Patrick  Baudisch, Hasso Plattner Institute"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Mutual Human Actuation", 
    "type": "paper"
  }, 
  "uistde107": {
    "abstract": "We propose the concept of the \u00c3\u00a2\u00e2\u0082\u00ac\u00c5\u0093Internet of Haptics\u00c3\u00a2\u00e2\u0082\u00ac\u00c2\u009d (IoH) that enables sharing experiences of others with the sense of touch. IoH allows to multicast haptic sensation from one Sensor-Node (Inter-Node) to multiple Actuator-Node (Ceive-Node) and to multicast from multiple Inter-Node to multiple Ceive-Node via the Internet. As a proof of concept, we developed the \u00c3\u00a2\u00e2\u0082\u00ac\u00c5\u0093HaptI/O\u00c3\u00a2\u00e2\u0082\u00ac\u00c2\u009d device which is a physical network node that can perform as a gateway to input or output the haptic information to/from the human body or tangible objects. We use the WebRTC as the baseline protocol for communication. Users can gain access IoH Web using a smartphone or PC and experience the haptic sensation by selecting the Inter-Node and Ceive-Node from a web browser. Multiple HaptI/O would be connected on the IoH server and transmit the haptic information from one node to multiple nodes as well as one to one mutual connection so that HaptI/O enables us to share our experiences with the sense of touch.", 
    "authors": [
      {
        "name": "Satoshi  Matsuzono, Keio University Graduate School of Media Design"
      }, 
      {
        "name": "Haruki  Nakamura, Keio University Graduate School of Media Design"
      }, 
      {
        "name": "Daiya  Kato, Keio University Graduate School of Media Design"
      }, 
      {
        "name": "Roshan  Peiris, Keio University Graduate School of Media Design"
      }, 
      {
        "name": "Kouta  Minamizawa, Keio University Graduate School of Media Design"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "HaptI/O: Physical Node for the Internet of Haptics", 
    "type": "paper"
  }, 
  "uistde108": {
    "abstract": "We propose a tactile element that can generate both an electrostatic force and an electrostimulus, and can be used to provide tactile feedback on a wide area of human skin such as the palm of the hand. Touching the flat surface through the our proposed tactile element allow the user to feel both uneven and rough textures. In addition, the element can be fabricated using double-sided inkjet printing with conductive ink. Use of a flexible substrate, such as a PET film or paper, allows the user to design a free-formed tactile element. In this demonstration, we describe the implementation of the proposed stimuli element and show examples of applications.", 
    "authors": [
      {
        "name": "Kunihiro  Kato, Meiji University"
      }, 
      {
        "name": "Homei  Miyashita, Meiji University"
      }, 
      {
        "name": "Hiroyuki  Kajimoto, The University of Electro-communications/Kajimoto-Lab"
      }, 
      {
        "name": "Hiroki  Ishizuka, Kagawa University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Tactile Element with Double-sided Inkjet Printing to Generate Electrostatic Forces and Electrostimuli", 
    "type": "paper"
  }, 
  "uistde110": {
    "abstract": "We propose a new type of printing system that incorporates sensors in a handheld printer to reflect in real time user intent in the results of printing on paper. This system achieves two key functions: \u00c3\u00a2\u00e2\u0082\u00ac\u00c5\u0093real-time embellishment\u00c3\u00a2\u00e2\u0082\u00ac\u00c2\u009d for altering printed content by reading user hand movements by pressure and optical sensors, and \u00c3\u00a2\u00e2\u0082\u00ac\u00c5\u0093local transcription\u00c3\u00a2\u00e2\u0082\u00ac\u00c2\u009d for selecting content to be output by tracing existing content on paper with a linear camera. We performed experiments to measure the accuracy of both techniques and evaluate their usefulness.", 
    "authors": [
      {
        "name": "Yuya  Kitazawa, Waseda University"
      }, 
      {
        "name": "Tomoko  Hashida, Waseda University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Printing System Reflecting User\u00c3\u00a2\u00e2\u0082\u00ac\u00e2\u0084\u00a2s Intent in Real Time Using a Handheld Printer", 
    "type": "paper"
  }, 
  "uistde112": {
    "abstract": "We present CritiqueKit, a mixed-initiative machine-learning system that helps students give better feedback to peers by reusing prior feedback, reducing it to be useful in a general context, and retraining the system about what is useful in real time. CritiqueKit exploits the fact that novices often make similar errors, leading reviewers to reuse the same feedback on many different submissions. It takes advantage of all prior feedback, and classifies feedback as the reviewer types it. CritiqueKit continually updates the corpus of feedback with new comments that are added, and it guides reviewers to improve their feedback, and thus the entire corpus, over time.", 
    "authors": [
      {
        "name": "C. Ailie  Fraser, UC San Diego"
      }, 
      {
        "name": "Tricia  Ngoon, UC San Diego"
      }, 
      {
        "name": "Ariel  Weingarten, UC San Diego"
      }, 
      {
        "name": "Mira  Dontcheva, Adobe Research"
      }, 
      {
        "name": "Scott  Klemmer, UC San Diego"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "CritiqueKit: A Mixed-Initiative, Real-Time Interface For Improving Feedback", 
    "type": "paper"
  }, 
  "uistde115": {
    "abstract": "shapeShift is a compact, high-resolution (7mm pitch), mobile tabletop shape display. We explore potential interaction techniques in both passive and active mobile scenarios. In the passive case, the user is able to freely move and spin the display as it renders elements. We introduce use cases for rendering lateral I/O elements, exploring volumetric datasets, and grasping and manipulating objects. On an active omnidirectional-robot platform, shapeShift can display moving objects and provide both vertical and lateral force feedback. We use the active platform as an encounter-type haptic device combined with a head-mounted display to dynamically simulate the presence of virtual content.", 
    "authors": [
      {
        "name": "Alexa  Siu, Stanford University"
      }, 
      {
        "name": "Eric  Gonzalez, Stanford University"
      }, 
      {
        "name": "Shenli  Yuan, Stanford University"
      }, 
      {
        "name": "Jason  Ginsberg, Stanford University"
      }, 
      {
        "name": "Allen  Zhao, Stanford University"
      }, 
      {
        "name": "Sean  Follmer, Stanford University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "shapeShift: A Mobile Tabletop Shape Display for Tangible and Haptic Interaction", 
    "type": "paper"
  }, 
  "uistde118": {
    "abstract": "An actuated shape-changing interface with faster response and smaller pixel size using a liquid material can provide real time tangible interaction with the digital world in physical space. To this end, we demonstrate an interface that displays user-defined patterns dynamically using liquid metal droplets as programmable micro robots on a flat surface. We built a prototype using an array of embedded electrodes and a switching circuit to control the jump of the droplets from electrode to electrode. The actuation and dynamics of the droplets under the finger provides mild tactile feedback to the user. Our demo is the first to show a planar visio-tactile display using liquid metal, and is a first step to make shape-changing physical ephemeral widgets on a tabletop interface.", 
    "authors": [
      {
        "name": "Deepak  Sahoo, Swansea University"
      }, 
      {
        "name": "Timothy  Neate, Swansea University"
      }, 
      {
        "name": "Yutaka  Tokuda, University of Sussex"
      }, 
      {
        "name": "Jennifer  Pearson, Swansea University"
      }, 
      {
        "name": "Simon  Robinson, Swansea University"
      }, 
      {
        "name": "Sriram  Subramanian, University of Sussex"
      }, 
      {
        "name": "Matt  Jones, Swansea University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "JDLED: Towards Visio-Tactile Displays based on Electrochemical Locomotion of Liquid-Metal Janus Droplets", 
    "type": "paper"
  }, 
  "uistde122": {
    "abstract": "Multiplanes is a virtual reality (VR) drawing system that provides users with the flexibility of freehand drawing and the ability to draw perfect shapes. Through the combination of both beautified and 2D drawing, Multiplanes addresses challenges in creating 3D VR drawings. To achieve this, the system beautifies user\u00c3\u00a2\u00e2\u0082\u00ac\u00e2\u0084\u00a2s strokes based on the most probable, intended shapes while the user is drawing them. It also automatically generates snapping planes and beautification trigger points based on previous and current strokes and the current controller pose. Based on geometrical relationships to previous strokes, beautification trigger points act as guides inside the virtual environment. Users can hit these points to (explicitly) trigger a stroke beautification. In contrast to other systems, when using Multiplanes, users do not need to manually set or do any kind of special gesture to activate, such guides allowing the user to focus on the creative process.", 
    "authors": [
      {
        "name": "Mayra Donaji  Barrera Machuca"
      }, 
      {
        "name": "Paul  Asente, Adobe Research"
      }, 
      {
        "name": "Jingwan  Lu, Adobe Research"
      }, 
      {
        "name": "Byungmoon  Kim, Adobe Research"
      }, 
      {
        "name": "Wolfgang  Stuerzlinger, Simon Fraser University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Multiplanes: Assisted Freehand VR Drawing", 
    "type": "paper"
  }, 
  "uistde124": {
    "abstract": "We present PhyShare, a new haptic user interface based on actuated robots. Virtual reality has recently been gaining wide adoption, and an effective haptic feedback in these scenarios can strongly support user's sensory in bridging virtual and physical world. Since participants do not directly observe these robotic proxies, we investigate the multiple mappings between physical robots and virtual proxies that can utilize the resources needed to provide a well rounded VR experience. PhyShare bots can act either as directly touchable objects or invisible carriers of physical objects, depending on different scenarios. They also support distributed collaboration, allowing remotely located VR collaborators to share the same physical feedback.", 
    "authors": [
      {
        "name": "Zhenyi  He, New York University"
      }, 
      {
        "name": "Fengyuan  Zhu, New York University"
      }, 
      {
        "name": "Ken  Perlin, New York University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "PhyShare: Sharing Physical Interaction in Virtual Reality", 
    "type": "paper"
  }, 
  "uistde125": {
    "abstract": "We introduce SkinBot; a lightweight robot that moves over the skin surface with a two-legged suction-based locomotion mechanism and that captures a wide range of body parameters with an exchangeable multipurpose sensing module. We believe that robots that live on our skin such as SkinBot will enable a more systematic study of the human body and will offer great opportunities to advance many areas such as telemedicine, human-computer interfaces, body care, and fashion.", 
    "authors": [
      {
        "name": "Artem  Dementyev, MIT"
      }, 
      {
        "name": "Javier  Hernandez, MIT"
      }, 
      {
        "name": "Sean  Follmer, Stanford University"
      }, 
      {
        "name": "Inrak  Choi, Stanford University"
      }, 
      {
        "name": "Joseph  Paradiso, MIT"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "SkinBot: A Wearable Skin Climbing Robot", 
    "type": "paper"
  }, 
  "uistde127": {
    "abstract": "We propose a novel shape-changing technique called Filum, which makes it possible to alter the shapes of textiles to better suit the requirements of people and the environment. Using strings and various sewing methods, ordinary textiles can be automatically shortened or shrunk into curved shapes. We demonstrate a series of novel interactions between people and textiles via three applications.", 
    "authors": [
      {
        "name": "Tomomi  Kono, Meiji University"
      }, 
      {
        "name": "Keita  Watanabe, Meiji University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Filum: A Sewing Technique to Alter Textile Shapes", 
    "type": "paper"
  }, 
  "uistde128": {
    "abstract": "Conquer it! is a lightweight proof-of-concept exertion game that demonstrates Body Channel Communication (BCC) in a smart environment. BCC employs the human body as communication medium to transfer digital data between physical objects by using electric fields that are coupled to the body. During the game participants are provided with BCC wearables, each of which represents a specific RGB color. When the user stands, walks on, or touches with a hand the BCC tiles, communication is automatically established: the corresponding sensor area decodes the message (RGB value) originating from the wearable and lights up according to that color for two seconds. The goal of the game is to try to light up as many tile cells simultaneously as possible. Participants can try to keep alive the colors by continuously moving around on the tiles. In the multiuser version, by stepping on or touching a blinking cell, users can immediately claim the area and overwrite the color of that subtile.", 
    "authors": [
      {
        "name": "Virag  Varga, ETH Zurich"
      }, 
      {
        "name": "Gergely  Vakulya, ETH Zurich"
      }, 
      {
        "name": "Alanson  Sample, Disney Research"
      }, 
      {
        "name": "Thomas  Gross, ETH Zurich"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Playful Interactions with Body Channel Communication: Conquer it!", 
    "type": "paper"
  }, 
  "uistde129": {
    "abstract": "We present MagTics, a novel flexible and wearable haptic interface based on magnetically actuated bi-directional tactile pixels (taxels). MagTics' thin form-factor and flexibility allows for rich haptic feedback in mobile settings.  We propose a novel actuation mechanism based on bi-stable electromagnetic latching that combines high framerate and holding force with low energy consumption and a soft and flexible form-factor.  We overcome limitations of traditional soft-actuators by placing several hard actuation cells, driven by flexible printed electronics, in a soft 3D printed case. A novel EM-shielding prevents magnet-magnet interactions and allows for high actuator densities. A prototypical implementation comprising of 4 actuated pins on a 1.7 cm pitch, with 2 mm travel, and generating 160 mN to 200 mN of latching force is used to implement a number of compelling application scenarios.", 
    "authors": [
      {
        "name": "Juan  Zarate, \u00c3\u2030cole polytechnique f\u00c3\u00a9d\u00c3\u00a9rale de Lausanne (EPFL)"
      }, 
      {
        "name": "Fabrizio  Pece, ETH Zurich"
      }, 
      {
        "name": "Velko  Vechev, Chalmers University of Technology"
      }, 
      {
        "name": "Nadine  Besse, \u00c3\u2030cole polytechnique f\u00c3\u00a9d\u00c3\u00a9rale de Lausanne"
      }, 
      {
        "name": "Olexandr  Gudozhnik, \u00c3\u2030cole polytechnique f\u00c3\u00a9d\u00c3\u00a9rale de Lausanne (EPFL)"
      }, 
      {
        "name": "Ronan  Hinchet, \u00c3\u2030cole Polytechnique F\u00c3\u00a9d\u00c3\u00a9rale de Lausanne"
      }, 
      {
        "name": "Herbert  Shea, \u00c3\u2030cole polytechnique f\u00c3\u00a9d\u00c3\u00a9rale de Lausanne (EPFL)"
      }, 
      {
        "name": "Otmar  Hilliges, ETH Zurich"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "MagTics: Flexible and Thin Form Factor Magnetic Actuators for Dynamic and Wearable Haptic Feedback", 
    "type": "paper"
  }, 
  "uistde130": {
    "abstract": "Physical controls are fabricated through complicated assembly of parts requiring expensive machinery and are prone to mechanical wear. One solution is to embed controls directly in interactive surfaces, but the proprioceptive part of gestural interaction that makes physical controls discoverable and usable solely by hand gestures is lost and has to be compensated, by vibrotactile feedback for instance. Vibrotactile actuators face the same aforementioned issues as for physical controls. We propose printed vibrotactile actuators and sensors. They are printed on plastic sheets, with piezoelectric ink for actuation, and with silver ink for conductive elements, such as wires and capacitive sensors. These printed actuators and sensors make it possible to design vibrotactile widgets on curved surfaces, without complicated mechanical assembly.", 
    "authors": [
      {
        "name": "Christian  Frisson, Inria"
      }, 
      {
        "name": "Julien  Decaudin, Inria"
      }, 
      {
        "name": "Thomas  Pietrzak, Univ. Lille"
      }, 
      {
        "name": "Alexander  Ng, University of Glasgow"
      }, 
      {
        "name": "Pauline  Poncet, CEA"
      }, 
      {
        "name": "Fabrice  Casset, CEA"
      }, 
      {
        "name": "Antoine  Latour, CEA"
      }, 
      {
        "name": "Stephen  Brewster, University of Glasgow"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Designing Vibrotactile Widgets with Printed Actuators and Sensors", 
    "type": "paper"
  }, 
  "uistde131": {
    "abstract": "In this paper we propose a method of non-contact stirring.  Ultrasonic waves have been studied for various applications.  However, devices using ultrasound have been devised only to  specialize in one role up to now. In recent years, we aim at  generalization of aerial ultrasonic equipment used for various  applications such as tactile presentation and super directional  speaker, and propose applications closely our real life.", 
    "authors": [
      {
        "name": "Yuta  Sato, University of Tsukuba"
      }, 
      {
        "name": "Kensuke  Abe, University of Tsukuba"
      }, 
      {
        "name": "Kotaro  Omomo, University of Tsukuba"
      }, 
      {
        "name": "Ryota  Kawamura, Graduate School of Library Information and Media Studies in University of Tsukuba"
      }, 
      {
        "name": "Kazuki  Takazawa, University of Tsukuba"
      }, 
      {
        "name": "Yoichi  Ochiai, University of Tsukuba"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Ultrasonic Cuisine: Proposal of ultrasonic non-contact stirring methods", 
    "type": "paper"
  }, 
  "uistde133": {
    "abstract": "We demonstrate TrussFab\u00c3\u00a2\u00e2\u0082\u00ac\u00e2\u0084\u00a2s editor for creating large-scale structures that are sturdy enough to carry human weight. TrussFab achieves the large scale by using plastic bottles as beams that form structurally sound node-link structures, also known as trusses, allowing it to handle the forces resulting from scale and load. During this hands-on demo at UIST, attendees will use the TrussFab software to design their own structures, validate their design using integrated structural analysis, and export their designs for 3D printing.", 
    "authors": [
      {
        "name": "Robert  Kovacs, Hasso Plattner Institute"
      }, 
      {
        "name": "Ludwig Wilhelm Wall, Hasso Plattner Institute"
      }, 
      {
        "name": "Anna  Seufert, Hasso Plattner Institute"
      }, 
      {
        "name": "Hsiang-Ting  Chen, Hasso Plattner Institute"
      }, 
      {
        "name": "Willi  M\u00c3\u00bcLler, Hasso Plattner Institute"
      }, 
      {
        "name": "Florian  Meinel, Hasso Plattner Institute"
      }, 
      {
        "name": "Yannis  Kommana, Hasso Plattner Institute"
      }, 
      {
        "name": "Thomas  Bl\u00c3\u00a4Sius"
      }, 
      {
        "name": "Thijs Jan Roumen, Hasso Plattner Institute"
      }, 
      {
        "name": "Oliver S. Schneider"
      }, 
      {
        "name": "Patrick  Baudisch, Hasso Plattner Institute"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Demonstrating TrussFab's Editor: Designing Sturdy Large-Scale Structures", 
    "type": "paper"
  }, 
  "uistde135": {
    "abstract": "Sensor technologies have been adapted to performing arts, and owing to the recent advancement of low-cost mobile electroencephalography devices, brain-computer interface (BCI) is integrated to dance performances as well. Nevertheless, BCI is less accessible to artists compared to other sensors because signal processing and machine learning are required. This paper proposes a work-in-progress example of BCI applications for performances that has been designed in collaboration with contemporary dancers. Its contribution is that the piece is not an add-on to a performance, but the implementation reflects practices of contemporary dance. ", 
    "authors": [
      {
        "name": "Naoto  Hieda, Concordia University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Mobile Brain-Computer Interface for Dance and Somatic Practice", 
    "type": "paper"
  }, 
  "uistde137": {
    "abstract": "We demonstrate a haptic feedback method that generates a compliance illusion on a rigid surface with a tangential force sensor and a vibrotactile actuator. The method assumes a conceptual model where a virtual object is placed on a textured surface and stringed to four walls with four springs. A two dimensional tangential force vector measured from the rigid surface is mapped to the virtual position of the virtual object on the textured surface. By playing vibration patterns that simulate the friction-induced vibrations made from the movement of the virtual object, we could make an illusion that the rigid surface feels like moving. We also demonstrate that the perceptual properties of the illusion, such as the stiffness of the virtual spring and the maximum travel distance of the virtual object, can be programmatically controlled.", 
    "authors": [
      {
        "name": "Seongkook  Heo"
      }, 
      {
        "name": "Geehyuk  Lee, KAIST"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Creating Haptic Illusion of Compliance for Tangential Force Input using Vibrotactile Actuator", 
    "type": "paper"
  }, 
  "uistde138": {
    "abstract": "Recent developments in wearable robots and human augmentation open up new possibilities of designing computational interfaces integrated to the body. Particularly, supernumerary robot is a recently established field of research that investigates a radical idea of adding robotic limbs to users. Such augmentations, however, pose a limit in how much we can add to the body due to weight or interference with other body parts. To address that, we explore the use of soft robots as supernumerary robotic fingers. We present a pair of soft robotic fingers driven by cables and servomotors, and applications using the robotic fingers in various contexts. ", 
    "authors": [
      {
        "name": "Yuhan  Hu, MIT"
      }, 
      {
        "name": "Sang-Won  Leigh, MIT"
      }, 
      {
        "name": "Pattie  Maes, MIT Media Lab"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Hand Development Kit: Soft Robotic Fingers as Prosthetic Augmentation of the Hand", 
    "type": "paper"
  }, 
  "uistde139": {
    "abstract": "We provide a hands-on demonstration of the potential of interactive systems based on electrical muscle stimulation (EMS). These wearable devices allow attendees, for example, to physically learn how to manipulate objects they never seen before, feel walls and forces in virtual reality, and so forth. In our demo we plan to not only demonstrate several of these EMS-based prototypes but also to provide instructions and free hardware for people to conduct their first projects using EMS.", 
    "authors": [
      {
        "name": "Pedro  Lopes, Hasso Plattner Institute"
      }, 
      {
        "name": "Patrick  Baudisch, Hasso Plattner Institute"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Demonstrating Interactive Systems based on Electrical Muscle Stimulation", 
    "type": "paper"
  }, 
  "uistde147": {
    "abstract": "This paper proposes a dynamic acoustic field generation system for a spot audio towards particular person indoors. Spot audio techniques have been explored by generating the ultrasound beams toward the target person in certain area however everyone in this area can hear the sound. Our system recognizes the positions of each person indoor using motion capture and 3D model data of the room. After that we control direction of parametric speaker in real-time so that sound reach only particular person by calculating the reflection of sound on surfaces such as wall and ceiling. We calculate direction of parametric speaker using a beam tracing method. We present generating methods of dynamic acoustic field in our system and conducted the experiments on human factor to evaluate performance of proposed system.", 
    "authors": [
      {
        "name": "Naoya  Muramatsu, University of Tsukuba"
      }, 
      {
        "name": "Kazuki  Ohshima, University of Tsukuba"
      }, 
      {
        "name": "Ryota  Kawamura, Graduate School of Library Information and Media Studies in University of Tsukuba"
      }, 
      {
        "name": "Yoichi  Ochiai, University of Tsukuba"
      }, 
      {
        "name": "Yuta  Sato, University of Tsukuba"
      }, 
      {
        "name": "Chun Wei  Ooi, University of Tsukuba"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Sonoliards: Rendering audible sound spots by reflecting the ultrasound beams", 
    "type": "paper"
  }, 
  "uistde149": {
    "abstract": "There are many everyday situations in which users need to enter their user identification (user ID), such as logging in to computer systems and entering secure offices. In such situations, contactless passive IC cards are convenient because users can input their user ID simply by passing the card over a reader. However, these cards cannot be used for successive interactions. To address this issue, we propose AccelTag, a contactless IC card equipped with an acceleration sensor and a liquid crystal display (LCD). AccelTag utilizes high-function RFID technology so that the acceleration sensor and the LCD can also be driven by a wireless power supply. With its built-in acceleration sensor, AccelTag can acquire its direction and movement when it is waved over the reader. We demonstrate several applications using AccelTag, such as displaying several types of information in the card depending on the user's requirements.", 
    "authors": [
      {
        "name": "Kazuya  Oharada, University of Tsukuba"
      }, 
      {
        "name": "Buntarou  Shizuki, University of Tsukuba"
      }, 
      {
        "name": "Shin  Takahashi, University of Tsukuba"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "AccelTag: A Passive Smart ID Tag with Acceleration Sensor for Interactive Applications", 
    "type": "paper"
  }, 
  "uistde151": {
    "abstract": "Virtual reality (VR) using head-mounted displays (HMDs) is becoming popular.  Smartphone-based HMDs (SbHMDs) are so low cost that users can easily experience VR.  Unfortunately, their input modality is quite limited.  We propose a real-time eye tracking technique that uses the built-in front facing camera to capture the user's eye.  It realizes stand-alone pointing functionality without any additional device.", 
    "authors": [
      {
        "name": "Hiroyuki  Hakoda, NTT DOCOMO"
      }, 
      {
        "name": "Wataru  Yamada, NTT DOCOMO"
      }, 
      {
        "name": "Hiroyuki  Manabe, NTT DOCOMO"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Eye Tracking Using Built-in Camera for Smartphone-based HMD", 
    "type": "paper"
  }, 
  "uistde152": {
    "abstract": "This paper proposes a new approach to enhancing interaction with general applications on smartphones.  Physical objects held on back surface of the smartphone, which can be captured by the rear camera with a mirror, work as input devices or controllers.  It does not require any additional electronic devices but offers tactile feedback.  The occlusion problem does not occur when using smartphone's back side, in terms of both display and camera viewing.  We implemented on an Android smartphone and confirmed that it provides richer interaction and low latency (100 ms).", 
    "authors": [
      {
        "name": "Nobutaka  Matsushima, NTT DOCOMO, INC."
      }, 
      {
        "name": "Wataru  Yamada, NTT DOCOMO"
      }, 
      {
        "name": "Hiroyuki  Manabe, NTT DOCOMO"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Attaching Objects to Smartphones Back Side for a Modular Interface", 
    "type": "paper"
  }, 
  "uistde153": {
    "abstract": "We present Feeling Fireworks, a tactile firework show. Feeling Fireworks is aimed at making fireworks more inclusive for blind and visually impaired users, in a novel experience that can be shared by all. Tactile effects are created using directable water jets that spray onto the rear of a flexible screen, with different nozzles for different firework effects. Our approach is low-cost and scales well, and allows for dynamic tactile effects to be rendered with high spatial resolution. A user study demonstrated that the tactile effects are meaningful analogs to the visual fireworks that they represent, with sighted users able to label the correct correspondence of tactile-to-visual effects by a large margin over chance. Beyond the specific application, the technology represents a novel and cost-effective approach for making large scalable tactile displays, with the potential for wider use.", 
    "authors": [
      {
        "name": "Dorothea  Reusser, Disney Research Zurich"
      }, 
      {
        "name": "Dorothea  Reusser, ETH Zurich"
      }, 
      {
        "name": "Espen  Knoop, Disney Research"
      }, 
      {
        "name": "Roland  Siegwart, ETH Zurich"
      }, 
      {
        "name": "Paul  Beardsley, Disney Research Zurich"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Feeling Fireworks", 
    "type": "paper"
  }, 
  "uistde155": {
    "abstract": "Chalktalk is a computer-based visual language based around real-time interaction with virtual objects in a blackboard-style environment. Its aim is to be a presentation and communication tool, using animation and interactivity to allow easy visualization of complex ideas and concepts. This demonstration will show Chalktalk in action, with focus on its ability to link these objects together to send data between them, as well as the flexible type system, named Atypical, that underpins this feature.", 
    "authors": [
      {
        "name": "Gabriel Barbosa Nunes, New York University"
      }, 
      {
        "name": "Ken  Perlin, New York University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Atypical: A Type System for Live Performances", 
    "type": "paper"
  }, 
  "uistf1006": {
    "abstract": "Low-fidelity prototyping at the early stages of user interface (UI) design can help designers and system builders quickly explore their ideas.  However, interactive behaviors in such prototypes are often replaced by textual descriptions because it usually takes even professionals hours or days to create animated interactive elements due to the complexity of creating them.  In this paper, we introduce SketchExpress, a crowd-powered prototyping tool that enables crowd workers to create reusable interactive behaviors easily and accurately. With the system, a requester\u00e2\u0080\u0094designers or end-users\u00e2\u0080\u0094describes aloud how an interface should behave and crowd workers make the sketched prototype interactive within minutes using a demonstrate-remix-replay approach.  These behaviors are manually demonstrated, refined using remix functions, and then can be replayed later. The recorded behaviors persist for future reuse to help users communicate with the animated prototype. We conducted a study with crowd workers recruited from Mechanical Turk, which demonstrated that workers could create animations using SketchExpress in 2.9 minutes on average with 27% gain in the quality of animations compared to the baseline condition of manual demonstration.", 
    "authors": [
      {
        "name": "Sang Won  Lee, University of Michigan"
      }, 
      {
        "name": "Yujin  Zhang, University of Michigan"
      }, 
      {
        "name": "Isabelle  Wong, University of Michigan"
      }, 
      {
        "name": "Yiwei  Yang, University of Michigan"
      }, 
      {
        "name": "Stephanie D O'Keefe, University of Michigan"
      }, 
      {
        "name": "Walter S Lasecki, University of Michigan"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "SketchExpress: Remixing Animations for More Effective Crowd-Powered Prototyping of Interactive Interfaces", 
    "type": "paper"
  }, 
  "uistf1143": {
    "abstract": "Live animation of 2D characters is a new form of storytelling that has started to appear on streaming platforms and broadcast TV. Unlike traditional animation, human performers control characters in real time so that they can respond and improvise to live events. Current live animation systems provide a range of animation controls, such as camera input to drive head movements, audio for lip sync, and keyboard shortcuts to trigger discrete pose changes via artwork swaps. However, managing all of these controls during a live performance is challenging. In this work, we present a new interactive system that specifically addresses the problem of triggering artwork swaps in live settings. Our key contributions are the design of a multi-touch triggering interface that overlays visual triggers around a live preview of the character, and a predictive triggering model that leverages practice performances to suggest pose transitions during live performances. We evaluate our system with quantitative experiments, a user study with novice participants, and interviews with professional animators.", 
    "authors": [
      {
        "name": "Nora S Willett, Princeton University"
      }, 
      {
        "name": "Wilmot  Li, Adobe Research"
      }, 
      {
        "name": "Jovan  Popovic, Adobe Research"
      }, 
      {
        "name": "Adam  Finkelstein, Princeton Univ."
      }
    ], 
    "award": true, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Triggering Artwork Swaps for Live Animation", 
    "type": "paper"
  }, 
  "uistf1172": {
    "abstract": "FlexStylus, a flexible stylus, detects deformation of the barrel as a vector with both a rotational and an absolute value, providing two degrees of freedom with the goal of improving the expressivity of digital art using a stylus device. We outline the construction of the prototype and the principles behind the sensing method, which uses a cluster of four fibre-optic based deformation sensors. We propose interaction techniques using the FlexStylus to improve menu navigation and tool selection. Finally, we describe a study comparing users\u00e2\u0080\u0099 ability to match a changing target value using a commercial pressure stylus and the FlexStylus\u00e2\u0080\u0099 absolute deformation. When using the FlexStylus, users had a significantly higher accuracy overall. This suggests that deformation may be a useful input method for future work considering stylus augmentation.", 
    "authors": [
      {
        "name": "Nicholas  Fellion, Carleton University"
      }, 
      {
        "name": "Thomas  Pietrzak, Univ. Lille"
      }, 
      {
        "name": "Audrey  Girouard, Carleton University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "FlexStylus: Leveraging Bend Input for Pen Interaction", 
    "type": "paper"
  }, 
  "uistf1201": {
    "abstract": "Squeezing sensations are one of the most common and intimate forms of human contact. In this paper, we investigate HapticClench, a device that generates squeezing sensations using shape memory alloys. We define squeezing feedback in terms of it perceptual properties and conduct a psychophysical evaluation of HapticClench. HapticClench is capable of generating up to four levels of distinguishable load and works well in distracted scenarios. HapticClench has a high spatial acuity and can generate spatial patterns on the wrist that the user can accurately recognize. We also demonstrate the use of HapticClench for communicating gradual progress of an activity, and for generating squeezing sensations using rings and loose bracelets.", 
    "authors": [
      {
        "name": "Aakar  Gupta, University of Toronto"
      }, 
      {
        "name": "Antony Albert Raj  Irudayaraj, University of Toronto"
      }, 
      {
        "name": "Ravin  Balakrishnan, University of Toronto"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "HapticClench: Investigating Squeeze Sensations using Memory Alloys", 
    "type": "paper"
  }, 
  "uistf1335": {
    "abstract": "Ungrounded haptic devices for virtual reality (VR) applications lack the ability to convincingly render the sensations of a grasped virtual object's rigidity and weight. We present Grabity, a wearable haptic device designed to simulate kinesthetic pad opposition grip forces and weight for grasping virtual objects in VR. The device is mounted on the index finger and thumb and enables precision grasps with a wide range of motion. A unidirectional brake creates rigid grasping force feedback. Two voice coil actuators create virtual force tangential to each finger pad through asymmetric skin deformation. These forces can be perceived as gravitational and inertial forces of virtual objects. The rotational orientation of the voice coil actuators is passively aligned with the real direction of gravity through a revolute joint, causing the virtual forces to always point downward. This paper evaluates the performance of Grabity through two user studies, finding promising ability to simulate different levels of weight with convincing object rigidity. The first user study shows that Grabity can convey various magnitudes of weight and force sensations to users by manipulating the amplitude of the asymmetric vibration. The second user study shows that users can differentiate different weights in a virtual environment using Grabity.", 
    "authors": [
      {
        "name": "Inrak  Choi, Stanford University"
      }, 
      {
        "name": "Heather  Culbertson, Stanford University"
      }, 
      {
        "name": "Mark Roman Miller, Stanford University"
      }, 
      {
        "name": "Alex  Olwal, Google, Inc."
      }, 
      {
        "name": "Sean  Follmer, Stanford University"
      }
    ], 
    "award": true, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Grabity: A Wearable Haptic Interface for Simulating Weight and Grasping in Virtual Reality", 
    "type": "paper"
  }, 
  "uistf1353": {
    "abstract": "This paper introduces a novel method for designing personalized orthopedic casts which are aware of thermal-comfort while satisfying mechanical requirements. Our pipeline starts from thermal images taken by an infrared camera, by which the distribution of thermal-comfort sensitivity is generated on the surface of a 3D scanned model. We formulate a hollowed Voronoi tessellation pattern to represent the covered region for a web-like cast design. The pattern is further optimized according to the thermal-comfort sensitivity calculated from thermal images. Working together with a thickness variation method, we generate a solid model for a personalized cast maximizing both thermal comfort and mechanical stiffness. To demonstrate the effectiveness of our approach, 3D printed models of personalized casts are tested on body parts of different individuals.", 
    "authors": [
      {
        "name": "Xiaoting  Zhang, Boston University"
      }, 
      {
        "name": "Guoxin  Fang, Delft University of Technology"
      }, 
      {
        "name": "Chengkai  Dai, Delft University of Technology"
      }, 
      {
        "name": "Jouke  Verlinden, TU Delft"
      }, 
      {
        "name": "Jun  Wu, TU Delft"
      }, 
      {
        "name": "Emily  Whiting, Boston University"
      }, 
      {
        "name": "Charlie C. L.  Wang, Delft University of Technology"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Thermal-Comfort Design of Personalized Casts", 
    "type": "paper"
  }, 
  "uistf1384": {
    "abstract": "Due to the development of 3D sensing and modeling techniques, the state-of-the-art mixed reality devices such as Microsoft Hololens have the ability of digitalizing the physical world.  This unique feature bridges the gap between virtuality and reality and largely elevates the user experience.  Unfortunately, the current solution only performs well if the virtual contents complement the real scene.  It can easily cause visual artifacts when the reality needs to be modified due to the virtuality (e.g., remove real objects to offer more space for virtual objects), a common scenario in mixed reality applications such as room redecoration and environment design.  We present a novel system, called emph{SceneCtrl}, that allows the user to interactively edit the real scene sensed by Hololens, such that the reality can be adapted to suit virtuality.  Our proof-of-concept prototype employs scene reconstruction and understanding to enable efficient editing such as deleting, moving, and copying real objects in the scene.  We also demonstrate emph{SceneCtrl} on a number of example scenarios in mixed reality, verifying the enhanced experience by resolving conflicts between virtuality and reality.", 
    "authors": [
      {
        "name": "Ya-Ting  Yue, The University of Hong Kong"
      }, 
      {
        "name": "Yongliang  Yang, University of Bath"
      }, 
      {
        "name": "Gang  Ren, Xiamen University of Technology"
      }, 
      {
        "name": "Wenping  Wang, The University of Hong Kong"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "SceneCtrl: Mixed Reality Enhancement via Efficient Scene Editing", 
    "type": "paper"
  }, 
  "uistf1491": {
    "abstract": "This paper describes a simple 3D display that can be built from a tablet computer and a plastic sheet folded into a cone. This display allows naturally viewing a three-dimensional object from any direction over a 360-degree path of travel without the use of a head mount or special glasses. Inspired by the classic Pepper's Ghost illusion, our approach uses a curved transparent surface to reflect the image displayed on a 2D display. By properly pre-distorting the displayed image our system can produce a perspective-correct image to the viewer that appears to be suspended inside the reflector. We use the gyroscope integrated into modern tablet computers to adjust the rendered image based on the relative orientation of the viewer. The end result is a natural and intuitive interface for inspecting a 3D object.  Our choice of a cone reflector is obtained by analyzing optical performance and stereo-compatibility over  rotationally-symmetric conic reflector shapes.  We also present the prototypes we built and measure the performance of our display through side-by-side comparisons with reference images.", 
    "authors": [
      {
        "name": "Xuan  Luo, University of Washington"
      }, 
      {
        "name": "Jason  Lawrence, Google Inc."
      }, 
      {
        "name": "Steven M. Seitz, University of Washington"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Pepper's Cone: An Inexpensive Do-It-Yourself 3D Display", 
    "type": "paper"
  }, 
  "uistf1526": {
    "abstract": "Email is more than just a communication medium. Email serves as an external memory for people---it contains our reservation numbers, meeting details, phone numbers, and more. Often, people need access to this information while on the go, which is cumbersome from mobile devices with limited I/O bandwidth. In this paper, we introduce WearMail, a conversational interface to retrieve specific information in email. WearMail is mostly automated but is made robust to information extraction tasks via a novel privacy-preserving human computation workflow. In WearMail, crowdworkers never have direct access to emails, but rather (i) generate an email filter to help the system find messages that may contain the desired information, and (ii) generate examples of the requested information that are then used to create custom, low-level information extractors that run automatically within the set of filtered emails. We explore the impact of varying levels of obfuscation on result quality, demonstrating that workers are able to deal with highly-obfuscated information nearly as well as with the original. WearMail introduces general mechanisms that let the crowd search and select private data without having direct access to the data itself. ", 
    "authors": [
      {
        "name": "Saiganesh  Swaminathan, Carnegie Mellon University"
      }, 
      {
        "name": "Raymond  Fok, University of Michigan"
      }, 
      {
        "name": "Fanglin  Chen, Carnegie Mellon University"
      }, 
      {
        "name": "Ting-Hao  Huang, Language Technologies Institute"
      }, 
      {
        "name": "Irene  Lin, Carnegie Mellon University"
      }, 
      {
        "name": "Rohan  Jadvani, Carnegie Mellon University"
      }, 
      {
        "name": "Walter  Lasecki, University of Michigan"
      }, 
      {
        "name": "Jeffrey  Bigham, Carnegie Mellon University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "WearMail: On-the-Go Access to Information in Your Email with a Privacy-Preserving Human Computation Workflow", 
    "type": "paper"
  }, 
  "uistf1660": {
    "abstract": "Gestures have become an important tool for natural interaction with computers and thus several wearables have been developed to detect hand gestures. However, many existing solutions are unsuitable for practical use due to low accuracy, high cost or poor ergonomics. We present SensIR, a bracelet that uses near-infrared sensing to infer hand gestures. The bracelet is composed of pairs of infrared emitters and receivers that are used to measure both the transmission and reflection of light through/off the wrist. SensIR improves the accuracy of existing infrared gesture sensing systems through the key idea of taking measurements with all possible combinations of emitters and receivers. Our study shows that SensIR is capable of detecting 12 discrete gestures with 93.3% accuracy. SensIR has several advantages compared to other systems such as high accuracy, low cost, robustness against bad skin coupling and thin form-factor.", 
    "authors": [
      {
        "name": "Jess  Mcintosh, University of Bristol"
      }, 
      {
        "name": "Asier  Marzo, University of Bristol"
      }, 
      {
        "name": "Mike  Fraser, University of Bristol"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "SensIR: Detecting Hand Gestures with a Wearable Bracelet using Infrared Transmission and Reflection", 
    "type": "paper"
  }, 
  "uistf1663": {
    "abstract": "Interaction in VR involves large body movements, easily inducing fatigue and discomfort. We propose Erg-O, a manipulation technique that leverages visual dominance to maintain the visual location of the elements in VR, while making them accessible from more comfortable locations. Our solution works in an open-ended fashion (no prior knowledge of the object the user wants to touch), can be used with multiple objects, and still allows interaction with any other point within user\u00e2\u0080\u0099s reach. We use optimization approaches to compute the best physical location to interact with each visual element, and space partitioning techniques to distort the visual and physical spaces based on those mappings and allow multi-object retargeting. In this paper we describe the Erg-O technique, propose two retargeting strategies and report the results from a user study on 3D selection under different conditions, elaborating on their potential and application to specific usage scenarios.", 
    "authors": [
      {
        "name": "Roberto Antonio Montano Murillo, University of Sussex"
      }, 
      {
        "name": "Sriram  Subramanian, University of Sussex"
      }, 
      {
        "name": "Diego  Martinez Plasencia, University of Sussex"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Erg-O: Ergonomic Optimization of Immersive Virtual Environments", 
    "type": "paper"
  }, 
  "uistf1706": {
    "abstract": "We present a low cost method to measure and characterize the end-to-end latency when using a touch system (tap latency) or an input device equipped with a physical button. Our method relies on a vibration sensor attached to a finger and a photo-diode to detect the screen response. Both are connected to a micro-controller connected to a host computer using a low-latency USB communication protocol in order to combine software and hardware probes to help determine where the latency comes from. We present the operating principle of our method before investigating the main sources of latency in several systems. We show that most of the latency originates from the display side. Our method can help application designers characterize and troubleshoot latency on a wide range of interactive systems.", 
    "authors": [
      {
        "name": "G\u00e9Ry  Casiez, Universit\u00e9 de Lille"
      }, 
      {
        "name": "Thomas  Pietrzak, Univ. Lille"
      }, 
      {
        "name": "Damien  Marchal, Universit\u00e9 de Lille"
      }, 
      {
        "name": "S\u00e9Bastien  Poulmane, Inria"
      }, 
      {
        "name": "Matthieu  Falce, Mathieu Falce Consulting"
      }, 
      {
        "name": "Nicolas  Roussel, Inria"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Characterizing Latency in Touch and Button-Equipped Interactive Systems", 
    "type": "paper"
  }, 
  "uistf1723": {
    "abstract": "We present a sphere-shaped interactive display system, named Qoom, as a new input and output device. Unlike existing sphere-shaped displays, Qoom is a perfectly spherical ball that can be rotated, thrown, or even kicked.   First, we discuss how spherical displays can be used in daily life and describe how users interact with spheres.   Then, we show how we developed the Qoom prototype that uses touch and rotation detection, real-time object tracking, and spherical projection mapping.   We implemented actions including touching, rotating, bouncing and throwing as controls.   We also developed applications for Qoom that utilize the unique advantages of ball displays.", 
    "authors": [
      {
        "name": "Shio  Miyafuji, Tokyo Institute of Technology"
      }, 
      {
        "name": "Zhengqing  Li, Tokyo Institute of Technology"
      }, 
      {
        "name": "Toshiki  Sato, Tokyo Institute of Technology"
      }, 
      {
        "name": "Hideki  Koike, Tokyo Institute of Technology"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Qoom: An Interactive Omnidirectional Ball Display", 
    "type": "paper"
  }, 
  "uistf1785": {
    "abstract": "We propose an immersive telepresence system for puppetry that transmits a human performer's body and facial movements into a puppet with audiovisual feedback to the performer. The cameras carried in place of puppet's eyes stream live video to the HMD worn by the performer, so that performers can see the images from the puppet's eyes with their own eyes and have a visual understanding of the puppet's ambience. In conventional methods to manipulate a puppet (a hand-puppet, a string-puppet, and a rod-puppet), there is a need to practice manipulating puppets, and there is difficulty carrying out interactions with the audience. Moreover, puppeteers must be positioned exactly where the puppet is. The proposed system addresses these issues by enabling a human performer to manipulate the puppet remotely using his or her body and facial movements. We conducted several user studies with both beginners and professional puppeteers. The results show that, unlike the conventional method, the proposed system facilitates the manipulation of puppets especially for beginners. Moreover, this system allows performers to enjoy puppetry and fascinate audiences.", 
    "authors": [
      {
        "name": "Mose  Sakashita, University of Tsukuba"
      }, 
      {
        "name": "Tatsuya  Minagawa, University of Tsukuba"
      }, 
      {
        "name": "Amy  Koike, University of Tsukuba"
      }, 
      {
        "name": "Ippei  Suzuki, University of Tsukuba"
      }, 
      {
        "name": "Keisuke  Kawahara, University of Tsukuba"
      }, 
      {
        "name": "Yoichi  Ochiai, University of Tsukuba"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "You as a Puppet: Evaluation of Telepresence User Interface for Puppetry", 
    "type": "paper"
  }, 
  "uistf1814": {
    "abstract": "We present MagTics, a novel flexible and wearable haptic interface based on magnetically actuated bidirectional tactile pixels (taxels).  MagTics' thin form factor and flexibility allows for rich haptic feedback in mobile settings.  We propose a novel actuation mechanism based on bistable electromagnetic latching that combines high frame rate and holding force with low energy consumption and a soft and flexible form factor.  We overcome limitations of traditional soft actuators by placing several hard actuation cells, driven by flexible printed electronics, in a soft 3D printed case. A novel EM-shielding prevents magnet-magnet interactions and allows for high actuator densities.  A prototypical implementation comprising of 4 actuated pins on a 1.7 cm pitch, with 2 mm travel, and generating 160 mN to 200 mN  of latching force is used to implement a number of compelling application scenarios including adding haptic and tactile display capabilities to wearable devices, to existing input devices and to provide localized haptic feedback in virtual reality.  Finally, we report results of a psychophysical study, conducted to inform future developments and to identify possible application domains.", 
    "authors": [
      {
        "name": "Fabrizio  Pece, ETH Zurich"
      }, 
      {
        "name": "Juan Jose Zarate, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne"
      }, 
      {
        "name": "Velko  Vechev, Chalmers University of Technology"
      }, 
      {
        "name": "Nadine  Besse, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne"
      }, 
      {
        "name": "Olexandr  Gudozhnik, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne"
      }, 
      {
        "name": "Herbert  Shea, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne"
      }, 
      {
        "name": "Otmar  Hilliges, ETH Zurich"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "MagTics: Flexible and Thin Form Factor Magnetic Actuators for Dynamic and Wearable Haptic Feedback", 
    "type": "paper"
  }, 
  "uistf1910": {
    "abstract": "The small screen size of a smartwatch limits user experience when watching or interacting with media. We propose a supplementary tactile feedback system to enhance the user experience with a method unique to the smartwatch form factor. Our system has a deformable surface on the back of the watch face, allowing the visual scene on screen to extend into 2.5D physical space. This allows the user to watch and feel virtual objects, such as experiencing a ball bouncing against the wrist. We devised two controlled experiments to analyze the influence of tactile display resolution on the illusion of virtual object presence. Our first study revealed that on average, a taxel can render virtual objects between 70% and 138% of its own size without shattering the illusion. From the second study, we found visual and haptic feedback can be separated by 4.5mm to 16.2mm for the tested taxels. Based on the results, we developed a prototype (called RetroShape) with 4\u00c3\u00974 10mm taxels using micro servo motors, and demonstrated its unique capability through a set of tactile-enhanced games and videos. A preliminary user evaluation showed that participants welcome RetroShape as a useful addition to existing smartwatch output. ", 
    "authors": [
      {
        "name": "Da-Yuan  Huang, National Taiwan University of Science and Technology"
      }, 
      {
        "name": "Ruizhen  Guo, Dartmouth College"
      }, 
      {
        "name": "Jun  Gong, Dartmouth College"
      }, 
      {
        "name": "Jingxian  Wang, Carnegie Mellon University"
      }, 
      {
        "name": "John M Graham, Dartmouth College"
      }, 
      {
        "name": "De-Nian  Yang, Academia Sinica"
      }, 
      {
        "name": "Xing-Dong  Yang, Dartmouth College"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "RetroShape: Leveraging Rear-Surface Shape Displays for 2.5D Interaction on Smartwatches", 
    "type": "paper"
  }, 
  "uistf1973": {
    "abstract": "The creation of 3D contents still remains one of the most crucial problems for the emerging applications such as 3D printing and Augmented Reality. In Augmented Reality, how to create virtual contents that seamlessly overlay with the real environment is a key problem for human-computer interaction and many subsequent applications. In this paper, we present a sketch-based interactive tool, which we term emph{SweepCanvas}, for rapid exploratory 3D modeling on top of an RGB-D image. Our aim is to offer end-users a simple yet efficient way to quickly create 3D models on an image. We develop a novel sketch-based modeling interface, which takes a pair of user strokes as input and instantly generates a curved 3D surface by sweeping one stroke along the other. A key enabler of our system is an optimization procedure that extracts pairs of spatial planes from the context to position and sweep the strokes. We demonstrate the effectiveness and power of our modeling system on various RGB-D data sets and validate the use cases via a pilot study.", 
    "authors": [
      {
        "name": "Yuwei  Li, ShanghaiTech University"
      }, 
      {
        "name": "Xi  Luo, ShanghaiTech University"
      }, 
      {
        "name": "Youyi  Zheng, Zhejiang University of Technology"
      }, 
      {
        "name": "Pengfei  Xu, Shenzhen University"
      }, 
      {
        "name": "Hongbo  Fu, City University of Hong Kong"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "SweepCanvas: Sketch-based 3D Prototyping on an RGB-D Image", 
    "type": "paper"
  }, 
  "uistf2014": {
    "abstract": "We present SoundCraft, a smartwatch prototype embedded with a microphone array, that localizes angularly, in azimuth and elevation, acoustic signatures: non-vocal acoustics that are produced using our hands. Acoustic signatures are common in our daily lives, such as when snapping or rubbing our fingers, tapping on objects or even when using an auxiliary object to generate the sound. We demonstrate that we can capture and leverage the spatial location of such naturally occurring acoustics using our prototype. We describe our algorithm, which we adopt from the MUltiple SIgnal Classification (MUSIC) technique [31], that enables robust localization and classification of the acoustics when the microphones are required to be placed at close proximity. SoundCraft enables a rich set of spatial interaction techniques, including quick access to smartwatch content, rapid command invocation, in-situ sketching, and also multi-user around device interaction. Via a series of user studies, we validate SoundCraft\u00e2\u0080\u0099s localization and classification capabilities in non-noisy and noisy environments.", 
    "authors": [
      {
        "name": "Teng  Han, University of Manitoba"
      }, 
      {
        "name": "Khalad  Hasan, University of Manitoba"
      }, 
      {
        "name": "Keisuke  Nakamura, Honda Research Institute"
      }, 
      {
        "name": "Randy  Gomez, Honda Research Institute Japan"
      }, 
      {
        "name": "Pourang  Irani, University of Manitoba"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "SoundCraft: Enabling Spatial Interactions on Smartwatches using Hand Generated Acoustics", 
    "type": "paper"
  }, 
  "uistf2024": {
    "abstract": "We introduce ImAxes immersive system for exploring multivariate data using fluid, modeless interaction.  The basic interface element is an embodied data axis.  The user can manipulate these axes like physical objects in the immersive environment and combine them into sophisticated visualisations.  The type of visualisation that appears depends on the proximity and relative orientation of the axes with respect to one another, which we describe with a formal grammar.  This straight-forward composability leads to a number of emergent visualisations and interactions, which we review, and then demonstrate with a detailed multivariate data analysis use case.", 
    "authors": [
      {
        "name": "Maxime  Cordeil, Monash University"
      }, 
      {
        "name": "Andrew  Cunningham, University of South Australia"
      }, 
      {
        "name": "Tim  Dwyer, Monash University"
      }, 
      {
        "name": "Bruce H Thomas, The University of South Australia"
      }, 
      {
        "name": "Kim  Marriott, Monash University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "ImAxes: Immersive Axes as Embodied Affordances for Interactive Multivariate Data Visualisation", 
    "type": "paper"
  }, 
  "uistf2052": {
    "abstract": "Eye contact is an important non-verbal cue in social signal processing and promising as a measure of overt attention in human-object interactions and attentive user interfaces. However, robust detection of eye contact across different users, gaze targets, camera positions, and illumination conditions is notoriously challenging. We present a novel method for eye contact detection that combines a state-of-the-art appearance-based gaze estimator with a novel approach for unsupervised gaze target discovery, i.e. without the need for tedious and time-consuming manual data annotation. We evaluate our method in two real-world scenarios: detecting eye contact at the workplace, including on the main work display, from cameras mounted to target objects, as well as during everyday social interactions with the wearer of a head-mounted egocentric camera. We empirically evaluate the performance of our method in both scenarios and demonstrate its effectiveness for detecting eye contact independent of target object type and size, camera position, and user and recording environment.", 
    "authors": [
      {
        "name": "Xucong  Zhang, Saarland Informatics Campus"
      }, 
      {
        "name": "Yusuke  Sugano, Osaka University"
      }, 
      {
        "name": "Andreas  Bulling, Saarland Informatics Campus"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "Everyday Eye Contact Detection Using Unsupervised Gaze Target Discovery", 
    "type": "paper"
  }, 
  "uistf2235": {
    "abstract": "We present Pyro, a micro thumb-tip gesture recognition technique based on thermal infrared signals radiating from the fingers. Pyro uses a compact, low-power passive sensor, making it suitable for wearable and mobile applications. To demonstrate the feasibility of Pyro, we developed a self-contained prototype consisting of the infrared pyroelectric sensor, a custom sensing circuit, and software for signal processing and machine learning. A ten-participant user study yielded a 93.9% cross-validation accuracy and 84.9% leave-one-session-out accuracy on six thumb-tip gestures. Subsequent lab studies demonstrated Pyro\u00e2\u0080\u0099s robustness to varying light conditions, hand temperatures, and background motion. We conclude by discussing the insights we gained from this work and future research questions.", 
    "authors": [
      {
        "name": "Jun  Gong, Dartmouth College"
      }, 
      {
        "name": "Yang  Zhang, Carnegie Mellon University"
      }, 
      {
        "name": "Xia  Zhou, Dartmouth College"
      }, 
      {
        "name": "Xing-Dong  Yang, Dartmouth College"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Pyro: Thumb-Tip Gesture Recognition Using Pyroelectric Infrared Sensing", 
    "type": "paper"
  }, 
  "uistf2241": {
    "abstract": "SmoothMoves is an interaction technique for augmented reality (AR) based on smooth pursuits head movements. It works by computing correlations between the movements of on-screen targets and the user\u00e2\u0080\u0099s head while tracking those targets. The paper presents three studies. The first suggests that head based input can act as an easier and more affordable surrogate for eye-based input in many smooth pursuits interface designs. A follow-up study grounds the technique in the domain of augmented reality, and captures the error rates and acquisition times on different types of AR devices: head-mounted (2.6%, 1965ms) and hand-held (4.9%, 2089ms). Finally, the paper presents an interactive lighting system prototype that demonstrates the benefits of using smooth pursuits head movements in interaction with AR interfaces. A final qualitative study reports on positive feedback regarding the technique\u00e2\u0080\u0099s suitability for this scenario. Together, these results indicate show SmoothMoves is viable, efficient and immediately available for a wide range of wearable devices that feature embedded motion sensing.", 
    "authors": [
      {
        "name": "Augusto  Esteves, Edinburgh Napier University"
      }, 
      {
        "name": "David  Verweij, Eindhoven University of Technology"
      }, 
      {
        "name": "Liza Jahan Suraiya, Ulsan National Institute of Science and Technology"
      }, 
      {
        "name": "Md. Rasel  Islam, Ulsan National Institute of Science and Technology"
      }, 
      {
        "name": "Youryang  Lee, UNIST"
      }, 
      {
        "name": "Ian  Oakley, Ulsan National Institute of Science and Technology"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "SmoothMoves: Smooth Pursuits Head Movements for Augmented Reality", 
    "type": "paper"
  }, 
  "uistf2291": {
    "abstract": "Two recent innovations in immersive media include the ability to capture very high resolution panoramic imagery, and the rise of consumer level heads-up displays for virtual reality. Unfortunately, zooming to examine the high resolution in VR breaks the basic contract with the user, that the FOV of the visual field matches the FOV of the imagery. In this paper, we study methods to overcome this restriction to allow high resolution panoramic imagery to be able to be explored in VR.    We introduce and test new interface modalities for exploring high resolution panoramic imagery in VR. In particular, we demonstrate that limiting the visual FOV of the zoomed in imagery to the central portion of the visual field, and modulating the transparency or zoom level of the imagery during rapid panning, reduce simulator sickness and help with targeting tasks. ", 
    "authors": [
      {
        "name": "Huiwen  Chang, Princeton University"
      }, 
      {
        "name": "Michael  Cohen, Facebook Research"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Panning and Zooming High-Resolution Panoramas in Virtual Reality Devices", 
    "type": "paper"
  }, 
  "uistf2357": {
    "abstract": "Analog circuit design is a complex, error-prone task in which the processes of gathering observations, formulating reasonable hypotheses, and manually adjusting the circuit raise significant barriers to an iterative workflow. We present Scanalog, a tool built on programmable analog hardware that enables users to rapidly explore different circuit designs using direct manipulation, and receive immediate feedback on the resulting behaviors without manual assembly, calculation, or probing. Users can interactively tune modular signal transformations on hardware with real inputs, while observing real-time changes at all points in the circuit. They can create custom unit tests and assertions to detect potential issues. We describe three interactive applications demonstrating the expressive potential of Scanalog. In an informal evaluation, users successfully conditioned analog sensors and described Scanalog as both enjoyable and easy to use. ", 
    "authors": [
      {
        "name": "Evan N Strasnick, Stanford University"
      }, 
      {
        "name": "Maneesh  Agrawala, Stanford University"
      }, 
      {
        "name": "Sean  Follmer, Stanford University"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "Scanalog: Interactive Design and Debugging of Analog Circuits with Programmable Hardware", 
    "type": "paper"
  }, 
  "uistf2440": {
    "abstract": "HCI researchers lack low latency and robust systems to support the design and development of interaction techniques using finger identification. We developed a low cost prototype using piezo based vibration sensors attached to each finger. By combining the events from an input device with the information from the vibration sensors we demonstrate how to achieve low latency and robust finger identification. Our prototype was evaluated in a controlled experiment, using two keyboards and a touchpad, showing recognition rates of 98.2% for the keyboard and, for the touchpad, 99.7% for single touch and 94.7% for two simultaneous touches. These results were confirmed in an additional laboratory style experiment with ecologically valid tasks. Last we present new interactions techniques made possible using this technology.", 
    "authors": [
      {
        "name": "Damien  Masson, Universit\u00e9 de Lille"
      }, 
      {
        "name": "Alix  Goguey, University of Saskatchewan"
      }, 
      {
        "name": "Sylvain  Malacria, Inria"
      }, 
      {
        "name": "G\u00e9Ry  Casiez, Universit\u00e9 de Lille"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "WhichFingers: Identifying Fingers on Touch Surfaces and Keyboards Using Vibration Sensors", 
    "type": "paper"
  }, 
  "uistf2447": {
    "abstract": "This paper enables data storage and interaction with smart fabric, without the need for onboard electronics or batteries. To do this, we present the first smart fabric design that harnesses the ferromagnetic properties of conductive thread. Specifically, we manipulate the polarity of magnetized fabric and encode different forms of data including 2D images and bit strings. These bits can be read by swiping a commodity smartphone across the fabric, using its inbuilt magnetometer. Our results show that magnetized fabric retains its data even after washing, drying and ironing. Using a glove made of magnetized fabric, we can also perform six gestures in front of a smartphone, with a classification accuracy of 90.1%. Finally, using magnetized thread, we create fashion accessories like necklaces, ties, wristbands and belts with data storage capabilities as well as enable authentication applications.  ", 
    "authors": [
      {
        "name": "Justin  Chan, University of Washington"
      }, 
      {
        "name": "Shyamnath  Gollakota, University of Washington"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Data Storage and Interaction using Magnetized Fabric", 
    "type": "paper"
  }, 
  "uistf2588": {
    "abstract": "We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.", 
    "authors": [
      {
        "name": "Andreas  Fender, Aarhus University"
      }, 
      {
        "name": "David  Lindlbauer, TU Berlin"
      }, 
      {
        "name": "Philipp  Herholz, TU Berlin"
      }, 
      {
        "name": "Marc  Alexa, TU Berlin"
      }, 
      {
        "name": "Joerg  Mueller, Aarhus University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior", 
    "type": "paper"
  }, 
  "uistf2681": {
    "abstract": "Smart rings have a unique form factor suitable for many applications, however, they offer little opportunity to provide the user with natural output. We propose passive kinesthetic force feedback as a novel output method for rotational input on smart rings. With this new output channel, friction force profiles can be designed, programmed, and felt by a user when they rotate the ring. This modality enables new interactions for ring form factors. We demonstrate the potential of this new haptic output method though Frictio, a prototype smart ring. In a controlled experiment, we determined the recognizability of six force profiles, including Hard Stop, Ramp-Up, Ramp-Down, Resistant Force, Bump, and No Force. The results showed that participants could distinguish between the force profiles with 94% accuracy. We conclude by presenting a set of novel interaction techniques that Frictio enables, and discuss insights and directions for future research.", 
    "authors": [
      {
        "name": "Teng  Han, University of Manitoba"
      }, 
      {
        "name": "Qian  Han, Department of Computer Science"
      }, 
      {
        "name": "Michelle  Annett"
      }, 
      {
        "name": "Fraser  Anderson, Autodesk Research"
      }, 
      {
        "name": "Da-Yuan  Huang, Dartmouth College"
      }, 
      {
        "name": "Xing-Dong  Yang, Dartmouth College"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Frictio: Passive Kinesthetic Force Feedback for Smart Ring Output", 
    "type": "paper"
  }, 
  "uistf2786": {
    "abstract": "Standard vehicle infotainment systems often include touch screens that allow the driver to control their mobile phone, navigation, audio, and vehicle configurations. For the driver\u00e2\u0080\u0099s safety, these interfaces are often disabled or simplified while the car is in motion. Although this reduced functionality aids in reducing distraction for the driver, it also disrupts the usability of infotainment systems for passengers. Current infotainment systems are unaware of the seating position of their user and hence, cannot adapt. We present Carpacio, a system that takes advantage of the capacitive coupling created between the touchscreen and the electrode present in the seat when the user touches the capacitive screen. Using this capacitive coupling phenomenon, a car infotainment system can intelligently distinguish who is interacting with the screen seamlessly, and adjust its user interface accordingly. Manufacturers can easily incorporate Carpacio into vehicles since the included seat occupancy detection sensor or seat heating coils can be used as the seat electrode. We evaluated Carpacio in eight different cars and five mobile devices and found that it correctly detected over 2600 touches with an accuracy of 99.4%.", 
    "authors": [
      {
        "name": "Edward Jay Wang, University of Washington"
      }, 
      {
        "name": "Jake  Garrison, University of Washington"
      }, 
      {
        "name": "Eric  Whitmire, University of Washington"
      }, 
      {
        "name": "Mayank  Goel, Carnegie Mellon University"
      }, 
      {
        "name": "Shwetak N Patel, University of Washington"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Carpacio: Repurposing Capacitive Sensors to Distinguish Driver and Passenger Touches on In-Vehicle Screens", 
    "type": "paper"
  }, 
  "uistf2860": {
    "abstract": "This study proposes BlowFab, a prototyping method used to create a 2.5-dimensional prototype in a short time by combining laser cutting and blow molding techniques. The user creates adhesive areas and inflatable areas by engraving and cutting multilayered plastic sheets using a laser cutter. These adhesive areas are fused automatically by overlapping two crafted sheets and softening them with a heater. The user can then create hard prototypes by injecting air into the sheets.    Objects can be bent in any direction by cutting incisions or engraving a resistant resin. The user can create uneven textures by engraving a pattern with a heat-resistant film. These techniques can be used for prototyping various strong inflatable objects. The finished prototype is strong and can be collapsed readily for storage when not required.     In this study, the design process is described using the proposed method.   The study also evaluates possible bending mechanisms and texture expression methods along with various usage scenarios and discusses the resolution, strength, and reusability of the prototype developed.", 
    "authors": [
      {
        "name": "Junichi  Yamaoka, Keio University"
      }, 
      {
        "name": "Ryuma  Niiyama, The University of Tokyo"
      }, 
      {
        "name": "Yasuaki  Kakehi, Keio University"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "BlowFab: Rapid Prototyping for Rigid and Reusable Objects using Inflation of Laser-cut Surfaces", 
    "type": "paper"
  }, 
  "uistf2895": {
    "abstract": "Touch sensing with multiple electrodes allows expressive touch interactions.  The adaptability and flexibility of the sensor are important in efficiently prototyping touch based systems. The proposed technique uses capacitive touch sensing and simplifies the connections as the electrodes are connected in series via capacitors and the interface circuit is connected to the electrode array by just two wires. The touched electrode is recognized by measuring the capacitance changes while switching the polarity of the signal. We show that the technique is capable of detecting different touches through simulations and actual measurements. User tests show that ten electrodes are successfully recognized after user calibration. They also show the proposal's other novel capabilities of multi-touch (2-touch) and `capacitor-free' design. Various forms of electrodes and applications are examined to elucidate the application range.", 
    "authors": [
      {
        "name": "Hiroyuki  Manabe, NTT DOCOMO"
      }, 
      {
        "name": "Wataru  Yamada, NTT DOCOMO"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "A Capacitive Touch Sensing Technique with Series-connected Sensing Electrodes", 
    "type": "paper"
  }, 
  "uistf2925": {
    "abstract": "Pointing is a fundamental interaction technique where user movement is translated to spatial input on a display. Conventionally, this is based on a rigid configuration of a display coupled with a pointing device that determines the types of movement that can be sensed, and the specific ways users can affect pointer input. Spontaneous spatial coupling is a novel input technique that instead allows any body movement, or movement of tangible objects, to be appropriated for touchless pointing on an ad hoc basis. Pointer acquisition is facilitated by the display presenting graphical objects in motion, to which users can synchronise to define a temporary spatial coupling with the body part or tangible object they used in the process. The technique can be deployed using minimal hardware, as demonstrated by MatchPoint, a generic computer vision-based implementation of the technique that requires only a webcam. We explore the design space of spontaneous spatial coupling, demonstrate the versatility of the technique with application examples, and evaluate MatchPoint performance using a multi-directional pointing task.", 
    "authors": [
      {
        "name": "Christopher  Clarke, Lancaster University"
      }, 
      {
        "name": "Hans  Gellersen, Lancaster University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "MatchPoint: Spontaneous Spatial Coupling of Body Movement for Touchless Pointing", 
    "type": "paper"
  }, 
  "uistf2935": {
    "abstract": "We present Dwell+, a method that boosts the effectiveness of typical dwell selection by augmenting the passive dwell duration with active haptic ticks which promptly drives rapid switches of modes forward through the user's skin sensations. In this way, Dwell+ enables multi-level dwell selection using rapid haptic ticks. To select a mode from a button, users dwell-touch the button until the mode of selection is haptically prompted.    Our haptic stimulation design consists of a short 10ms vibrotacile feedback that indicates a mode arriving and a break that separates consecutive modes. We first tested the effectiveness of 170ms, 150ms, 130ms, and 110ms intervals between modes for a 10-level selection. The results reveal that 3-beats-per-chunk rhythm design, e.g., displaying longer 25ms vibrations initially for all three modes, could potentially achieve higher accuracy. The second study reveals significant improvement wherein a 94.5% accuracy was achieved for a 10-level Dwell+ selection using the 170ms interval with 3-beats-per-chunk design, and a 93.82% rate of accuracy using the more frequent 150ms interval with similar chunks for 5-level selection. The performance of conducting touch and receiving vibration from disparate hands was investigated for our final study to provide a wider range of usage. Our applications demonstrated implementing Dwell+ across interfaces, such as text input on a smartwatch, enhancing touch space for HMDs, boosting modalities of stylus-based tool selection, and extending the input vocabulary of physical interfaces.", 
    "authors": [
      {
        "name": "Yi-Chi  Liao, National Taiwan University"
      }, 
      {
        "name": "Yen-Chiu  Chen, National Taiwan University"
      }, 
      {
        "name": "Liwei  Chan, National Chiao Tung University"
      }, 
      {
        "name": "Bing-Yu  Chen, National Taiwan University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Dwell+: Multi-Level Mode Selection Using Vibrotactile Cues", 
    "type": "paper"
  }, 
  "uistf3019": {
    "abstract": "Tutorials are vital for helping people perform complex software-based tasks in domains such as programming, data science, system administration, and computational research. However, it is tedious to create detailed step-by-step tutorials for tasks that span multiple interrelated GUI and command-line applications. To address this challenge, we created Torta, an end-to-end system that automatically generates step-by-step GUI and command-line app tutorials by demonstration, provides an editor to trim, organize, and add validation criteria to these tutorials, and provides a web-based viewer that can validate step-level progress and automatically run certain steps. The core technical insight that underpins Torta is that combining operating-system-wide activity tracing and screencast recording makes it easier to generate mixed-media (text+video) tutorials that span multiple GUI and command-line apps. An exploratory study on 10 computer science teaching assistants (TAs) found that they all preferred the experience and results of using Torta to record programming and sysadmin tutorials relevant to classes they teach rather than manually writing tutorials. A follow-up study on 6 students found that they all preferred following the Torta tutorials created by those TAs over the manually-written versions.", 
    "authors": [
      {
        "name": "Alok  Mysore, UC San Diego"
      }, 
      {
        "name": "Philip J Guo, UC San Diego"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Torta: Generating Mixed-Media GUI and Command-Line App Tutorials Using Operating-System-Wide Activity Tracing", 
    "type": "paper"
  }, 
  "uistf3196": {
    "abstract": "We propose a novel interactive system to simplify the process of indoor 3D CAD room modeling. Traditional room modeling methods require users to measure room and furniture dimensions, and manually select models that match the scene from large catalogs. Users then employ a mouse and keyboard interface to construct walls and place the objects in their appropriate locations. In contrast, our system leverages the sensing capabilities of a 3D aware mobile device, recent advances in object recognition, and a novel augmented reality user interface, to capture indoor 3D room models in-situ. With a few taps, a user can mark the surface of an object, take a photo, and the system retrieves and places a matching 3D model into the scene, from a large online database. User studies indicate that this modality is significantly quicker, more accurate, and requires less effort than traditional desktop tools.", 
    "authors": [
      {
        "name": "Aditya  Sankar, University of Washington"
      }, 
      {
        "name": "Steve  Seitz, University of Washington"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Interactive Room Capture on 3D-Aware Mobile Devices", 
    "type": "paper"
  }, 
  "uistf3285": {
    "abstract": "While gaze holds a lot of promise for hands-free interaction with public displays, remote eye trackers with their confined tracking box restrict users to a single stationary position in front of the display. We present EyeScout, an active eye tracking system that combines an eye tracker mounted on a rail system with a computational method to automatically detect and align the tracker with the user\u00e2\u0080\u0099s lateral movement. EyeScout addresses key limitations of current gaze-enabled large public displays by offering two novel gaze-interaction modes for a single user: In \"Walk then Interact\" the user can walk up to an arbitrary position in front of the display and interact, while in \"Walk and Interact\" the user can interact even while on the move. We report on a user study that shows that EyeScout is well perceived by users, extends a public display's sweet spot into a sweet line, and reduces gaze interaction kick-off time to 3.5 seconds \u00e2\u0080\u0093 a 62% improvement over state of the art solutions. We discuss sample applications that demonstrate how EyeScout can enable position and movement-independent gaze interaction with large public displays.", 
    "authors": [
      {
        "name": "Mohamed  Khamis, LMU Munich"
      }, 
      {
        "name": "Axel  Hoesl, LMU Munich"
      }, 
      {
        "name": "Alexander  Klimczak, LMU Munich"
      }, 
      {
        "name": "Martin  Reiss, LMU Munich"
      }, 
      {
        "name": "Florian  Alt, LMU Munich"
      }, 
      {
        "name": "Andreas  Bulling, Saarland Informatics Campus"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "EyeScout: Active Eye Tracking for Position and Movement Independent Gaze Interaction with Large Public Displays", 
    "type": "paper"
  }, 
  "uistf3335": {
    "abstract": "We present iSphere, a flying spherical display that can display high resolution and bright images in all directions from anywhere in 3D space. Our goal is to build a new platform which can physically and directly emerge arbitrary bodies in the real world. iSphere flies by itself using a built-in drone and creates a spherical display by rotating arcuate multi light-emitting diode (LED) tapes around the drone. As a result of the persistence of human vision, we see it as a spherical display flying in the sky.  The proposed method yields large display surfaces, high resolution, drone mobility, high visibility and 360\u00c2\u00b0 field of view. Previous approaches fail to match these characteristics, because of problems with aerodynamics and payload. We construct a prototype and validate the proposed method. The unique characteristics and benefits of flying spherical display surfaces are discussed and we describe application scenarios based on iSphere such as guidance, signage and telepresence.", 
    "authors": [
      {
        "name": "Wataru  Yamada, NTT DOCOMO"
      }, 
      {
        "name": "Kazuhiro  Yamada, NTT DOCOMO"
      }, 
      {
        "name": "Hiroyuki  Manabe, NTT DOCOMO"
      }, 
      {
        "name": "Daizo  Ikeda, NTT DOCOMO"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "iSphere: Self-Luminous Spherical Drone Display", 
    "type": "paper"
  }, 
  "uistf3396": {
    "abstract": "Visualizations of run-time program state help novices form proper mental models and debug their code. We push this technique to the extreme by posing the following question: What if a live programming environment for an imperative language always displays the entire history of all run-time values for all program variables all the time? To explore this question, we built a prototype live IDE called Omnicode (\"Omniscient Code\") that continually runs the user's Python code and uses a scatterplot matrix to visualize the entire history of all of its numerical values, along with meaningful numbers derived from other data types. To filter the visualizations and hone in on specific points of interest, the user can brush and link over the scatterplots or select portions of code. They can also zoom in to view detailed stack and heap visualizations at each execution step. An exploratory study on 10 novice programmers discovered that they found Omnicode to be useful for debugging, forming mental models, explaining their code to others, and discovering moments of serendipity that would not have been likely within an ordinary IDE.", 
    "authors": [
      {
        "name": "Hyeonsu  Kang, UC San Diego"
      }, 
      {
        "name": "Philip J Guo, UC San Diego"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Omnicode: A Novice-Oriented Live Programming Environment with Always-On Run-Time Value Visualizations", 
    "type": "paper"
  }, 
  "uistf3412": {
    "abstract": "We motivate, design, and prototype a modular smartphone designed to make temporary device lending trustworthy and convenient. The concept is that the phone can be separated into pieces, so a child, friend, or even stranger can begin an access-controlled interaction with one piece, while the own-er retains another piece to continue their tasks and monitor activity. This is grounded in a survey capturing attitudes towards device lending, and an exploratory study probing how people might lend pieces of different kinds of modular smartphones. Design considerations are generated for a hardware form factor and software interface to support different lending scenarios. A functional prototype combining three smartphones into a single modular device is described and used to demonstrate a lending interaction design. A usability test validates the concept using the prototype.", 
    "authors": [
      {
        "name": "Teddy  Seyed, University of Calgary"
      }, 
      {
        "name": "Xing-Dong  Yang, Dartmouth College"
      }, 
      {
        "name": "Daniel  Vogel, University of Waterloo"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "A Modular Smartphone for Lending", 
    "type": "paper"
  }, 
  "uistf3450": {
    "abstract": "The rise of Maker communities and open-source electronic prototyping platforms have made electronic circuit projects increasingly popular around the world.   Although there are software tools that support the debugging and sharing of circuits, they require users to manually create the virtual circuits in software, which can be time-consuming and error-prone.  We present CircuitSense, a system that automatically recognizes the wires and electronic components placed on breadboards. It uses a combination of passive sensing and active probing to detect and generate the corresponding circuit representation in software in real-time. CircuitSense bridges the gap between the physical and virtual representations of circuits. It enables users to interactively construct and experiment with physical circuits while gaining the benefits of using software tools. It also dramatically simplifies the sharing of circuit designs with online communities.", 
    "authors": [
      {
        "name": "Te-Yen  Wu, National Taiwan University"
      }, 
      {
        "name": "Bryan  Wang, National Taiwan University"
      }, 
      {
        "name": "Jiun-Yu  Lee, National Taiwan University"
      }, 
      {
        "name": "Hao-Ping  Shen, National Taipei University"
      }, 
      {
        "name": "Yu-Chian  Wu, National Taiwan University"
      }, 
      {
        "name": "Yu-An  Chen, National Taiwan University"
      }, 
      {
        "name": "Pin-Sung  Ku, National Taiwan University"
      }, 
      {
        "name": "Ming-Wei  Hsu, National Taiwan University"
      }, 
      {
        "name": "Yu-Chih  Lin, National Taiwan University"
      }, 
      {
        "name": "Mike Y. Chen, National Taiwan University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "CircuitSense: Automatic Sensing of Physical Circuits and Generation of Virtual Circuits to Support Software Tools.", 
    "type": "paper"
  }, 
  "uistf3497": {
    "abstract": "We present AirCode, a technique that allows the user to tag physically fabricated objects with given information. An AirCode tag consists of a group of carefully designed air pockets placed beneath the object surface. These air pockets are easily produced during the fabrication process of the object, without any additional material or postprocessing. Meanwhile, the air pockets affect only the scattering light transport under the surface, and thus are hard to notice to our naked eyes. But, by using a computational imaging method, the tags become detectable. We present a tool that automates the design of air pockets for the user to encode information. AirCode system also allows the user to retrieve the information from captured images via a robust decoding algorithm. We demonstrate our tagging technique with applications for metadata embedding, robotic grasping, as well as conveying object affordances.  ", 
    "authors": [
      {
        "name": "Dingzeyu  Li, Columbia University"
      }, 
      {
        "name": "Avinash S Nair, Columbia University"
      }, 
      {
        "name": "Shree K Nayar, Columbia University"
      }, 
      {
        "name": "Changxi  Zheng, Columbia University"
      }
    ], 
    "award": true, 
    "hm": false, 
    "subtype": "paper", 
    "title": "AirCode: Unobtrusive Physical Tags for Digital Fabrication", 
    "type": "paper"
  }, 
  "uistf3506": {
    "abstract": "Virtual reality filmmakers creating 360-degree video currently rely on  cinematography techniques that were developed for traditional narrow  field of view film. They typically edit together a sequence of shots  so that they appear at a fixed-orientation irrespective of the viewer's  field of view. But because viewers set their own camera orientation  they may miss important story content while looking in the wrong  direction.  We present new interactive shot orientation techniques that are  designed to help viewers see all of the important content in  360-degree video stories.  Our viewpoint-oriented technique  reorients the shot at each cut so that the most  important content lies in the the viewer's current field of view. Our  active reorientation technique, lets the viewer press a button to  immediately reorient the shot so that important content lies in their  field of view.  We present a 360-degree video player which implements these techniques  and conduct a user study which finds that users spend  5.2-9.5% more time viewing the important points (manually labelled) of the scene with our techniques  compared to the traditional fixed-orientation cuts.   In practice, 360-degree video creators may label important content, but we also provide an automatic method for determining important content in existing 360-degree videos.", 
    "authors": [
      {
        "name": "Amy  Pavel, Berkeley"
      }, 
      {
        "name": "Bjoern  Hartmann, University of California, Berkeley"
      }, 
      {
        "name": "Maneesh  Agrawala, Stanford University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Shot Orientation Controls for Interactive Cinematography with 360 Video", 
    "type": "paper"
  }, 
  "uistf3567": {
    "abstract": "The dramatic decrease in price and increase in availability of hobbyist electronics has led to a wide array of embedded and interactive devices. While electronics have become more widespread, developing and prototyping the required circuitry for these devices is still difficult, requiring knowledge of electronics, components, and programming. In this paper, we present Trigger-Action-Circuits (TAC), an interactive system that leverages generative design to produce circuitry, firmware, and assembly instructions, based on high-level, behavioural descriptions. TAC is able to generate multiple candidate circuits from a behavioural description, giving the user a number of alternative circuits that may be best suited to their use case (e.g., based on cost, component availability or ease of assembly). The generated circuitry uses off-the-shelf, commodity electronics, not specialized hardware components, enabling scalability and extensibility. TAC supports a range of common components and behaviors that are frequently required for prototyping electronic circuits. A user study demonstrated that TAC helps users avoid problems encountered during circuit design and assembly, with users completing their circuits significantly faster than with traditional methods.", 
    "authors": [
      {
        "name": "Fraser  Anderson, Autodesk Research"
      }, 
      {
        "name": "Tovi  Grossman, Autodesk Research"
      }, 
      {
        "name": "George  Fitzmaurice, Autodesk Research"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Trigger-Action-Circuits: Leveraging Generative Design to Enable Novices to Design and Build Circuitry", 
    "type": "paper"
  }, 
  "uistf3584": {
    "abstract": "Most of our daily activities take place in the physical world, which inherently imposes physical constraints. In contrast, the digital world is very flexible, but usually isolated from its physical counterpart. To combine these two realms, many Mixed Reality (MR) techniques have been explored, at different levels in the continuum. In this work we present an integrated Mixed Reality ecosystem that allows users to incrementally transition from pure physical to pure virtual experiences in a unique reality. This system stands on a conceptual framework composed of 6 levels. This paper presents these levels as well as the related interaction techniques.", 
    "authors": [
      {
        "name": "Joan Sol  Roo, INRIA Bordeaux Sud-Ouest"
      }, 
      {
        "name": "Martin  Hachet, Inria"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "One Reality: Augmenting How the Physical World is Experienced by combining Multiple Mixed Reality Modalities", 
    "type": "paper"
  }, 
  "uistf3598": {
    "abstract": "CommandBoard offers a simple, efficient and incrementally learnable technique for issuing gesture commands from a soft keyboard. We transform the area above the keyboard into a command-gesture input space that lets users draw unique command gestures or type command names followed by execute. Novices who pause see an in-context dynamic guide, whereas experts simply draw. Our studies show that CommandBoard\u00e2\u0080\u0099s inline gesture shortcuts are significantly faster (almost double) than markdown symbols and significantly preferred by users. We demonstrate additional techniques for more complex commands, and discuss trade-offs with respect to the user\u00e2\u0080\u0099s knowledge and motor skills, as well as the size and structure of the command space.", 
    "authors": [
      {
        "name": "Jessalyn  Alvina, Inria, Universit\u00e9 Paris-Saclay"
      }, 
      {
        "name": "Carla Florencia Griggio, Univ. Paris-Sud, CNRS, Inria, Universit\u00e9 Paris-Saclay"
      }, 
      {
        "name": "Xiaojun  Bi, Stony Brook University"
      }, 
      {
        "name": "Wendy E. Mackay, Univ. Paris-Sud, CNRS, Inria, Universit\u00e9 Paris-Saclay"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "CommandBoard: Creating a General-Purpose Command Gesture Input Space for Soft Keyboard", 
    "type": "paper"
  }, 
  "uistf3635": {
    "abstract": "Facial expressions transmit a variety of social, grammatical, and affective signals. For technology to leverage this rich source of communication, tools that better model the breadth of information they convey are required. MiFace is a novel framework for creating expression lexicons that map signal values to parameterized facial muscle movements. In traditional mapping paradigms using posed photographs, na\u00c3\u00afve judges select from predetermined label sets and movements are inferred by trained experts. The set of generally accepted expressions established in this way is limited to six basic displays of affect. In contrast, our approach generatively simulates muscle movements on a 3D avatar. By applying natural language processing techniques to crowdsourced free-response labels for the resulting images, we efficiently converge on an expression\u00e2\u0080\u0099s value across signal categories. Two studies returned 218 discriminable facial expressions with 51 unique labels. The six basic emotions are included, but we additionally define such nuanced expressions as embarrassed, curious, and hopeful.  ", 
    "authors": [
      {
        "name": "Crystal  Butler, New York University"
      }, 
      {
        "name": "Stephanie Ann Michalowicz, New York University"
      }, 
      {
        "name": "Lakshmi  Subramanian, New York University"
      }, 
      {
        "name": "Winslow  Burleson, New York University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "More than a Feeling: The MiFace Framework for Defining Facial Communication Mappings", 
    "type": "paper"
  }, 
  "uistf3763": {
    "abstract": "When bringing animated characters to life, artists often augment the primary motion of a figure by adding secondary animation -- subtle movement of parts like hair, foliage or cloth that complements and emphasizes the primary motion. Traditionally, artists add secondary motion to animated illustrations only through arduous manual effort, and often eschew it entirely. Emerging ``live'' performance applications allow both novices and experts to perform the primary motion of a character, but only a virtuoso performer could manage the degrees of freedom needed to specify both primary and secondary motion together. This paper introduces physically-inspired rigs that propagate the primary motion of layered, illustrated characters to produce plausible secondary motion. These composable elements are rigged and controlled via a small number of parameters to produce an expressive range of effects. Our approach supports a variety of the most common secondary effects, which we demonstrate with an assortment of characters of varying complexity.", 
    "authors": [
      {
        "name": "Nora S Willett, Princeton University"
      }, 
      {
        "name": "Wilmot  Li, Adobe Research"
      }, 
      {
        "name": "Jovan  Popovic, Adobe Research"
      }, 
      {
        "name": "Floraine  Berthouzoz, Adobe Research"
      }, 
      {
        "name": "Adam  Finkelstein, Princeton Univ."
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Secondary Motion for Performed 2D Animation", 
    "type": "paper"
  }, 
  "uistf3771": {
    "abstract": "We introduce Codestrates, a literate computing approach to developing interactive software. Codestrates blurs the distinction between the use and development of applications. It builds on the literate computing approach, commonly found in interactive notebooks such as Jupyter notebook. Literate computing weaves together prose and live computation in the same document. However, literate computing in interactive notebooks are limited to computation and it is challenging to extend their user interface, reprogram their functionality, or develop stand-alone applications. Codestrates builds literate computing capabilities on top of Webstrates and demonstrates how it can be used for (i) collaborative interactive notebooks, (ii) extending its functionality from within itself, and (iii) developing reprogrammable applications.", 
    "authors": [
      {
        "name": "Roman  R\u00e4Dle, Aarhus University"
      }, 
      {
        "name": "Midas  Nouwens, Aarhus University"
      }, 
      {
        "name": "Kristian B Antonsen, Aarhus University"
      }, 
      {
        "name": "James R Eagan, Telecom ParisTech"
      }, 
      {
        "name": "Clemens N Klokmose, Aarhus University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Codestrates: Literate Computing with Webstrates", 
    "type": "paper"
  }, 
  "uistf3775": {
    "abstract": "We present StrutModeling, a computationally enhanced construction kit that enables users without a 3D modeling background to prototype 3D models by assembling struts and hub primitives in physical space. Physical 3D models are immediately captured in software and result in readily available models for 3D printing. Given the concrete physical format of StrutModels, modeled objects can be tested and fine tuned in the presence of existing objects and specific needs of users. StrutModeling avoids puzzling with pieces by contributing an adjustable strut and universal hub design. Struts can be adjusted in length and snap to magnetic hubs in any configuration. As such, arbitrarily complex models can be modeled, tested, and adjusted during the design phase. In addition, the embedded sensing capabilities allow struts to be used as measuring devices for lengths and angles, and tune physical mesh models according to existing physical objects.", 
    "authors": [
      {
        "name": "Danny  Leen, KULeuven"
      }, 
      {
        "name": "Raf  Ramakers, UHasselt-tUL-imec"
      }, 
      {
        "name": "Kris  Luyten, UHasselt-tUL-imec"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "StrutModeling: A Low-Fidelity Construction Kit to Iteratively Model, Test, and Adapt 3D Objects", 
    "type": "paper"
  }, 
  "uistf3835": {
    "abstract": "The predominant interaction paradigm of current audio spatialization tools, which are primarily geared towards expert users, imposes a design process in which users are characterized as stationary, limiting the application domain of these tools. Navigable 3D sonic virtual realities, on the other hand, can support many applications ranging from soundscape prototyping to spatial data representation. Although modern game engines provide a limited set of audio features to create such sonic environments, the interaction methods are inherited from the graphical design features of such systems, and are not specific to the auditory modality. To address such limitations, we introduce INVISO, a novel web-based user interface for designing and experiencing rich and dynamic sonic virtual realities. Our interface enables both novice and expert users to construct complex immersive sonic environments with 3D dynamic sound components. INVISO is platform-independent and facilitates a variety of mixed reality applications, such as those where users can simultaneously experience and manipulate a virtual sonic environment. In this paper, we detail the interface design considerations for our audio-specific VR tool. To evaluate the usability of INVISO, we conduct two user studies: The first demonstrates that our visual interface effectively facilitates the generation of creative audio environments; the second demonstrates that both expert and non-expert users are able to use our software to accurately recreate complex 3D audio scenes. ", 
    "authors": [
      {
        "name": "Anil  Camci, University of Michigan"
      }, 
      {
        "name": "Kristine  Lee, University of Illinois at Chicago"
      }, 
      {
        "name": "Cody J Roberts, University of Illinois at Chicago"
      }, 
      {
        "name": "Angus G Forbes, University of Illinois at Chicago"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "INVISO: A Cross-platform User Interface for Creating Virtual Sonic Environments", 
    "type": "paper"
  }, 
  "uistf3858": {
    "abstract": "Virtual Reality allow users to explore virtual environments naturally, by moving their head and body. However, the size of the environments they can explore is limited by real world constraints, such as the tracking technology or the physical space available. Existing techniques removing these limitations often break the metaphor of natural navigation in VR (e.g. steering techniques), involve control commands (e.g., teleporting) or hinder precise navigation (e.g., scaling user\u00e2\u0080\u0099s displacements). This paper proposes NaviFields, which quantify the requirements for precise navigation of each point of the environment, allowing natural navigation within relevant areas, while scaling users\u00e2\u0080\u0099 displacements when travelling across non-relevant spaces. This expands the size of the navigable space, retains the natural navigation metaphor and still allows for areas with precise control of the virtual head. We present a formal description of our NaviFields technique, which we compared against two alternative solutions (i.e., homogeneous scaling and natural navigation). Our results demonstrate our ability to cover larger spaces, introduce minimal disruption when travelling across bigger distances and improve very significantly the precise control of the viewpoint inside relevant areas.", 
    "authors": [
      {
        "name": "Roberto Antonio Montano Murillo, University of Sussex"
      }, 
      {
        "name": "Elia  Gatti, University of Sussex"
      }, 
      {
        "name": "Miguel  Oliver Segovia, Universidad de Castilla de la Mancha"
      }, 
      {
        "name": "Marianna  Obrist, University of Sussex"
      }, 
      {
        "name": "Jose Pascual Molina Masso, Universidad de Castilla-La Mancha"
      }, 
      {
        "name": "Diego  Martinez Plasencia, Sussex University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "NaviFields: Relevance fields for adaptive VR navigation", 
    "type": "paper"
  }, 
  "uistf3948": {
    "abstract": "Electric current and voltage are fundamental to learning, understanding, and debugging circuits. Although both can be measured using tools such as multimeters and oscilloscopes, electric current is much more difficult to measure because users have to unplug parts of a circuit and then insert the measuring tools in serial. Furthermore, users need to restore the circuits back to its original state after measurements have been taken. In practice, this cumbersome process poses a formidable barrier to knowing how current flows throughout a circuit. We present CurrentViz, a system that can sense and visualize the electric current flowing through a circuit, which helps users quickly understand otherwise invisible circuit behavior. It supports fully automatic, ubiquitous, and real-time collection of amperage information of breadboarded circuits. It also supports visualization of the amperage data on a circuit schematic to provide an intuitive view into the current state of a circuit.  ", 
    "authors": [
      {
        "name": "Te-Yen  Wu, National Taiwan University"
      }, 
      {
        "name": "Hao-Ping  Shen, National Taipei University"
      }, 
      {
        "name": "Yu-Chian  Wu, National Taiwan University"
      }, 
      {
        "name": "Yu-An  Chen, National Taiwan University"
      }, 
      {
        "name": "Pin-Sung  Ku, National Taiwan University"
      }, 
      {
        "name": "Ming-Wei  Hsu, National Taiwan University"
      }, 
      {
        "name": "Jun-You  Liu, National Taiwan University"
      }, 
      {
        "name": "Yu-Chih  Lin, National Taiwan University"
      }, 
      {
        "name": "Mike Y. Chen, National Taiwan University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "CurrentViz: Sensing and Visualizing Electric Current of Breadboarded Circuits", 
    "type": "paper"
  }, 
  "uistf3971": {
    "abstract": "Mobile app designers often look at examples during the early stages of the design process. In this work, we present an approach for low-overhead collection of performance data for such mobile app designs examples found in the wild. It requires no development overhead since designers can directly work with the app they are interested in instead of building prototypes based on it. It uses anonymous, unsupervised, crowdworkers to use apps while collecting detailed data about the user interactions and the app UIs in the background. Performance measures of interests (such as time on task, completion rates etc.) are then computed from these interaction traces. We demonstrate the usefulness of this approach through case studies highlighting the differences in user behaviors between apps and present examples of usability issues that it can help discover.", 
    "authors": [
      {
        "name": "Biplab  Deka, University of Illinois at Urbana-Champaign"
      }, 
      {
        "name": "Zifeng  Huang, University of Illinois at Urbana-Champaign"
      }, 
      {
        "name": "Chad D Franzen, University of Illinois at Urbana Champaign"
      }, 
      {
        "name": "Jeffrey  Nichols, Google, Inc."
      }, 
      {
        "name": "Yang  Li, Google Research"
      }, 
      {
        "name": "Ranjitha  Kumar, University of Illinois at Urbana-Champaign"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "ZIPT: Zero-Integration Performance Testing of Mobile App Designs", 
    "type": "paper"
  }, 
  "uistf4070": {
    "abstract": "Research experiences today are limited to a privileged few at select universities. Providing open access to research experiences would enable global upward mobility and increased diversity in the scientific workforce. How can we coordinate a crowd of diverse volunteers on open-ended research? How could a PI have enough visibility into each person's contributions to recommend them for further study? We present Crowd Research, a crowdsourcing technique that coordinates open-ended research through an iterative cycle of open contribution, synchronous collaboration, and peer assessment. To aid upward mobility and recognize contributions in publications, we introduce a decentralized credit system: participants allocate credits to each other, which a graph centrality algorithm translates into a collectively-created author order. Over 1,500 people from 62 countries have participated, 74% from institutions with low access to research. Over two years and three projects, this crowd has produced articles at top-tier Computer Science venues, and participants have gone on to leading graduate programs.", 
    "authors": [
      {
        "name": "Rajan  Vaish, Stanford University"
      }, 
      {
        "name": "Snehalkumar `Neil' S. Gaikwad, MIT"
      }, 
      {
        "name": "Geza  Kovacs, Stanford University"
      }, 
      {
        "name": "Andreas  Veit, Cornell University"
      }, 
      {
        "name": "Ranjay A Krishna, Stanford University"
      }, 
      {
        "name": "Imanol  Arrieta Ibarra, Stanford University"
      }, 
      {
        "name": "Camelia  Simoiu, Stanford University"
      }, 
      {
        "name": "Michael  Wilber, Cornell Tech"
      }, 
      {
        "name": "Serge  Belongie, Cornell University"
      }, 
      {
        "name": "Sharad C.  Goel, Stanford University"
      }, 
      {
        "name": "James  Davis, University of California, Santa Cruz"
      }, 
      {
        "name": "Michael S Bernstein, Stanford University"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "Crowd Research: Open and Scalable University Laboratories", 
    "type": "paper"
  }, 
  "uistf4170": {
    "abstract": "We present a jaw, face, or head movement (face-related movement) recognition system called CanalSense. It recognizes face-related movements using barometers embedded in earphones. We find that face-related movements change air pressure inside the ear canals, which shows characteristic changes depending on the type and degree of the movement. We also find that such characteristic changes can be used to recognize face-related movements. We conduct an experiment to measure the accuracy of recognition. As a result, random forest shows per-user recognition accuracies of 87.6% for eleven face-related movements and 87.5% for four OpenMouth levels.", 
    "authors": [
      {
        "name": "Toshiyuki  Ando, University of Tsukuba"
      }, 
      {
        "name": "Yuki  Kubo, University of Tsukuba"
      }, 
      {
        "name": "Buntarou  Shizuki, University of Tsukuba"
      }, 
      {
        "name": "Shin  Takahashi, University of Tsukuba"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "CanalSense: Face-Related Movement Recognition System based on Sensing Air Pressure in Ear Canals", 
    "type": "paper"
  }, 
  "uistf4174": {
    "abstract": "As three-dimensional printers become more available, 3D printed models can serve as important learning materials, especially for blind people who perceive the models tactilely. Such models can be much more powerful when augmented with audio annotations that describe the model and their elements. We present Markit and Talkit, a low-barrier toolkit for creating and interacting with 3D models with audio annotations. Makers (e.g., hobbyists, teachers, and friends of blind people) can use Markit to mark model elements and associate then with text annotations. A blind user can then print the augmented model, launch the Talkit application, and access the annotations by touching the model and following Talkit\u00e2\u0080\u0099s verbal cues. Talkit uses an RGB camera and a microphone to sense users\u00e2\u0080\u0099 inputs so it can run on a variety of devices. We evaluated Markit with eight sighted \u00e2\u0080\u009cmakers\u00e2\u0080\u009d and Talkit with eight blind people. On average, non-experts added two annotations to a model in 275 seconds (SD=70) with Markit. Meanwhile, with Talkit, blind people found a specified annotation on a model in an average of 7 seconds (SD=8).", 
    "authors": [
      {
        "name": "Lei  Shi, Cornell Tech"
      }, 
      {
        "name": "Yuhang  Zhao, Cornell Tech"
      }, 
      {
        "name": "Shiri  Azenkot, Cornell Tech"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Markit and Talkit: A Low-Barrier Toolkit to Augment 3D Printed Models with Audio Annotations", 
    "type": "paper"
  }, 
  "uistf4234": {
    "abstract": "We present a new, publicly available dataset of mobile app designs with the goal of advancing the development of data-driven mobile app design tools. This dataset was mined from over 9.7K existing Android apps from 27 categories using a combination of human and automated crawling of apps. The dataset contains more than 72K unique UI screens captured as screenshots and view hierarchies, metadata from the Google Play Store (category, ratings, etc.), and the set of human interaction traces captured in the crawl. We sketch the diverse set of applications that could be enabled by this dataset and demonstrate that the dataset\u00e2\u0080\u0099s scale can support deep learning techniques by training and evaluating an autoencoder for UI layout similarity.", 
    "authors": [
      {
        "name": "Biplab  Deka, University of Illinois at Urbana-Champaign"
      }, 
      {
        "name": "Zifeng  Huang, University of Illinois at Urbana-Champaign"
      }, 
      {
        "name": "Chad D Franzen, University of Illinois at Urbana Champaign"
      }, 
      {
        "name": "Joshua  Hibschman, Northwestern University"
      }, 
      {
        "name": "Dan  Afergan, Google, Inc."
      }, 
      {
        "name": "Yang  Li, Google Research"
      }, 
      {
        "name": "Jeffrey  Nichols, Google, Inc."
      }, 
      {
        "name": "Ranjitha  Kumar, University of Illinois at Urbana-Champaign"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "RICO: A Mobile App Dataset for Building Data-Driven Design Applications", 
    "type": "paper"
  }, 
  "uistf4289": {
    "abstract": "Over the last decades, there have been numerous efforts in wearable computing research to enable interactive textiles. Most work focus, however, on integrating sensors for planar touch gestures, and thus do not fully take advantage of the flexible, deformable and tangible material properties of textile. In this work, we introduce SmartSleeve, a deformable textile sensor, which can sense both surface and deformation gestures in real-time. It expands the gesture vocabulary with a range of expressive interaction techniques, and we explore new opportunities using advanced deformation gestures, such as, Twirl, Twist, Fold, Push and Stretch. We describe our sensor design, hardware implementation and its novel non-rigid connector architecture. We provide a detailed description of our hybrid gesture detection pipeline that uses learning-based algorithms and heuristics to enable real-time gesture detection and tracking. Its modular architecture allows us to derive new gestures through the combination with continuous properties like pressure, location, and direction. Finally, we report on the promising results from our evaluations which demonstrate real-time classification. ", 
    "authors": [
      {
        "name": "Patrick  Parzer, University of Applied Sciences Upper Austria"
      }, 
      {
        "name": "Adwait  Sharma, University of Applied Sciences Upper Austria"
      }, 
      {
        "name": "Anita  Vogl, University of Applied Sciences Upper Austria"
      }, 
      {
        "name": "J\u00fcRgen  Steimle, Saarland University"
      }, 
      {
        "name": "Alex  Olwal, Google, Inc."
      }, 
      {
        "name": "Michael  Haller, University of Applied Sciences Upper Austria"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "SmartSleeve: Real-time Sensing of Surface and Deformation Gestures on Flexible, Interactive Textiles, using a Hybrid Gesture Detection Pipeline", 
    "type": "paper"
  }, 
  "uistf4298": {
    "abstract": "Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process. ", 
    "authors": [
      {
        "name": "Zoya  Bylinskii, MIT"
      }, 
      {
        "name": "Nam Wook  Kim, Harvard University"
      }, 
      {
        "name": "Peter  O'Donovan, Adobe Research"
      }, 
      {
        "name": "Sami  Alsheikh, MIT"
      }, 
      {
        "name": "Spandan  Madan, Harvard University"
      }, 
      {
        "name": "Hanspeter  Pfister, Harvard University"
      }, 
      {
        "name": "Fredo  Durand, MIT"
      }, 
      {
        "name": "Bryan  Russell, Adobe Research"
      }, 
      {
        "name": "Aaron  Hertzmann, Adobe Research"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "Learning Visual Importance for Graphic Designs and Data Visualizations", 
    "type": "paper"
  }, 
  "uistf4355": {
    "abstract": "We present iSoft, a single volume soft sensor capable of sensing real-time continuous contact and unidirectional stretching. We propose a low-cost and an easy way to fabricate such piezoresistive elastomer-based soft sensors for instant interactions. We employ an electrical impedance tomography (EIT) technique to estimate changes of resistance distribution on the sensor caused by fingertip contact. To compensate for the rebound elasticity of the elastomer and achieve real-time continuous contact sensing, we apply a dynamic baseline update for EIT. The baseline updates are triggered by fingertip contact and movement detections. Further, we support unidirectional stretching sensing using a model-based approach which works separately with continuous contact sensing. We also provide a software toolkit for users to design and deploy personalized interfaces with customized sensors. Through a series of experiments and evaluations, we validate the performance of contact and stretching sensing. Through example applications, we show the variety of examples enabled by iSoft.", 
    "authors": [
      {
        "name": "Sang Ho  Yoon, Purdue University"
      }, 
      {
        "name": "Ke  Huo, Purdue University"
      }, 
      {
        "name": "Yunbo  Zhang, Purdue University"
      }, 
      {
        "name": "Guiming  Chen, Purdue University"
      }, 
      {
        "name": "Luis  Paredes, Purdue University"
      }, 
      {
        "name": "Subramanian  Chidambaram, Purdue University"
      }, 
      {
        "name": "Karthik  Ramani, Purdue University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "iSoft: A Customizable Soft Sensor with Real-time Continuous Contact and Stretching Sensing", 
    "type": "paper"
  }, 
  "uistf4432": {
    "abstract": "In this paper, we introduce two different transforming steering wheel systems that can be utilized to augment user experience for future partially autonomous and fully autonomous vehicles. The first one is a robotic steering wheel that can mechanically transform by using its actuators to move the various components into different positions. The second system is a LED steering wheel that can visually transform by using LEDs embedded along the rim of wheel to change colors. Both steering wheel systems contain onboard microcontrollers developed to interface with our driving simulator. The main function of these two systems is to provide emergency warnings to drivers in a variety of safety critical scenarios, although the design space that we propose for these steering wheel systems also includes the use as interactive user interfaces.     To evaluate the effectiveness of the emergency alerts, we conducted a driving simulator study examining the performance of participants (N=56) after an abrupt loss of autonomous vehicle control. Drivers who experienced the robotic steering wheel performed significantly better than those who experienced the LED steering wheel. The results of this study suggest that alerts utilizing mechanical movement are more effective than purely visual warnings.  ", 
    "authors": [
      {
        "name": "Brian  Mok, Stanford University"
      }, 
      {
        "name": "Mishel  Johns, Stanford University"
      }, 
      {
        "name": "Stephen  Yang, Stanford University"
      }, 
      {
        "name": "Wendy  Ju, Stanford University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Reinventing the Wheel: Transforming Steering Wheel Systems for Autonomous Vehicles", 
    "type": "paper"
  }, 
  "uistf4474": {
    "abstract": "360-degree video contains a full field of environmental content. However, browsing these videos, either on screens or through head-mounted displays (HMDs), users consume only a subset of the full field of view per a natural viewing experience. This causes a search problem when a region-of-interest (ROI) in a video is outside of the current field of view (FOV) on the screen, or users may search for non-existing ROIs.     We propose Outside-In, a visualization technique which re-introduces off-screen regions-of-interest (ROIs) into the main screen as spatial picture-in-picture (PIP) previews. The geometry of the preview windows further encodes a ROI's relative location vis-\u00c3\u00a0-vis the main screen view, allowing for effective navigation. In an 18-participant study, we compare Outside-In with traditional arrow-based guidance within three types of 360-degree video. Results show that Outside-In outperforms in regard to understanding spatial relationship, the storyline of the content and overall preference. Two applications are demonstrated for use with Outside-In in 360-degree video navigation with touchscreens, and live telepresence.", 
    "authors": [
      {
        "name": "Yung-Ta  Lin, National Taiwan University"
      }, 
      {
        "name": "Yi-Chi  Liao, National Taiwan University"
      }, 
      {
        "name": "Shan-Yuan  Teng, National Taiwan University"
      }, 
      {
        "name": "Yi-Ju  Chung, National Taiwan University"
      }, 
      {
        "name": "Liwei  Chan, National Chiao Tung University"
      }, 
      {
        "name": "Bing-Yu  Chen, National Taiwan University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Outside-In: Visualizing Out-of-Sight Regions-of-Interest in a 360 Video Using Spatial Picture-in-Picture Previews", 
    "type": "paper"
  }, 
  "uistf4499": {
    "abstract": "Usability has a distinct subjective component, yet surprisingly little is known about its neural basis and relation to the neuroanatomy of aesthetics. To begin closing this gap, we conducted two functional magnetic resonance imaging studies in which participants were shown static webpages (in the first study) and videos of interaction with webpages (in the second study). The webpages were controlled so as to exhibit high and low levels of perceived usability and perceived aesthetics. Our results show unique links between perceived usability and brain areas involved in functions such as emotional processing (left fusiform gyrus, superior frontal gyrus), anticipation of physical interaction (precentral gyrus), task intention (anterior cingulate cortex), and linguistic processing (medial and bilateral superior frontal gyri). We use these findings to discuss the brain correlates of perceived usability and the use of fMRI for usability evaluation and for generating new user experiences.  ", 
    "authors": [
      {
        "name": "Chi  Thanh Vi, University of Sussex"
      }, 
      {
        "name": "Kasper  Hornb\u00e6K, University of Copenhagen"
      }, 
      {
        "name": "Sriram  Subramanian, University of Sussex"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Neuroanatomical Correlates of Perceived Usability", 
    "type": "paper"
  }, 
  "uistf4553": {
    "abstract": "The Maker movement has encouraged more people to start working with electronics and embedded processors. A key challenge in developing and debugging custom embedded systems is understanding their behavior, particularly at the boundary between hardware and software. Existing tools such as step debuggers and logic analyzers only focus on software or hardware, respectively. This paper presents a new development environment designed to illuminate the boundary between embedded code and circuits. Bifr\u00c3\u00b6st automatically instruments and captures the progress of the user\u00e2\u0080\u0099s code, variable values, and the electrical and bus activity occurring at the interface between the processor and the circuit it operates in. This data is displayed in a linked visualization that allows navigation through time and program execution, enabling comparisons between variables in code and signals in circuits. Automatic checks can detect low-level hardware configuration and protocol issues, while user-authored checks can test particular application semantics. In an exploratory study with ten participants, we investigated how Bifr\u00c3\u00b6st influences debugging workflows.", 
    "authors": [
      {
        "name": "Will  Mcgrath, Stanford University"
      }, 
      {
        "name": "Daniel  Drew, University of California, Berkeley"
      }, 
      {
        "name": "Jeremy  Warner, UC Berkeley"
      }, 
      {
        "name": "Majeed  Kazemitabaar, University of Maryland, College Park"
      }, 
      {
        "name": "Mitchell  Karchemsky, UC Berkeley"
      }, 
      {
        "name": "David A Mellis, UC Berkeley"
      }, 
      {
        "name": "Bjoern  Hartmann, University of California, Berkeley"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Bifr\u00c3\u00b6st : Visualizing and Checking Behavior of Embedded Systems across Hardware and Software", 
    "type": "paper"
  }, 
  "uistf4597": {
    "abstract": "Collaborative review and feedback is an important part of conventional filmmaking and now Virtual Reality (VR) video production as well. However, conventional collaborative review practices do not easily translate to VR video because VR video is normally viewed in a headset, which makes it difficult to align gaze, share context, and take notes. This paper presents CollaVR, an application that enables multiple users to review a VR video together while wearing headsets. We interviewed VR video professionals to distill key considerations in reviewing VR video. Based on these insights, we developed a set of networked tools that enable filmmakers to collaborate and review video in real-time. We conducted a preliminary expert study to solicit feedback from VR video professionals about our system and assess their usage of the system with and without collaboration features.", 
    "authors": [
      {
        "name": "Cuong  Nguyen, Portland State University"
      }, 
      {
        "name": "Stephen  Diverdi, Adobe Research"
      }, 
      {
        "name": "Aaron  Hertzmann, Adobe Research"
      }, 
      {
        "name": "Feng  Liu, Portland State University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "CollaVR: Collaborative In-Headset Review for VR Video", 
    "type": "paper"
  }, 
  "uistf4607": {
    "abstract": "The emergence of personal computing devices offers both a challenge and opportunity for displaying text: small screens can be hard to read, but also support higher resolution. To fit content on a small screen, text must be small. This small text size can make computing devices unusable, in particular to low-vision users, whose vision is not correctable with glasses. Usability is also decreased for sighted users straining to read the small letters, especially without glasses at hand. We propose animated scripts called livefonts for displaying English with improved legibility for all users. Because paper does not support animation, traditional text is static. However, modern screens support animation, and livefonts capitalize on this capability. We evaluate our livefont variations' legibility through a controlled lab study with low-vision and sighted participants, and find our animated scripts to be legible across vision types at approximately half the size (area) of traditional letters, while previous smartfonts (static alternate scripts) did not show a significant legibility advantage for low-vision users. We evaluate the learnability of our livefont with low-vision and sighted participants, and find it to be comparably learnable to static smartfonts after two thousand practice sentences.", 
    "authors": [
      {
        "name": "Danielle  Bragg, University of Washington"
      }, 
      {
        "name": "Shiri  Azenkot, Cornell Tech"
      }, 
      {
        "name": "Kevin  Larson, Microsoft"
      }, 
      {
        "name": "Ann  Bessemans, Hasselt University"
      }, 
      {
        "name": "Adam  Kalai, Microsoft Research"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Designing and Evaluating Livefonts", 
    "type": "paper"
  }, 
  "uistf4623": {
    "abstract": "Redubbing is an extensively used technique to correct errors in voiceover recordings. It involves re-recording a part of a voiceover, identifying the corresponding section of audio in the original recording that needs to be replaced, and using low level audio tools to replace the audio. Although this sequence of steps can be performed using traditional audio editing tools, the process can be tedious when dealing with long voiceover recordings and prohibitively difficult for users not familiar with such tools. To address this issue, we present AutoDub, a novel system for redubbing voiceover recordings. Using our system, a user simply needs to re-record the part of the voiceover that needs to be replaced. Our system automatically locates the corresponding part in the original recording and performs the low level audio processing to replace it. The system can be easily incorporated in any existing sophisticated audio editor or can be employed as a functionality in an audio-guided user interface. User studies involving participation from novice, knowledgeable and expert users indicate that our tool is preferred to a traditional audio editor based redubbing approach by all categories of users due to its faster and easier redubbing capabilities.", 
    "authors": [
      {
        "name": "Shrikant  Venkataramani, University of Illinois at Urbana Champaign"
      }, 
      {
        "name": "Paris  Smaragdis, University of Illinois at Urbana Champaign"
      }, 
      {
        "name": "Gautham  Mysore, Adobe Research"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "AutoDub: Automatic Redubbing for Voiceover Editing", 
    "type": "paper"
  }, 
  "uistf4637": {
    "abstract": "We present DreamSketch, a novel 3D design interface that combines the free-form and expressive qualities of sketching with the computational power of generative design algorithms. In DreamSketch, a user coarsely defines the problem by sketching the design context. Then, a generative design algorithm produces multiple solutions that are augmented as 3D objects in the sketched context. The user can interact with the scene to navigate through the generated solutions. The combination of sketching and generative algorithms enables designers to explore multiple ideas and make better informed design decisions during the early stages of design. Design study sessions with designers and mechanical engineers demonstrate the expressive nature and creative possibilities of DreamSketch.", 
    "authors": [
      {
        "name": "Rubaiat Habib  Kazi, Autodesk Research"
      }, 
      {
        "name": "Tovi  Grossman, Autodesk Research"
      }, 
      {
        "name": "Hyunmin  Cheong, Autodesk"
      }, 
      {
        "name": "Ali B. Hashemi, Autodesk"
      }, 
      {
        "name": "George  Fitzmaurice, Autodesk Research"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "DreamSketch: Early Stage 3D Design Explorations with Sketching and Generative Design", 
    "type": "paper"
  }, 
  "uistf4658": {
    "abstract": "Data science courses and tutorials have grown popular in recent years, yet they are still taught using production-grade programming tools (e.g., R, MATLAB, and Python IDEs) within desktop computing environments. Although powerful, these tools present high barriers to entry for novices, forcing them to grapple with the extrinsic complexities of software installation and configuration, data file management, data parsing, and Unix-like command-line interfaces. To lower the barrier for novices to get started with learning data science, we created DS.js, a bookmarklet that embeds a data science programming environment directly into any existing webpage. By transforming any webpage into an example-centric IDE, DS.js eliminates the aforementioned complexities of desktop-based environments and turns the entire web into a rich substrate for learning data science. DS.js automatically parses HTML tables and CSV/TSV data sets on the target webpage, attaches code editors to each data set, provides a data table manipulation and visualization API designed for novices, and gives instructional scaffolding in the form of bidirectional previews of how the user's code and data relate.", 
    "authors": [
      {
        "name": "Xiong  Zhang, University of Rochester"
      }, 
      {
        "name": "Philip J Guo, UC San Diego"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "DS.js: Turn Any Webpage into an Example-Centric Live Programming Environment for Learning Data Science", 
    "type": "paper"
  }, 
  "uistf4706": {
    "abstract": "We propose a system for real-time six degrees of freedom (6DoF) tracking of a passive stylus that achieves sub-millimeter accuracy, which is suitable for writing or drawing in mixed reality applications.  Our system is particularly easy to implement, requiring only a monocular camera, a 3D printed dodecahedron, and hand-glued binary square markers. The accuracy and performance we achieve are due to model-based tracking using a calibrated model and a combination of sparse pose estimation and dense alignment.  We demonstrate the system performance in terms of speed and accuracy on a number of synthetic and real datasets, showing that it can be competitive with state-of-the-art multi-camera motion capture systems.  We also demonstrate several applications of the technology ranging from 2D and 3D drawing in VR to general object manipulation and board games.", 
    "authors": [
      {
        "name": "Po-Chen  Wu, National Taiwan University"
      }, 
      {
        "name": "Robert  Wang, Facebook Inc."
      }, 
      {
        "name": "Kenrick  Kin, Facebook Inc."
      }, 
      {
        "name": "Christopher  Twigg, Facebook Inc."
      }, 
      {
        "name": "Shangchen  Han, Facebook Inc."
      }, 
      {
        "name": "Ming-Hsuan  Yang, University of California at Merced"
      }, 
      {
        "name": "Shao-Yi  Chien, National Taiwan University"
      }
    ], 
    "award": false, 
    "hm": true, 
    "subtype": "paper", 
    "title": "DodecaPen: Accurate 6DoF Tracking of a Passive Stylus", 
    "type": "paper"
  }, 
  "uistf4922": {
    "abstract": "Reflector is a novel direct pointing method that utilizes hidden design space on reflective screens. By aligning a part of the user\u00e2\u0080\u0099s onscreen reflection with objects rendered on the screen, Reflector enables (1) distance-independent and (2) private pointing on commodity screens. Reflector can be implemented easily in both desktop and mobile conditions through a single camera installed at the edge of the screen. Reflector\u00e2\u0080\u0099s pointing performance was compared to today\u00e2\u0080\u0099s major direct input devices: eye trackers and touchscreens. We demonstrate that Reflector allows the user to point more reliably, regardless of distance from the screen, compared to an eye tracker. Further, due to the private nature of an onscreen reflection, Reflector shows a shoulder surfing success rate 20 times lower than that of touchscreens for the task of entering a 4-digit PIN.", 
    "authors": [
      {
        "name": "Jong-In  Lee, KAIST"
      }, 
      {
        "name": "Sunjun  Kim, KAIST"
      }, 
      {
        "name": "Masaaki  Fukumoto, Microsoft Research"
      }, 
      {
        "name": "Byungjoo  Lee, KAIST"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Reflector: Distance-Independent, Private Pointing on a Reflective Screen", 
    "type": "paper"
  }, 
  "uistf4937": {
    "abstract": "Here we report the new soft sensor \u00e2\u0080\u009cFoamSense\u00e2\u0080\u009d that can measure the deformation state of a volumetric soft object such as compressed, bent, twisted and sheared (Figure 1). This sensor is made by impregnating a porous soft object with conductive ink. The design process of FoamSense is explained. We then summarized the features and basic characteristics of some porous materials for designing these sensors appropriately. We also proposed the potential of using digital fabrication for controlling the carrier structure of FoamSense. Proposed porous structure showed an anisotropic sensor characteristic. We discussed the potential and limitation of this approach. Three possible applications are proposed by using FoamSense. FoamSense supports a richer interaction between the user and soft objects.", 
    "authors": [
      {
        "name": "Satoshi  Nakamaru, Keio University"
      }, 
      {
        "name": "Ryosuke  Nakayama, Keio University"
      }, 
      {
        "name": "Ryuma  Niiyama, The University of Tokyo"
      }, 
      {
        "name": "Yasuaki  Kakehi, Keio University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "FoamSense: Design of three dimensional soft sensors with porous materials", 
    "type": "paper"
  }, 
  "uistf4994": {
    "abstract": "Human actuation is the idea of using people to provide large-scale force feedback to users. The Haptic Turk system, for example, used four human actuators to lift and push a virtual reality user; TurkDeck used ten human actuators to place and animate props for a single user. While the experience of human actuators was decent, it was still inferior to the experience these people could have had, had they participated as a user. In this paper, we address this issue by making everyone a user. We introduce mutual human actuation, a version of human actuation that works without dedicated human actuators. The key idea is to run pairs of users at the same time and have them provide human actuation to each other. Our system, Mutual Turk, achieves this by (1)\u00c2\u00a0offering shared props through which users can exchange forces while obscuring the fact that there is a human on the other side, and (2)\u00c2\u00a0synchronizing the two users\u00e2\u0080\u0099 timelines such that their way of manipulating the shared props is consistent across both virtual worlds. We demonstrate mutual human actuation with an example experience in which users pilot kites though storms, tug fish out of ponds, are pummeled by hail, battle monsters, hop across chasms, push loaded carts, and ride in moving vehicles.", 
    "authors": [
      {
        "name": "Lung-Pan  Cheng, Hasso Plattner Institute"
      }, 
      {
        "name": "Sebastian  Marwecki, Hasso Plattner Institute"
      }, 
      {
        "name": "Patrick  Baudisch, Hasso Plattner Institute"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Mutual Human Actuation", 
    "type": "paper"
  }, 
  "uistpp111": {
    "abstract": "Children with ASD (Autism Spectrum Disorder) have social communication difficulties partly due to their abnormal avoidance of eye contact on human faces, yet they have a normal visual processing strategy on cartoon face. In this paper, we present KinToon, a face-to-face communication enhancement system to help ASD children in their training lessons. Our system use Kinect to scan human face and extract key points from facial contour, and match them to corresponding key points of a cartoon face. A modified cartoon face is projected to the communicator\u00c3\u00a2\u00e2\u0082\u00ac\u00e2\u0084\u00a2s face to achieve the effect of dynamic \u00c3\u00a2\u00e2\u0082\u00ac\u00c5\u0093makeup\u00c3\u00a2\u00e2\u0082\u00ac\u00c2\u009d. ASD children will finally talk to the communicator with dynamic cartoon makeup, which would reduce their stress of interacting with people and make them easier to understand emotions. The interactive devices were applied to an ASD training lesson, and our creative approach was examined to be relatively effective in encouraging ASD children to fetch more emotional information and have more eye contact with people by eye tracking.", 
    "authors": [
      {
        "name": "Cheng  Zheng, Zhejiang University"
      }, 
      {
        "name": "Caowei  Zhang, Zhejiang University"
      }, 
      {
        "name": "Xuan  Li, Hangzhou"
      }, 
      {
        "name": "Fan  Zhang, Zhejiang University"
      }, 
      {
        "name": "Bing  Li, Computer Science"
      }, 
      {
        "name": "Chuqi  Tang, Zhejiang University"
      }, 
      {
        "name": "Cheng  Yao, Computer Science and Technology"
      }, 
      {
        "name": "Ting  Zhang, Zhejiang University"
      }, 
      {
        "name": "Fangtian  Ying, College of Computer Science and Technology"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "KinToon: A Kinect Facial Projector for Communication Enhancement for ASD Children", 
    "type": "paper"
  }, 
  "uistpp117": {
    "abstract": "Perceptual illusions enable designers to go beyond hardware limitations to create rich haptic content. Nevertheless, spatio-temporal interactions for thermal displays have not been studied thoroughly. We focus on the apparent motion of hot and cold thermal pulses delivered at the thenar eminence of both hands. Here we show that 1000 ms hot and cold thermal pulses overlapping for about 40% of their actuation time are likely to produce a continuous apparent motion sensation. Furthermore, we show that the quality of the illusion (defined as the motion's temporal continuity) was more sensitive to changes in SOA for cold pulses in relation to hot pulses.", 
    "authors": [
      {
        "name": "Daniel  Gongora, Keio University"
      }, 
      {
        "name": "Roshan L Peiris, Keio University"
      }, 
      {
        "name": "Kouta  Minamizawa, Yokohama City"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Towards Intermanual Apparent Motion of Thermal Pulses", 
    "type": "paper"
  }, 
  "uistpp120": {
    "abstract": "We developed methods and implemented a system prototype to help people find specific signboards in areas with densely located signboards. In addition, we examined whether the proposed methods would reduce the search time of a specific signboard. The result showed that the proposed method was superior in cases where there were multiple signboards to be searched and background saturation was low.", 
    "authors": [
      {
        "name": "Shigeo  Kitamura, Kansai University"
      }, 
      {
        "name": "Mitsunori  Matsushita, Kansai University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Information Identification Support Method for Areas with Densely Located Signboards", 
    "type": "paper"
  }, 
  "uistpp125": {
    "abstract": "In augmented and virtual reality, there may be many 3D planar windows with 2D texts, images, and videos on them. Projective Windows is a technique using projective geometry to bring any near or distant window instantly to the fingertip and then to scale and position it simultaneously with a single, continuous flow of hand motion.", 
    "authors": [
      {
        "name": "Joon Hyub  Lee, KAIST"
      }, 
      {
        "name": "Sang-Gyun  An, KAIST"
      }, 
      {
        "name": "Yongkwan  Kim, KAIST"
      }, 
      {
        "name": "Seok-Hyung  Bae, KAIST"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Projective Windows: Arranging Windows in Space Using Projective Geometry", 
    "type": "paper"
  }, 
  "uistpp128": {
    "abstract": "In Japan, the necessity of saving energy is rising due to the nuclear accident caused by the Great East Japan Earthquake that occurred on March 11, 2011. Reduction of energy usage is required due to rapid increases in electricity consumption due to the scorching summer heat in recent years. The common ways to provide information on energy consumption mainly occur through \u00c3\u00a2\u00e2\u0082\u00ac\u00c5\u0093visualization\u00c3\u00a2\u00e2\u0082\u00ac\u00c2\u009d of information. On the contrary, olfactory stimulation can be performed while working, and it is effective also when the degree of arousal is low. This study considers applications on the basis of the concept of \u00c3\u00a2\u00e2\u0082\u00ac\u00c5\u0093smellization\u00c3\u00a2\u00e2\u0082\u00ac\u00c2\u009d of information using olfactive stimulation. In this paper, we introduce the configuration and operation examples of a system developed for evoking public energy conservation behavior using smell.", 
    "authors": [
      {
        "name": "Tsutomu  Miyasato"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Smellization\u00c2\u009d of Warnings against Overuse Power used to Promote Energy Saving Behavior", 
    "type": "paper"
  }, 
  "uistpp133": {
    "abstract": "Audio Podcasts have gained popularity because they are a compelling form of storytelling and are easy to consume. However, they are not as easy to produce since resources are invested in the research, recording, and editing process and the average length of an episode is over an hour. Some audio podcasts could benefit from visuals to increase engagement and learning, but manually curating them can be arduous and time-consuming. We introduce a tool for automatically visualizing audio podcasts, currently focused on the genre of travelogues. Our system works by first time-aligning the transcript of a given podcast, using NLP techniques to extract entities and track how interesting or relevant they are throughout the podcast, and then retrieving visual data appropriately to describe them, either through transitions on a map or professionally taken photographs with captions. By automatically creating a visual narrative to accompany a podcast, we hope our tool can provide listeners with a better sense of the podcast's topic.", 
    "authors": [
      {
        "name": "Jihyeon  Lee, Stanford University"
      }, 
      {
        "name": "Mitchell  Gordon, Stanford University"
      }, 
      {
        "name": "Maneesh  Agrawala, Stanford University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Automatically Visualizing Audio Travel Podcasts", 
    "type": "paper"
  }, 
  "uistpp134": {
    "abstract": "Wearable devices combining with VR/AR technology become a research hotspot these years. In some research, tactile displays are put on the skin and synchronized with VR/AR environment. Researchers try to use these display to simulate varied embodied feeling to enhance the immersion in the VR/AR environment. In the field of game entertainment, based on the scenario, sometimes the feeling of passing through the body need to be presented to the user. However this is physically impossible. Thus we make a exploration attempting to simulate this feeling by thermal feedback. Here we use two thermal modules bonding on the two side of the wrist( inside and outside). When we actuate two modules sequentially, user would perceive the stimuli and interpret this into a feeling of passing though. In the paper, we will introduce the interface and describe the experiment to determine the principle for thermo-tactile illusion of passing through.", 
    "authors": [
      {
        "name": "Wei  Peng, Keio University Graduate School of Media Design"
      }, 
      {
        "name": "Roshan L Peiris, Keio University Graduate School of Media Design"
      }, 
      {
        "name": "Kouta  Minamizawa, Yokohama"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Exploring of Simulating Passing through Feeling on the Wrist: Using thermal feedback", 
    "type": "paper"
  }, 
  "uistpp135": {
    "abstract": "In this paper, we propose a grouping scheme that classifies applications into groups for individual users by utilizing their geometrical information on a tabletop system. The proposed scheme investigates the geometrical information of each application, such as its position on the display and its rotational information, and then groups the applications of each individual user by utilizing a classifier with the geometrical information. We evaluate the proposed scheme with lab experiments, and the results show that, on average, 95.6% of applications are well classified into their users.", 
    "authors": [
      {
        "name": "Jonggyu  Park, Sungkyunkwan University"
      }, 
      {
        "name": "Inhyeok  Kim, Sungkyunkwan University"
      }, 
      {
        "name": "Young Ik  Eom, Suwon"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Grouping Applications Using Geometrical Information of Applications on Tabletop Systems", 
    "type": "paper"
  }, 
  "uistpp136": {
    "abstract": "We present HapticDrone, a concept to generate controllable and comparable force feedback for direct haptic interaction with a drone. As a proof-of-concept study this paper focuses on creating haptic feedback only in 1D direction. To this end, an encountered-type, safe and un-tethered haptic display is implemented. An overview of the system and details on how to control the force output of drones is provided. Our current prototype generates forces up to 1.53 N upwards and 2.97 N downwards. This concept serves as a first step towards introducing drones as mainstream haptic devices.", 
    "authors": [
      {
        "name": "Muhammad  Abdullah, Kyung Hee University"
      }, 
      {
        "name": "Minji  Kim, Kyung Hee University"
      }, 
      {
        "name": "Waseem  Hassan, Yongin-si"
      }, 
      {
        "name": "Yoshihiro  Kuroda, Osaka University"
      }, 
      {
        "name": "Seokhee  Jeon, Kyung Hee University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "HapticDrone - An Encountered-Type Kinesthetic Haptic Interface with Controllable Force Feedback: Initial Example for 1D Haptic Feedback", 
    "type": "paper"
  }, 
  "uistpp138": {
    "abstract": "\u00c3\u00a2\u00e2\u0082\u00ac\u00c5\u0093Walk-In Music\u00c3\u00a2\u00e2\u0082\u00ac\u00c2\u009d is a system that provides a new walking experience through synchronized music and pseudo-gravity. This system synchronizes each step with the music being listened to and creates a feeling of generating music through walking. It creates a Walk-In state where music and walking are consistent at all times. In this state, when changing the speed of music, the pedestrian may feel pseudo-gravity based on pseudo-haptics. Our results indicate that by changing the speed of music during the Walk-In state, the walking speed became faster and slower. We call this a Walk-Shift. This demonstrated the possibility of controlling personal walking by music. Walk-In Music has created a pleasant experience by music, and proposed a new relationship between people and music that leads to behavior changes. ", 
    "authors": [
      {
        "name": "Haruto  Murata, Keio Graduate School of Media Design"
      }, 
      {
        "name": "Youssef  Bouzarte, Kreio Graduate School of Media Design, Graduate School, Yokohama, Kanagawa, Japan"
      }, 
      {
        "name": "Junichi  Kanebako, Yokohama"
      }, 
      {
        "name": "Kouta  Minamizawa, Keio University Graduate School of Media Design"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Walk-In Music : Walking experience with synchronized music and its effect of pseudo-gravity", 
    "type": "paper"
  }, 
  "uistpp144": {
    "abstract": "Crowd-powered conversational assistants have found to be more robust than automated systems, but do so at the cost of higher response latency and monetary costs. One promising direction is to combined the two approaches for high quality and low cost solutions. However, traditional offline approaches of building automated systems with the crowd requires first collecting training data from the crowd, and then training a model before an online system can be launched. In this paper, we introduce Evorus, a crowd-powered conversational assistant with online-learning capability that automate itself over time. Evorus expands a previous crowd-powered conversation system by reducing its reliance on the crowd over time while maintaining the robustness and reliability of human intelligence, by (i) allowing new chatbots to be added to help contribute possible answers, (ii) learning to reuse past responses to similar queries over time, and (iii) learning to reduce the amount of crowd oversight necessary to retain quality. Our deployment study with 28 users show that automated responses were chosen 12.84% of the time, and voting cost was reduced by 6%. Evorus introduced a new framework for constructing crowd-powered conversation systems that can gradually automate themselves using machine learning, a concept that we believe can be generalize to other types of crowd-powered systems for future research.", 
    "authors": [
      {
        "name": "Ting-Hao K. Huang, Carnegie Mellon University"
      }, 
      {
        "name": "Joseph Chee  Chang, Carnegie Mellon University"
      }, 
      {
        "name": "Saiganesh  Swaminathan, Pittsburgh"
      }, 
      {
        "name": "Jeffrey P Bigham, Carnegie Mellon University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Evorus: A Crowd-powered Conversational Assistant That Automates Itself Over Time", 
    "type": "paper"
  }, 
  "uistpp145": {
    "abstract": "The main goal of our research is to develop a haptic display that conveys the shapes, hardness, and textures of objects displayed on near-future 3D haptic TVs. We present a novel handheld device, GraspForm. This device renders the surface shapes and hardness of a virtual object that is represented in an absolute position in real space. GraspForm has a 2\u00c3\u0083\u00e2\u0080\u00942 matrix of actuated hemispheres for one fingertip, two actuated pads for the palm, and a force feedback actuator for the thumb. Our first experimental results showed that eight participants succeeded in recognizing the side geometries of a cylinder and a square prism regardless of the availability of visual information.", 
    "authors": [
      {
        "name": "Takuya  Handa, NHK Science & Technology Research Laboratories"
      }, 
      {
        "name": "Makiko  Azuma, NHK Science & Technology Research Laboratories"
      }, 
      {
        "name": "Toshihiro  Shimizu, Setagaya-ku"
      }, 
      {
        "name": "Satoru  Kondo, NHK Science & Technology Research Laboratories"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "GraspForm: Shape and Hardness Rendering on Handheld Device toward Universal Interface for 3D Haptic TV", 
    "type": "paper"
  }, 
  "uistpp152": {
    "abstract": "In this project, by combining thermal feedback with Virtual Reality (VR) and utilizing thermal stimuli to present temperature data of weather, we attempted to provide a multi-sensory experience for enhancing users' perception of environment in virtual space.  By integrating thermal modules with the current VR head mounted display to provide thermal feedback directly on the face, and by setting thermal stimulus to provide similar feeling towards real air temperature, we developed an application in which users are able to \"feel\" the weather in VR environment.  An user experiment was also conducted to evaluate our design, according to which we verified that thermal feedback can improve users' experience in perceiving environment, and this research also provided a new approach for setting thermal feedback for presenting environmental information in virtual space.", 
    "authors": [
      {
        "name": "Zikun  Chen, Keio University"
      }, 
      {
        "name": "Roshan L Peiris, Keio University"
      }, 
      {
        "name": "Kouta  Minamizawa, Yokohama City"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "A Thermally Enhanced Weather Checking System in VR", 
    "type": "paper"
  }, 
  "uistpp153": {
    "abstract": "Global Positioning System (GPS) technology is widely used for outdoor navigation, but it is still challenging to apply this technology to a mid-scale or indoor environment. Using GPS in this way raises issues, such as reliability, deployment cost, and maintenance. Recently, companies like Google have begun to provide accurate indoor mapping. However, current implementations rely on both Wi-Fi and cellular technologies which have a hard time identifying the user\u00c3\u00a2\u00e2\u0082\u00ac\u00e2\u0084\u00a2s exact location in an indoor environment. There are two research questions in this paper: (1) How do we design a flexible and cost efficient indoor navigation system for organizations? (2) How to find an optimized path in a mid-scale/local environment. Here we propose Jaguar, which is a novel navigation system that utilizes a customized KML map with NFC technologies to address above questions. Our system includes an Android mobile application, a web-based map authoring tool and an implementation of a Cartesian plane based path finding algorithm. The initial testing of the system shows successful adaptation for a school campus environment.", 
    "authors": [
      {
        "name": "Brandon  Dalton, Wentworth Institute of Technology"
      }, 
      {
        "name": "Chen-Hsiang  Yu, Wentworth Institute of Technology"
      }, 
      {
        "name": "Mira  Yun, Wentworth Institute of Technology"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Jaguar: Indoor Navigation System for Organizations", 
    "type": "paper"
  }, 
  "uistpp154": {
    "abstract": "As communication technologies continue to rapidly evolve, older adults face challenges to access systems and devices, which may increase their social isolation. Our project investigates the design of a digital pen and paper-based communication system that allows users to connect to their family and friends' e-mail inboxes. Given the unique needs of older adults, we opted for a participatory design approach, prototyping the system with 22 older adults through a series of design workshops in two locations. Four individuals used our resulting system over a period of two weeks. Based on their feedback and a review of design workshops, we are currently in the process of refining our interface and preparing for a larger deployment study.", 
    "authors": [
      {
        "name": "Taciana  Pontual Falc\u00c3\u00a3O, McGill University"
      }, 
      {
        "name": "Xiaofeng  Yong, McGill University"
      }, 
      {
        "name": "Elisabeth  Sulmont, McGill University"
      }, 
      {
        "name": "Robert Douglas Ferguson, McGill"
      }, 
      {
        "name": "Karyn  Moffatt, McGill University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "A Digital Pen and Paper Email System for Older Adults", 
    "type": "paper"
  }, 
  "uistpp157": {
    "abstract": "Virtual Reality (VR) has numerous mechanisms for making a virtual scene more compellingly real.  Most effort has been focused on visual and auditory techniques for immersive environments, although some commercial systems now include relatively crude haptic effects through handheld controllers or haptic suits.  We present results from a pilot experiment demonstrating the use of Electrical Muscle Stimulation (EMS) to trick participants into thinking a surface is dangerously hot even though it is below 50C.  This is accomplished by inducing an artificial heat withdrawal reflex by contracting the participant's bicep shortly after contact with the virtual hot surface.  Although the effects of multiple experimental confounds need to be quantified in future work, results so far suggest that EMS could potentially be used to modify temperature perception in VR and AR contexts.  Such an illusion has applications for VR gaming as well as emergency response and workplace training and simulation, in addition to providing new insights into the human perceptual system.", 
    "authors": [
      {
        "name": "Pascal E.  Fortin, McGill University"
      }, 
      {
        "name": "Jeffrey R Blum, McGill University"
      }, 
      {
        "name": "Jeremy R Cooperstock, McGill University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Raising the Heat : Electrical Muscle Stimulation for Simulated Heat Withdrawal Response", 
    "type": "paper"
  }, 
  "uistpp158": {
    "abstract": "We propose a method for determining grip force based on active bone-conducted sound sensing, which is an active acoustic sensing. In our previous studies, we estimated the joint angle, hand pose, and contact force by emitting a vibration to the body. We aspired to expand to an additional application of an active bone-conducted sound sensing, thus, we tried to estimate the grip force by creating a wrist-type device. The grip force was determined by using the power spectral density as the features, and gradient boosted regression trees (GBRT). Through evaluation experiments, the average error of the estimated grip force was around 15 N. Moreover, we confirmed that the grip strength could be determined with high accuracy.", 
    "authors": [
      {
        "name": "Nobuhiro  Funato, Tokai University"
      }, 
      {
        "name": "Kentaro  Takemura, Tokai University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Grip Force Estimation by Emitting Vibration", 
    "type": "paper"
  }, 
  "uistpp162": {
    "abstract": "Presbyopia is a symptom in which the elasticity of the lens is weakened and the image is not formed. However, as the use of smart phones increases, the age at which presbyopia symptoms appear is gradually decreasing. The closer the distance is from the smartphone, the less the focus of the eyes will be which making the letters and pictures on the smartphone screen appear blurred. In this study, we conducted a study on the interactions that helped to improve health of eye for people with presbyopia or those who have a habit that can facilitating presbyopia. As the distance of the smartphone from the eye is increased, the font size is increased to upgrading readability and the prototype is tested by 20 experimenters.", 
    "authors": [
      {
        "name": "Jiyeon  Lee, Sungkyunkwan University"
      }, 
      {
        "name": "Inseok  Hong, Sungkyunkwan University"
      }, 
      {
        "name": "Jongsung  Lee, Suwon-si"
      }, 
      {
        "name": "Jundong  Cho, Dep. of Human ICT Convergence"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "EYE DEAR : Smartphone text resizing interaction for the eye health of the presbyopia population", 
    "type": "paper"
  }, 
  "uistpp165": {
    "abstract": "Screencasts, where computer screen is broadcast to a large audience on the web, are becoming popular as an online educational tool. Among various types of screencast content, popular are the contents that involve text editing, including computer programming. There are emerging platforms that support such text-based screencasts by recording every character insertion/deletion from the creator and reconstructing its playback on the viewer's screen. However, these platforms lack rich support for creating and editing the screencast itself, mainly due to the difficulty of manipulating recorded text changes; the changes are tightly coupled in sequence, thus modifying arbitrary part of the sequence is not trivial.   We present a non-linear editing tool for text-based screencasts. With the proposed selective history rewrite process, our editor allows users to substitute an arbitrary part of a text-based screencast while preserving overall consistency of the rest of the text-based screencast.", 
    "authors": [
      {
        "name": "Jungkook  Park, University of Minnesota"
      }, 
      {
        "name": "Yeong Hoon  Park, University of Minnesota"
      }, 
      {
        "name": "Alice  Oh, KAIST"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Non-Linear Editor for Text-Based Screencast", 
    "type": "paper"
  }, 
  "uistpp168": {
    "abstract": "Children with ASD (Autism Spectrum Disorder) have difficulties in expressing their feelings and needs, their teachers have to be very familiar with them to adjust teaching contents in related training lessons. In this paper, we present an adaptive training system with EEG (Electroencephalogram) devices for autistic children. We designed an EEG helmet to monitor children\u00c3\u00a2\u00e2\u0082\u00ac\u00e2\u0084\u00a2s attention levels, and a video chat system with virtual cartoon faces covered on teacher\u00c3\u00a2\u00e2\u0082\u00ac\u00e2\u0084\u00a2s face. Cartoon faces are synchronized with the performer\u00c3\u00a2\u00e2\u0082\u00ac\u00e2\u0084\u00a2s facial movements to help trainers express themselves in an exaggerated way. When the attention reduction is detected by the EEG helmet, cartoon face will adjust automatically, and try to draw their attention back through changing cartoon types, colors, brightness, etc. Each change and feedback from children will be traced by the helmet and analyzed for improvements. By continuous iterative learning, the system will become smarter in avoiding children\u00c3\u00a2\u00e2\u0082\u00ac\u00e2\u0084\u00a2s physical exhaustion. The system was introduced in the form of a specific training lesson to an ASD school, and preliminary experiment has indicated an encouraging result.", 
    "authors": [
      {
        "name": "Cheng  Zheng, Zhejiang University"
      }, 
      {
        "name": "Caowei  Zhang, Zhejiang University"
      }, 
      {
        "name": "Xuan  Li, Hangzhou"
      }, 
      {
        "name": "Bing  Li, Zhejiang University"
      }, 
      {
        "name": "Fan  Zhang, Zhejiang University"
      }, 
      {
        "name": "Xin  Liu, Zhejiang University"
      }, 
      {
        "name": "Cheng  Yao, Zhejiang University"
      }, 
      {
        "name": "Yijun  Zhao, Zhejiang University"
      }, 
      {
        "name": "Fangtian  Ying, Zhejiang University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "An EEG-based Adaptive Training System for ASD Children", 
    "type": "paper"
  }, 
  "uistpp169": {
    "abstract": "We present Ani-Bot, a modular robotics system that allows users to construct Do-It-Yourself (DIY) robots and use mixed-reality approach to interact with them. Ani-Bot enables novel user experience by embedding Mixed-Reality Interaction (MRI) in the three phases of interacting with a modular construction kit, namely, Creation, Tweaking, and Usage. In this paper, we first present the system design that allows users to instantly perform MRI once they finish assembling the robot. Further, we discuss the augmentations offered by MRI in the three phases in specific.", 
    "authors": [
      {
        "name": "Yuanzhi  Cao, Purdue University"
      }, 
      {
        "name": "Zhuangying  Xu, Purdue University"
      }, 
      {
        "name": "Terrell Kendall Glenn, Purdue University"
      }, 
      {
        "name": "Ke  Huo, Purdue University"
      }, 
      {
        "name": "Karthik  Ramani, Purdue University"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Ani-Bot: A Mixed-Reality Modular Robotics System", 
    "type": "paper"
  }, 
  "uistpp172": {
    "abstract": "The emergence of social reading services has enabled readers to participate actively in reading activities by means of sharing and feedback. Readers can state their opinion on a book by providing feedback. However, because current e-books are published with fixed, unchangeable content, it is difficult to reflect the reader's feedback on them. In this paper, we propose a system for an adaptive e-book that dynamically updates itself on user participation. To achieve this, we designed a Feedback Block Model and a Feedback Engine. In the Feedback Block Model, at the time of publication, the author defines the type of feedback expected from readers. After publication, the Feedback Engine collects and aggregates the readers\u00c3\u00a2\u00e2\u0082\u00ac\u00e2\u0084\u00a2 feedback. The Feedback Engine can be configured with drag-and-drop block programming, and hence, even authors inexperienced in programming can create an adaptive e-book.", 
    "authors": [
      {
        "name": "Ja-Ryoung  Choi, KAIST"
      }, 
      {
        "name": "Suin  Kim, KAIST"
      }, 
      {
        "name": "Soon-Bum  Lim, Seoul"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "The Feedback Block Model for an Adaptive E-Book", 
    "type": "paper"
  }, 
  "uistpp178": {
    "abstract": "Voice assistant technology has expanded the design space for voice-activated consumer products and audio-centric user experience. To navigate this emerging design space, Speech Synthesis Markup Language (SSML) provides a standard to characterize synthetic speech based on parametric control of the prosody elements, i.e. pitch, rate, volume, contour, range, and duration. However, the existing voice assistants utilizing Text-to-Speech (TTS) lack expressiveness. The need of a new production workflow for more efficient and emotional audio content using TTS is discussed. A prototype that allows a user to produce TTS-based content in any emotional tone using voice input is presented. To evaluate the new workflow enabled by the prototype, an initial comparative study is conducted against the parametric approach. Preliminary quantitative and qualitative results suggest the new workflow is more efficient based on time to complete tasks and number of design iterations, while maintaining the same level of user preferred production quality.", 
    "authors": [
      {
        "name": "Yuan-Yi  Fan, I.AM+"
      }, 
      {
        "name": "Soyoung  Shin, I.AM+"
      }, 
      {
        "name": "Vids  Samanta, I.AM+"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Contour: An Efficient Voice-enabled Workflow for Producing Text-to-Speech Content", 
    "type": "paper"
  }, 
  "uistpp181": {
    "abstract": "Recent work in 3D printing has focused on tools and techniques to design deformation behaviors using mechanical structures such as joints and metamaterials. In this poster, we explore how to embed and control mechanical springs to create deformable 3D-printed objects. We propose an initial design space of 3D-printable spring-based structures to support a wide range of expressive behaviors, including stretch and compress, bend, twist, and all possible combinations. The poster concludes with a brief feasibility test and enumerates future work.", 
    "authors": [
      {
        "name": "Liang  He, Cornell University"
      }, 
      {
        "name": "Huaishu  Peng, Cornell University"
      }, 
      {
        "name": "Joshua  Land, College Park"
      }, 
      {
        "name": "Mark D Fuge, University of Maryland"
      }, 
      {
        "name": "Jon E Froehlich, University of Maryland, College Park"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Designing 3D-Printed Deformation Behaviors Using Spring-Based Structures: An Initial Investigation", 
    "type": "paper"
  }, 
  "uistpp183": {
    "abstract": "Blind users browse the web using screen readers. Screen readers read the content on a web page sequentially via synthesized speech. The linear nature of this process makes it difficult to obtain an overview of the web page, which creates navigation challenges. To alleviate this problem, we have developed ScreenTrack, a browser extension that summarizes a web page\u00c3\u00a2\u00e2\u0082\u00ac\u00e2\u0084\u00a2s accessibility features into a short, dynamically generated soundtrack. Users can quickly gain an overview of the presence of web elements useful for navigation on a web page. Here we describe ScreenTrack and discuss future research plans.", 
    "authors": [
      {
        "name": "Tao  Wang, UC Irvine"
      }, 
      {
        "name": "David  Redmiles, UC Irvine"
      }
    ], 
    "award": false, 
    "hm": false, 
    "subtype": "paper", 
    "title": "Auditory Overview of Web Pages for Screen Reader Users", 
    "type": "paper"
  }
}