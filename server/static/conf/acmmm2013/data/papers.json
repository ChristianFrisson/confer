entities={"216": {"abstract": "\"Recognition of complex events captured under real-world settings, has emerged ,as a challenging area of research across both computer vision and multimedia ,community. In this dissertation, we present a bottom-up approach towards an ,end to end framework for complex event recognition in widely available, ,consumer uploaded web-scale videos. We structure our approach into the ,following key stages and highlight our novel contributions in each of these ,stages: (a) Extraction of novel semi-global features, (b) Construction of ,semantically meaningful intermediate concept based representations from ,these features, and (c) Modeling temporal interactions between the ,spatio-temporal concepts. Thorough experimental validations in each of these,stages demonstrate the efficacy of our proposed methods in the context of ,complex event recognition.\"", "authors": [{"name": "Subhabrata Bhattacharya"}], "title": "Recognition of Complex Events in Open-Source Web-Scale Videos: A Bottom up approach"}, "217": {"abstract": "\"In the context of a social gathering, such as cocktail party, the precious moments are generally captured by professional photographers or the participants. The latter case is generally undesired because the participants would like to enjoy the event rather than being occupied by the tedious photo capturing task. Motivated by this scenario, we propose an automated social event photo capture framework for which, given the multiple sensor data streams as input, will output the visually appealing photos of the social event. Our proposal consists of three components: (1) social attribute extraction from multiple sensors; (2) social interaction analysis; and (3) active camera control. Progresses and advances have been presented, with expected contribution and future direction concluded in the end.\"", "authors": [{"name": "Tian Gan"}], "title": "Social Interaction Detection Using A Multi-sensor Approach"}, "214": {"abstract": "\"Consider for example the image sequence in the figure on the right. The spider in front of the camera or the snow on the lens are examples of events that deviate from the context since they violate the expectations, and therefore are considered interesting. On the other hand, weather changes or a camera shift, does not considerably raise human attention, even though large regions of the image are influenced. In this work we firstly investigate what humans consider as ``interesting'' in image sequences. Secondly we propose a computer vision algorithm to automatically spot these interesting events. To this end, we integrate multiple cues inspired by cognitive concepts and discuss why and to what extent the automatic discovery of visual interestingness is possible.\"", "authors": [{"name": "Helmut Grabner"}, {"name": " Fabian Nater"}, {"name": " Michel Druey and Luc Van Gool"}], "title": "Visual Interestingness in Image Sequences"}, "215": {"abstract": "\"Inter-Destination Media Synchronization (IDMS) is essential for enabling pleasant shared media experiences. The goal of my PhD thesis is to design, develop and evaluate an advanced RTP/RTCP-based IDMS solution fitting the requirements of the emerging distributed media consumption paradigm. In particular, standard compliant extensions to RTCP are being specified to allow for an accurate and adaptive IDMS control when using RTP for streaming media. Moreover, the suitability of several architectural schemes for exchanging the IDMS information, algorithms for allowing a dynamic IDMS monitoring and control, as well as adjustment techniques to synchronize suitability is being investigated. The satisfactory performance of our IDMS solution is being proved through both simulation and user perception tests. The expected outcome of my PhD thesis is to provide a standardized IDMS solution for managed streaming services.\"", "authors": [{"name": "Mario Montagud"}], "title": "\"Design, Development and Evaluation of an Adaptive and Standardized RTP/RTCP-based IDMS Solution\""}, "212": {"abstract": "\"Real-world scene recognition has been one of the most challenging research topics in computer vision, due to the tremendous intraclass variability and the wide range of scene categories. In this paper, we successfully apply an evolutionary methodology to automatically synthesize domain-adaptive holistic descriptors for the task of scene recognition, instead of using hand-tuned descriptors. We address this as an optimization problem by using multi-objective genetic programming (MOGP). Specifically, a set of primitive operators and filters are first randomly assembled in theMOGP framework as tree-based combinations, which are then evaluated by two,objective fitness criteria i.e., the classification error and the tree complexity. Finally, the best-so-far solution selected by MOGP is regarded as the (near-)optimal feature descriptor for scene recognition.,We have evaluated our approach on three realistic scene datasets: MIT urban and nature, SUN and UIUC Sport. Experimental results consistently show that our MOGP-generated descriptors achieve significantly higher recognition accuracies compared with state-of-the-art hand-crafted and machine-learned features.\"", "authors": [{"name": "Li Liu"}, {"name": " Ling Shao and Xuelong Li"}], "title": "Building Holistic Descriptors for Scene Recognition: A Multi-objective Genetic Programming Approach"}, "213": {"abstract": "\"Scene text is widely observed in our daily life and has many important multimedia applications. Unlike documented text, scene text usually exhibits large variations in font and language, and suffers from low resolution, occlusions and complex background. In this paper, we present a novel scale-based region growing algorithm for scene text detection. We first distinguish SIFT features in text regions from those in background by exploring the inter- and intra-statistics of SIFT features. Then scene text regions in images are identified by scale-based region growing, which explores the geometric context of SIFT keypoints in local regions. Our algorithm is very effective to detect multilingual text in various fonts, sizes, orientations and with complex background. In addition, it offers important clues with efficiency to numerous applications of local features, such as visual search. We evaluate our algorithm on three datasets and achieve the state-of-the-art performance. Furthermore, cross-training experiments among different datasets show that our algorithm is insensitive to the trained classifier.\"", "authors": [{"name": "Junhua Mao"}, {"name": " Houqiang Li"}, {"name": " Wengang Zhou"}, {"name": " Shuicheng Yan and Qi Tian"}], "title": "Scale-based region growing for scene text detection"}, "210": {"abstract": "\"Recent years have witnessed the extensive studies on distance metric learning (DML) for improving visual similarity search in multimedia information retrieval tasks. Despite their successes, most existing DML methods suffer from two critical limitations: (i) they typically attempt to learn a linear distance function on the input feature space, in which the assumption of linearity limits their capacity of measuring the similarity on complex patterns in real-world applications; (ii) they are often designed for learning distance metrics on single-modal data, which may not effectively handle the similarity measures for multimedia objects of multimodal representations. To address these limitations, in this paper, we propose a novel framework of online multimodal deep similarity learning (OMDSL), which aims to optimally integrate multiple deep neural networks pretrained with stacked denoised autoencoder. In particular, the proposed framework explores a unified two-stage online learning scheme that consists of (i) learning a flexible nonlinear transformation function for each individual modality, and (ii) learning to find the optimal combination of multiple diverse modalities simultaneously in a coherent process. We conduct an extensive set of experiments to evaluate the performance of the proposed algorithms for multi-modal image retrieval tasks, in which the encouraging results validate the effectiveness of the proposed technique.\"", "authors": [{"name": "Pengcheng Wu"}, {"name": " Steven C.H. Hoi"}, {"name": " Hao Xia"}, {"name": " Peilin Zhao"}, {"name": " Dayong Wang and Chunyan Miao"}], "title": "Online Multimodal Deep Similarity Learning with Application to Image Retrieval"}, "211": {"abstract": "\"Recently visual saliency has attracted wide attention of researchers in the computer vision and multimedia field. However, most of the visual saliency-related research was conducted on still images for studying static saliency. In this paper, we give a comprehensive comparative study for the first time of dynamic saliency (video shots) and static saliency (key frames of the corresponding video shots), and two key observations are obtained: 1) video saliency is often different from, yet quite related with, image saliency, and 2) camera motions, such as tilting, panning or zooming, affect dynamic saliency signicantly. Motivated by these observations, we propose a novel camera motion and image saliency aware model for dynamic saliency prediction. The extensive experiments on two static-vs-dynamic saliency datasets collected by us show that our proposed method outperforms the state-of-the-art methods for dynamic saliency prediction. Finally, we also introduce the application of dynamic saliency prediction for dynamic video captioning, assisting people with hearing impairments to better entertain videos with only o-screen voices, e.g., documentary films, news videos and sports videos.\"", "authors": [{"name": "Tam Nguyen"}, {"name": " Mengdi Xu"}, {"name": " Guangyu Gao"}, {"name": " Mohan Kankanhalli"}, {"name": " Qi Tian and Shuicheng Yan"}], "title": "Static Saliency vs. Dynamic Saliency: A Comparative Study"}, "164": {"abstract": "\"This paper presents a novel structural model based scene recognition method. In order to resolve regular grid image division methods which cause low content discriminability for scene recognition in previous methods, we partition an image into a pre-defined set of regions by superpixel segmentation. And then classification is modelled by introducing a structural model which has the capability of organizing unordered features of image patches. In the implementation, CENTRIST which is robust to scene recognition is used as original image feature, and bag-of-words representation is used to capture the local appearances of an image. In addition, we incorporate adjacent superpixel's differences as edge features. Our models are trained using structural SVM. Two state-of-art scene datasets are adopted to evaluate the proposed method. The experiment results show that the recognition accuracy is significantly improved by the proposed method.\"", "authors": [{"name": "Shuhui Bu"}, {"name": " Zhenbao Liu"}, {"name": " Kun Zhou"}], "title": "Superpixel Segmentation based Structural Scene Recognition  "}, "130": {"abstract": "\"The Waisda? video labeling game is a crowsourcing tool to collect user-generated metadata for video clips. It follows the paradigm of games-with-a-purpose, where two or more users play against each other by entering tags that describe the content of the video. Players score points by entering the same tags as one of the other players. As a result each video that is played in the game is annotated with tags that are anchored to a time point in the video. Waisda? has been deployed in two pilot projects with videos from Dutch broadcasters. With the open source version of Waisda? crowdsourcing of video annotation becomes available for any online video collection.\"", "authors": [{"name": "Michiel Hildebrand"}, {"name": " Maarten Brinkerink"}, {"name": " Riste Gligorov"}, {"name": " Martijn Steenbergen Van"}, {"name": " Johan Huijkman"}, {"name": " Johan Oomen"}], "title": "Waisda? Video Labeling Game"}, "137": {"abstract": "\"3D collaborative Tele-Immersive environments allow reconstruction of real world 3D scenes in the virtual world across multiple physical locations. This kind of reconstruction results in a lot of 3D data being transmitted over the internet in real time. The current systems allow for transmission at low frame rates due to the large volume of data and network bandwidth restrictions. In this paper we propose a prediction based approach that generates future frames by animating the live model based on few skeleton points. By doing so the magnitude of data transmitted is reduced to few hundred bytes. The prediction errors are corrected when an entire frame is received. This approach allows minimal amounts (few bytes) of data to be transmitted per frame, thus allowing for high frame rates and still maintain an acceptable visual quality of reconstruction at the receiver side.\"", "authors": [{"name": "Karthik Venkatraman"}, {"name": " Suraj Raghuraman"}, {"name": " Balakrishnan Prabhakaran"}, {"name": " Xiaohu Guo"}, {"name": " Zhanyu Wang"}], "title": "A 3D TELE-IMMERSION STREAMING APPROACH USING SKELETON-BASED PREDICTION "}, "136": {"abstract": "\"In this paper, we propose a new pipeline to synthesize virtual views. It allows us to generate virtual views far away from each other, each presenting the exact same level of quality. This inter-view consistency is key to seamlessly navigate between viewpoints. Its computational cost is also lower than that of existing approaches. We compare the proposed approach with state-of-the-art methods and show the effectiveness of this new view synthesis pipeline.\"", "authors": [{"name": "David Wolinski"}, {"name": " Olivier Le Meur"}, {"name": " Josselin Gautier"}], "title": "3D view synthesis with inter-view consistency "}, "135": {"abstract": "\"Being able to detect and recognize human activities is essential for 3D collaborative applications for efficient quality of service provisioning and device management. A broad range of research has been devoted to analyze media data to identify human activity, which requires the knowledge of data format, application-specific coding technique and computationally expensive image analysis. In this paper, we propose human activity  detection technique based on application generated metadata and corresponding system metadata. Our approach does not depend on specific data format or coding technique. We test our algorithm with different cyber-physical setup, and show that a high accuracy (about 97%) can be achieved by using a careful learning model.\"", "authors": [{"name": "Aadhar Jain"}, {"name": " Ahsan Arefin"}, {"name": " Raoul Rivas"}, {"name": " Chien-Nan Chen"}, {"name": " Klara Nahrstedt"}], "title": "3D Teleimmersive Activity Classification Based on Application-System Metadata "}, "165": {"abstract": "\"A platform of exploratory networked robotic cameras was created, informing new directions in computer vision engineering and utilizing an aesthetic approach to experimentation. Initiated by research in autonomous swarm robotic camera behavior, SwarmVision is an installation consisting of multiple Pan-Tilt-Zoom cameras on rails positioned above spectators in an exhibition space, where each camera behaves autonomously based on its own rules of computer vision and control. Each of the cameras is programmed to detect visual information of interest based on a different algorithm, and each negotiates with the other two, influencing what subject matter to study in a collective way.  The emergent behaviors of the system suggest potential new approaches in scene reconstruction, video-based behavior analysis and other areas of vision and imaging research\"", "authors": [{"name": "Danny Bazo"}, {"name": " George Legrady"}, {"name": " Marco Pinter"}], "title": "Swarm Vision "}, "139": {"abstract": "\"In this paper, we propose a multimodal approach to create the mapping between gesture and sound in interactive music systems. Specifically, we propose to use a multimodal HMM to conjointly model gesture and sound parameters. Our approach is compatible with a learning method that allows users to define the gesture_sound relationships interactively. We describe an implementation of this method for the control of physical modeling sound synthesis. Our model is promising to capture expressive gesture variations while guaranteeing a consistent relationship between gesture and sound.\"", "authors": [{"name": "Jules Fran\u008doise"}, {"name": " Norbert Schnell"}, {"name": " Frederic Bevilacqua"}], "title": "A multimodal probabilistic model for gesture-based control of sound synthesis "}, "138": {"abstract": "\"In this paper, we propose a novel networked animation system to generate and stream 3D volumetric deformation of soft objects. Multigrid technique is used in this framework to accelerate the converging rate of the optimization of the nonlinear deformation energy. The computation is performed from coarsest mesh at the top level to the finest mesh at the bottom level and then goes back to top again. Such V-shape calculation provides great flexibility for networked environment. Clients are able to receive the data streaming corresponding to different mesh resolution. A more compact form of deformation data packaging is also used in this system such that an cube element only needs six parameters instead of 24 variables as used in regular mesh representation, which significantly reduces the network overhead for the streaming.\"", "authors": [{"name": "Yuan Tian"}, {"name": " Yin Yang"}, {"name": " Xiaohu Guo"}, {"name": " Balakrishnan Prabhakaran"}], "title": "A Multigrid Approach for Bandwidth and Display Resolution Aware Streaming of 3D Deformations "}, "166": {"abstract": "\"This paper proposes to use the Fisher representation to capture temporal variation in video. In particular, we propose to represent a set of features extracted from each frame of a video with respect to the gradient of the log-likelihood of a previously learned Gaussian Mixture Model. While the temporal order is lost, it captures both subtle temporal variations such as the ones caused by a moving bicycle and drastic temporal variations such as the changing of shots in a documentary.,,We demonstrate that our framework is highly general. We report improvements using a high variety of features, ranging from frame-based features, to body-part based features, and even to audio features. Additionally, we report excellent results on a wide variety of datasets: we obtain state-of-the-art results on the UCF50 sports dataset. We improve the state-of-the-art on the MediaEval 2012 genre classification benchmark and the ADL daily activity recognition dataset.\"", "authors": [{"name": "Ionut Mironica"}, {"name": " Jasper Uijlings"}, {"name": " Negar Rostamzadeh"}, {"name": " Bogdan Ionescu"}, {"name": " Nicu Sebe"}], "title": "Time Matters! Capturing Temporal Variation in Video using Fisher Kernels "}, "24": {"abstract": "\"To preserve the precious traditional heritage Chinese shadow puppetry, we propose the puppetry eHeritage, including a creator module and a manipulator module. The creator module accepts a frontal view face image and a profile face image of the user as input, and automatically generates the corresponding puppet, which looks like the original person and meanwhile has some typical characteristics of traditional Chinese shadow puppetry.  In order to create the puppet, we first extract the central profile curve and warp the reference puppet eye and eyebrow to the shape of the frontal view eye and eyebrow. Then we transfer the puppet texture to the real face area. The manipulator module can accept the script provided by the user as input and automatically generate the motion sequences.  Technically, we first learn atomic motions from a set of shadow puppetry videos. A scripting system converts the user's input to atomic motions, and finally synthesizes the animation based on the atomic motion instances. For better visual effects, we propose the sparsity optimization over simplexes formulation to automatically assemble weighted instances of different atomic actions into a smooth shadow puppetry animation sequence. We evaluate the performance of the creator module and the manipulator module sequentially. Extensive experimental results on the creation of puppetry characters and puppetry plays well demonstrate the effectiveness of the proposed system.\"", "authors": [{"name": "Min Lin"}, {"name": " Zhenzhen Hu"}, {"name": " Si Liu"}, {"name": " Richang Hong"}, {"name": " Meng Wang and Shuicheng Yan"}], "title": "eHeritage of Shadow Puppetry: Creation and Manipulation"}, "25": {"abstract": "\"In this paper, we present a method to generate aesthetic video from a robotic camera by incorporating a virtual camera operating on a delay, and a hybrid controller which uses feedback from both the robotic and virtual cameras. Our strategy employs a robotic camera to follow a coarse region-of-interest identified by a computer vision system, and then resamples the captured images to synthesize the video that would have been recorded along a smooth, aesthetic camera trajectory. The smooth motion trajectory is obtained by operating the virtual camera on a short delay so that perfect knowledge of immediate future events is known.,,Previous autonomous camera installations have employed either robotic cameras or stationary wide-angle cameras with subregion cropping. Robotic cameras track the subject using realtime sensor data, and regulate a smoothness-latency trade-off through control gains. Fixed cameras, on the other hand, post-process the data and suffer significant reductions in image resolution when the subject moves freely over a large area.,,Our approach provides a solution for broadcasting events from locations where camera operators cannot access, and offers broadcasters additional cameras angles without the overhead of additional human operators. Experiments on our prototype system for college basketball illustrate how our approach better mimics human operators compared to traditional robotic control approaches, while avoiding the loss in resolution that occurs from fixed camera system.\"", "authors": [{"name": "Peter Carr"}, {"name": " Mike Mistry and Iain Matthews"}], "title": "Hybrid Robotic/Virtual Pan-Tilt-Zoom Cameras for Autonomous Event Recording"}, "26": {"abstract": "\"Connecting people to the resources they need is a fundamental task for any society. We present the idea of a technology that can be used by the middle tier of a society so that it uses people's mobile devices and social networks to connect the needy with providers. We conceive of a world observatory called the Social Life Network (SLN) that connects together people and things and monitors for people's needs as their life situations evolve. To enable such a system we need SLN to register and recognize situations by combining people's activities and data streaming from personal devices and environment sensors, and based on the situations make the connections when possible. But is this a multimedia problem? We show that many pattern recognition, machine learning, sensor fusion and information retrieval techniques used in multimedia-related research are deeply connected to the SLN problem. We sketch the functional architecture of such a system and show the place for these techniques.\"", "authors": [{"name": "Amarnath Gupta; Ramesh Jain"}], "title": "Social Life Networks: A Multimedia Problem?"}, "27": {"abstract": "\"One of the main findings of cognitive sciences is that automatic processes of which we are unaware shape, to a significant extent, our perception of the environment. The phenomenon applies not only to the real world, but also to multimedia data we consume every day. Whenever we look at pictures, watch a video or listen to audio recordings, our conscious attention efforts focus on the observable content, but our cognition spontaneously perceives intentions, beliefs, values, attitudes and other constructs that, while being outside of our conscious awareness, still shape our reactions and behavior. So far, multimedia technologies have neglected such a phenomenon to a large extent. This paper argues that taking into account cognitive effects is possible and it can also improve multimedia approaches. As a supporting proof-of-concept, the paper shows not only that there are visual patterns correlated with the personality traits of 300 Flickr users to a statistically significant extent, but also that the personality traits (both self-assessed and attributed by others) of those users can be inferred from the images these latter post as \"\"favourite\"\" \"", "authors": [{"name": "Marco Cristani; Alessandro Vinciarelli; Cristina Segalin; Alessandro Perina"}], "title": "Unveiling the Multimedia Unconscious: Implicit Cognitive Processes and Multimedia Content Analysis"}, "20": {"abstract": "\"This paper studies the use of everyday words to describe images. The common saying has it that a picture is worth a thousand words, here we ask which thousand? The pro- liferation of tagged social multimedia data presents a chal- lenge to understanding collective tag use at large scale _ one can ask if patterns of photo tag use help understand tag-tag relations, and how it can be leveraged to improve visual search and recognition. We propose a new method to jointly analyzing three distinct visual knowledge resources: Flickr, ImageNet/WordNet, and ConceptNet. This allows us to quantify the visual relevance of both tags and their relationships. We propose a novel network estimation algo- rithm, Inverse Concept Rank, to infer incomplete tag rela- tionships. We then design an algorithm for image annotation that takes into account both image and tag features. We analyze over 5 million photos with over 20,000 visual tags. The statistics from this collection leads to good results for image tagging, relationship estimation, and generalizing to unseen tags. This is a first step in analyzing picture tags and everyday semantic knowledge. Potential other appli- cations include generating natural language descriptions of pictures, as well as validating and supplementing knowledge databases.\"", "authors": [{"name": "Lexing Xie and Xuming He"}], "title": "Picture Tags and World Knowledge: Learning Tags and Tag Relations from Visual Semantic Sources"}, "21": {"abstract": "\"Automatic tagging of videos is a key to many video applications, such as browsing, search, and recommendation. The research on this topic, either model-based or data driven-based methodologies, highly relies on computing the video similarity in a high dimensional feature or semantic space. This is particularly unreliable and computationally expensive for video in which large variance and high complexity exist. To address the above issue, we propose a novel crowdsourcing approach to video tagging by learning high order video relationship from user searching behavior. Based on the analysis on a large-scale click-through data, we have found that user searching behavior brings great potential for video tagging, as the query-click data naturally build latent semantic relationship between videos (clicks) and tags (queries). We first learn relationship between videos by co-click-based and polynomial semantic video similarity. We then propose two video tagging methods, i.e., weighted neighbor tagging and tag propagation. Our approach can solve tagging, tag ranking, and tag enrichment in one single framework. We conduct experiments on a collection of click-through data with over 15 million queries and 20 million clicked video URLs during November to December 2012 from a commercial video search engine. We show that our proposed crowdsourcing approach is highly computationally efficient, easy to scale up, and extensible with more click-through data. Thus, it represents a promising paradigm to solve tagging problem.\"", "authors": [{"name": "Ting Yao"}, {"name": " Tao Mei"}, {"name": " Chong-Wah Ngo and Shipeng Li"}], "title": "Annotate for Free: Video Tagging by Mining User Search Behavior"}, "22": {"abstract": "\"Live sports broadcasts require a coordinated team of camera operators, directors, and technical personnel to control and switch between multiple cameras to tell the evolving story of a game.  In this paper, we present an unimodal interface concept that allows one person to cover live sporting action by controlling multiple cameras and by directing which view to broadcast. The interface exploits the structure of sports broadcasts which typically switch between a zoomed out (game-camera) view to capture the team-level play and a zoomed in (iso-camera) view to capture the player-level focus of the play. The operator controls live pan-tilt-zoom cameras by pointing at a location on the touch screen, and selects a camera by switching between one or two points of contact. The selected view is superimposed on top of a wide-angle view from a context-camera to provide spatiotemporal context for the control of each camera. We show that by unifying directorial and camera control functions, we can achieve comparable broadcast quality to a multi-person crew, while reducing cost, logistical, and communication complexities.\"", "authors": [{"name": "Peter Carr"}, {"name": " Patrick Lucey"}, {"name": " Iain Matthews"}, {"name": " Eric Foote and Yaser Sheikh"}], "title": "One-Man-Band: A Touch Screen Interface for Producing Live Multi-Camera Sports Broadcasts"}, "23": {"abstract": "\"Currently, human_computer interaction (HCI) is primarily focused on human-centric interactions; however, people experience many nonhuman-centric interactions during the course of a day. Interactions with nature, such as experiencing the sounds of birds or trickling water, can imprint the beauty of nature in our memory. In this context, this paper presents an interface of such nonhuman-interactions to observe people\u00cds reaction to the interactions through an imaginable interaction with a mythological creature. Tele Echo Tube (TET) is a speaking tube interface that acoustically interacts with a deep mountain echo through the slightly vibrating lampshade like interface.  TET allows users to interact with the mountain echo in real time through an augmented echo sounding experience with the vibration over satellite data network. This novel interactive system can create an imaginable presence of the mythological creature in the undeveloped natural locations beyond our cultural and imaginable boundaries. The results indicate that users take the reflection of the sound as a cue that triggers the non-linguistic believability in a form of mythological metaphor of the mountain echo.This echo-like experience of believable interaction in augmented reality between a human and gave the users an imaginable presence of mountain echo with high degree of excitement. This paper describes the development and integration of non-human-centric design protocols, requirements, methods, and context evaluation.\"", "authors": [{"name": "Hiroki Kobayashi"}, {"name": " Michitaka Hirose"}, {"name": " Akio Fujiwara"}, {"name": " Kazuhiko Nakamura"}, {"name": " Seaki Kaoru and Kaoru Saito"}], "title": "Tele Echo Tube: beyond Cultural and Imaginable Boundaries"}, "160": {"abstract": "\"One of the most crucial techniques associated with Computer Vision is technology that deals with facial recognition, especially, the automatic estimation of facial expressions. However, in real-time facial expression recognition, when a face turns sideways, the expressional feature extraction becomes difficult as the view of camera changes and recognition accuracy degrades significantly. Therefore, quite many conventional methods are proposed, which are based on static images or limited to situations in which the face is viewed from the front. In this paper, a method that uses Look-Up-Table (LUT) AdaBoost combining with the three-dimensional average face is proposed to solve the problem mentioned above. In order to evaluate the proposed method, the experiment compared with the conventional method was executed. These approaches show promising results and very good success rates. This paper covers several methods that can improve results by making the system more robust.\"", "authors": [{"name": "Jinhui Chen"}, {"name": " Yasuo Ariki"}, {"name": " Tetsuya Takiguchi"}], "title": "Robust Facial Expressions Recognition Using 3D Average Face and Ameliorated AdaBoost "}, "28": {"abstract": "\"We address the challenge of sentiment analysis from visual content. In contrast to existing methods which infer sentiment or emotion directly from visual low-level features, we propose a novel approach based on understanding of the visual concepts that are strongly related to sentiments. Our key contribution is two-fold: first, we present a method built upon psychological theories and web mining to automatically construct a large-scale Visual Sentiment Ontology (VSO) consisting of more than 3,000 Adjective Noun Pairs (ANP). Second, we propose SentiBank, a novel visual concept detector library that can be used to detect the presence of 1,200 ANPs in an image. The VSO and SentiBank are distinct from existing work and will open a gate towards various applications enabled by automatic sentiment analysis. Experiments on detecting sentiment of image tweets demonstrate significant improvement in detection accuracy when comparing the proposed SentiBank based predictors with the text-based approaches. The effort also leads to a large publicly available resource consisting of a visual sentiment ontology, a large detector library, and the training/testing benchmark for visual sentiment analysis.\"", "authors": [{"name": "Damian Borth; Rongrong Ji; Thomas Breuel; Shih-Fu Chang"}], "title": "Large-scale Visual Sentiment Ontology and Detectors Using Adjective Noun Pairs"}, "29": {"abstract": "\"Action recognition is an important problem in multimedia understanding. This paper addresses this problem by building an expressive model for automatic action recognition from video data.We model one action instance in the video with an ensemble of spatio-temporal compositions: a number of discrete temporal anchor frames, each of which is further decomposed to a layout of deformable parts. In this way, our model can identify a Spatio-Temporal And-Or Graph (STAOG) to represent the latent structure of actions e.g., triple jumping, swinging and high jumping. The STAOG model comprises four layers: (i) a batch of leaf-nodes in bottom for detecting various action parts within video patches; (ii) the or-nodes over bottom, i.e. switch variables to activate their children leaf-nodes for structural variability; (iii) the and-nodes within an anchor frame for verifying spatial composition; and (iv) the root-node at top for aggregating scores over temporal anchor frames. Moreover, the contextual interactions are defined between leaf-nodes in both spatial and temporal domains. In the model training stage, we develop a novel discriminative learning algorithm which iteratively determine the structural configuration (e.g., the production of leaf-nodes associated with the or-nodes) along with the optimization of multi-layer parameters. The learning method is weakly supervised without requiring elaborate supervision and initialization. By fully exploiting spatio-temporal compositions and interactions, our approach handles well large intra-class action variance (e.g., different views, individual appearances, spatio-temporal structures). The experimental results on the challenging databases (i.e. UCF YouTube and Olympic sports) demonstrate superior performance of our approach over other state-of-the-arts methods.\"", "authors": [{"name": "Xiaodan Liang"}, {"name": " Liang Lin and Liangliang Cao"}], "title": "Learning Latent Spatio-Temporal Compositional Model for Human Action Recognition"}, "161": {"abstract": "\"In this paper, we introduce a new approach to music structure segmentation that is based on the joint estimation of structural segments, keys and chords in one probabilistic framework. More precisely, the boundaries of a structure segment are determined by detecting key changes and by utilizing the difference in prior probability of chord transitions according to their position in a structural segment. In contrast to many of the recent approaches to structural segmentation, this system does not work with self-similarity matrices, although it has been designed to integrate this kind of approach into the framework at a later stage. However, just the current version of the system, using only the estimated harmony, is already producing encouraging results, especially with respect to the precise localization of the boundaries.\"", "authors": [{"name": "Johan Pauwels"}, {"name": " Geoffroy Peeters"}], "title": "\"Segmenting music through the joint estimation of keys, chords and structural boundaries \""}, "4": {"abstract": "\"This paper considers the problem of automatically discovering geo-informative attributes for location recognition and exploration. The attribute is expected to be both discriminative and representative, which corresponds to a distinctive visual pattern and associates with semantic interpretation. For solution, we analyze the attribute at region level. Each segmented region in the training set is assigned a binary latent variable indicating its discriminative capability. A latent learning framework is proposed for discriminative region detection and geo-informative attribute discovery. Moreover, we use user-generated content to obtain the semantic interpretation for the discovered visual attribute. The proposed approach are evaluated on one challenging dataset including GoogleStreetView and Flickr photos. Experimental results show that: (1) geo-informative attribute are discriminative and useful for location recognition; (2) the discovered semantic interpretation is meaningful and can be exploited for further explorations.\"", "authors": [{"name": "Quan Fang"}, {"name": " Jitao Sang and Changsheng Xu"}], "title": "GIANT: Geo-Informative Attributes for locatioN recogniTion and exploration"}, "8": {"abstract": "\"It is undoubtedly that an image's content determines how people assess the image's aesthetic level. Previous works have shown that image contrast, saliency features, and the composition of objects may jointly decide whether an image looks good or not. In addition to the ``content'' of an image, however, the way an image is ``presented'' to viewers may also affect how much it is appreciated. For example, one might expect a picture always looks better when it is displayed in a larger size. Is this ``the-bigger-the-better'' rule always true? If not, under what situations it becomes invalid?,,This paper devotes to analyze how an image's resolution (pixels) and physical dimension (inches) affect how much viewers appreciate this image. Based on a large-scale aesthetic assessments of 100 images displayed in a variety of resolutions and physical dimensions, we show that an image's display size significantly affects its aesthetic rating in a complicated way; normally a picture looks better with a bigger display size, but it may look relatively worse depending on its content. We develop a set of regression models to predict a picture's absolute and relative aesthetic levels at a given display size based on its content and compositional features, and, simultaneously, we analyze the essential features that lead to the size-dependent property of image aesthetics. We hope that this work will shed some light on future research by revealing that both content and presentation should be considered in image aesthetics evaluation.\"", "authors": [{"name": "Wei-Ta Chu"}, {"name": " Yu-Kuang Chen and Kuan-Ta Chen"}], "title": "Size Does Matter: How Does Image Display Size Affect Aesthetic Perception?"}, "119": {"abstract": "\"Analyzing web content, particularly multimedia content, for security applications is of great interest. However, it often requires deep expertise in data analytics that is not always accessible to non-experts. Our approach is to use scientific workflows that capture expert-level methods to examine web content. We use workflows to analyze the image and text components of multimedia web posts separately, as well as by a multimodal fusion of both image and text data. In particular, we re-purpose workflow fragments to do the multimedia analysis and create additional components for the fusion of the image and text modalities. In this paper, we present preliminary work which focuses on a Human Trafficking Detection task to help deter human trafficking of minors by thus fusing image and text content from the web. We also examine how workflow fragments save time and effort in multimedia content analysis while bringing together multiple areas of machine learning and computer vision. We further export these workflow fragments using linked data as web objects.\"", "authors": [{"name": "Ricky Sethi"}, {"name": " Yolanda Gil"}, {"name": " Hyunjoon Jo and Andrew Philpot"}], "title": "Multimedia Content Analysis for Security Applications Using Scientific Workflows"}, "120": {"abstract": "\"We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Affero GPL license. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. The library is also wrapped in Python and includes a number of predefined executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music descriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, specifically the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications.\"", "authors": [{"name": "Dmitry Bogdanov"}, {"name": " Nicolas Wack"}, {"name": " Emilia G\u00d1mez"}, {"name": " Sankalp Gulati"}, {"name": " Perfecto Herrera"}, {"name": " Oscar Mayor"}, {"name": " Gerard Roma"}, {"name": " Justin Salamon"}, {"name": " Jose Zapata"}, {"name": " Xavier Serra"}], "title": "ESSENTIA: an Audio Analysis Library for Music Information Retrieval"}, "121": {"abstract": "\"While cloud gaming opens new business opportunity, it also poses tremendous challenges as the Internet only provides best-effort service and gamers are hard to please. Although researchers have various ideas to improve cloud gaming systems, existing cloud gaming systems are closed and proprietary, and cannot be used to evaluate these ideas. We present GamingAnywhere, the first open-source cloud gaming system, which is extensible, portable, and configurable. GamingAnywhere may be used by: (i) researchers and engineers to implement and test their new ideas, (ii) service providers to develop cloud gaming services, and (iii) gamers to set up private cloud gaming systems. Details on GamingAnywhere are given in this paper. We firmly believe GamingAnywhere will stimulate many studies on cloud gaming and highly-interactive distributed systems.\"", "authors": [{"name": "Chun-Ying Huang"}, {"name": " De-Yu Chen"}, {"name": " Cheng-Hsin Hsu "}, {"name": " Kuan-Ta Chen"}], "title": "GamingAnywhere: An Open-Source Cloud Gaming Testbed"}, "122": {"abstract": "\"Golden Retriever Image Retrieval Engine (GRire) is an open source light weight Java library developed for Content Based Image Retrieval (CBIR) tasks, employing the Bag of Visual Words (BOVW) model. It provides a complete framework for creating CBIR system including image analysis tools, classifiers, weighting schemes etc., for efficient indexing and retrieval procedures. Its eminent feature is its extensibility, achieved through the open source nature of the library as well as a user-friendly embedded plug-in system. GRire is available on-line along with install and development documentation on http://www.grire.net and on its Google Code page http://code.google.com/p/grire. It is distributed either as a Java library or as a standalone Java application, both GPL licensed.\"", "authors": [{"name": "Lazaros Tsochatzidis"}, {"name": " Chryssanthi Iakovidou"}, {"name": " Savvas Chatzichristofis"}, {"name": " Yiannis Boutalis"}], "title": "Golden Retriever - A Java Based Open Source Image Retrieval Engine"}, "123": {"abstract": "\"ImpoveMyCity is an open source platform that enables residents to directly report to their public administration local issues about their neighborhood such as discarded trashbins, faulty street lights, broken tiles on sidewalks, illegal advertising boards, etc. The reported issues are automatically transmitted to the appropriate office in public administration so as to schedule their settlement. Reporting is feasible both through a web- and a smartphone-based front-end that adopt a map-based visualization, which makes reporting a user-friendly and intriguing process. The management and routing of incoming issues is performed through a back-end infrastructure that serves as an integrated management system with easy to use interfaces. Apart from reporting a new issue, both front-ends allow the citizens to add comments or vote on existing issues, which adds a social dimension on the collected content. Finally, the platform makes also provision for informing the citizens about the progress status of the reported issue and in this way facilitate the establishment of a two-way dialogue between the citizen and public administration.\"", "authors": [{"name": "Ioannis Tsampoulatidis"}, {"name": " Dimitrios Ververidis"}, {"name": " Panagiotis Tsarchopoulos"}, {"name": " Spiros Nikolopoulos"}, {"name": " Ioannis Kompatsiaris"}, {"name": " Nicos Komninos"}], "title": "ImproveMyCity - An open source platform for direct citizen-government communication"}, "124": {"abstract": "\"Content based image retrieval has been around for some time. There are lots of different test data sets, lots of published methods and techniques, and manifold retrieval challenges, where content based image retrieval is of interest. LIRE is a Java library, that provides a simple way to index and retrieve millions of images based on the images' contents. LIRE is robust and well tested and is not only recommended by the websites of ImageCLEF and MediaEval, but is also employed in industry. This paper gives an overview on LIRE, its use, capabilities and reports on retrieval and runtime performance.\"", "authors": [{"name": "Mathias Lux"}], "title": "LIRE: Open Source Image Retrieval in Java"}, "125": {"abstract": "\"In this paper, we present Orcc (Orcc is available at http://orcc.sf.net), an open-source development environment that aims to enhance multimedia development by offering all the advantages of dataflow programming: flexibility, portability and scalability. To do so, Orcc embeds two rich eclipse-based editors that provide an easy writing of dataflow applications, a simulator that permits the quick validation of the written code, and a multi-target compiler that is able to translate any dataflow program, written in the RVC-CAL language, into an equivalent description in both hardware and software languages. Orcc has already been used to successfully write tens of multimedia applications, such as a video decoder supporting the new High Efficiency Video Coding standard, that clearly demonstrates the viability of the environment to develop complex applications. Moreover, the results shows scalable performances on multi-core platforms and achieves real-time decoding frame-rate on HD sequences.\"", "authors": [{"name": "Herv_ Yviquel"}, {"name": " Antoine Lorence"}, {"name": " Khaled Jerbi"}, {"name": " Gildas Cocherel"}, {"name": " Alexandre Sanchez"}, {"name": " Micka\u00d4l Raulet"}], "title": "Orcc: Multimedia development made easy"}, "126": {"abstract": "\"We present recent developments in the openSMILE feature extraction toolkit, which unites feature extraction paradigms from speech, music, and general sound events with basic video features for multi-modal processing. Descriptors from audio and video can be processed jointly in a single framework allowing for time synchronization of parameters, on-line incremental processing as well as off-line and batch processing, and the extraction of statistical functionals of the contour, such as moments, peaks, regression parameters, etc. Postprocessing of the features includes statistical classifiers such as support vector machine models or export to popular toolkits such as Weka or HTK. Available low-level descriptors include popular speech, music and video features including Mel-frequency and similar cepstral and spectral coefficients, Chroma, CENS, loudness, voice quality, local binary pattern and color histograms. Besides, voice activity detection, pitch tracking and face detection are supported. openSMILE is implemented in C++, using standard open source libraries for on-line audio and video input. It is fast, runs on Unix and Windows platforms, and has a modular, component based architecture which makes extensions via plug-ins easy. openSMILE is distributed under a research license and can be downloaded from http://opensmile.sourceforge.net/. \"", "authors": [{"name": "Florian Eyben"}, {"name": " Felix Weninger"}, {"name": " Florian Gro\u00a4"}, {"name": " Bj_rn Schuller"}], "title": "\"Recent Developments in openSMILE, the Munich Open-Source Multimedia Feature Extractor\""}, "127": {"abstract": "\"Cave Automatic Virtual Environments (CAVEs) are small enclosed virtual reality rooms that are made using multiple projectors displaying images on the front and side walls as well as the floor. The projected images from all the projectors are synchronised by a computer that controls the virtual environment. Traditionally a CAVE is used by a single user at any one time and by utilising some form of motion sensing the user's head position can be tracked to allow for first person virtual perception. Often stereoscopic projectors are used to complete the virtual reality effect. Professional CAVE installations are very expensive and can cost upwards of several hundred thousand dollars. CAVEs are becoming increasingly popular as the costs of high resolution projectors and high power graphics cards are decreasing thereby allowing advanced computer users to construct amateur CAVEs. However, one of the greatest disadvantages of CAVE systems has always been the lack of low cost specialised 3D content creation software. In this paper we present an open source and easy to use CAVE software creation toolkit called SCReen Adjusted Panoramic Effect (SCRAPE). We believe that SCRAPE is a key component in an overall vision that aims to bring easy to set up, easy to use, portable CAVE systems to all types of non-expert users.\"", "authors": [{"name": "Carl Flynn"}, {"name": " David Monaghan"}, {"name": " Noel E. O Connor"}], "title": "SCReen Adjusted Panoramic Effect - SCRAPE"}, "128": {"abstract": "\"In this paper we present Stage Framework, an HTML5 and CSS3 framework for digital book, magazine and newspaper publishing. The framework offers publishers the means and tools for publishing editorial content in the HTML5 format using a single web application. The approach is cross-platform and is based on open web standards. Stage Framework serves as an alternative for platform-specific native publications using pure HTML5 to deliver book, magazine and newspaper content while retaining the familiar gesture interaction of native applications. Available gesture actions include for example the page swipe and kinetic scrolling. The magazine browsing view relies entirely on CSS3 3D Transforms and Transitions, thus utilizing hardware acceleration in most devices and platforms. The web application also features a magazine stand which, can be used to offer issues of multiple publications. Developed as a part of master\u00cds thesis research, the framework has been published under the GPL and MIT licenses and is available to everyone via the framework website and the GitHub repository.\"", "authors": [{"name": "Rami Aamulehto"}, {"name": " Mikko Kuhna"}, {"name": " Pirkko Oittinen"}], "title": "Stage Framework - An HTML5 and CSS3 Framework for Digital Publishing"}, "129": {"abstract": "\"Automatic detection and interpretation of social signals carried by voice, gestures, mimics, etc. will play a key-role for next-generation interfaces as it paves the way towards a more intuitive and natural human-computer interaction. The paper at hand introduces Social Signal Interpretation (SSI), a framework for real-time recognition of social signals. SSI supports a large range of sensor devices, filter and feature algorithms, as well as, machine learning and pattern recognition tools. It encourages developers to add new components using SSI's C++ API, but also addresses front end users by offering a XML-based language to build pipelines with a text editor. SSI is freely available under GPL at http://openssi.net.\"", "authors": [{"name": "Johannes Wagner"}, {"name": " Florian Lingenfelser"}, {"name": " Tobias Baur"}, {"name": " Ionut Damian"}, {"name": " Felix Kistler"}, {"name": " Elisabeth Andre"}], "title": "The Social Signal Interpretation (SSI) Framework - Multimodal Signal Processing and Recognition in Real-Time"}, "167": {"abstract": "\"Social media platforms now allow users to share images alongside their textual posts. These image tweets make up a fast-growing percentage of tweets, but have not been studied in depth unlike their text-only counterparts.,,We study a large corpus of image tweets in order to uncover what people post about and the correlation between the tweet's image and its text. We show that an important functional distinction is between visually-relevant and visually-irrelevant tweets, and that we can successfully build an automated classifier utilizing text, image and social context features to distinguish these two classes, obtaining a macro F1 of 70.5%.\"", "authors": [{"name": "Tao Chen"}, {"name": " Dongyuan Lu"}, {"name": " Min-Yen Kan"}, {"name": " Peng Cui"}], "title": "Understanding and Classifying Image Tweets "}, "118": {"abstract": "\"The field of image processing in the encrypted domain has been given increasing attention for the extensive potential applications, for example, providing efficient and secure solutions for privacy-preserving applications in untrusted environment. One obstacle to the widespread use of these techniques is the ciphertext expansion of high orders of magnitude caused by the existing homomorphic encryptions. In this paper, we provide a way to tackle this issue for image processing in the encrypted domain. By using characteristics of image format, we develop an image encryption scheme to limit ciphertext expansion while preserving the homomorphic property. The proposed encryption scheme first encrypts image pixels with an existing probabilistic homomorphic cryptosystem, and then compresses the whole encrypted image in order to save storage space. Our scheme has a much smaller ciphertext expansion factor compared with the element-wise encryption scheme, while preserves the homomorphic property. It is not necessary to require additional interactive protocols when applying secure signal processing tools to the compressed encrypted image. We present a fast algorithm for the encryption and the compression of the proposed image encryption scheme, which speeds up the computation and makes our scheme much more efficient. The detailed analysis on the security, ciphertext expansion ratio, and computational complexity are conducted. Our experiments demonstrate the validity of the proposed algorithms. The proposed scheme is suitable to be employed as an image encryption method for the applications in secure image processing.\"", "authors": [{"name": "Peijia Zheng and Jiwu Huang"}], "title": "An Efficient Image Homomorphic Encryption Scheme with Small Ciphertext Expansion"}, "59": {"abstract": "", "authors": [{"name": "Marco A. Hudelist"}, {"name": " Klaus Schoeffmann"}, {"name": " Laszlo Boeszoermenyi"}], "title": "Mobile Video Browsing with the ThumbBrowser"}, "58": {"abstract": "", "authors": [{"name": "Kiia Korpi"}, {"name": " Kiyoharu Aizawa"}], "title": "Kanji Snap - an OCR-based smartphone application for learning Japanese kanji characters"}, "55": {"abstract": "of user recommendation and social image annotation.", "authors": [{"name": "Subhabrata Bhattacharya"}], "title": "Towards a Comprehensive Computational Model for Aesthetic Assessment of Videos"}, "54": {"abstract": "\"observed links, leading to improvement in social media tasks\"", "authors": [{"name": "Brendan Jou; Hongzhi Li; Joseph G. Ellis; Daniel Morozoff-Abegauz; Shih-Fu Chang"}], "title": "Structured Exploration of Who; What; When; and Where in Heterogenous Multimedia News Sources"}, "57": {"abstract": "", "authors": [{"name": "Mei-Chen Yeh"}, {"name": " Chia-Ju Lu"}, {"name": " Chih-Fan Hsu"}], "title": "Real-time Salient Object Detection"}, "51": {"abstract": "deep learning model to test the idea on link analysis tasks", "authors": [{"name": "Chidansh Bhatt; Andrei Popescu-Belis; Maryam Habibi; Sandy Ingram; Stefano Masneri; et al."}], "title": "Multi-factor Segmentation for Topic Visualization and Recommendation: the MUST-VIS System"}, "50": {"abstract": "\"proposed framework, we design a novel relational generative\"", "authors": [{"name": "Kong-Wah Wan; Wei-Yun Yau; Sujoy Roy"}], "title": "Metadata Enrichment For News Video Retrieval -- A Graph-based Propagation Approach"}, "53": {"abstract": "features well embed both the media content and their", "authors": [{"name": "Chun-Che Wu; Kuan-Yu Chu; Yin-Hsi Kuo; Yan-Ying Chen; Wen-Yu Lee; Winston Hsu "}], "title": "Search-Based Relevance Association with Auxiliary Contextual Cues"}, "52": {"abstract": "in the social media networks. We show that the derived latent", "authors": [{"name": "Eleni Mantziou; Symeon Papadopoulos; Yiannis Kompatsiaris"}], "title": "Scalable Training with Approximate Incremental Laplacian Eigenmaps and PCA"}, "201": {"abstract": "\"In the context of a social gathering, such as cocktail party, the precious moments are generally captured by professional photographers or the participants. The latter case is generally undesired because the participants would like to enjoy the event instead of being occupied by the tedious photo capturing task. Motivated by this scenario, a set of cameras are used to automatically capture the photos. Instead of performing dense analysis on all cameras for photo capturing, we firstly detect the occurrence and location of social interactions via F-formation detection. In the sociology literature, F-formation is a concept to define social interactions, where each detection only requires the spatial location and orientation of each object. These information can be robustly retrieved with additional Kinect depth sensors. In this paper, we propose an extended F-formation system for robust detection of interactions and interactants. The extended F-formation system employs a heat map based feature representation for each individual, namely Interaction Space (IS), to model their respective location, orientation, and temporal information. Using the temporal encoded IS for each detected interactant, we propose a best view camera selection framework to detect the corresponding best view camera for each detected social interaction. The extended F-formation system is evaluated with synthetic data on multiple scenarios. To demonstrate the effectiveness of the proposed system, we display a set of the best view camera snapshots obtained from real-world experimental environment.\"", "authors": [{"name": "Tian Gan"}, {"name": " Yongkang Wong"}, {"name": " Daqing Zhang and Mohan Kankanhalli"}], "title": "Temporal encoded F-formation System for Improved Social Interaction Detection and Its Application"}, "199": {"abstract": "\"Among the vast information available on the web, social media streams capture what people currently pay attention to and how they feel about certain topics. Awareness of such trending topics plays a crucial role in multimedia systems such as trend aware recommendation and automatic vocabulary selection for video concept detection systems.,,Correctly utilizing trending topics requires a better understanding of their various characteristics in different social media streams. To this end, we present the first comprehensive study across three major online and social media streams, Twitter, Google, and Wikipedia, covering thousands of trending topics during an observation period of an entire year. Our results indicate that depending on one's requirements one does not necessarily have to turn to Twitter for information about current events and that some media streams strongly emphasize content of specific categories. As our second key contribution we further present a novel approach for the challenging task of forecasting the life cycle of trending topics in the very moment they emerge. Our fully automated approach is based on a nearest neighbor forecasting technique exploiting our assumption that semantically similar topics exhibit similar behavior.,,We demonstrate on a large-scale dataset of Wikipedia page view statistics that forecasts by the proposed approach are about 9-48k views closer to the actual viewing statistics compared to baseline methods and achieve a mean average percentage error of 45-19% for time periods of up to 14 days.\"", "authors": [{"name": "Tim Althoff"}, {"name": " Damian Borth"}, {"name": " J_rn Hees and Andreas Dengel"}], "title": "Analysis and Forecasting of Trending Topics in Online Media Streams"}, "179": {"abstract": "", "authors": [{"name": "Giuliano Armano"}, {"name": " Alessandro Giuliani"}, {"name": " Alberto Messina"}, {"name": " Maurizio Montagnuolo"}], "title": "CAMMA: Contextual Advertising system for Multimodal News Aggregations        "}, "200": {"abstract": "\"Despite considerable progress in recent years on Tag-based Social Image Retrieval (TagIR), state-of-the-art TagIR systems fail to provide a systematic framework for end users to ask why certain images are not in the result set of a given query and provide an explanation for such missing results. However, as humans, such why-not questions are natural when expected images are missing in the query results returned by a TagIR system. Clearly, it would be very helpful to users if they could pose follow-up why-not questions to seek clarifications on missing images in query results. In this work, we take the first step to systematically answer the why-not questions posed by end-users on TagIR systems. Our answer not only involves the reason why desired images are missing in the results but also suggestion on how the query can be altered so that the user can view these missing images in sufficient number. We present three explanation models, namely result reordering, query relaxation, and query substitution, that enable us to explain a variety of why-not questions. We present an algorithm called WINE (Why-not questIon aNswering Engine) that exploits these models to answer why-not questions efficiently. Experiments on NUS-WIDE dataset demonstrate effectiveness as well as benefits of WINE.\"", "authors": [{"name": "Sourav S Bhowmick"}, {"name": " Aixin Sun and Ba Quan Truong"}], "title": "\"Why Not, WINE?: Towards Answering Why-Not Questions in Social Image Search\""}, "195": {"abstract": "\"Mobile video is quickly becoming a mass consumer phenomenon. More and more people are using their smartphones to search and browse video contents on the move. In this paper, we develop an innovative instant mobile video search system through which users can discover videos by simply pointing their phones at a screen to capture a very few seconds of what they are watching. The system is able to index a large-scale of video data using a new layered audio-video indexing approach on the cloud, as well as extract light-weight joint audio-video signatures in real time and perform searching progressively on the mobile devices. Unlike most existing mobile video search applications which simply send the original video query to the cloud, the proposed mobile system is one of the first attempts towards instant and progressive video search leveraging the light-weight computing capacity of mobile devices. The system is characterized by four unique properties: 1) joint audio-video signature to deal with the large aural and visual variances associated with the query video captured by the mobile phone, 2) layered audio-video indexing to holistically exploit the complementarity between audio and video signals, 3) light-weight fingerprinting to comply with mobile processing capacity, and 4) progressive query process to significantly reduce computational costs and improve user experience---the search process can stop anytime once a confident result is achieved. We have collected 1,400 query videos captured by 25 mobile users from the videos with a total length of 600 hours. The experiments show that our system outperforms state-of-the-arts by achieving 90.79% precision when the query video is less than 10 seconds and 70.07% even the query video less than 5 seconds.\"", "authors": [{"name": "Wu Liu"}, {"name": " Tao Mei"}, {"name": " Yongdong Zhang"}, {"name": " Jintao Li and Shipeng Li"}], "title": "\"Listen, Look, and Gotcha: Instant Video Search with Mobile Phones by Layered Audio-Video Indexing\""}, "197": {"abstract": "\"In multimedia information retrieval, most classic approaches tend to represent different modalities of media in the same feature space. Existing approaches take either one-to-one paired data or uni-directional ranking examples as training samples, which do not make full use of bi-directional ranking examples to obtain a better performance. In this paper, we consider learning a cross-media representation model from the perspective of optimizing a listwise ranking problem while taking advantage of bi-directional ranking examples. We propose a general cross-media ranking algorithm to optimize the bi-directional listwise ranking loss with a latent space  embedding, which we call Bi-directional Cross-Media Semantic Representation Model (Bi-CMSRM). The latent  space embedding is discriminatively learned by the structural large margin learning for optimization with certain ranking criteria (Mean Average Precision in this paper) directly. We evaluate Bi-CMSRM on the Wikipedia and NUS-WIDE datasets. Experimental results show that the proposed method obtains significant improvements over the state-of-the-art methods for cross-media retrieval. We also show that the utilization of the bi-directional ranking examples achieves a much better performance than that of only using the uni-directional ranking examples.\"", "authors": [{"name": "Fei Wu"}, {"name": " Xinyan Lu"}, {"name": " Zhongfei Zhang"}, {"name": " Shuicheng Yan"}, {"name": " Yong Rui and Yueting Zhuang"}], "title": "Cross-Media Semantic Representation via Bi-directional Learning to Rank"}, "178": {"abstract": "", "authors": [{"name": "Marco Bertini"}, {"name": " Alberto Del Bimbo;\u00e6George Ioannidis;\u00e6Emile Bijk;\u00e6Isabel Trancoso"}, {"name": " Hugo Meinedo"}], "title": "euTV: a system for media monitoring and publishing        "}, "191": {"abstract": "", "authors": [{"name": "Huijie Lin"}, {"name": " Jia Jia"}, {"name": " Hanyu Liao"}, {"name": " Lianhong Cai"}], "title": "WeCard: A Multimodal Solution for Making Personalized Electronic Greeting Cards        "}, "190": {"abstract": "", "authors": [{"name": "Duong-Trung-Dung Nguyen"}, {"name": " Mukesh Saini;\u00e6Vu-Thanh Nguyen"}, {"name": " Wei Tsang Ooi"}], "title": "Jiku Director: An Online Mobile Video Mashup System        "}, "115": {"abstract": "\"Previous research has found that the distance metric for similarity estimation is determined by the underlying data noise distribution. The well known Euclidean(L2) and  Manhattan (L1) metrics are then justified when the additive noise are Gaussian and Exponential, respectively. However, finding a suitable distance metrics for local features is still a challenge when the underlying noise distribution is unknown and could be neither Gaussian nor Exponential. To address this issue, we introduce a modeling framework for arbitrary noise distributions and propose a generalized distance metric for local features based on this framework. We prove that the proposed distance is equivalent to the L1 or the L2 distance when the noise is Gaussian or Exponential. Furthermore, we justify the Hamming metric when the noise meets the given conditions. In that case, the proposed distance is a linear mapping of the Hamming distance. The proposed metric has been extensively tested on a benchmark data set with five state-of-the-art local features: SIFT, SURF, BRIEF, ORB and BRISK. Experiments show that our framework better models the real noise distributions and that more robust results can be obtained by using the proposed distance metric.\"", "authors": [{"name": "Zhendong Mao"}, {"name": " Yongdong Zhang"}, {"name": " Qi Tian"}], "title": "What are the distance metrics for local features? "}, "114": {"abstract": "\"We propose a weakly-supervised learning method for object detection using color and depth images of a real environment attached with object labels. The proposed method applies Multiple Instance Learning to find proper instances of the objects in training images. This method is novel in the sense that it learns multiple objects simultaneously in a way to balance the scores of each training sample across all object classes. Moreover, we combine 3D features considering different properties, that is, color texture, grayscale texture, and surface curvature, to improve the performance. We show that our method surpasses a conventional method using color and depth images. Furthermore, we evaluate its performance with our new dataset consisting of color and depth images with weak labels of 100 objects and various backgrounds.\"", "authors": [{"name": "Asako Kanezaki"}, {"name": " Tatsuya Harada"}, {"name": " Yasuo Kuniyoshi"}], "title": "Weakly-supervised Multi-class Object Detection Using Multi-type 3D Features "}, "88": {"abstract": "\"In this paper we present hand and foot based immersive,multimodal interaction approach for smart phone. A smart,phone based immersive football game is designed as a proof,of concept. The foot gesture is detected and tracked using,template modeling and Tracking-Learning-Detection (TLD),framework. Our system combines input modalities and pro-,vides a coordinated output to both modalities along with,audio and video. Finally we have conducted a user study,for checking validity and limitations of proposed hand and,foot based multimodal interaction for smart phones. Our,preliminary evaluation demonstrates the efficiency and easy,of use of proposed multimodal interaction approach.\"", "authors": [{"name": "Zhihan Lv"}, {"name": " Shafiq Ur R_hman"}, {"name": " Muhammad Sikandar Lal Khan"}], "title": "Hand and Foot Gesture Interaction for Handheld Devices "}, "89": {"abstract": "\"This paper studies a new way of accessing videos in a non-linear fashion. Existing non-linear access methods allow users to jump into videos at points that depict specific visual concepts or that are likely to elicit affective reactions. We believe that deep-link comments, which occur unprompted on social video sharing platforms, offer a new opportunity beyond existing methods. With deep-link comments, viewers express themselves about a particular moment in a video by including a time-code. Deep-link comments are special because they reflect viewer perceptions of noteworthiness, that includes, but extends beyond depicted conceptual content and induced affective reactions. Based on deep-link comments collected from YouTube, we develop a Viewer Expressive Reaction Variety (VERV) taxonomy that captures how viewers deep-link. We validate the taxonomy with a user study on a crowdsourcing platform and discuss how it extends conventional relevance criteria. We carry out experiments which show that deep-link comments can be automatically filtered and sorted into VERV categories.\"", "authors": [{"name": "Raynor Vliegendhart"}, {"name": " Babak Loni"}, {"name": " Martha Larson"}, {"name": " Alan Hanjalic"}], "title": "How Do We Deep-Link? Leveraging User-Contributed Time-Links for Non-Linear Video Access "}, "111": {"abstract": "\"Points of interest (POIs) are a core component of geograph-,ical databases and of location based services. POI acqui-,sition was classicaly performed by domain experts but as-,sociated costs and access difficulties in many regions of the,world limit the coverage of manually built databases. With,the availability of large geotagged multimedia datasets on,the Web, a sustained research effort was dedicated to au-,tomatic POI discovery and characterization. However, in,spite of its practical importance, POI localization was only,marginally addressed. To compute POI coordinates an as-,sumption was made that the more data were available, the,more precise the localization will be. Here we shift the focus,of the localization process from data quantity to data qual-,ity. Given a set of geotagged photos associated to a POI,,close-up classification is used to trigger a spatial clustering,process. To evaluate the newly introduced method against,different other localization schemes, we create an accurate,ground truth. We show that significant localization error,reductions are obtained compared to a coordinate averaging,approach and to a X-Means clustering scheme.\"", "authors": [{"name": "Adrian Popescu"}, {"name": " Aymen Shabou"}], "title": "Towards Precise POI Localization with Social Media "}, "110": {"abstract": "\"Cover song or version identification, with the goal to automatically identify all versions of a piece of music, is typically addressed by locally comparing all subsequences of a given query document against all possible subsequences of all documents in a given database. In this paper we provide a proof-of-concept approach to reduce the number of subsequences that need to be compared. As a main contribution, we introduce the concept of cover group thumbnail, which is the most representative, essential subsequence for an entire group of versions. Opposed to previous approaches, we jointly consider all versions of a given song to compute a single cover group template, which then shows a high degree of robustness against version-specific aspects. To compute such a template, we introduce a modification of a recent audio thumbnailing technique. In our experiments, we analyze whether an entire cover group can be characterized by a single subsequence and show that cover group templates can be a valuable tool for facilitating template-based version identification.\"", "authors": [{"name": "Peter Grosche"}, {"name": " Meinard M\u00d9ller"}, {"name": " Joan Serr\u00f6"}], "title": "Towards Cover Group Thumbnailing "}, "113": {"abstract": "\"The bag of visual words model (BoW) and its variants have,demonstrate their effectiveness for visual applications and have,been widely used by researchers. The BoW model first extracts local,features and generates the corresponding codebook, the elements of a,codebook are viewed as visual words. The local features within each,image are then encoded to get the final histogram representation.,However, the codebook is dataset dependent and has to be generated,for each image dataset. This costs a lot of computational time and,weakens the generalization power of the BoW model. To solve these,problems, in this paper, we propose to undo the dataset bias by,codebook linear transformation. To represent every points within the,local feature space using Euclidean distance, the number of bases,should be no less than the space dimensions. Hence, each codebook,can be viewed as a linear transformation of these bases. In this,way, we can transform the pre-learned codebooks for a new dataset.,However, not all of the visual words are equally important for the,new dataset, it would be more effective if we can make some,selection using sparsity constraints and choose the most,discriminative visual words for transformation. We propose an,alternative optimization algorithm to jointly search for the optimal,linear transformation matrixes and the encoding parameters. Image,classification experimental results on several image datasets show,the effectiveness of the proposed method.\"", "authors": [{"name": "Chunjie Zhang"}, {"name": " Shuhui Wang"}, {"name": " Qingming Huang"}, {"name": " Chao Liang"}, {"name": " Jing Liu"}, {"name": " Qi Tian"}], "title": "Undo the codebook bias by linear transformation for visual applications "}, "112": {"abstract": "\"Due to increasing globalization, urban societies are becoming more multicultural. The availability of large-scale digital mobility traces e.g. from tweets or checkins provides an opportunity to explore multiculturalism that until recently could only be addressed using small scale survey-based methods. In this paper we examine a basic facet of multiculturalism through the lens of language use across multiple cities. Using data obtained from Foursquare, we present an exploratory analysis of linguistic differences and similarities across five urban agglomerations in a multicultural western European country.\"", "authors": [{"name": "Darshan Santani"}, {"name": " Daniel Gatica-Perez"}], "title": "Tower of Babel? Languages and Venues on Foursquare "}, "82": {"abstract": "\"Bag-of-words approaches have been shown to achieve state-of-the-art performance in large-scale multimedia event detection. However, the commonly used histogram representation of bag-of-words requires large codebook sizes and expensive nonlinear kernel based classifiers for optimal performance. To address these two issues, we present a two-part generative model for compact visual representation, based on the i-vector approach recently proposed for speech and audio modeling. First, we use a Gaussian mixture model (GMM) to model the joint distribution of local descriptors. Second, we use a low-dimensional factor representation that constrains the GMM parameters to a subspace that preserves most of the information. We further extend this method to incorporate overlapping spatial regions, forming a highly compact yet informative visual representation that achieves superior performance with fast linear classifiers. We evaluate the method on a large ~45000 video dataset used in the TRECVID 2011 MED evaluation. With linear classifiers, the proposed representation, with one-tenth of the storage footprint, outperforms soft quantization histograms used in the top performing TRECVID 2011 MED systems.\"", "authors": [{"name": "Xiaodan Zhuang"}, {"name": " Shuang Wu"}, {"name": " Pradeep Natarajan"}, {"name": " Rohit Prasad"}, {"name": " Prem Natarajan"}], "title": "Compact bag-of-words visual representation for effective linear classification "}, "83": {"abstract": "\"Stereo images and videos are very popular in recent years, and techniques for processing this media are attracting a lot of attention. In this paper, we extend the shift-map method for stereo image editing. Our method simultaneously processes the left and right images on pixel level using a global optimization algorithm. It enforces photo consistence between the two images and preserves 3D scene structures. It also address the occlusion and disocclusion problem, which may enable many stereo image editing functions, such as depth mapping, object depth adjustment and non-homogeneous image resizing. Our experiments show that the proposed method produces high quality results with various editing functions.\"", "authors": [{"name": "Tao Yan"}, {"name": " Shengfeng He"}, {"name": " Rynson Lau"}, {"name": " Yun Xu"}], "title": "Consistent Stereo Image Editing "}, "80": {"abstract": "\"Image tag relevance estimation aims to automatically determine what people spontaneously label about images is factually present in the pictorial content. Different from previous works, which either use only positive examples of a given tag or use positive and random negative examples, we argue the importance of relevant positive and relevant negative examples for tag relevance estimation. We propose a system that automatically selects positive and negative examples, deemed most relevant with respect to the given tag from crowd-annotated images. While applying models for many tags could be cumbersome, our system trains efficient ensembles of Support Vector Machines per tag, enabling fast classification. % with the selected examples.,Experiments on two benchmark sets show that the proposed system compares favorably against five present day methods. Given extracted visual features, for each image our system can process up to 3,787 tags per second. The new system is both effective and efficient for tag relevance estimation.\"", "authors": [{"name": "Xirong Li"}, {"name": " Cees Snoek"}], "title": "Classifying Tag Relevance with Relevant Positive and Negative Examples "}, "81": {"abstract": "\"A collage is an artistic composition made by assembling different parts to create a new whole. This procedure can be applied for assembling tridimensional objects. In this paper we present CollARt, a Mobile Augmented Reality application which permits to create 3D photo collages. Virtual pieces are textured with pictures taken with the camera and can be blended with real objects. A preliminary user study (N=12) revealed that participants were able to create interesting works of art. The evaluation also suggested that the possibility of itinerantly mixing virtual pieces with the real world increases creativity.\"", "authors": [{"name": "Asier Marzo"}, {"name": " Oscar Ardaiz"}], "title": "CollARt: a Tool for Creating 3D Photo Collages Using Mobile Augmented Reality "}, "86": {"abstract": "\"The main challenge of facial landmark localization in real-world application is that the large changes of head pose and facial expressions cause substantial image appearance variations. To avoid high dimensional regression in the 3D and 2D facial pose spaces simultaneously, we propose a hierarchical pose regression approach, estimating the head rotation, face components, and facial landmarks hierarchically. The regression process works in a unified cascaded fern framework. We present generalized gradient boosted ferns (GBFs) for the regression framework, which give better performance than traditional ferns. The framework also achieves real time performance. We verify our method on the latest benchmark datasets. The results show that it outperforms state-of-the-art methods in both accuracy and speed.\"", "authors": [{"name": "Zhanpeng Zhang"}, {"name": " Wei Zhang"}, {"name": " Jianzhuang Liu"}, {"name": " Xiaoou Tang"}], "title": "Facial Landmark Localization based on Hierarchical Pose Regression with Cascaded Random Ferns "}, "87": {"abstract": "\"The selection of discriminative features is an important and effective technique for many multimedia tasks. Using irrelevant features in classification or clustering tasks could deteriorate the performance. Thus, designing efficient feature selection algorithms to remove the irrelevant features is a possible way to improve the classification or clustering performance. With the successful usage of sparse models in image and video classification and understanding, imposing structural sparsity in \\emph{feature selection} has been widely investigated during the past years. Motivated by the merit of sparse models, we propose a novel feature selection method using a sparse model in this paper. Different from the state of the art, our method is built upon $\\ell _{2,p}$-norm and simultaneously considers both the global and local (GLocal) structures of data distribution. Our method is more flexible in selecting the discriminating features as it is able to control the degree of sparseness. Moreover, considering both global and local structures of data distribution makes our feature selection process more effective. An efficient algorithm is proposed to solve the $\\ell_{2,p}$-norm sparsity optimization problem in this paper. Experimental results performed on real-world image and video datasets show the effectiveness of our feature selection method compared to several state-of-the-art methods.\"", "authors": [{"name": "Yan Yan"}, {"name": " Zhigang Ma"}, {"name": " Gaowen Liu"}, {"name": " Nicu Sebe"}], "title": "GLocal Structural Feature Selection with Sparsity for Multimedia Data Understanding "}, "84": {"abstract": "\"The automated selection of satisfying subsets from large collections of photos is a central challenge in multimedia research. Objective criteria like the depiction of persons or the photo quality are met by existing approaches. But it is difficult to know the users' personal interest, which plays an important role in the selection process. The expected spread of devices with eye tracking support in the near future allows us to measure this interest in a new way. In an experiment with 12 participants, we derive the most interesting photos of a collection for every person from gaze information recorded during the viewing of the photos. We can show that the eye tracking information delivers valuable information about the users' preferences by comparing the results to a manual selection. The selection based on gaze information significantly outperforms baseline approaches and improves the results by up to 17%. For photo sets of personal interest to the user this improvement is even up to 23%.\"", "authors": [{"name": "Tina Walber"}, {"name": " Chantal Neuhaus"}, {"name": " Ansgar Scherp"}, {"name": " Steffen Staab"}, {"name": " Ramesh Jain"}], "title": "Creation of Individual Photo Selections: Read Preferences from the Users' Eyes "}, "85": {"abstract": "\"Ranking on image search has attracted considerable attentions. Many graph-based algorithms have been proposed to solve this problem. Despite their remarkable success, these approaches are restricted to the single image networks. To improve the ranking performance, one effective strategy is to work beyond the single image graph by leveraging fruitful information from manual semantic labeling (i.e., tags) associated with images, which leads to the technique of co-ranking images and tags, a representative method that aims to explore the reinforcing relationship between image and tag graphs. The idea of co-ranking is implemented by adopting the paradigm of random walks. However, there are two problems hidden in co-ranking remained to be open: the high computational complexity and the problem of out-of-sample. To address the challenges above, in this paper, we cast the co-ranking process into a Bregman divergence optimization framework under which we transform the original random walk into an equivalent optimal kernel matrix learning problem. Enhanced by this new formulation, we derive a novel extension to achieve a better performance for both in-sample and out-of-sample cases. Extensive experiments are conducted to show the effectiveness and efficiency of our approach.\"", "authors": [{"name": "Lin Wu"}, {"name": " Yang Wang"}], "title": "Efficient Image and Tag Co-Ranking: A Bregman Divergence Optimization Method "}, "198": {"abstract": "\"Cross media retrieval systems have received considerable interest in recent years. Due to the semantic gap between low level features and high level semantic concepts of multimedia data, many researchers have explored joint-model techniques in cross media retrieval systems. Previous joint-model approaches usually focus on two traditional ways to design cross media retrieval systems: (a) Fusing features,from di_erent multimedia data. (b) Learning di_erent models for di_erent multimedia data and fusing their outputs. However, the process of fusing features or outputs will lose both low and high level abstraction information of media data. Hence, both ways do not really reveal the semantic correlations among the heterogenous multimedia data. In this paper, we introduce a novel method for the cross media retrieval task, named Parallel Field Alignment cross media Retrieval (PFAR), which integrates a manifold alignment framework from the perspective of vector _elds. Instead of fusing original features or outputs, we consider the cross media retrieval as a manifold alignment problem and use parallel _eld, which can e_ectively preserve the metric of media data manifolds, to model heterogenous multimedia data and project their relationship into a common latent semantic space during the process of manifold alignment. After alignment, the semantic correlations are also determined. In this way, the cross media retrieval task can be resolved by the determined semantic correlations. Comprehensive experimental results have demonstrated the e_ectiveness of our approach.\"", "authors": [{"name": "Xiangbo Mao"}, {"name": " Binbin Lin"}, {"name": " Xiaofei He"}, {"name": " Deng Cai and Jian Pei"}], "title": "Parallel Field Alignment for Cross Media Retrieval"}, "206": {"abstract": "", "authors": [{"name": ""}], "title": "Doctorial Symposium Poster Session"}, "3": {"abstract": "\"Beauty e-Experts, a fully automatic system for hairstyle and facial makeup recommendation and synthesis, is developed in this work. Given a user-provided frontal face image with short/bound hair and no/light makeup, the Beauty e-Experts system can not only recommend the most suitable hairdo and makeup, but also show the synthetic effects. To obtain enough knowledge for beauty modeling, we build the Beauty e-Experts Database, which contains $1,505$ attractive female photos with a variety of beauty attributes and beauty-related attributes annotated. Based on this Beauty e-Experts Dataset, two problems are considered for the Beauty e-Experts system: what to recommend and how to wear, which describe a similar process of selecting hairstyle and cosmetics in our daily life. For the what-to-recommend problem, we propose a multiple tree-structured super-graphs model to explore the complex relationships among the high-level beauty attributes, mid-level beauty-related attributes and low-level image features, and then based on this model, the most compatible beauty attributes for a given facial image can be efficiently inferred. For the how-to-wear problem, an effective and efficient facial image synthesis module is designed to seamlessly synthesize the recommended hairstyle and makeup into the user facial image. Extensive experimental evaluations and analysis on testing images of various conditions well demonstrate the effectiveness of the proposed system.\"", "authors": [{"name": "Luoqi Liu"}, {"name": " Hui Xu"}, {"name": " Junliang Xing"}, {"name": " Si Liu"}, {"name": " Xi Zhou and Shuicheng Yan"}], "title": "Wow! You Are So Beautiful Today!"}, "177": {"abstract": "", "authors": [{"name": "Marco Bertini;\u00e6Alberto Del Bimbo"}, {"name": " Andrea Ferracani"}, {"name": " Francesco Gelli"}, {"name": " Daniele Maddaluno"}, {"name": " Daniele Pezzatini"}], "title": "\"A novel framework for collaborative video recommendation, interest discovery and friendship suggestion based on semantic profiling        \""}, "7": {"abstract": "\"Crowdsourcing strategy gains rising applications in Quality of Experience (QoE) evaluation community, which advocates mass collaboration and the wisdom of the commons, whence more economic than conventional laboratory studies. However, a major challenge for crowdsourcing test is the detection and control of outliers, which may arise due to different test conditions, human errors or natural variations in raters. For this purpose, it is desired to develop a robust evaluation methodology to deal with crowdsourceable data which are possibly incomplete, imbalanced, and distributed on a graph. In this paper, we propose a robust rating scheme based on robust regression and Hodge Decomposition on graphs, to assess QoE in crowdsourcing scenario. The scheme shows that removal of outliers in crowdsourcing experiment would be helpful for purifying data; thus could provide us more reliable QoE evaluation results. The effectiveness of proposed scheme is further confirmed by experimental studies on both simulated examples and real-world data.\"", "authors": [{"name": "Qianqian Xu"}, {"name": " Jiechao Xiong"}, {"name": " Qingming Huang and Yuan Yao"}], "title": "Robust Evaluation for Quality of Experience in Crowdsourcing"}, "108": {"abstract": "\"We present a generic event detection system evaluated in the Surveillance Event Detection (SED) task of TRECVID 2012. We investigate a statistical approach with spatio-temporal features applied to seven event classes, which were defined by the SED task. This approach is based on local spatio-temporal descriptors, called MoSIFT and generated by pair-wise video frames. A Gaussian Mixture Model(GMM) is learned to model the distribution of the low level features. Then for each sliding window, the Fisher vector encoding is used to generate the sample representation. The model is learnt using a Linear SVM for each event. The main novelty of our system is the introduction of Fisher vector encoding into video event detection. Fisher vector encoding has demonstrated great success in image classification. The key idea is to model the low level visual features as a Gaussian Mixture Model and to generate an intermediate vector representation for bag of features. FV encoding uses higher order statistics in place of histograms in the standard BoW. FV has several good properties: (a) it can naturally separate the video specific information from the noisy local features and (b) we can use a linear model for this representation. We build an efficient implementation for FV encoding which can attain a 10 times speed-up over real-time. We also take advantage of  non-trivial object localization techniques to feed into the video event detection, e.g. multi-scale detection and non-maximum suppression. This approach outperformed the results of all other teams submissions in TRECVID SED 2012 on four of the seven event types.\"", "authors": [{"name": "Qiang Chen"}, {"name": " Yang Cai"}, {"name": " Lisa Brown"}, {"name": " Ankur Datta"}, {"name": " Quanfu Fan"}, {"name": " Rogerio Feris"}, {"name": " Shuicheng Yan"}, {"name": " Alex Hauptmann"}, {"name": " Sharathchandra Pankanti"}], "title": "Spatio-Temporal Fisher Vector Coding for Surveillance Event Detection "}, "109": {"abstract": "\"The state-of-the-art partial-duplicate image search systems reply heavily on the match of local features like SIFT. Independently matching local features across two images ignores the overall geometry structure and therefore may incur many false matches. To reduce such matches, several geometry verification methods have been proposed. This paper introduces a new geometry verification method named as Strong Geometry Consistency (SGC), which uses the orientation, scale and location information of the local feature points to accurately and quickly remove the false matches. We also propose a simple scale weighting (SW) strategy, which gives feature points with larger scales greater weights, based on the intuition that a larger-scale feature point tends to be more robust for image search as it occupies a larger area of an image. Extensive experiments performed on three popular datasets show that SGC significantly outperforms state-of-the-art geometry verification methods, and SW can further boost the performance with marginal additional computation.\"", "authors": [{"name": "Junqiang Wang"}, {"name": " Jinhui Tang"}], "title": "Strong Geometrical Consistency in Large Scale Partial-duplicate Image Search "}, "102": {"abstract": "\"We present a work that explores the feasibility of face recognition technologies for analyzing identities in works of portraiture, and in the process provide additional evidence to settle some long-standing questions in art history. Works of portrait art bear the mark of visual interpretation of the artist. Moreover, the number of samples available to model these effects is often limited. From a set of portraiture of Renaissance era where the identitites of subjects is known, we derive appropriate features that are based on domain knowledge of artistic renderings and learn statistical models for the distribution of the match and non-match scores, which we refer to as portrait feature space (PFS). Thereafter, we use this PFS on a number of cases that have been ``open questions\\ to art historians. They are usually in the form of validating two portraits as belonging to the same person. Using statistical hypothesis tests on the PFS, we provide quantitative measures of similarity for each of these questions. It is, to the best of our knowledge, the first study that applies automated face recognition technologies to the analysis of portraits of multiple subjects in various forms - paintings, death masks, sculptures.\"\"\"", "authors": [{"name": "Ramya Srinivasan"}, {"name": " Amit Roy-Chowdhury"}, {"name": " Conrad Rudolph"}, {"name": " Jeanette Kohl"}], "title": "Recognizing the Royals -Leveraging Computerized Face Recognition for Identifying Subjects in Ancient Artworks "}, "103": {"abstract": "\"Discovering salient objects from videos in an unsupervised way is not a trivial problem. Noticing that salient objects though changing their categories are always across the whole video and the density of the salient video object region is usually large, we formulate the salient object detection in a video as an optimal path discovery problem. A globally optimal solution is also obtained by the proposed dynamic programming algorithm. By taking the temporal coherence of salient objects into consideration, the proposed algorithm obtains more accurate detection results. Without any prior knowledge of the salient objects, our method can automatically detect the salient objects across different scales and aspect ratios. Experimental results on two public datasets demonstrate the effectiveness of the proposed method on salient video object detection.\"", "authors": [{"name": "Ye Luo"}, {"name": " Junsong Yuan"}], "title": "Salient Object Detection in Videos by Optimal Spatio-Temporal Path Discovery "}, "100": {"abstract": "\"We aim to query web video for complex events using only a handful of video query examples, where the standard approach learns a ranker from hundreds of examples. We consider a semantic signature representation, consisting of off-the-shelf concept detectors, to capture the variance in semantic appearance of events. Since it is unknown what similarity metric and query fusion to use in such an event retrieval setting, we perform three experiments on unconstrained web videos from the TRECVID event detection task. It reveals that: retrieval with semantic signatures using normalized correlation as similarity metric outperforms a low-level bag-of-words alternative, multiple queries are best combined using late fusion with an average operator, and event retrieval is preferred over event classification when less than eight positive video examples are available.\"", "authors": [{"name": "Masoud Mazloom"}, {"name": " Amirhossein Habibian"}, {"name": " Cees Snoek"}], "title": "Querying for Video Events by Semantic Signatures from Few Examples "}, "101": {"abstract": "\"With the advance of cloud computing, growing applications have been migrating to the cloud for its robustness and scalability. However, sending raw data to the cloud-based service providers will generally risk our privacy; especially for cloud-based surveillance system, where privacy is one of the major concerns as continuously recording daily life. Thus, privacy-preserving intelligent analytics are in dire needs. In this preliminary research, we investigate real-time privacy-preserving moving object detection in the encrypted cloud-based surveillance videos. Moving object detection is one of the core techniques and can further enable other applications (e.g., object tracking, action recognition, etc.). One possible approach is using homomorphic encryption which provides corresponding operations between unencrypted and encrypted data. However, homomorphic encryption is impractical in real case because of formidable computations and bulky storage consumption. In this paper, we propose an efficient and secure encryption framework, which entails real-time analytics (e.g., moving object detection) in encrypted video streams. Experiments confirm that the proposed method can achieve similar accuracy as detection on original raw frames.\"", "authors": [{"name": "Kuan-Yu Chu"}, {"name": " Yin-Hsi Kuo"}, {"name": " Winston H. Hsu"}], "title": "Real-Time Privacy-Preserving Moving Object Detection in the Cloud "}, "106": {"abstract": "\"One of the most successful method to link all similar images within a large collection is min-Hash, which is a way to significantly speed-up the comparison of images when the underlying image representation is bag-of-words. However, the quantization step of min-Hash introduces important information loss. In this paper, we propose a generalization of min-Hash, called Sim-min-Hash, to compare sets of real-valued vectors.  We demonstrate the effectiveness of our approach when combined with the Hamming embedding similarity. Experiments on large-scale popular benchmarks demonstrate that Sim-min-Hash is more accurate and faster than min-Hash for similar image search. Linking a collection of one million images described by 2 billion local descriptors is done in 7 minutes on a single core machine.\"", "authors": [{"name": "Wan-Lei Zhao"}, {"name": " Herv_ J_gou"}, {"name": " Guillaume Gravier"}], "title": "Sim-Min-Hash "}, "107": {"abstract": "\"This paper presents a Spatialized Audio Multiparty Teleconferencing (SAMT) system. Based on our recently developed 3D sound source localization (SSL) and 3D audio capture and reproduction technologies with a low-cost, compactly designed microphone array, the SAMT system is realized and optimized to provide a radically new communication experience in one-to-many and many-to-many virtual meetings. In essence, the system offers 3D audio capture capability and spatial audio perception with multiple participants at a site, which still falls short in teleconferencing solutions. In addition, being able to identify and automatically track the active speaker, the system allows more compelling visual presentations for effective communications. Extensive experiments have shown that the proposed SAMT system runs reliably and comfortably in real time on one's laptop or desktop PC, and it needs only a low-cost miniature microphone array and a consumer depth camera. With a really minimal deployment requirement, we present a variety of user experiences created by the SAMT system.\"", "authors": [{"name": "Viet Anh Nguyen"}, {"name": " Shengkui Zhao"}, {"name": " Tien Dung Vu"}, {"name": " Douglas L. Jones"}, {"name": " Minh N. Do"}], "title": "Spatialized Audio Multiparty Teleconferencing with Commodity Miniature Microphone Array "}, "104": {"abstract": "\"The separation of different sound sources from polyphonic music recordings ,constitutes a complex task since one has to account for different musical ,and acoustical aspects. In the last years, various score-informed procedures ,have been suggested where musical cues such as pitch, timing, and ,track information are used to support the source separation process. ,In this paper, we discuss a framework for decomposing a given music ,recording into note-wise audio events which serve as elementary building blocks. ,In particular, we introduce ,an interface that employs the additional score information to provide,a natural way for a user to interact with these audio events.,By simply selecting arbitrary note groups within ,the score a user can access, modify, or analyze corresponding events,in a given audio recording. In this way, our framework not only opens up new ways for ,audio editing applications, but also serves as a valuable tool ,for evaluating and better understanding the results of source ,separation algorithms.\"", "authors": [{"name": "Jonathan Driedger"}, {"name": " Harald Grohganz"}, {"name": " Thomas Praetzlich"}, {"name": " Sebastian Ewert"}, {"name": " Meinard Mueller"}], "title": "Score-Informed Audio Decomposition and Applications "}, "105": {"abstract": "\"In this work, we address the problem of complex event detection on unconstrained videos. We introduce a novel multi-way feature pooling approach which leverages segment-level (sub-clip) information. The approach is simple and widely applicable to diverse audio-visual features for multimedia videos. Our approach uses a set of clusters discovered via unsupervised clustering of segment-level features. Depending on feature characteristics, not only scene-based clusters but also motion/audio-based clusters can be incorporated. Then, every video is represented with multiple descriptors, where each descriptor is designed to relate to one of the pre-built clusters. For classication, intersection kernel SVMs are used where the kernel between a pair of videos is obtained by combining multiple kernels computed from every corresponding per-cluster descriptor pairs. Evaluation on TRECVID '11 MED dataset shows a signicant improvement by the proposed approach beyond the state-of-the-art.\"", "authors": [{"name": "Ilseo Kim"}, {"name": " Sangmin Oh"}, {"name": " Arash Vahdat"}, {"name": " Kevin Cannons"}, {"name": " A. G. Amitha Perera"}, {"name": " Greg Mori"}], "title": "Segmental Multi-way Local Pooling for Video Recognition "}, "39": {"abstract": "The current trend in social media analysis and application is", "authors": [{"name": "Zhaoquan Yuan; Jitao Sang; Yan Liu; Changsheng Xu"}], "title": "Latent Feature Learning in Social Media Network"}, "38": {"abstract": "\"The semantic gap between low-level visual features and high-level semantics has been investigated for decades but still remains a big challenge in multimedia. When \"\"search\"\" became one of the most frequently used applications, \"\"intent gap\"\", the gap between query expressions and users' search intents, emerged. Researchers have been focusing on three approaches to bridge the semantic and intent gaps: 1) developing more representative features, 2) exploiting better learning approaches or statistical models to represent the semantics, and 3) collecting more training data with better quality. However, it remains a challenge to close the gaps. In this paper, we argue that the massive amount of click data from commercial search engines provides a data set that is unique in the bridging of the semantic and intent gap. Search engines generate millions of click data (a.k.a. image-query pairs), which provide almost \"\"unlimited\"\" yet strong connections between semantics and images, as well as connections between users' intents and queries. To study the intrinsic properties of click data and to investigate how to effectively leverage this huge amount of data to bridge semantic and intent gap is a promising direction to advance multimedia research. In the past, the primary obstacle is that there is no such dataset available to the public research community. This changes as Microsoft has released a new large-scale real-world image click data to public. This paper presents preliminary studies on the power of large-scale click data with a variety of experiments, such as building large-scale concept detectors, tag processing, search, definitive tag detection, intent analysis, etc., with the goal to inspire deeper researches based on this dataset.\"", "authors": [{"name": "Xian-Sheng Hua; Jingdong Wang; Jing Wang; Jin Li"}], "title": "Clickage: Towards Bridging Semantic and Intent Gaps via Mining Click Logs of Search Engines"}, "33": {"abstract": "\"In this work, we address the request and service scheduling challenge posed by the non-linear access pattern in P2P media streaming systems. With non-linear accesses, on-demand and prefetch requests co-exist. Existing solutions often result in  unnecessary content retrieval from the server, due to request contentions (requests compete for the limited service capacity) either within the same or across two categories of requests. We propose an unified scheduling solution that not only alleviates the contentions, but also works towards achieving the synergy effect between the two categories of requests. In particular, we propose a novel request binning algorithm to avoid requests issued from the same peer ``stepping on each other''. Ties among on-demand requests are broken by a properly designed serving and rejection policy. More importantly, we design a prefetch metric that factors in the need of on-demand requests and it is used to prioritize prefetch requests at both requesting and serving peers. Finally, we propose a prefetch request issuing algorithm that can fully utilize abundant upload bandwidth for prefetching. Evaluation with traces collected from Second Life shows that our scheme outperforms existing solutions with significant margin. The adoption of our scheme can significantly reduce the server load in P2P streaming systems where the non-linear access pattern is intrinsic.\"", "authors": [{"name": "Zhen Wei Zhao and Wei Tsang Ooi"}], "title": "Unifying Request and Service Scheduling for P2P Non-linear Media Access Systems"}, "32": {"abstract": "\"Multimedia event detection (MED) is an effective technique for video indexing and retrieval. In this paper, we focus on detecting the generic, complicated events that occur in long video clips. Current classifier training for MED relies on label information and treats the negative videos equally. However, many negative videos may resemble the positive videos in different degrees. Intuitively, we may capture more informative cues from the negative videos if we assign them fine-grained labels, thus benefiting the classifier learning. Aiming for this, we use a statistical method to mine both the positive and negative examples to get the decisive attributes of a specific event. Based on these decisive attributes, we assign the fine-grained labels to negative examples to treat them differently for more effective exploitation. The resulting fine-grained labels may be not accurate enough to characterize the negative videos. Hence, we propose to jointly optimize the fine-grained labels with the knowledge from the visual features and the attributes representations, which brings mutual reciprocality. Our model obtains two kinds of classifiers, one from the attributes and one from the features, which incorporate the informative cues from the fine-grained labels. The output of both classifiers on the testing videos are fused for detection. To the best of our knowledge, this is the first research on adaptively assigning fine-grained labels for MED. Extensive experiments on the challenging TRECVID MED 2012 development set have validated the efficacy of our proposed approach.\"", "authors": [{"name": "Zhigang Ma"}, {"name": " Yi Yang"}, {"name": " Zhongwen Xu and Alex Hauptmann"}], "title": "We Are Not Equally Negative: Fine-grained Labeling for Multimedia Event Detection"}, "31": {"abstract": "\"We present a new method to classify human activities by leveraging on the cues available from depth images alone. Towards this end, we propose a descriptor which couples depth and spatial information of the segmented body to describe a human pose. Unique poses (i.e. codewords) are then identified by a spatial-based clustering step. Given a video sequence of depth images, we segment humans from the depth images and represent these segmented bodies as a sequence of codewords. We exploit unique poses of an activity and the temporal ordering of these poses to learn subsequences of codewords which are strongly discriminative for the activity. Each discriminative subsequence acts as a classifier and we learn a boosted ensemble of discriminative subsequences to assign a confidence score for the activity label of the test sequence. Unlike existing methods which demand accurate tracking of 3D joint locations or couple depth with color image information as recognition cues, our method requires only the segmentation masks from depth images to recognize an activity. Experimental results on the publicly available Human Activity Dataset (which comprises 12 challenging activities) demonstrate the validity of our method, where we attain a precision/recall of 78.1%/75.4% when the person was not seen before in the training set, and 94.6%/93.1% when the person was seen before. To our knowledge, this is the best result published so far for this depth image dataset.\"", "authors": [{"name": "Raj Gupta"}, {"name": " Alex Yong-Sang Chia and Deepu Rajan"}], "title": "Human Activities Recognition using Depth Images"}, "30": {"abstract": "\"Articulated configuration of human body parts is an essential representation of human motion, therefore is well suited for classifying human actions. In this work,  we propose a novel approach to exploring the discriminative  pose sub-patterns for effective action classification.  These pose sub-patterns are extracted from a predefined set of 3D poses represented by hierarchical motion angles. The basic idea is motivated by the two observations: (1)  There exist representative sub-patterns in each action class, from which the action class can be easily differentiated. (2) These sub-patterns frequently appear in the action class.  By constructing a connection between frequent sub-patterns and the discriminative measure, we develop the SSPI, namely, the Support Sub-Pattern Induced  learning algorithm for simultaneous feature selection and feature learning.  Based on the algorithm, discriminative pose sub-patterns can be identified and used as a series of  ``magnetic centers'' on the surface of normalized super-sphere for feature transform. The ``attractive forces'' from the sub-patterns determine the direction and step-length of the transform. This transformation makes a feature more discriminative while maintaining dimensionality invariance. Comprehensive experimental studies conducted on a large scale motion capture dataset demonstrate the  effectiveness of the proposed approach for action classification and the superior performance over the state-of-the-art techniques.\"", "authors": [{"name": "Xu Zhao"}, {"name": " Yuncai Liu and Yun Fu"}], "title": "Exploring Discriminative Pose Sub-Patterns for Effective Action Classification"}, "37": {"abstract": "\"Because of the popularity of touch-screen devices, it has become a highly desirable feature to retrieve images from a huge repository by matching with a hand-drawn sketch. Although searching images via keywords or an example image has been successfully launched in some commercial search engines of billions of images, it is still very challenging for both academia and industry to develop a sketch-based image retrieval system on a billion-level database. In this work, we systematically study this problem and try to build a system to support query-by-sketch for two billion images. The raw edge pixel and Chamfer matching are selected as the basic representation and matching in this system, owning to the superior performance compared with other methods in extensive experiments. To get a more compact feature and a faster matching, a vector-like Chamfer feature pair is introduced, based on which the complex matching is reformulated as the crossover dot-product of feature pairs. Based on this new formulation, a compact shape code is developed to represent each image/sketch by projecting the Chamfer features to a linear subspace followed by a non-linear source coding. Finally, the multi-probe Kmedoids-LSH is leveraged to index database images, and the compact shape codes are further used for fast reranking. Extensive experiments show the effectiveness of the proposed features and algorithms in building such a sketch-based image search system.\"", "authors": [{"name": "Xinghai Sun; Changhu Wang; Chao Xu; Lei Zhang"}], "title": "Towards Building a Sketch-based Image Search Engine on a Billion-Level Database"}, "36": {"abstract": "\"This paper reports research into video-mediated synchronous communication within social groups, with members interacting from different physical locations. The ultimate aim of the research is to create a more natural medium for interaction, aware of the context in which it operates, able to continuously adapt itself to the communication needs and optimise the way in which it captures and transmits aspects of the communication. This, is hypothesised, can be achieved by equipping each location with multiple controllable video cameras and microphone arrays and mixing the resulting content through techniques similar to those used in television\u0084a process referred to as \u00f1orchestration\u00ee. Through orchestration, each location should be able to receive the appropriate perspectives and levels of detail, resulting in communication experiences in which the spatial separation between participants is minimised. The paper defines the concept of orchestration and presents two major evaluation experiments that provide convincing supporting evidence for the main assumption.\"", "authors": [{"name": "Marian Ursu"}, {"name": " Martin Groen"}, {"name": " Manolis Falelakis"}, {"name": " Michael Frantzis and Vilmos Zsombori"}], "title": "Orchestration: TV-Like Mixing Grammars applied to Video-Communication for Social Groups"}, "35": {"abstract": "\"Inter-Destination Media Synchronization (IDMS) is essential in the emerging media consumption paradigm, which is radically evolving from passive and isolated services towards dynamic and interactive group shared experiences. This paper concentrates on improving a standardized RTP/RTCP-based solution for IDMS. In particular, novel Early Event-Driven (EED) RTCP feedback reporting mechanisms are designed to overcome latency issues and to enable higher flexibility, dynamism and accuracy when using RTP/RTCP for IDMS. The faster reaction on dynamic situations (e.g., detection of asynchrony or channel change delays) and a higher granularity for synchronizing media-related events, while preserving the RTCP bandwidth bounds, are proved through simulation tests.\"", "authors": [{"name": "Mario Montagud"}, {"name": " Fernando Boronat and Hans Stokking"}], "title": "Early Event-Driven (EED) RTCP Feedback for Rapid IDMS"}, "34": {"abstract": "\"Video streaming on the Internet is popular and the need to store and stream video content using CDNs is continually on the rise thanks to services such as Hulu and Netflix.   Adaptive HTTP video streaming using the deployed CDN infrastructure has become the de facto standard for meeting the increasing demand for video streaming on the Internet.   The storage architecture that is used for storing and streaming the video content is the focus of this study.  Hard-disk as the storage medium has been the norm for enterprise-class storage servers for the longest time.  More recently, multi-tiered storage servers (incorporating SSDs) such as Sun\u00cds ZFS and Facebook\u00cds flashcache offer an alternative to disk-based storage servers for enterprise applications.  Both these systems use the SSD as a cache between the DRAM and the hard disk.  The thesis of our work is that the current-state-of-the art in multi-tiered storage systems, architected for general-purpose enterprise workloads, do not cater to the unique needs of HTTP video streaming.  The key insight in this paper is the need to pay attention to video streaming over caching in architecting such storage servers.  Further, to reduce the ill-effects of write amplification in SSDs, it is imperative to pay attention to the granularity of write requests sent to the SSDs.  We present FlashStream, a multi-tiered storage architecture that addresses the unique needs of HTTP video streaming.  Like ZFS and flashcache, it also incorporates SSDs as a cache between the DRAM and the hard disk.   The key architectural elements of FlashStream include optimal write granularity,to overcome the write amplification effect of flash memory SSDs and a QoS-sensitive caching strategy that monitors the activity of the flash memory SSDs to ensure that video streaming performance is not hampered by the caching activity.   We have implemented FlashStream and experimentally compare it with ZFS and flashcache for HTTP streaming workloads.  We show that FlashStream outperforms both these systems for the same hardware configuration.  Specifically, it is better by a factor of two compared to its nearest competitor, namely ZFS.\"", "authors": [{"name": "Moonkyung Ryu and Umakishore Ramachandran"}], "title": "FlashStream: A multi-tiered storage architecture for HTTP video streaming incorporating Flash Memory SSDs"}, "205": {"abstract": "\"While watching live sports broadcasts we do not feel so emotionally connected with the performers and the in-venue fans as if we were watching it live, where the event takes place. Moreover, both remote and in-venue fans do not share a social connection, resulting in defragmented social experiences. This work intends to establish a new paradigm that explores the use of mobile devices to enhance remote fans\u00cd interaction with a live event. This new paradigm will provide them with an emotional and social experience by bringing the stadium atmosphere, its immersion, and emotional levels, to remote supporters. As a result, remote fans will be more engaged in the broadcasted sports, and both, remote and in-venue, fans will all feel part of the same community.\"", "authors": [{"name": "Pedro Centieiro"}], "title": "Bringing the Sport Stadium Atmosphere to Remote Fans "}, "176": {"abstract": "", "authors": [{"name": "Brendan Jou;\u00e6Hongzhi Li;\u00e6Joseph G. Ellis"}, {"name": " Dan Morozoff"}, {"name": " Shih-Fu Chang"}], "title": "News Rover: Exploring Topical Structures and Serendipity in Heterogeneous Multimedia News        "}, "60": {"abstract": "", "authors": [{"name": "Che-Hao Hsu"}, {"name": " Kai-Lung Hua"}, {"name": " Wen-Huang Cheng"}], "title": "Physiognomy Master: A Novel Personality Analysis System Based on Facial Features        "}, "61": {"abstract": "", "authors": [{"name": "Wu Liu"}, {"name": " Feibin Yang"}, {"name": " Yongdong Zhang"}, {"name": " Qinghua Huang"}, {"name": " Tao Mei"}], "title": "LAVES: A Instant Mobile Video Search System Based on Layered Audio-Video Indexing        "}, "62": {"abstract": "", "authors": [{"name": "Frederic Font"}, {"name": " Gerard Roma"}, {"name": " Xavier Serra"}], "title": "Freesound Technical Demo for ACM Multimedia 2013        "}, "63": {"abstract": "", "authors": [{"name": "Yu You"}, {"name": " Ville-Veikko Mattila"}], "title": "Visualizing Web Mash-ups for In-Situ Vision-Based Mobile AR Applications        "}, "64": {"abstract": "", "authors": [{"name": "Oscar Mayor"}, {"name": " Quim Llimona;\u00e6Marco Marchini;\u00e6Panos Papiotis"}, {"name": " Esteban Maestre"}], "title": "\"repoVizz: a framework for remote storage, browsing, annotation, and exchange of multi-modal data        \""}, "65": {"abstract": "", "authors": [{"name": "Pierre Letessier"}, {"name": " Nicolas Herv_"}, {"name": " Julien Champ"}, {"name": " Alexis Joly"}, {"name": " Olivier Buisson"}, {"name": " Amel Hamzaoui"}], "title": "Small objects query suggestion in a large web-image collection        "}, "66": {"abstract": "", "authors": [{"name": "Amirhossein Habibian"}, {"name": " Cees Snoek"}], "title": "Video2Sentence and Vice Versa        "}, "67": {"abstract": "", "authors": [{"name": "Christoph Korinke"}, {"name": " Mohamad Rabbath"}, {"name": " Dennis Lamken"}, {"name": " Susanne Boll"}], "title": "A tool for catching back your preferred videos from physical collages        "}, "68": {"abstract": "", "authors": [{"name": "Herv_ Go\u00d4au"}, {"name": " Pierre Bonnet"}, {"name": " Alexis Joly"}, {"name": " Vera Bakic"}, {"name": " Julien Barbe"}, {"name": " Souheil Selmi"}, {"name": " Jennifer Carr_"}, {"name": " Daniel Barthelemy"}, {"name": " Nozha Boujemaa"}, {"name": " Jean-Fran\u008dois Molino"}, {"name": " Gr_goire Duch_"}, {"name": " Aur_lien P_ronnet"}], "title": "Pl@ntNet Mobile App        "}, "69": {"abstract": "", "authors": [{"name": "Benjamin Guthier"}, {"name": " Kalun Ho"}, {"name": " Stephan Kopf"}, {"name": " Wolfgang Effelsberg"}], "title": "Determining Exposure Values from HDR Histograms for Smartphone Photography        "}, "175": {"abstract": "", "authors": [{"name": "Jules Fran\u008doise;\u00e6Norbert Schnell"}, {"name": " Frederic Bevilacqua"}], "title": "Gesture-based control of physical modeling sound synthesis: a mapping-by-demonstration approach        "}, "174": {"abstract": "", "authors": [{"name": "Zhenzhen Hu"}, {"name": " Min Lin"}, {"name": " Si Liu"}, {"name": " Meng Wang"}, {"name": " Richang Hong"}, {"name": " Shuicheng Yan"}], "title": "eHeritage of Shadow Puppetry: Creation and Manipulation        "}, "173": {"abstract": "", "authors": [{"name": "Pengye Xia"}, {"name": " Klara Nahrstedt"}], "title": "TEEVE Endpoint: Towards the Ease of 3D Tele-Immersive Application Development"}, "172": {"abstract": "", "authors": [{"name": "Nicolas Herv_"}, {"name": " Marie-Luce Viaud"}, {"name": " J_r\u00aame Thi\u008fvre"}, {"name": " Agn\u008fs Saulnier"}, {"name": " Pierre Letessier"}, {"name": " Julien Champ"}, {"name": " Olivier Buisson"}, {"name": " Alexis Joly"}], "title": "OTMedia: The French TransMedia News Observatory        "}, "171": {"abstract": "\"In this paper we investigate a new problem called visual business recognition. Automatic identification of businesses in images is an interesting task with plenty of potential applications especially for mobile device users. We propose a multimodal approach which incorporates business directories, textual information, and web images in a unified framework. We assume the query image is associated with a coarse location tag and utilize business directories for extracting an over complete list of nearby businesses which may be visible in the image. We use the name of nearby businesses as search keywords in order to automatically collect a set of relevant images from the web and perform image matching between them and the query. Additionally, we employ a text processing method customized for business recognition which is assisted by nearby business names; we fuse the results of image matching and text processing in a probabilistic framework to recognize the businesses. We tested the proposed algorithm on  a challenging set of user-uploaded and street view images from the cities of San Francisco, CA and Pittsburgh, PA with promising results for this new application.\"", "authors": [{"name": "Amir Roshan Zamir"}, {"name": " Afshin Dehghan"}], "title": "Visual Business Recognition - A Multimodal Approach "}, "170": {"abstract": "\"Detecting violent scenes in movies is an important video content,understanding functionality e.g., for providing automated youth protection,services. One key issue in designing algorithms for violence,detection is the choice of discriminative features. In this paper, we,employ mid-level audio features and compare their discriminative,power against low-level audio features. We fuse these mid-level,audio cues with low-level visual ones at the decision level in order,to further improve the performance of violence detection. We use,Mel-Frequency Cepstral Coefficients (MFCC) as audio and average,motion as visual features. In order to learn a violence model,,we choose two-class support vector machines (SVMs). Our experimental,results on detecting violent video shots in Hollywood,movies show that mid-level audio features are more discriminative,and provide more precise results than low-level ones. The detection,performance is further enhanced by fusing the mid-level audio cues,with low-level visual ones using an SVM-based decision fusion.\"", "authors": [{"name": "Esra Acar"}, {"name": " Frank Hopfgartner"}, {"name": " Sahin Albayrak"}], "title": "Violence Detection in Hollywood Movies by the Fusion of Visual and Mid-level Audio Cues "}, "203": {"abstract": "\"In this paper we address the issue of mapping between gesture and sound in interactive music systems. Our approach, we call mapping by demonstration, aims at learning the mapping from examples provided by users while interacting with the system. We propose a general framework for modeling gesture-sound sequences based on a probabilistic, multimodal, and hierarchical model. Two orthogonal modeling aspects are detailed and we describe planned research directions to improve and evaluate the proposed models.\"", "authors": [{"name": "Jules Fran\u008doise"}], "title": "Gesture-Sound Mapping by demonstration in Interactive Music Systems "}, "181": {"abstract": "", "authors": [{"name": "Damian Borth"}, {"name": " Tao Chen"}, {"name": " Rong-Rong Ji"}, {"name": " Shih-Fu Chang"}], "title": "SentiBank: Large-Scale Ontology and Classifiers for Detecting Sentiment and Emotions in Visual Content        "}, "182": {"abstract": "", "authors": [{"name": "Junsheng Fu"}, {"name": " Lixin Fan"}, {"name": " Yu You"}], "title": "Augmented and Interactive Video Playback Based On Global Camera Pose        "}, "183": {"abstract": "", "authors": [{"name": "Matt Yu"}, {"name": " Peter Vajda"}, {"name": " David Chen"}, {"name": " Sam Tsai"}, {"name": " Maryam Daneshi"}, {"name": " Andre Araujo"}, {"name": " Huizhong Chen"}, {"name": " Bernd Girod"}], "title": "EigenNews: A Personalized News Video Delivery Platform        "}, "180": {"abstract": "", "authors": [{"name": "Andrea Ferracani"}, {"name": " Alberto Del Bimbo"}, {"name": " Daniele Pezzatini"}], "title": "Flarty: recommending art routes using check-ins latent topics        "}, "2": {"abstract": "\"Multimedia is the combination of several media forms. Information designers, educationalists and artists are concerned with questions such as: Is text, or audio or video, or a combination of all three, the best format for the message? Should another modality (e.g., haptics/touch, olfaction) be invoked instead to make the message more effective and/or the experience more engaging? How does the setting affect perception/reception? How does framing affect people\u00cds experience of multimedia? How is the artifact changed through interaction with audience members?  In this presentation, I will talk about people\u00cds experience of multimedia artifacts like videos. I will discuss the ways in which framing affects how we experience multimedia. Framing can be intentional_ scripted creations produced with clear intent by technologists, designers, media producers, media artists, film-makers, archivists, documentarians and architects. Framing can also be unintentional.  Everyday acts of interest and consumption turn us, the viewers, into co-producers of the experiences of the multimedia artifacts we have viewed. We download, annotate, comment and share multimedia artifacts online. Our actions are reflected in viewcounts, displayed comments and content ranking. Our actions therefore change how multimedia artifacts are interpreted and understood by others.  Drawing on examples from the history of film and of performance art, from current social media research and from research conducted with collaborators over the past 16 years, I will illustrate how content understanding is modulated by context, by the \u00f1framing\u00ee of the content. I will consider three areas of research that are addressing the issue of framing, and that have implications for our understanding of \u00efmultimedia\u00cd consumption, now and in the future: (1) The psychology and psychophysiology of multimedia as multimodal experience; (2) Emerging practices with contemporary social media capture and sharing from personal devices; and (3) Innovations in social media and audience analytics focused on more deeply understanding media consumption.  I will conclude with some technical excitements, design/development challenges and experiential possibilities that lie ahead.\"", "authors": [{"name": "Elizabeth Churchill"}], "title": "Keynote 1:  Multimedia Framed"}, "162": {"abstract": "\"Complex event detection is very challenging in open source such as You-Tube videos, where usually comprise very diverse visual contents involving various object, scene and action concepts. Not all of them, however, are relevant to the target event. In other words, a video may contain a lot of ``junk'' information for recognition. Therefore, we propose a semantic pooling approach to tackle this issue. Unlike the conventional pooling over the entire video or over specific spatial regions of a video, we employ a discriminative approach to acquire abstract semantic ``regions'' for pooling. For this purpose, we  first associate low-level visual words with semantic concepts via their co-occurrence relationship. We then pool the low-level features separately according to their semantic information.  The proposed semantic pooling strategy also provides a new mechanism for incorporating semantic concepts for low-level feature based event recognition. We evaluate our approach on TRECVID MED \\cite{trecvid11} dataset and the results show that semantic pooling consistently improves the performance compared with conventional pooling strategies.\"", "authors": [{"name": "Qian Yu"}, {"name": " Jingen Liu"}], "title": "Semantic Pooling for Complex Event Detection "}, "187": {"abstract": "", "authors": [{"name": "Xingyu Gao"}], "title": "GeSoDeck: A Geo-Social Event Detection and Tracking System        "}, "184": {"abstract": "", "authors": [{"name": "Andr_ Mour\u00dco"}, {"name": " Joao Magalhaes"}], "title": "NovaEm_tions: Winning with a smile        "}, "6": {"abstract": "\"This paper presents a novel Attributes-augmented Semantic,Hierarchy (A2SH) towards bridging both the semantic,and intention gaps in Content-based Image Retrieval. A2SH,structuralizes semantic concepts into various semantic levels.,Each concept in A2SH is augmented with its related,attributes, which are exploited as the intermediate bridge,to connect the concept and low-level visual content. Based,on A2SH, we propose a new hierarchical semantic similarity,function, which characterizes the semantic affinities among,images precisely and thus can lead to accurate image retrieval.,Moreover, we develop a hybrid feedback mechanism,to allow users to deliver their search intent by providing,hybrid feedbacks on attributes and/or images. The hybrid,feedbacks augmented by A2SH enable the search system to,shape users\u00cd intent accurately and narrow down the search,to users\u00cd target quickly. Extensive experiments on a largescale,data set of 1.2 million web images well demonstrate,the superiority of the proposed A2SH based image retrieval,framework over other state-of-the-art solutions.\"", "authors": [{"name": "Hanwang Zhang"}, {"name": " Zheng-Jun Zha"}, {"name": " Yang Yang"}, {"name": " Shuicheng Yan"}, {"name": " Yue Gao and Tat-Seng Chua"}], "title": "Attributes-augmented Semantic Hierarchy for Image Retrieval"}, "186": {"abstract": "", "authors": [{"name": "Xiaoyan Wang"}, {"name": " Lifeng Sun"}, {"name": " Shou Wang"}], "title": "Group TV: A Cloud based Social TV for Group Social Experience        "}, "188": {"abstract": "", "authors": [{"name": "You Yang"}, {"name": " Qiong Liu"}, {"name": " Yue Gao"}, {"name": " Binbin Xiong"}, {"name": " Li Yu"}, {"name": " Xiangyu Chen"}, {"name": " Qi Tian"}], "title": "3D Facetime: A 2D and 3D Switchable Video Communication System        "}, "189": {"abstract": "", "authors": [{"name": "Xinghai Sun"}, {"name": " Changhu Wang"}, {"name": " Avneesh Sud"}, {"name": " Chao Xu"}, {"name": " Lei Zhang"}], "title": "MagicBrush: Image Search by Color Sketch        "}, "202": {"abstract": "Generating Social Media Snippets for Mobile Browsing", "authors": [{"name": "Wenyuan Yin"}, {"name": " Tao Mei and Chang Wen Chen"}], "title": "Generating Social Media Snippets for Mobile Browsing"}, "196": {"abstract": "\"Over the recent years, the problem of video location estimation (i.e., estimating the longitude/latitude coordinates of a video without GPS information) has been approached with diverse methods and ideas in the research community and significant improvements have been made. So far, however, systems have only been compared against each other and no systematic study on human performance has been conducted. Based on a human-subject study with over 11,000 experiments, this article presents a human baseline for location estimation for different combinations of modalities (au- dio, audio/video, audio/video/text). Furthermore, this article reports on the comparison of the accuracy of state-of-the-art location estimation systems with the human baseline. Although the overall performance of humans\u00cd multimodal video location estimation is better than current machine learning approaches, the difference is quite small: For 41 % of the test set, the machine\u00cds accuracy was superior to the humans. We present case studies and discuss why machines did better for some videos and not for others. Our analysis suggests new directions and priorities for future work on the improvement of location inference algorithms.\"", "authors": [{"name": "Jaeyoung Choi"}, {"name": " Venkatesan Ekambaram"}, {"name": " Howard Lei"}, {"name": " Pascal Kelm"}, {"name": " Luke Gottlieb"}, {"name": " Thomas Sikora"}, {"name": " Kannan Ramchandran and Gerald Friedland"}], "title": "Human vs Machine: Establishing a Human Baseline for Multimodal Location Estimation"}, "185": {"abstract": "", "authors": [{"name": "Jia Chen"}, {"name": " Qin Jin"}, {"name": " Weipeng Zhang;\u00e6Shenghua Bao"}, {"name": " Zhong Su;\u00e6Yong Yu"}], "title": "Tell Me What Happened Here in History        "}, "99": {"abstract": "\"In the scene classification, a scene can be considered as a set of object cliques. Objects inside each clique have semantic correlations with each other, while two objects from different cliques are relatively independent. To utilize these correlations for better recognition performance, we propose a new method - Object Coding on the Semantic Graph to address the scene classification problem. We first exploit prior knowledge by making statistics on a large number of labeled images and calculating the dependency degree between objects. Then, a graph is built to model the semantic correlations between objects. This semantic graph captures semantics by treating the objects as vertices and the objects affinities as the weights of edges. By encoding these semantic knowledge into the semantic graph, object coding is conducted to automatically select a set of object cliques that have strongly semantic correlations to represent a specific scene. The experimental results show that the Object Coding on semantic graph can improve the,classification accuracy.\"", "authors": [{"name": "Jingjing Chen"}, {"name": " Yahong Han"}, {"name": " Xiaochun Cao"}, {"name": " Qi Tian"}], "title": "Object Coding on the Semantic Graph for Scene Classification "}, "98": {"abstract": "\"In conventional online learning based tracking studies, fixshape,appearance modeling is often incorporated for training,samples generation, as it is simple and convenient to be applied.,However, for more general non-rigid and articulated,object, this strategy may regard some background areas as,foreground, which is likely to deteriorate the learning process.,Recently published work utilize more than one patches,to represent non-rigid object with foreground object segmentation,,but most of these segmentation for target representation,are performed only in single frame manner. Since the,motion information between the consecutive frames was not,considered by these approaches, when the backgrounds are,similar to the target, accurate segmentation is hard to be,achieved.,In this work, we proposed a novel model for non-rigid,object segmentation by incorporating consecutive gradients,flow between pair-wise frames into a Gibbs energy function.,With help from motion information, the irregular target areas,can be segmented more accurately during precise boundary,convergence. The proposed segmentation model is incorporated,into a semi-supervised online tracking framework,for training samples generation. We test the proposed tracking,on challenging videos involving heavy intrinsic variations,and occlusions. As a result, the experiments demonstrate a,significant improvement in tracking accuracy and robustness,in comparison with other state-of-art tracking works.\"", "authors": [{"name": "Tao Zhuo"}, {"name": " Yanning Zhang"}, {"name": " Peng Zhang"}, {"name": " Wei Huang"}, {"name": " Hichem Sahli"}], "title": "Non-Rigid Target Tracking based on 'Flow-Cut' in Pair-Wise Frames with online Hough Forests "}, "168": {"abstract": "\"Emotions are playing significant roles in daily life, making emotion prediction important. To date, most of state-of-the-art methods make emotion prediction for the masses which are invalid for individuals. In this paper, we propose a novel emotion prediction method for individuals based on user interest and social influence. To balance user interest and social influence, we further propose a simple yet efficient weight learning method in which the weights are obtained from users' behaviors. We perform experiments in real social media network, with 4,257 users and 2,152,037 microblogs. The experimental results demonstrate that our method outperforms traditional methods with significant performance gains.\"", "authors": [{"name": "Yun Yang"}, {"name": " Peng Cui"}, {"name": " Wenwu Zhu"}, {"name": " Shiqiang Yang"}], "title": "User Interest and Social Influence Based Emotion Prediction for Individuals "}, "169": {"abstract": "\"This paper proposes an approach to modeling the relationship between users\u00cd moods and music-listening behavior by using contextual emotions from user-generated articles. To conduct this study, we employ the audio information extracted from the music users listen to and the emotional text information extracted from the articles written by users. The audio information represents various perceptual dimensions of music listening, including danceability, loudness, mode, and tempo; the emotional text information consists of three dimensional affective states within an article: Valence, Arousal and Dominance. With these information, we adopt the Factorization Machine approach is applied to combine these different factors for music recommendation. In our experiments, a large dataset crawled from a real-world social blogging website, LiveJournal, is used to assess the performance of the proposed emotional context features. The experimental results show that, when the emotional context information from user-generated articles is included, the performance of recommendation is significantly better than the traditional collaborative filtering approach and content-based approach.\"", "authors": [{"name": "Chih-Ming Chen"}, {"name": " Jen-Yu Liu"}, {"name": " Yi-Hsuan Yang"}, {"name": " Ming-Feng Tsai"}], "title": "Using Emotional Context from Article for Contextual Music Recommendation "}, "91": {"abstract": "\"In this paper, we introduce a novel method called VisualTextualRank,        which aims to automatically extract video shots corresponding to given,        action keywords from tagged Web videos employing their metadata and,        visual features. Our proposed method analyzes the visual link structures,        between video shots simultaneously with the textual link structures,        between videos and their tags. We tested our framework on 30 human,        action categories used by baseline method. Experimental results show,        that our VisualTextualRank improve significantly the performance of,        video shot extraction system for experimented action categories.\"", "authors": [{"name": "Do Hang Nga"}, {"name": " Keiji Yanai"}], "title": "Large-scale Web Video Shot Ranking Based on Visual Features and Tag Co-occurrence "}, "90": {"abstract": "\"This paper proposes a novel method of discovering a set of image contents sharing a specific context and implicit meaning with the help of image collections from social curation platforms as corpora. Socially curated contents are promising to analyze various kinds of multimedia information, since they are manually filtered and organized based on individual preferences, interests or perspectives. Our proposed method fully exploits the process of social curation: (1) How image contents are manually grouped together by users, and (2) how image contents are distributed in the platform. Our method reveals the fact that every image content includes really various contexts that cannot necessarily be verbalized by texts. A preliminary experiment with a small collection of a million of images yields a promising result.\"", "authors": [{"name": "Akisato Kimura"}, {"name": " Katsuhiko Ishiguro"}, {"name": " Alejandro Marcos Alvarez"}, {"name": " Kaori Kataoka"}, {"name": " Kazuhiko Murasaki"}, {"name": " Makoto Yamada"}], "title": "Image context discovery from socially curated contents "}, "93": {"abstract": "\"Establishing correct correspondences between two images,has a wide range of applications, such as 2D and 3D registration,,structure from motion, and image retrieval. In,this paper, we propose a new matching method based on,spatial constraints. The proposed method has linear time,complexity, and is efficient when applying it to image retrieval.,The main assumption behind our method is that,,the local geometric structure among a feature point and its,neighbors, is not easily affected by both geometric and photometric,transformations, and thus should be preserved in,their corresponding images. We model this local geometric,structure by linear coefficients that reconstruct the point,from its neighbors. The method is flexible, as it can not,only estimate the number of correct matches between two,images efficiently, but also determine the correctness of each,match accurately. Furthermore, it is simple and easy to be,implemented. When applying the proposed method on reranking,images in an image search engine, it outperforms,the-state-of-the-art techniques.\"", "authors": [{"name": "Shanmin Pang"}, {"name": " Jianru Xue"}, {"name": " Nanning Zheng"}, {"name": " Qi Tian"}], "title": "Locality Preserving Verification for Image Search "}, "92": {"abstract": "\"People Re-identification is a challenging problem in surveillance and forensics and it aims at associating multiple instances of the same person which have been acquired from different points of view and after a temporal gap. Image-based appearance features are usually adopted but, additionally to their intrinsically low discriminability, they are subject to perspective and view-point issues. ,We propose to completely change the approach by mapping local descriptors on a 3D body model for creating a view-independent signature. An original bone-wise color descriptor is generated and reduced with PCA to compute the person signature. The virtual bone set used to map appearance features is learned using a recursive splitting approach. Finally, people matching for re-identification is performed using the Relaxed Pairwise Metric Learning, which simultaneously provides feature reduction and weighting. Experiments on a specific dataset created with the Microsoft Kinect sensor and the OpenNi libraries prove the advantages of the proposed technique with respect to state of the art methods based on 2D or non-articulated 3D body models.\"", "authors": [{"name": "Davide Baltieri"}, {"name": " Roberto Vezzani"}, {"name": " Rita Cucchiara"}], "title": "Learning Articulated Body Models for People Re-identification "}, "95": {"abstract": "\"Currently, video surveillance plays a very important role in the fields of public safety and security. For storing the videos that usually contain extremely long sequences, it requires huge space. Video compression techniques can be used to release the storage load to some extent, such as H.264/AVC. However, the existing codecs are not sufficiently effective and efficient  for encoding surveillance videos as they do not specifically consider the characteristic of surveillance videos, \\textit{i.e.} the background of surveillance video has intensive redundancy. If we have a background dictionary, the background of frames can be easily reconstructed. In other words, motion matters more in surveillance videos. This paper introuduces a novel framework for compressing such videos. We first train a background dictionary based on a small number of observed frames. With the well-trained background dictionary, we then seperate every frame into the background and motion (foreground), and store the encoded motion together with the reconstruction coefficient of background corresponding to the background dictionary. The decoding is carried out on the encoded frame in a reversed procedure. The experimental results on extensive surveillance videos demonstrate that our proposed method significantly reduces the size of video while gains much higer PSNR compared to the state of the art codecs.\"", "authors": [{"name": "Xiaojie Guo"}, {"name": " Siyuan Li"}, {"name": " Xiaochun Cao"}], "title": "Motion Matters: A Novel Framework for Compressing Surveillance Videos "}, "94": {"abstract": "\"Forensic detection of resampled digital images has become an important technology among many others to establish the integrity of digital visual content. This paper proposes a moment feature based method to detect resampled digital images. Rather than concentrating on the positions of characteristic resampling peaks, we utilize a moment feature to exploit the periodic interpolation characteristics in the frequency domain. Not only the positions of resampling peaks but also the amplitude distribution is taken into consideration. With the extracted moment feature, a trained SVM classifier is used to detect resampled digital images. Extensive experimental results show the validity and efficiency of the proposed method.\"", "authors": [{"name": "Lu Li"}, {"name": " Jianru Xue"}, {"name": " Zhiqiang Tian"}, {"name": " Nanning Zheng"}], "title": "Moment Feature Based Forensic Detection of Resampled Digital Images "}, "97": {"abstract": "\"Most photo sharing sites give their users the opportunity to manually label images. The labels collected that way are usually very incomplete due to the size of the image collections: most images are not labeled according to all the categories they belong to, and, conversely, many class have relatively few representative examples. Automated image systems that can deal with small amounts of labeled examples and unbalanced classes are thus necessary to better organize and annotate images. In this work, we propose a multiview semi-supervised bipartite ranking model which allows to leverage the information contained in unlabeled sets of images in order to improve the prediction performance, using multiple descriptions, or views of images. For each topic class, our approach first learns as many view-specific rankers as available views using the labeled data only. These rankers are then improved iteratively by adding pseudo-labeled pairs of examples on which all view-specific rankers agree over the ranking of examples within these pairs. We report on experiments carried out on the NUS-WIDE dataset, which show that the multiview ranking process improves predictive performances when a small number of labeled examples is available specially for unbalanced topic class. We show also that our approach achieves significant improvements over a state-of-the art semi-supervised multiview classification model.\"", "authors": [{"name": "Ali Fakeri-Tabrizi"}, {"name": " Massih-Reza Amini"}, {"name": " Patrick Gallinari"}], "title": "Multiview Semi-Supervised Ranking for Automatic Image Annotation "}, "163": {"abstract": "\"This paper proposes a user friendly tool for exploring one's individual photo gallery for selecting or even creating the best shot of a scene between its multiple alternatives. This functionality is realized through a graphical user interface where multiple images of a scene is selected from the generated panorama of the scene. Once the perspective is selected user is able to go through the image and explore possible alternatives coming from the other images. Using this tool, once can explore his photo gallery efficiently. Moreover, additional compositions from other images are also possible. With such additions one can go from a burst of photograph to a single best one. Even funny compositions of images, where you can duplicate a person in the same image, is possible with our proposed tool.\"", "authors": [{"name": "Emrah Tasli"}, {"name": " Jan Van Gemerts"}, {"name": " Theo Gevers"}], "title": "Spot the Differences: From a Photograph Burst to the Single Best Picture "}, "11": {"abstract": "\"Human-computer interaction (HCI) is expanding towards natural modalities of human expression. Gestures, body movements and other affective interaction techniques can change the way computers interact with humans. In this paper, we propose to extend existing interaction paradigms by including facial expression as a controller in computer games. NovaEm_tions is a multiplayer game where players score by acting an emotion through a facial expression. We designed an algorithm to offer an engaging interaction experience using the facial expression. Despite the novelty of the interaction method, our game scoring algorithm kept players engaged and competitive. A user study done with 46 users showed the success and potential for the usage of affective-based interaction in computer games. Moreover, we released a novel facial expression dataset with 42,911 images. These face images were captured in a novel and realistic setting: users playing games where a player\u00cds facial expression has an impact on the game score.\"", "authors": [{"name": "Andr_ Mour\u00dco and Jo\u00dco Magalh\u00dces"}], "title": "Competitive affective gaming: Winning with a smile"}, "10": {"abstract": "\"The rapid development of technologies in both hardware and software have made content-based multimedia services feasible on mobile devices such as smartphones and tablets; and the strong needs for mobile visual search and recognition,have been emerging. While many real applications of visual recognition require a large scale recognition systems, the same technologies that support server-based scalable visual recognition may not be feasible on mobile devices due to,the resource constraints. Although the client-server framework ensures the scalability, the realtime response subject to the limitation on network bandwidth. Therefore, the main challenge for mobile visual recognition system should be the,recognition bitrate, which is the amount of data transmission under the same recognition performance. For this work, we exploit and compare various strategies such as compact features, feature compression, feature signatures by hashing,,image scaling, etc., to enable low bitrate mobile visual recognition. We argue that thumbnail image is a competitive candidate for low bitrate visual recognition because it carries multiple features at once and multi-feature fusion is important as the size of semantic space increases. Our evaluations on two subsets of ImageNet, both contain more than 10,000 images with 19 and 137 categories, verify the efficacy,of thumbnail images. We further suggest a new strategy that combines single (local) feature signature and the thumbnail image, which achieves significant bitrate reduction from (average) 102,570 to 4,661 bytes with merely (overall) 10% performance degradation.\"", "authors": [{"name": "Yu-Chuan Su"}, {"name": " Tzu-Hsuan Chiu"}, {"name": " Yan-Ying Chen"}, {"name": " Chun-Yen Yeh and Winston H. Hsu"}], "title": "Enabling Low Bitrate Mobile Visual Recognition _ A Performance versus Bandwith Evaluation"}, "13": {"abstract": "\"This work improves the realism of synthesis and performance of ,string quartet music by generating audio through physical ,modelling of the violins, viola, and cello.  To perform music with the,physical models, virtual musicians interpret the musical score,and generate actions which control the physical models.,The resulting audio and haptic signals are examined with support,vector machines, which adjust the bowing parameters in order to,establish and maintain a desirable timbre.  This intelligent feedback,control is trained with human input, but after the initial,training is completed, the virtual musicians perform autonomously.,The system can synthesize and control different instruments of the,same type (e.g., multiple distinct violins) and has been tested on,two distinct string quartets (total of 8 violins, 2 violas, 2,cellos).  In addition to audio, the system creates a video animation of the,instrument(s) performing the sheet music.\"", "authors": [{"name": "Graham Percival"}, {"name": " Nicholas Bailey and George Tzanetakis"}], "title": "Physical Modelling and Supervised Training of a Virtual String Quartet"}, "12": {"abstract": "\"In this work, we evaluate the feasibility of tracking-based interaction using a mobile phone's or tablet's camera in order to create and edit 3D objects in augmented reality applications. We present a feasibility study investigating if and how gestures made with your finger can be used to create such objects. A revised interface design is evaluated in a user study with 24 subjects that reveals a high usability and entertainment value, but also identifies issues such as ergonomic discomfort and imprecise input for complex tasks. Hence, our results suggest a huge potential for this type of interaction in the entertainment, edutainment, and leisure domain, but limited usefulness in serious applications\"", "authors": [{"name": "Wolfgang Huerst and Joris Dekker"}], "title": "Tracking-based interaction for object creation in mobile augmented reality"}, "14": {"abstract": "\"This paper proposes a decomposition approach to listener modelling that exploits music structure annotation information. Automatically identifying repeated patterns and contrasting sections, also known as structure analysis, is a popular task in Music Information Retrieval (MIR). The repetitions that occur in music are very salient to listeners and they can characterize a piece. Self-similarity matrices (SSMs) have long been used for visualizing musical structure and they form the basis of many previous music structure analysis algorithms. Noting that listeners pay attention to many musical features (e.g., rhythm, timbre, and harmony), some approaches have calculated several SSMs and summed or multiplied them together. However, since the reasons behind a listener\u00cds analysis can change throughout a piece, the best way to combine SSMs may differ for each section. We propose to exploit this in a novel application of SSMs derived from audio recordings: using them to learn about the reasoning behind the annotation. This would allow us to mine music perception insights from existing data sets.,Our method uses quadratic programming (QP) to minimize the difference between an SSM derived from the annotation and a linear combination of decomposed SSMs derived from each of several audio features. The optimal weights suggest how the listener weighed the information from different features to provide the structural analysis. The usefulness of the method is illustrated using examples of listener annotations. The results substantiate the claim that the importance of different musical parameters varies throughout a piece, and that taking this into account may be important for perceptual and user experience modelling. Finally, we propose variations on our method and some applications.\"", "authors": [{"name": "Jordan B. L. Smith and Elaine Chew"}], "title": "Using Quadratic Programming to Estimate Musical Attention from Self-Similarity Matrices"}, "19": {"abstract": "\"With the rapid development of social networks, tagging has become an important means responsible for such rapid development. A robust tagging method must have the capability to meet the two challenging requirements: limited labeled training samples and noisy labeled training samples. In this paper, we investigate this challenging problem of learning with limited and noisy tagging and propose a discriminative model, called SpSVM-MC, that exploits both labeled and unlabeled data through a semi-parametric regularization and takes advantage of the multi-label constraints into the optimization. While SpSVM-MC is a general method for learning with limited and noisy tagging, in the evaluations we focus on the specific application of noisy image tagging with limited labeled training samples on a benchmark dataset. Theoretic analysis and extensive evaluations in comparison with state-of-the-art literature demonstrate that SpSVM-MC outstands with a superior performance.\"", "authors": [{"name": "Yingming Li"}, {"name": " Zhongang Qi"}, {"name": " Zhongfei Zhang and Ming Yang"}], "title": "Learning with Limited and Noisy Tagging"}, "18": {"abstract": "\"In this paper, we particularly investigate the problem of accelerating sparse coding for scalable image annotation. Sparse coding has been proved to be effective in many multimedia applications (e.g.  classification, content analytic and retrieval). However, its off-the-shelf solvers are generally inefficient on large-scale dataset. In general, sparsity essentially implies a very strong prior that most reconstruction coefficients should be zero. By leveraging this prior, we develop a general and efficient framework, which can derive an accurate solution to the large-scale sparse coding problem through solving a series of much smaller-scale subproblems. In this framework, an active variable set, which expands and shrinks iteratively, is maintained, with each snapshot of the active variable set corresponding to a subproblem. Meanwhile, the convergence of our proposed framework to global optimum is theoretically provable. More importantly, to further accelerate the proposed framework, a sub-linear time complexity hashing strategy, e.g. Locality-Sensitive Hashing, is seamlessly applied to efficiently search nearest neighbors of the intermediate residue vector in the expansion stage. Theoretic convergence of the hash-accelerated framework is also guaranteed. Extensive empirical experiments on NUS-WIDE as well as its subset NUS-WIDE-LITE and IMAGENET data sets demonstrate that the orders-of-magnitude acceleration is achieved by the proposed framework for large-scale image annotation, along with zero/negligible accuracy loss for the cases without/with hashing speed-up compared to the expensive off-the-shelf solvers.\"", "authors": [{"name": "Junshi Huang"}, {"name": " Hairong Liu"}, {"name": " Jialie Shen and Shuicheng Yan"}], "title": "Towards Efficient Sparse Coding for Scalable Image Annotation"}, "117": {"abstract": "\"Artists and fashion designers have recently been creating a new form of art _ Camouflage Art _ which can be used to fool computer vision algorithms. This digital art technique combines makeup and hair styling, or other modifications such as facial painting to help avoid automatic face-detection. In this paper, we present a tool that can facilitate digital art design for such camouflage. This tool can find the prominent/decisive features from facial images that constitute the face being recognized; and provide suggestions for camouflage options (makeup, styling, paints) on particular facial features or facial parts. Testing of this tool shows that it can effectively aid artists or designers in creating camouflage designs that are able to thwart face recognition techniques. The evaluation of suggested camouflages, applied to 40 celebrities, across eight different face recognition systems (both non-commercial or commercial) shows that 82.5% ~ 100% of the time the subject is unrecognizable using the suggested camouflage.\"", "authors": [{"name": "Ranran Feng and Balakrishnan Prabhakaran"}], "title": "Facilitating Fashion Camouflage Art"}, "116": {"abstract": "\"In this paper, we address the problem of robust face recognition using single sample per person. Given only one training image per subject of interest, our proposed method is able to recognize query images with illumination or expression changes, or even the corrupted ones due to occlusion. In order to model the above intra-class variations, we advocate the use of external data (i.e., images of subjects not of interest) for learning an exemplar-based dictionary. This observed dictionary provide auxiliary yet representative information for handling intra-class variation, while the gallery set containing one training image per class preserves separation between dierent subjects. Our experiments on two face image datasets conrm the effectiveness and robustness of our approach, which is shown to outperform state-of-the-art sparse representation based methods.\"", "authors": [{"name": "De-An Huang"}, {"name": " Yu-Chiang Frank Wang"}], "title": "With One Look: Robust Face Recognition Using Single Sample Per Person "}, "204": {"abstract": "\"Body movement has received increasing attention in music technology research during the last years. Some new musical interfaces make use of gestures to control music in a meaningful and intuitive way. A typical approach is to use the orchestra conducting paradigm, in which the computer that generates the music would be a virtual orchestra conducted by the user. However, although conductors' gestures are complex and their meaning can vary depending on the musical context, this context-dependency is still to explore. We propose a method to study context-dependency of body and facial gestures of conductors in orchestral classical music based on temporal clustering of gestures into actions, followed by an analysis of the evolution of audio features after action occurrences. For this, multi-modal data (audio, video, motion capture) will be recorded in real live concerts and rehearsals situations using unobtrusive techniques.\"", "authors": [{"name": "Alvaro Sarasua"}], "title": "Context-Aware Gesture Recognition in Classical Music Conducting "}, "151": {"abstract": "\"Users react differently to irrelevant and relevant tags associated with content which can be used for labeling large media databases. In this paper, we present a method to assess tag relevance to images using the non-verbal bodily responses, namely, electroencephalogram (EEG), facial expressions, and eye gaze. We conducted experiments in which 28 images were shown to 28 subjects once with correct and another time with incorrect tags. The goal of our study is to automatically detect the responses to irrelevant tags and consequently filter them out. Therefore, we trained classifiers to detect the tag relevance from spontaneous bodily responses. We evaluated the performance of our system using a subject independent approach. The precision at top 5% and top 10% detection were calculated and results of different modalities and different classifiers were compared. The results show that eye gaze outperforms the other modalities in tag relevance detection both overall and for top ranked results.\"", "authors": [{"name": "Mohammad Soleymani"}, {"name": " Sebastian Kaltwang"}, {"name": " Maja Pantic"}], "title": "Human Behavior Sensing for Tag Relevance Assessment "}, "150": {"abstract": "\"Spectral hashing (SpH) is an efficient and simple binary hashing method, which assumes that data are sampled from a multidimensional uniform distribution. However, this assumption is too restrictive in practice. In this paper we propose an improved method, Fitted Spectral Hashing, to relax this distribution assumption. Our work is based on the fact that one-dimensional data of any distribution could be mapped to a uniform distribution without changing the local neighbor relations among data items. We have found that this mapping on each PCA direction has certain regular pattern, and could fit data well by S-Curve function, Sigmoid function. With more parameters Fourier function also fit data well. Thus with Sigmoid function and Fourier function, we propose two binary hashing methods. Experiments show that our methods are efficient and outperform state-of-the-art methods.\"", "authors": [{"name": "Yu Wang"}, {"name": " Sheng Tang"}, {"name": " Yalin Zhang"}, {"name": " Jintao Li"}], "title": "Fitted Spectral Hashing "}, "153": {"abstract": "\"Common techniques represent images by quantizing local descriptors and summarizing their distribution in a histogram. In this paper we propose to employ a parametric description and compare its capabilities to histogram based approaches. We use the multivariate Gaussian distribution, applied over the SIFT descriptors, extracted with dense sampling on a spatial pyramid. Every distribution is converted to a high-dimensional descriptor, by concatenating the mean vector and the projection of the covariance matrix on the Euclidean space tangent to the Riemannian manifold. Experiments on Caltech-101 and ImageCLEF2011 are performed using the Stochastic Gradient Descent solver, which allows to deal with large scale datasets and high dimensional feature spaces.\"", "authors": [{"name": "Giuseppe Serra"}, {"name": " Costantino Grana"}, {"name": " Marco Manfredi"}, {"name": " Rita Cucchiara"}], "title": "Modeling Local Descriptors with Multivariate Gaussians for Object and Scene Recognition "}, "152": {"abstract": "\"In this paper, a new method that exploits related videos for the problem of event detection is proposed, where related videos are videos that are closely but not fully associated with the event of interest. In particular, the Weighted Margin SVM formulation is modified so that related class observations can be effectively incorporated in the optimization problem. The resulting Relevance Degree SVM is especially useful in problems where only a limited number of training observations is provided, e.g., for the EK10Ex subtask of TRECVID MED, where only ten positive and ten related samples are provided for the training of a complex event detector. Experimental results on the TRECVID MED 2011 dataset verify the effectiveness of the proposed method.\"", "authors": [{"name": "Christos Tzelepis"}, {"name": " Nikolaos Gkalelis"}, {"name": " Vasileios Mezaris"}, {"name": " Ioannis Kompatsiaris"}], "title": "Improving event detection using related videos and Relevance Degree Support Vector Machines "}, "155": {"abstract": "\"Segments, such as sentence boundaries in the texts, or an-,notated regions in the images, can be considered as useful,structural constraints (i.e., priors) for unsupervised topic,modeling. However, the segment unit (e.g., words in texts,or visual words in images) inside a given segment may be,irrelevant to the topic of the segment due to its character-,istics.How to appropriately utilize the characteristics of,each segment unit in a given segment is desirable for many,applications. This paper proposes a model introducing a,latent variable pi into Latent Dirichlet Allocation (LDA) to,capture the characteristics of segment units, which is called,piLDA. That is to say, piLDA is conducted to determine,whether each segment unit is assigned (or selected) to the,topic embedded in their corresponding segment. Compared,with other approaches that assume all the segment units,in one segment share a common topic, our proposed piLDA,has the acute selective ability to discover the discriminative,segment units (e.g., informative words or visual words). We,conduct piLDA for the clustering of news articles and images,,the performance of piLDA in terms of normalized mutual in-,formation (NMI) and clustering purity is better than other,approaches.\"", "authors": [{"name": "Siliang Tang"}, {"name": " Hanqi Wang"}, {"name": " Fei Wu"}, {"name": " Ming Chen"}, {"name": " Yueting Zhuang"}], "title": "piLDA: Document clustering with selective structural constraints "}, "154": {"abstract": "\"The goal of this paper is to simultaneously segment the object regions appearing in a set of images of the same object class, known as object co-segmentation. Different from typical methods, simply assuming that the regions common among images are the object regions, we additionally consider the disturbance from some consistent backgrounds, and indicate not only common regions but salient ones among images to be the object regions. To this end, we propose a Discriminative Low Rank Matrix recovery (DLRR) algorithm to divide the over-completely segmented regions (i.e., super-pixels) of a given image set into object and non-object ones. In DLRR, a low-rank matrix recovery term is adopted to detect salient regions in an image, while a discriminative learning term is used to distinguish the object regions from all the super-pixels. An additional regularized term is imported to jointly measure the disagreement between the predicted saliency and the objectiveness probability corresponding to each super-pixel of the image set. For the unified learning problem by connecting the above three terms, we design an efficient optimization procedure based on block-coordinate descent. Extensive experiments are conducted on two public datasets, i.e., MSRC and iCoseg, and the comparisons with some state-of-the-arts demonstrate the effectiveness of our work.\"", "authors": [{"name": "Yong Li"}, {"name": " Jing Liu"}, {"name": " Zechao Li"}, {"name": " Yang Liu"}, {"name": " Hanqing Lu"}], "title": "Object Co-segmentation Via Discriminative Low Rank Matrix Recovery "}, "157": {"abstract": "\"We present RealSense, a technology that enables users to easily share media files with proximate users by detecting the relative direction of each other only with built-in orientation sensors on smartphones. With premise that users are arranged as a circle and every user is facing the center of that circle, RealSense continuously collects the directional heading of each phone to calculate the virtual position of each user in real time during the sharing.,RealSense users can simply share photos to others by swiping and throwing the photo to another user\u00cds direction without remembering or searching the name and even the device id of a specific receiver. We evaluated the orientation sensor error and the minimal arc degree for selection, and compared RealSense with linear menu, pie menu and NFC. Our results show that the limitation that requires participants to face toward the circle center is rather acceptable and participants preferred RealSense than other sharing interactions especially they were unacquainted with each other.\"", "authors": [{"name": "Chien Peng Lin"}, {"name": " Cheng Yao Wang"}, {"name": " Hou Ren Chen"}, {"name": " Wei Chen Chu"}, {"name": " Mike Chen"}], "title": "RealSense: Directional Interaction for Proximate Mobile Sharing Using Built-in Orientation Sensors "}, "156": {"abstract": "\"Although with emerging potential towards improving text-based image search engines, the state-of-the-art reranking performance is still far from satisfactory. One intrinsic issue comes from its visual similarity metric during reranking operations, i.e. not matter with what kind of feature fusion and metric learning, the initial feature pools are based solely on visual statistics.One possibility is to incorporate visual concepts, or so-called attributes, into the above metric learning, to which end, however, whose optimal combination retains unknown. In this paper, we propose an image reranking approach, with the principle to leverage high-level attribute detections among top returned images to adapt the initial visual dictionary into a query-specific version. We start from learning an offline transposition probability between attributes and words, which is then leverage to adapt the dictionary online, as a query-dependent, semantic-induced metric learning. Extensive evaluations on several benchmark datasets with comparisons to a group of state-of-the-arts have demonstrated the effectiveness and efficiency of the proposed approach.\"", "authors": [{"name": "Jialong Wang"}, {"name": " Cheng Deng"}, {"name": " Wei Liu"}, {"name": " Rongrong Ji"}], "title": "Query-Dependent Visual Dictionary Adaptation for Image Reranking "}, "159": {"abstract": "\"Recent works on image retrieval have proposed to index images by compact representations encoding powerful local descriptors, such as the closely related vector of aggregated local descriptors and Fisher vector. By combining them with a suitable coding technique, it is possible to encode an image in a few dozen bytes while achieving excellent retrieval results. This paper revisits some assumptions proposed in this context regarding the handling of ``visual burstiness'', and shows that ad-hoc choices are implicitly done which are not desirable. Focusing on VLAD without loss of generality, we propose to modify several steps of the original design. Albeit simple, these modifications significantly improve VLAD and make it compare favorably against the state of the art, as demonstrated by experiments on large-scale image retrieval benchmarks.\"", "authors": [{"name": "Jonathan Delhumeau"}, {"name": " Philippe-Henri Gosselin"}, {"name": " Herv_ J_gou"}, {"name": " Patrick P_rez"}], "title": "Revisiting the VLAD image representation "}, "158": {"abstract": "\"Recent studies in image memorability showed that the memorability of an image is a measurable quantity and is closely correlated with semantic attributes. However, the intrinsic characteristics of memorability are not yet fully understood. It has been reported that in contrast to a popular belief unusualness or aesthetic beauty of the image may not be positively correlated with the image memorability. This counter-intuitive characteristic of memorability hinders a better understanding of image memorability and its applicability. In this paper, we investigate two new spatial features that are closely correlated with the image memorability yet intuitively explainable. We propose the Weighted Object Area (WOA) and Relative Area Rank (RAR), and empirically demonstrate their useful correlation with the image memorability. Results show that both WOA and RAR can positively improve the memorability prediction. In addition, we provide evidences that the RAR can effectively capture object-centric unusualness of size.\"", "authors": [{"name": "Jongpil Kim"}, {"name": " Sejong Yoon"}, {"name": " Vladimir Pavlovic"}], "title": "Relative Spatial Features for Image Memorability "}, "207": {"abstract": "\"Binary hashing has been widely used for efficient similarity search. Learning efficient binary codes has become a research focus and it is still a challenge. In many cases, the real-world data often lies on a low-dimensional manifold, which should be taken into account to capture meaningful neighbors. The importance of a manifold is its topology, which represents the neighborhood relationships between its subregions and the relative proximities between the neighbors of each subregion, e.g. the relative ranking of neighbors of each subregion. Most existing hashing methods try to preserve the neighborhood relationships by mapping similar points to close codes, while ignoring the neighborhood rankings. Moreover, most hashing methods lack in providing a good ranking for query results since they use Hamming distance as the similarity metric, and in practice, there are often a lot of results sharing the same distance to a query. In this paper, we propose a novel hashing method to solve these two issues jointly. The proposed method is referred to as Topology Preserving Hashing (TPH). TPH is distinct from prior works by preserving the neighborhood rankings of data points in Hamming space. The learning stage of TPH is formulated as a generalized eigendecomposition problem with closed form solutions. Experimental comparisons with other state-of-the-art methods on three noted image benchmarks demonstrate the efficacy of the proposed method.\"", "authors": [{"name": "Lei Zhang"}, {"name": " Yongdong Zhang"}, {"name": " Jinhui Tang"}, {"name": " Xiaoguang Gu"}, {"name": " Jintao Li and Qi Tian"}], "title": "Topology Preserving Hashing for Similarity Search"}, "48": {"abstract": "and present a general feature learning framework based on", "authors": [{"name": "Yingwei Pan; Ting Yao; Kuiyuan Yang; Houqiang Li; Chong-Wah Ngo; Jingdong Wang; Tao Mei"}], "title": "Image Search by Graph-based Label Propagation with Image Representation from DNN"}, "49": {"abstract": "\"the popular deep architecture. In particular, following the\"", "authors": [{"name": "Xiaoyin Che; Haojin Yang; Christoph Meinel "}], "title": "Lecture Video Segmentation by Automatically Analyzing the Synchronized Slides"}, "46": {"abstract": "\"of social media data. Therefore, we propose to transfer the\"", "authors": [{"name": "Yu-Chuan Su; Tzu-Hsuan Chiu; Guan-Long Wu; Chun-Yen Yeh; Felix Wu; Winston Hsu "}], "title": "Flickr-tag Prediction using Multi-modal Fusion and Meta Information"}, "47": {"abstract": "\"focus from the model development to latent feature learning,\"", "authors": [{"name": "Zongbo Hao; Qianni Zhang; Ebroul Ezquierdo; Nan Sang"}], "title": "Human Action Recognition by Fast Dense Trajectories"}, "44": {"abstract": "provide evidence that specially learned feature well addresses", "authors": [{"name": "Yanran Wang; Qi Dai; Rui Feng; Yu-Gang Jiang "}], "title": "Beauty is Here: Evaluating Aesthetics in Videos Using Multimodal Features and Free Training Data"}, "45": {"abstract": "\"the diverse, heterogeneous and collective characteristics\"", "authors": [{"name": "Gokhan Yildirim; Appu Shaji; Sabine S\u00d9sstrunk"}], "title": "Estimating Beauty Ratings of Videos using Supervoxels"}, "42": {"abstract": "we claim that representation is critical to the end tasks and", "authors": [{"name": "Litian Sun; Kiyoharu Aizawa "}], "title": "Action Recognition using Invariant Features under Unexampled Viewing Condition"}, "43": {"abstract": "contributes much to the model development module. We", "authors": [{"name": "Shannon Chen; Pengye Xia; Klara Nahrstedt "}], "title": "Activity-Aware Adaptive Compression: A Morphing-Based Frame Synthesis Application in 3DTI"}, "41": {"abstract": "\"development modules to meet the end tasks. In this work,\"", "authors": [{"name": "Leonidas Guibas"}], "title": "The Space Between The Images"}, "5": {"abstract": "\"Online human gesture recognition has a wide range of applications,in computer vision, especially in human-computer interaction,applications. Recent introduction of cost-effective depth cameras,brings on a new trend of research on body-movement gesture,recognition. However, there are two major challenges: i) how to,continuously recognize gestures from unsegmented streams, and ii),how to differentiate different styles of a same gesture from other,types of gestures. In this paper, we solve these two problems with,a new effective and efficient feature extraction method that uses,a dynamic matching approach to construct a feature vector for each,frame and improves sensitivity to the features of different,gestures and decreases sensitivity to the features of gestures,within the same class. Our comprehensive experiments on MSRC-12,Kinect Gesture and MSR-Action3D datasets have demonstrated a,superior performance than the stat-of-the-art approaches.\"", "authors": [{"name": "Xin Zhao"}, {"name": " Xue Li"}, {"name": " Chaoyi Pang"}, {"name": " Xiaofeng Zhu and Michael Sheng"}], "title": "Online Human Gesture Recognition from Motion Data Streams"}, "9": {"abstract": "\"Immensely popular video sharing websites such as YouTube have become the most important sources of music information for Internet users and the most prominent platform for sharing live music. The audio quality of the huge amount of live music recordings, however, varies significantly due to factors such as environmental noises, locations, and recording devices. Yet, most video search engines do not take audio quality into consideration when retrieving and ranking results. Given the fact that most users prefer live music videos with better audio quality, we propose the first automatic, non-reference audio quality assessment framework for live music video search online. We first construct two annotated datasets of live music recordings. The first dataset contains 500 human-annotated pieces, and the second contains 2,400 synthetic pieces systematically generated by adding noise effects to clean recordings. Then we formulate the assessment task as a ranking problem and try to solve it using a learning-based scheme. To validate the effectiveness of our framework, we performed both objective and subjective evaluations. Results show that our framework significantly improve the ranking performance of live music recording retrieval and can prove useful for various real-world music applications.\"", "authors": [{"name": "Zhonghua Li"}, {"name": " Ju-Chiang Wang"}, {"name": " Jingli Cai"}, {"name": " Zhiyan Duan"}, {"name": " Hsin-Min Wang and Ye Wang"}], "title": "Non-Reference Audio Quality Assessment for Online Live Music Recordings"}, "146": {"abstract": "\"Most existing video quality metrics measure temporal distortions based on optical-flow estimation, which typically has limited descriptive power of visual dynamics and low efficiency. This paper presents a unified and efficient framework to measure temporal distortions based on a spacetime texture representation of motion. We first propose an effective motion-tuning scheme to capture temporal distortions along motion trajectories by exploiting the distributive characteristic of the spacetime texture. Then we reuse the motion descriptors to build a self-information based spatiotemporal saliency model to guide the spatial pooling. At last, a comprehensive quality metric is developed by combining the temporal distortion measure with spatial distortion measure. Our method demonstrates high efficiency and excellent correlation with the human perception of video quality.\"", "authors": [{"name": "Peng Peng"}, {"name": " Kevin Cannons"}, {"name": " Ze-Nian Li"}], "title": "Efficient Video Quality Assessment Based on Spacetime Texture Representation "}, "147": {"abstract": "\"Hierarchical classification (HC) is a popular and efficient way for detecting the semantic concepts from the images. However, the conventional HC, which always selects the branch with the highest classification response to go on, has the risk of propagating serious errors from higher levels of the hierarchy to the lower levels. We argue that the highest-response-first strategy is too arbitrary, because the candidate nodes are considered individually which ignores the semantic relationship among them. In this paper, we propose a novel method for HC, which is able to utilize the semantic relationship among candidate nodes and their children to recover the responses of unreliable classifiers of the candidate nodes, with the hope of providing the branch selection a more globally valid and semantically consistent view. The experimental results show that the proposed method outperforms the conventional HC methods and achieves a satisfactory balance between the accuracy and efficiency.\"", "authors": [{"name": "Shiai Zhu"}, {"name": " Xiao-Yong Wei"}, {"name": " Chong-Wah Ngo"}], "title": "Error Recovered Hierarchical Classification "}, "144": {"abstract": "\"Adopting continuous dimensional annotations for a ective analysis has been gaining rising attention by researchers over the past years. Due to the idiosyncratic nature of this problem, many subproblems have been identi ed, spanning from the fusion of multiple continuous annotations (crowdsourcing) to exploiting output-correlations amongst emotion dimensions. In this paper, we rstly empirically answer several important questions which have found partial or no answer at all so far in related literature. In more detail, we study the correlation of each emotion dimension (i) with respect to other emotion dimensions, (ii) to basic emotions (e.g., happiness, anger). As a measure for comparison, we use video and audio features. Interestingly enough, we nd that (i) each emotion dimension is more correlated with other emotion dimensions rather than with face and audio features, and similarly (ii) that each basic emotion is more correlated with emotion dimensions than with audio and video features. Motivated by these ndings, we present a novel regression algorithm (Correlated-Spaces Regression, CSR), inspired by Canonical Correlation Analysis (CCA) which learns output- correlations and performs supervised dimensionality reduction and multimodal fusion by (i) projecting features extracted from all modalities and labels onto a common space where their inter-correlation is maximised and (ii) learning mappings from the projected feature space onto the projected, uncorrelated label space.\"", "authors": [{"name": "Mihalis A. Nicolaou"}, {"name": " Stefanos Zafeiriou"}, {"name": " Maja Pantic"}], "title": "Correlated-Spaces Regression for learning continuous emotion dimensions "}, "145": {"abstract": "\"As a collaborative wiki-based encyclopedia, Wikipedia provides a huge amount of articles on various categories. In addition to its text corpus, Wikipedia also contains plenty of images which makes the articles more intuitive for readers to understand. To better organize these visual and textual data, one promising area of research is to jointly model the topics across multi-modal data (i.e. cross-media) from Wikipedia. In this work, we propose to learn the projection matrices that map the data from heterogeneous feature spaces into a unified latent topic space. Different from previous approaches, by imposing the L1 regularizers to the projection matrices, only a small number of relevant visual/textual words are associated with each topic, which makes our model more interpretable and robust. Furthermore, the correlations of Wikipedia data in different modalities are explicitly considered in our model. The effectiveness of the proposed topic extraction algorithm is verified by several experiments conducted on real Wikipedia datasets.\"", "authors": [{"name": "Xikui Wang"}, {"name": " Yang Liu"}, {"name": " Donghui Wang"}, {"name": " Fei Wu"}], "title": "Cross-media Topic Mining on Wikipedia "}, "142": {"abstract": "\"Fusion of audio and visual modalities has been shown superior,over each individual modality in many multimodal,recognition tasks. It is usually achieved either by classifier,fusion or feature fusion schemes. While the latter has been,known to be more effective, in most cases it is attained as,a simple concatenation of audio and visual features. Such,feature fusion scheme treats features from each modality independently,,which can pose a problem when one of the,modalities has noisy features. In this paper, we investigate,whether feature fusion based on explicit modelling of interactions,between audio and visual features can enhance performance,of the classifier that performs feature fusion using,simple concatenation of the audio-visual features. To this,end, we propose a log-linear model, named Bimodal Loglinear,regression, which accounts for interactions between,the features of the two modalities. The performance of,the target classifiers is measured in the task of laughter-vsspeech,discrimination, since both laughter and speech are,naturally audiovisual events. Our experiments, conducted,on the MAHNOB laughter database, suggest that feature,fusion based on explicit modelling of interactions between,the audio-visual features leads overall better performance of,the log-linear classifiers for laughter and speech, compared,to when simple concatenation of the features is used.\"", "authors": [{"name": "Ognjen Rudovic"}, {"name": " Stavros Petridis"}, {"name": " Maja Pantic"}], "title": "Bimodal Log-linear Regression for Fusion of Audio and Visual Features "}, "143": {"abstract": "\"This paper focuses on fine-grained classification by detecting photographed text in images. We introduce a new text detection method that does not try to detect all possible foreground text regions but instead aims to reconstruct the scene background to eliminate non-text regions. Object cues such as color, contrast, and objectiveness are used in corporation with a random forest classifier to detect background pixels in the scene. Results on two publicly available datasets ICDAR03 and a fine-grained \\emph{Building} subcategories of ImageNet shows the effectiveness of the proposed method.\"", "authors": [{"name": "Sezer Karaoglu"}, {"name": " Jan van Gemert"}, {"name": " Theo Gevers"}], "title": "Con-Text: Text Detection Using Background Connectivity for Fine-Grained Object Classification "}, "140": {"abstract": "\"Web image re-ranking aims to automatically refine the initial text-based image search results by employing visual information. A strong line of work in image re-ranking relies on building image graphs that requires computing distances between image pairs. In this paper, we present Anchor Concept Graph Distance (ACG Distance), a novel distance measure for image re-ranking. For a given textual query, an Anchor Concept Graph (ACG) is automatically learned from the initial text-based search results. The nodes of the ACG (i.e., anchor concepts) and their correlations well model the semantic structure of the images to be re-ranked. Images are projected to the anchor concepts. The projection vectors undergo a diffusion process over the ACG, and then are used to compute the ACG distance. The ACG distance reduces the semantic gap and better represents distances between images. Experiments on the MSRA-MM and INRIA datasets show that the ACG distance consistently outperforms existing distance measures and significantly improves start-of-the-art methods in image re-ranking.\"", "authors": [{"name": "Shi Qiu"}, {"name": " Xiaogang Wang"}, {"name": " Xiaoou Tang"}], "title": "Anchor Concept Graph Distance for Web Image Re-ranking "}, "141": {"abstract": "\"In this paper we propose an approach for automatically recognizing ancient Egyptian hieroglyph from photographs. To this end we first manually annotated and segmented a large collection of 3,400+ hieroglyphs. In our automatic approach we localize and segment each individual hieroglyph, determine the reading order and subsequently evaluate 5 visual descriptors in 3 different matching schemes to evaluate visual hieroglyph recognition. In addition to visual-only cues, we use a corpus of Egyptian texts to learn language models that help re-rank the visual output.\"", "authors": [{"name": "Morris Franken"}, {"name": " Jan van Gemert"}], "title": "Automatic Egyptian Hieroglyph Recognition by Retrieving Images as Texts "}, "209": {"abstract": "\"Most existing cross-modal hashing methods suffer from the scalability issue in the training phase. In this paper, we propose a novel cross-modal hashing approach with a linear time complexity to the training data size, to enable scalable indexing for multimedia search across multiple modals. Taking both the intra-similarity in each modal and the inter-similarity across different modals into consideration, the proposed approach aims at effectively learning hash functions from large-scale training datasets. More specifically, for each modal, we first partition the training data into $k$ clusters and then represent each training data point with its distances to $k$ centroids of the clusters. Interestingly, such a $k$-dimensional data representation can reduce the time complexity of the training phase from traditional $O(n^2)$ or higher to $O(n)$, where $n$ is the training data size, leading to practical learning on large-scale datasets. We further prove that this new representation preserves the intra-similarity in each modal. To preserve the inter-similarity among data points across different modals, we transform the derived data representations into a common binary subspace in which binary codes from all the modals are ``consistent'' and comparable. The transformation simultaneously outputs the hash functions for all modals, which are used to convert unseen data into binary codes. Given a query of one modal, it is first mapped into the binary codes using the modal's hash functions, followed by matching the database binary codes of any other modals. Experimental results on two benchmark datasets confirm the scalability and the effectiveness of the proposed approach in comparison with the state of the art.\"", "authors": [{"name": "Xiaofeng Zhu"}, {"name": " Zi Huang"}, {"name": " Heng Tao Shen and Xin Zhao"}], "title": "Linear Cross-Modal Hashing for Effective Multimedia Search"}, "208": {"abstract": "\"In this paper, we propose a novel method to learn similarity-preserving hash functions for approximate nearest neighbor (NN) search. The key idea is to learn hash functions by maximizing the alignment between the similarity orders computed from the original space and the ones in the hamming space. The problem of mapping the NN points into different hash codes is taken as a classification problem in which the points are categorized into several groups according to the hamming distances to the query. The hash functions are optimized from the classifiers pooled over the training points. Experimental results demonstrate the superiority of our approach over existing state-of-the-art hashing techniques.\"", "authors": [{"name": "Jianfeng Wang"}, {"name": " Jingdong Wang"}, {"name": " Nenghai Yu and Shipeng Li"}], "title": "Order preserving hashing for approximate nearest neighbor search"}, "148": {"abstract": "\"Processing visual content in images and videos is a challenging task associated with the development of modern computer vision. Because salient point approaches can represent distinctive and affine invariant points in images, many approaches have been proposed over the past decade. Each method has particular advantages and limitations and may be appropriate in different contexts. In this paper we evaluate the performance of a wide set of salient point detectors and descriptors. We begin by comparing diverse salient point algorithms (SIFT, SURF, BRIEF, ORB, FREAK, BRISK, STAR, GFTT and FAST) with regard to repeatability, recall and precision and then move to accuracy and stability in real-time video tracking.\"", "authors": [{"name": "Song Wu"}, {"name": " Michael Lew"}], "title": "Evaluation of salient point methods "}, "149": {"abstract": "\"Image/video collection summarization is an emerging paradigm,to provide an overview of contents stored in massive databases.,Existing algorithms require at least O(N) time to generate,a summary, which cannot be applied to online scenarios.,Assuming that contents are represented as a graph, we propose ,a fast image/video collection summarization algorithm,using local graph clustering. After a query node is specied, ,our algorithm first finds a small sub-graph covering the,query node without looking at the whole graph, and then,selects fewer number of nodes diverse to each other. Our,algorithm thus provides a summary in constant time in the,number of contents. Experimental results demonstrate that,our algorithm is signicantly faster than a state-of-the-art,method, with comparable summarization quality.\"", "authors": [{"name": "Shuhei Tarashima"}, {"name": " Go Irie"}, {"name": " Ken Tsutsuguchi"}, {"name": " Hiroyuki Arai"}, {"name": " Yukinobu Taniguchi"}], "title": "Fast Image/Video Collection Summarization with Local Clustering "}, "77": {"abstract": "\"To achieve maximum mobility, device-less approaches for home appliance remote control have received increasing attention in recent years. In this paper, we propose a screen-less virtual touch panel, called AirTouch Panel, which can be positioned at any place with various orientations around users. The proposed virtual touch panel provides a potential ability to remotely control the home appliances, such as television, air conditioner, and so on. The proposed system allows users to anchor the panel at the place with comfortable poses. If the users want to change panel\u00cds position or orientation, they only need to re-anchor it, and then the panel will be reset. In this paper, our main contribution is to design a re-anchorable virtual panel for digital home remote control. Most importantly, we explore the design of such imaginary interface through three user studies. In our user studies, we analyze task completion time, satisfaction rate, and the number of miss-click. We are interested in the feasibility issues, for example, proper click gesture, panel size and button size, etc. Moreover, based on the AirTouch Panel, we also developed an intelligent TV to demonstrate the usability for controlling home appliance.\"", "authors": [{"name": "Shih-Yao Lin"}, {"name": " Chuen-Kai Shie"}, {"name": " Shen-Chi Chen"}, {"name": " Yi-Ping Hung"}], "title": "AirTouch Panel: A Re-Anchorable Virtual Touch Panel "}, "76": {"abstract": "", "authors": [{"name": "Guanfeng Wang"}, {"name": " Beomjoo Seo"}, {"name": " Yifang Yin"}, {"name": " Roger Zimmermann"}, {"name": " Zhijie Shen"}], "title": "OSCOR: An Orientation Sensor Data Correction System for Mobile Generated Contents        "}, "75": {"abstract": "", "authors": [{"name": "Luoqi Liu"}, {"name": " Hui Xu"}, {"name": " Si Liu"}, {"name": " Junliang Xing"}, {"name": " Xi Zhou"}, {"name": " Shuicheng Yan"}], "title": "Wow! You Are So Beautiful Today!        "}, "74": {"abstract": "", "authors": [{"name": "Yichao Jin"}, {"name": " Tian Xie"}, {"name": " Yonggang Wen"}, {"name": " Haiyong Xie"}], "title": "Multi-Screen Cloud Social TV: Transforming TV Experience into 21th Century        "}, "73": {"abstract": "", "authors": [{"name": "Chao Dong"}, {"name": " Shifeng Chen"}, {"name": " Xiaoou Tang"}], "title": "AdVisual: A Visual-based Advertising System        "}, "72": {"abstract": "", "authors": [{"name": "Sandro Hardy;\u00e6Stefan G_bel"}, {"name": " Ralf Steinmetz"}], "title": "Adaptable and Personalized Game-based Training Systems for Fall Prevention        "}, "71": {"abstract": "", "authors": [{"name": "Peng Wu"}, {"name": " Rares Vernica"}, {"name": " Qian Lin"}], "title": "Cloud Based Multimedia Analytic Platform        "}, "70": {"abstract": "", "authors": [{"name": "Julien Law-To"}, {"name": " Gregory Grefenstette"}, {"name": " R_mi Landais"}], "title": "Semantic Dispatching of Multimedia News with MEWS        "}, "79": {"abstract": "\"Due to the semantic gap, the low-level features are not able to,semantically represent images well. Besides, traditional semantic,related image representation may not be able to cope with large,inter class variations and are not very robust to noise. To solve these problems, in this paper, we propose a novel image representation method in the sub-semantic space. First, examplar classifiers are trained by separating each training image from the others and serve as the weak semantic similarity measurement. Then a graph is constructed by combining the visual similarity and weak semantic similarity of these training images. We partition this graph into visually and semantically similar sub-sets. Each sub-set of images are then used to train classifiers in order to separate this sub-set from the others. The learned sub-set classifiers are,then used to construct a sub-semantic space based representation of images. This sub-semantic space is not only more semantically meaningful but also more reliable and resistant to noise. Finally, we make categorization of images using this sub-semantic space based representation on several public datasets to demonstrate the,effectiveness of the proposed method.\"", "authors": [{"name": "Chunjie Zhang"}, {"name": " Shuhui Wang"}, {"name": " Qingming Huang"}, {"name": " Chao Liang"}, {"name": " Jing Liu"}, {"name": " Qi Tian"}], "title": "Beyond bag of words: Image representation in sub-semantic space "}, "78": {"abstract": "\"Background subtraction, the task to detect moving objects,in a scene, is an important step in video analysis. In this paper, we propose an efficient background subtraction method,based on coherent trajectory decomposition. We assume,that the trajectories from background lie in a low-rank subspace, and foreground trajectories are sparse outliers in this,background subspace. Meanwhile, the Markov Random Field,(MRF) is used to encode the spatial coherency and trajectory consistency. With the low-rank decomposition and the,MRF, our method can better handle videos with moving,camera and obtain coherent foreground. Experimental results on a video dataset show our method achieves very competitive performance.\"", "authors": [{"name": "Zhixiang Ren"}, {"name": " Liang-Tien Chia"}, {"name": " Deepu Rajan"}], "title": "Background Subtraction via Coherent Trajectory Decomposition "}}