entities={
  "research863": {
    "title": "A Data Quality Metric (DQM): How to Estimate the Number of Undetected Errors in Data Sets", 
    "abstract": "Data cleaning, whether manual or algorithmic, is rarely perfect leaving a dataset with an unknown number of false positives and false negatives after cleaning. In many scenarios, quantifying the number of remaining errors is challenging because our data integrity rules themselves may be incomplete, or the available gold-standard datasets may be too small to extrapolate. As  the  use  of  inherently fallible crowds becomes more prevalent in data cleaning problems, it is important to have estimators to quantify the extent of such errors. We propose novel species estimators to estimate the number of distinct remaining errors in a dataset after it has been cleaned by a set of crowd workers -- essentially, quantifying the utility of hiring additional workers to clean the dataset. This problem requires new estimators that are robust to false positives and false negatives, and we empirically show on three real-world datasets  that existing species estimators are unstable for this problem, while our proposed techniques quickly converge.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Brown University", 
        "name": "Yeounoh Chung"
      }, 
      {
        "affiliation": "UC Berkeley", 
        "name": "Sanjay Krishnan"
      }, 
      {
        "affiliation": "Brown University", 
        "name": "Tim Kraska"
      }
    ], 
    "type": "research", 
    "id": "research863"
  }, 
  "research314": {
    "title": "A Declarative Query Processing System for Nowcasting", 
    "abstract": "Nowcasting is the practice of using social media data to quantify ongoing real-world phenomena. It has been used by researchers to measure flu activity, unemployment behavior, and more. However, the typical nowcasting workflow requires either slow and tedious manual searching of relevant social media messages or automated statistical approaches that are prone to spurious and low-quality results.  In this paper, we propose a method for declaratively specifying a nowcasting model; this method involves processing a user query over a very large social media database, which can take hours. Due to the human-in-the-loop nature of constructing nowcasting models, slow runtimes place an extreme burden on the user. Thus we also propose a novel set of query optimization techniques, which allow users to quickly construct nowcasting models over very large datasets. Further, we propose a novel query quality alarm that helps users estimate phenomena even when historical ground truth data is not available. These contributions allow us to build a declarative nowcasting data management system, RaccoonDB, which yields high-quality results in interactive time.  We evaluate RaccoonDB using 40 billion tweets collected over five years. We show that our automated system saves work over traditional manual approaches while improving result quality---57% more accurate in our user study---and that its query optimizations yield a 424x speedup, allowing it to process queries 123x faster than a 300-core Spark cluster, using only 10% of the computational resources. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Michigan", 
        "name": "Dolan Antenucci"
      }, 
      {
        "affiliation": "University of Michigan", 
        "name": "Michael Anderson"
      }, 
      {
        "affiliation": "University of Michigan", 
        "name": "Michael Cafarella"
      }
    ], 
    "type": "research", 
    "id": "research314"
  }, 
  "research1320": {
    "title": "A Forward Scan based Plane Sweep Algorithm for Parallel Interval Joins", 
    "abstract": "The interval join is a basic  operation that finds application in temporal,  spatial, and uncertain databases.  Although a number of centralized and distributed algorithms have been proposed for the efficient evaluation of interval joins,  classic plane sweep approaches have not been considered at their full potential. A recent piece of related work proposes an optimized approach based on plane sweep for modern hardware, showing that it greatly outperforms previous work. However, this approach depends on the development of a complex data structure and its parallelization has not been adequately studied. In this paper, we explore the applicability of a largely ignored forward scan (FS) based plane sweep algorithm, which is extremely simple to implement. We proposed two novel optimized versions of FS that greatly reduce its cost, making it competitive to the state-of-the-art single-threaded algorithm.  In addition, we show the drawbacks of a previously proposed hash-based partitioning approach for parallel join processing and suggest a domain-based partitioning approach that does not produce duplicate results.  Within our approach we propose a novel breakdown of the partition join jobs into a small number of independent mini-join jobs with varying cost and manage to avoid redundant comparisons. Finally, we show how these mini joins can be scheduled in a smaller number of CPU cores and propose an adaptive domain partitioning, aiming at load balancing. We include an experimental study that  demonstrates the efficiency of our optimized FS and the scalability of our parallelization framework.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "", 
        "name": "Panagiotis Bouros"
      }, 
      {
        "affiliation": "Hong Kong University", 
        "name": "Nikos Mamoulis"
      }
    ], 
    "type": "research", 
    "id": "research1320"
  }, 
  "research333": {
    "title": "A General Framework for Estimating Graphlet Statistics via Random Walk", 
    "abstract": "Graphlets are induced subgraph patterns and have been frequently applied to characterize the local topology structures of graphs across various domains, e.g., online social networks (OSNs) and biological networks. Discovering and computing graphlet statistics are highly challenging. First, the massive size of real-world graphs makes the exact computation of graphlets extremely expensive. Secondly, the graph topology may not be readily available so one has to resort to web crawling using the available application programming interfaces (APIs). In this work, we propose a general and novel framework to estimate graphlet statistics of ``any size''. Our framework is based on collecting samples through consecutive steps of random walks. We derive an analytic bound on the needed sample size (via the Chernoff-Hoeffding technique) to guarantee the convergence of our unbiased estimator. To further improve the accuracy, we introduce two novel optimization techniques to reduce the lower bound on the sample size. Experimental evaluations demonstrate that our methods outperform the state-of-the-art method up to an order of magnitude both in terms of accuracy and time cost.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "The Chinese University of HK", 
        "name": "Xiaowei Chen"
      }, 
      {
        "affiliation": "University of Science and Technology of China", 
        "name": "Yongkun Li"
      }, 
      {
        "affiliation": "Xi\u00e2\u0080\u0099an Jiaotong University", 
        "name": "Pinghui Wang"
      }, 
      {
        "affiliation": "The Chinese University of Hong Kong", 
        "name": "John C.S. Lui"
      }
    ], 
    "type": "research", 
    "id": "research333"
  }, 
  "research411": {
    "title": "A General and Parallel Platform for Mining Co-Movement Patterns over Large-scale Trajectories", 
    "abstract": "Discovering co-movement patterns from large-scale trajectory  databases is an important mining task and has a wide spectrum of applications. Previous studies have identified several types of interesting co-movement patterns and showcased their usefulness. In this paper, we make two key contributions to this research field. First, we propose a more general co-movement pattern to unify those defined in the past literature. Second, we propose two types of parallel and scalable frameworks and deploy them on Apache Spark. To the best of our knowledge, this is the first work to mine co-movement patterns in real life trajectory databases with hundreds of millions of points. Experiments on three real life large-scale trajectory datasets have verified the efficiency and scalability of our proposed solutions.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "NUS", 
        "name": "QI FAN"
      }, 
      {
        "affiliation": "", 
        "name": "Dongxiang Zhang"
      }, 
      {
        "affiliation": "", 
        "name": "Huayu Wu"
      }, 
      {
        "affiliation": "NUS", 
        "name": "Kian-Lee Tan"
      }
    ], 
    "type": "research", 
    "id": "research411"
  }, 
  "research1304": {
    "title": "A holistic view of stream partitioning costs", 
    "abstract": "Stream processing has become the dominant processing model for monitoring and real\u00e2\u0080\u0093time analytics. Modern Parallel Stream Processing Engines (pSPEs) have made it feasible to increase the performance in both monitoring and analytical queries by parallelizing a query\u00e2\u0080\u0099s execution and distributing the load on multiple workers. A determining factor for the performance of a pSPE is the partitioning algorithm used to disseminate tuples to the workers. Until now, partitioning methods in pSPEs have been similar to the ones used in parallel databases and only recently load-aware algorithms have been employed to improve the effectiveness of parallel execution. We identify and demonstrate the need to incorporate aggregation costs in the partitioning model when executing stateful operations in parallel, in order to minimize the overall latency and/or throughput. Towards this, we propose new stream partitioning algorithms,that consider both tuple imbalance and aggregation cost. We evaluate our proposed algorithms and show that they can achieve up to an order of magnitude better performance, compared to the current state of the art.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Pittsburgh", 
        "name": "Nikos R. Katsipoulakis"
      }, 
      {
        "affiliation": "University of Pittsburgh", 
        "name": "Alexandros   Labrinidis"
      }, 
      {
        "affiliation": "University of Pittsburgh", 
        "name": "Panos Chrysanthis"
      }
    ], 
    "type": "research", 
    "id": "research1304"
  }, 
  "research1321": {
    "title": "ASAP: A Streaming Operator for Smoothing Time Series Visualizations", 
    "abstract": "Time series visualization of streaming telemetry (i.e., charting of key metrics such as server load over time) is increasingly prevalent in recent application deployments.  Existing systems simply plot the raw data streams as they arrive, potentially obscuring large-scale deviations due to local variance and noise. We propose an alternative: to better prioritize attention in time series exploration and monitoring visualizations, smooth the time series as much as possible to remove noise and thus highlight long-term deviations. We develop a new technique for automatically smoothing streaming time series that adaptively optimizes this trade-off between noise reduction (i.e., variance) and outlier retention (i.e., kurtosis). We introduce metrics to quantitatively assess the quality of the choice of smoothing parameter and provide an efficient streaming analytics operator, ASAP, that optimizes these metrics by combining techniques from stream processing, user interface design, and signal processing via a novel autocorrelation-based pruning strategy and pixel-aware preaggregation. We demonstrate that ASAP is able to improve users' accuracy in identifying significant deviations in time series by up to 38.4\\% while reducing response times by up to 44.3\\%. Moreover, ASAP delivers these results several orders of magnitude faster than alternative optimization strategies.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Stanford University", 
        "name": "Kexin Rong"
      }, 
      {
        "affiliation": "Stanford University", 
        "name": "Peter Bailis"
      }
    ], 
    "type": "research", 
    "id": "research1321"
  }, 
  "research512": {
    "title": "AdaptDB: Adaptive Partitioning for Distributed Joins", 
    "abstract": "Big data analytics often involves complex join queries over two or more tables. Such join processing is expensive in a distributed setting both because large amounts of data must be read from disk, and because of data shuffling across the network. Many techniques based on data partitioning have been proposed to reduce the amount of data that must be  accessed, often focusing on finding the best partitioning scheme for a particular workload, rather than adapting to changes in the workload over time.  In this paper, we present AdaptDB, an adaptive storage manager for  analytical database workloads in a distributed setting. It works by partitioning datasets across a cluster and incrementally refining data partitioning as queries are run. AdaptDB introduces a novel hyper join that avoids expensive data shuffling by identifying storage blocks of the joining tables that overlap on the join attribute, and only joining those blocks.  Hyper join performs well when each block in one table overlaps with few blocks in the other table, since that will minimize the number of blocks that have to be accessed.  To minimize the number of overlapping blocks for common join queries, AdaptDB users smooth repartitioning to repartition small portions of the tables on join attributes as queries run. A prototype of AdaptDB running on top of Spark improves query performance by 2-3x on TPC-H as well as real-world dataset,  versus a system that employs scans and shuffle-joins. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "MIT", 
        "name": "Yi Lu"
      }, 
      {
        "affiliation": "MIT", 
        "name": "Anil Shanbhag"
      }, 
      {
        "affiliation": "Microsoft", 
        "name": "Alekh Jindal"
      }, 
      {
        "affiliation": "MIT", 
        "name": "Samuel Madden"
      }
    ], 
    "type": "research", 
    "id": "research512"
  }, 
  "research138": {
    "title": "Adaptive NUMA-aware data placement and task scheduling for analytical workloads in main-memory column-stores", 
    "abstract": "Non-uniform memory access (NUMA) architectures pose numerous performance challenges for main-memory column-stores in scaling up analytics on modern multi-socket multi-core servers. A NUMA-aware execution engine needs a strategy for data placement and task scheduling that prefers fast local memory accesses over remote memory accesses, and avoids an imbalance of resource utilization, both CPU and memory bandwidth, across sockets. State-of-the-art systems typically use a static strategy that always partitions data across sockets, and always allows inter-socket task stealing.  In this paper, we show that adapting data placement and task stealing to the workload can improve throughput by up to a factor of 4 compared to a static approach. We focus on highly concurrent workloads dominated by operators working on a single table or table group (copartitioned tables). Our adaptive data placement algorithm tracks the resource utilization of tasks, partitions of tables and table groups, and sockets. When a utilization imbalance across sockets is detected, the algorithm corrects it by moving or repartitioning tables. Also, inter-socket task stealing is dynamically disabled for memory-intensive tasks that could otherwise hurt performance.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "EPFL", 
        "name": "Iraklis Psaroudakis"
      }, 
      {
        "affiliation": "SAP SE", 
        "name": "Tobias Scheuer"
      }, 
      {
        "affiliation": "SAP SE", 
        "name": "Norman May"
      }, 
      {
        "affiliation": "SAP SE", 
        "name": "Abdelkader Sellami"
      }, 
      {
        "affiliation": "EPFL", 
        "name": "Anastasia Ailamaki"
      }
    ], 
    "type": "research", 
    "id": "research138"
  }, 
  "industrial1212": {
    "title": "Adaptive Statistics in Oracle 12c", 
    "abstract": "Database Management Systems (DBMS) continue to be the foundation of mission critical applications, both OLTP and Analytics. They provide a safe, reliable and efficient platform to store and retrieve data. SQL is the lingua franca of the database world. A database developer writes a SQL statement to specify data sources and express the desired result and the DBMS will figure out the most efficient way to implement it. The query optimizer is the component in a DBMS responsible for finding the best execution plan for a given SQL statement based on statistics, access structures, location, and format. At the center of a query optimizer is a cost model that consumes the above information and helps the optimizer make decisions related to query transformations, join order, join methods, access paths, and data movement. The final execution plan produced by the query optimizer depends on the quality of information used by the cost model, as well as the sophistication of the cost model. In addition to statistics about the data, the cost model also relies on statistics generated internally for intermediate results, e.g. size of the output of a join operation. This paper presents the problems caused by incorrect statistics of intermediate results, survey the existing solutions and present our solution introduced in Oracle 12c. The solution includes validating the generated statistics using table data and via the automatic creation of auxiliary statistics structures. We limit the overhead of the additional work by confining their use to cases where it matters the most, caching the computed statistics, and using table samples. The statistics management is automated. We demonstrate the benefits of our approach based on experiments using two SQL workloads, a benchmark that uses data from the Internal Movie Data Base (IMDB) and a real customer workload. ", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "Oracle", 
        "name": "Mohamed Zait"
      }, 
      {
        "affiliation": "Oracle", 
        "name": "Sunil Chakkappen"
      }, 
      {
        "affiliation": "Oracle Labs", 
        "name": "Suratna Budalakoti"
      }, 
      {
        "affiliation": "Oracle", 
        "name": "Satyanarayana Valluri"
      }, 
      {
        "affiliation": "Oracle", 
        "name": "Ramarajan  Krishnamachari"
      }, 
      {
        "affiliation": "Oracle Labs", 
        "name": "Alan Wood"
      }
    ], 
    "type": "industrial", 
    "id": "industrial1212"
  }, 
  "research602": {
    "title": "Adaptive Work Placement for Query Processing on Heterogeneous Computing Resources", 
    "abstract": "The hardware landscape is currently changing from homogeneous multi-core systems towards heterogeneous systems with many different computing units, each with their own characteristics. This trend is a great opportunity for database systems to increase the overall performance if the heterogeneous resources can be utilized efficiently. To achieve this, the main challenge is to place the right work on the right computing unit. Current approaches tackling this placement for query processing assume that data cardinalities of intermediate results can be correctly estimated. However, this assumption does not hold for complex queries. To overcome this problem, we propose an adaptive placement approach being independent of cardinality estimation of intermediate results. Our approach is incorporated in a novel adaptive placement sequence. Additionally, we implement our approach as an extensible virtualization layer, to demonstrate the broad applicability with multiple database systems. In our evaluation, we clearly show that our approach significantly improves OLAP query processing on heterogeneous hardware, while being adaptive enough to react to changing cardinalities of intermediate query results. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "TU Dresden", 
        "name": "Tomas Karnagel"
      }, 
      {
        "affiliation": "TU", 
        "name": "Dirk Habich"
      }, 
      {
        "affiliation": "Technische Universit\u00c3\u00a4t Dresden", 
        "name": "Wolfgang Lehner"
      }
    ], 
    "type": "research", 
    "id": "research602"
  }, 
  "research1383": {
    "title": "An Efficient Probabilistic Approach for Graph Similarity Search", 
    "abstract": "Graph similarity search is a common and fundamental operation in graph databases. One of the most popular graph similarity measures is the Graph Edit Distance (GED) mainly because of its broad applicability and high interpretability. Despite its prevalence, exact GED computation is proved to be NP-hard, which could result in unsatisfactory computational efficiency on large graphs. However, exactly accurate search results are usually unnecessary for real-world applications especially when the responsiveness is far more important than the accuracy. Thus, in this paper, we propose a novel probabilistic approach to efficiently estimate GED, which is further leveraged for the graph similarity search. Specifically, we first take branches as elementary structures in graphs, and introduce a novel graph similarity measure by comparing branches between graphs, i.e., Graph Branch Distance (GBD), which can be efficiently calculated in polynomial time. Then, we formulate the relationship between GED and GBD by considering branch variations as the result ascribed to graph edit operations, and model this process by probabilistic approaches. By applying our model, the GED between any two graphs can be efficiently estimated by their GBD, and these estimations are finally utilized in the graph similarity search. Extensive experiments show that our approach has better accuracy, efficiency and scalability than other comparable methods in the graph similarity search over real and synthetic data sets.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "HKUST", 
        "name": "Zijian Li"
      }, 
      {
        "affiliation": "", 
        "name": "Xun Jian"
      }, 
      {
        "affiliation": "Kent State University", 
        "name": "Xiang Lian"
      }, 
      {
        "affiliation": "HKUST", 
        "name": "Lei Chen"
      }
    ], 
    "type": "research", 
    "id": "research1383"
  }, 
  "research509": {
    "title": "An Evaluation of Distributed Concurrency Control", 
    "abstract": "Increasing transaction volumes have led to a resurgence of interest in distributed transaction processing. In particular, partitioning data across several servers can improve throughput by allowing servers to process transactions in parallel. But executing transactions across servers limits the scalability and performance of these systems.  In this paper, we quantify the effects of distribution on concurrency control protocols in a distributed environment. We evaluate six classic and modern protocols in an in-memory distributed database evaluation framework called Deneva, providing an apples-to-apples comparison between each. Our results expose severe limitations of distributed transaction processing engines. Moreover, in our analysis, we identify several protocol-specific scalability bottlenecks. We conclude that to achieve truly scalable operation, distributed concurrency control solutions must seek a tighter coupling with either novel network hardware (in the local area) or applications (via data modeling and semantically-aware execution), or both.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "MIT", 
        "name": "Rachael Harding"
      }, 
      {
        "affiliation": "", 
        "name": "Dana Van Aken"
      }, 
      {
        "affiliation": "Stanford University", 
        "name": "Peter Bailis"
      }, 
      {
        "affiliation": "CMU", 
        "name": "Andrew Pavlo"
      }, 
      {
        "affiliation": "MIT", 
        "name": "Michael Stonebraker"
      }
    ], 
    "type": "research", 
    "id": "research509"
  }, 
  "research500": {
    "title": "An Experimental Comparison of Partitioning Strategies in Distributed Graph Processing", 
    "abstract": "In this paper, we study the problem of choosing among partitioning strategies in distributed graph processing systems. To this end, we evaluate and characterize both the performance and resource usage of different partitioning strategies under various popular distributed graph processing systems, applications, input graphs, and execution environments. Through our experiments, we found that no single partitioning strategy is the best fit for all situations, and that the choice of partitioning strategy has a significant effect on resource usage and application run-time. Our experiments demonstrate that the choice of partitioning strategy depends on (1) the degree distribution of input graph, (2) the type and duration of the application, and (3) the cluster size. Based on our results, we present rules of thumb to help users pick the best partitioning strategy for their particular use cases. We present results from each system, as well as from all partitioning strategies implemented in one common system (PowerLyra).", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University Of Illinois at UC", 
        "name": "Shiv Verma"
      }, 
      {
        "affiliation": "UIUC", 
        "name": "Luke Leslie"
      }, 
      {
        "affiliation": "", 
        "name": "Yosub Shin"
      }, 
      {
        "affiliation": "UIUC", 
        "name": "Indranil Gupta"
      }
    ], 
    "type": "research", 
    "id": "research500"
  }, 
  "research851": {
    "title": "An Experimental Evaluation of Point-of-interest Recommendation in Location-based Social Networks [Experiments and Analyses]", 
    "abstract": "Point-of-interest (POI) recommendation is an important service to Location-Based Social Networks (LBSNs) that can benefit both users and businesses. In recent years, a number of POI recommender systems have been proposed, but there is still a lack of systematical comparison thereof. In this paper, we provide an all-around evaluation of 11 state-of-the-art POI recommendation models. From the evaluation, we obtain several important findings, based on which we can better understand and utilize POI recommendation models in various scenarios. We anticipate this work to provide readers with an overall picture of the cutting-edge research on POI recommendation.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Nanyang Technological Univ.", 
        "name": "Yiding Liu"
      }, 
      {
        "affiliation": "", 
        "name": "Tuan-Anh Pham"
      }, 
      {
        "affiliation": "", 
        "name": "Gao Cong"
      }, 
      {
        "affiliation": "UIUC", 
        "name": "Quan Yuan"
      }
    ], 
    "type": "research", 
    "id": "research851"
  }, 
  "research513": {
    "title": "An Experimental Evaluation of SimRank-based Similarity Search Algorithms[Experiments and Analyses]", 
    "abstract": "Given a graph, SimRank is one of the most popular measures of the similarity between two vertices. We focus on efficiently calculating SimRank, which has been studied intensively over the last decade. This has led to many algorithms that efficiently calculate or approximate SimRank being proposed by researchers. Despite these abundant research efforts, there is no systematic comparison of these algorithms. In this paper, we conduct a study to compare these algorithms to understand their pros and cons.  We first introduce a taxonomy for different algorithms that calculate SimRank and classify each algorithm into one of the following three classes, namely, {\\em iterative-}, {\\em non-iterative-}, and {\\em random walk-based} method. We implement ten algorithms published from 2002 to 2015, and compare them using both synthetic and real-world graphs. To ensure the fairness of our study, our implementations use the same data structure and execution framework, and we try our best to optimize each of these algorithms. Our study reveals that none of these algorithms dominates the others: algorithms based on iterative method often have higher accuracy while algorithms based on random walk can be more scalable. One non-iterative algorithm has good effectiveness and efficiency on graphs with medium size. Thus, depending on the requirements of different applications, the optimal choice of algorithms differs. This paper provides an empirical guideline for making such choices.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Peking University", 
        "name": "Zhipeng Zhang"
      }, 
      {
        "affiliation": "PKU", 
        "name": "Yingxia Shao"
      }, 
      {
        "affiliation": "Peking University", 
        "name": "Bin Cui"
      }, 
      {
        "affiliation": "ETH", 
        "name": "Ce Zhang"
      }
    ], 
    "type": "research", 
    "id": "research513"
  }, 
  "research1371": {
    "title": "An Optimizable Query Language for Unified Scale-out Data Cleaning", 
    "abstract": "Data cleaning has become an indispensable part of data analysis due to the increasing amount of dirty data. Data scientists spend most of their time preparing dirty data before it can be used for data analysis. At the same time, the existing tools that attempt to automate the data cleaning procedure typically focus on a specific use case and operation. Still, even such specialized tools exhibit long running times or fail to process large datasets. Therefore, from a user\u00e2\u0080\u0099s perspective, one is forced to use a different, potentially inefficient tool for each category of errors. This paper addresses the coverage and efficiency problems of data cleaning. It introduces CleanM (pronounced clean\u00e2\u0080\u0099em), a language  which can express multiple types of cleaning operations. CleanM goes through a three-level translation process for optimization  purposes; a different family of optimizations is applied in each abstraction level. Thus, CleanM can express complex data cleaning tasks, optimize them in a unified way, and deploy them in a scale-out  fashion. We validate the applicability of CleanM by using it on top of CleanDB, a newly designed and implemented framework which can query heterogeneous data. When compared to existing data cleaning solutions, CleanDB a) covers more data corruption cases, b) is up to \u00e2\u0088\u00bc 2\u00c3\u0097 faster on average, scales better, and can handle cases for which its competitors are unable to terminate, and c) uses a single interface for querying and for data cleaning.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "EPFL", 
        "name": "Stella Giannakopoulou"
      }, 
      {
        "affiliation": "EPFL", 
        "name": "Manos Karpathiotakis"
      }, 
      {
        "affiliation": "", 
        "name": "Benjamin Gaidioz"
      }, 
      {
        "affiliation": "EPFL", 
        "name": "Anastasia Ailamaki"
      }
    ], 
    "type": "research", 
    "id": "research1371"
  }, 
  "research742": {
    "title": "Attribute-Driven Community Search", 
    "abstract": "Recently, community search over graphs has gained significant interest. In applications such as analysis of protein-protein interaction (PPI) networks, citation graphs, and collaboration networks, nodes tend to have attributes. Unfortunately, most previous community search algorithms ignore attributes and result in communities with poor cohesion w.r.t. their node attributes. In this paper, we study the problem of attribute-driven community search, that is, given an undirected graph $G$ where nodes are associated with attributes, and an input query $Q$ consisting of nodes $V_q$ and attributes $W_q$, find the communities containing $V_q$, in which most community members are densely inter-connected and have similar attributes.   We formulate this problem as finding attributed truss communities (ATC), i.e., finding connected and close k-truss subgraphs containing $V_q$, with the largest attribute relevance score. We design a framework of desirable properties that good score function should satisfy. We show that the problem is NP-hard. However, we develop an efficient greedy algorithmic framework to iteratively remove nodes with the least popular attributes, and shrink the graph into an ATC. In addition, we also build an elegant index to maintain $k$-truss structure and attribute information, and propose efficient query processing algorithms. Extensive experiments on large real-world networks with ground-truth communities show that our algorithms significantly outperform the state of the art and demonstrates their efficiency and effectiveness.  ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Hong Kong Baptist University", 
        "name": "Xin Huang"
      }, 
      {
        "affiliation": "UBC", 
        "name": "Laks Lakshmanan"
      }
    ], 
    "type": "research", 
    "id": "research742"
  }, 
  "research853": {
    "title": "Auto-Join: Joining Tables by Leveraging Transformations", 
    "abstract": "Traditional equi-join relies solely on string comparisons to perform joins. However, in scenarios such as ad-hoc data analysis in spreadsheets, users increasingly need to join tables whose join-columns use different textual representa- tions, for which transformations are needed before equi-join can be executed. We develop an Auto-Join system that can automatically search over a rich space of operators to compose a program, whose execution makes input tables equi-join-able. Our evaluation using real test cases collected from both public web tables and proprietary enterprise tables shows that the proposed system can perform the desired syntactic joins with high quality.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Toronto", 
        "name": "Erkang Zhu"
      }, 
      {
        "affiliation": "Microsoft Research", 
        "name": "Yeye He"
      }, 
      {
        "affiliation": "Microsoft Research", 
        "name": "Surajit Chaudhuri"
      }
    ], 
    "type": "research", 
    "id": "research853"
  }, 
  "research679": {
    "title": "Automatic Algorithm Transformation for Efficient Multi-Snapshot Analytics on Temporal Graphs", 
    "abstract": "Analytical graph algorithms commonly compute metrics for a graph at one point in time. In practice it is often also of interest how metrics change over time, e.g., to find trends. For this purpose, algorithms must be executed for multiple graph snapshots. We present Single Algorithm Multiple Snapshots (SAMS), a novel approach to execute algorithms concurrently for multiple graph snapshots. SAMS automatically transforms graph algorithms to leverage similarities between the analyzed graph snapshots. The automatic transformation interleaves algorithm executions on multiple snapshots, synergistically shares their graph accesses and traversals, and optimizes the algorithm's data layout. Thus, SAMS can amortize the cost of random data accesses and improve memory bandwidth utilization---two main cost factors in graph analytics. We extensively evaluate SAMS using six well-known algorithms and multiple synthetic as well as real-world graph datasets. Our measurements show that in multi-snapshot analyses, SAMS offers runtime improvements of up to two orders of magnitude over traditional snapshot-at-a-time execution.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Technical University of Munich", 
        "name": "Manuel Then"
      }, 
      {
        "affiliation": "Technical University of Munich", 
        "name": "Timo Kersten"
      }, 
      {
        "affiliation": "Technical University of Munich", 
        "name": "Stephan Guennemann"
      }, 
      {
        "affiliation": "Technical University of Munich", 
        "name": "Alfons Kemper"
      }, 
      {
        "affiliation": "Technical University of Munich", 
        "name": "Thomas Neumann"
      }
    ], 
    "type": "research", 
    "id": "research679"
  }, 
  "research1389": {
    "title": "Benchmarking Distributed Stream Processing Engines [Experiments and Analyses]", 
    "abstract": "Over the last years, stream data processing has been gaining attention both in industry and in academia due to its wide range of applications. To fulfill the need for scalable and efficient stream analytics, numerous open source stream data processing systems (SDPSs) have been developed, with high throughput and low latency being their key performance targets. In this paper, we propose a framework to evaluate the performance of three SDPSs, namely Apache Storm, Apache Spark, and Apache Flink. Our evaluation focuses in particular on measuring the throughput and latency of windowed operations, such as windowed aggregation and join. For this benchmark, we design workloads based on real-life, industrial use-cases. The main contribution of this work is three-fold. First, we give a definition of latency and throughput for stateful operators. Second, we completely separate the system under test and driver, so that the measurement results are closer to actual system performance under real conditions. Third, we build the first driver to test the actual sustainable performance of a system under test. Our detailed evaluation shows that there is no single winner, but rather, each system shines in individual use-cases.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "DFKI", 
        "name": "Jeyhun Karimov"
      }, 
      {
        "affiliation": "TU Berlin", 
        "name": "Tilmann Rabl"
      }, 
      {
        "affiliation": "SAP Innovation Center", 
        "name": "Asterios Katsifodimos"
      }, 
      {
        "affiliation": "", 
        "name": "Roman Samarev"
      }, 
      {
        "affiliation": "", 
        "name": "Henri Heiskanen"
      }, 
      {
        "affiliation": "", 
        "name": "Volker Markl"
      }
    ], 
    "type": "research", 
    "id": "research1389"
  }, 
  "research747": {
    "title": "Bias-Aware Sketches", 
    "abstract": "Linear sketching algorithms have been widely used for processing large-scale distributed and streaming datasets. Their popularity is largely due to the fact that linear sketches can be naturally composed in the distributed model and be efficiently updated in the streaming model.  The errors of linear sketches are typically expressed in terms of the sum of coordinates of the input vector excluding those largest ones, or, the mass on the tail of the vector.  Thus, the precondition for these algorithms to perform well is that the mass on the tail is small, which is, however, not always the case -- in many real-world datasets the coordinates of the input vector have a {\\em bias}, which will generate a large mass on the tail.    In this paper we propose linear sketches that are {\\em bias-aware}. We rigorously prove that they achieve strictly better error guarantees than the corresponding existing sketches, and demonstrate their practicality and superiority via an extensive experimental evaluation on both real and synthetic datasets.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Indiana University", 
        "name": "Jiecao Chen"
      }, 
      {
        "affiliation": "Indiana University", 
        "name": "Qin Zhang"
      }
    ], 
    "type": "research", 
    "id": "research747"
  }, 
  "research409": {
    "title": "BlueCache: A Scalable Distributed Flash-based Key-value Store", 
    "abstract": "A key-value store (KVS), such as memcached and Redis, is widely used as a caching layer to augment the slower persistent backend storage in data centers. DRAM-based KVS provides fast key-value access, but its scalability is limited by the cost, power and space needed by the machine cluster to support a large amount of DRAM. This paper offers a 10X to 100X cheaper solution based on flash storage and hardware accelerators. In BlueCache key-value pairs are stored in flash storage and all KVS operations, including the flash controller are directly implemented in hardware. Furthermore, BlueCache includes a fast interconnect between flash controllers to provide a scalable solution. We show that BlueCache has 4.18X higher throughput and consumes 25X less power than a flash-backed KVS software implementation on x86 servers. We further show that BlueCache can outperform DRAM-based KVS when the latter has more than 7.4% misses for a read-intensive application. BlueCache is an attractive solution for both rack-level appliances and data-center-scale key-value cache.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "MIT", 
        "name": "Shuotao Xu"
      }, 
      {
        "affiliation": "Inha University", 
        "name": "Sungjin Lee"
      }, 
      {
        "affiliation": "Massachusetts Institute of Technology", 
        "name": "Sang-Woo Jun"
      }, 
      {
        "affiliation": "Massachusetts Institute of Technology", 
        "name": "Ming Liu"
      }, 
      {
        "affiliation": "Accelerated Tech", 
        "name": "Jamey Hicks"
      }, 
      {
        "affiliation": "", 
        "name": "Arvind Arvind"
      }
    ], 
    "type": "research", 
    "id": "research409"
  }, 
  "research682": {
    "title": "Bridging the Gap Between HPC and Big Data Frameworks", 
    "abstract": "Apache Spark is a popular framework for data analytics with attractive features such as fault tolerance and interoperability with the Hadoop ecosystem. Unfortunately, many analytics operations in Spark are an order of magnitude or more slower compared to native implementations written with high performance computing tools such as MPI. There is a need to bridge the performance gap while retaining the benefits of the Spark ecosystem such as availability, productivity, and fault tolerance. In this paper, we propose a system for integrating MPI with Spark and analyze the costs and benefits of doing so for four distributed graph and machine learning applications. We show that offloading computation to an MPI environment from within Spark provides 3.1-17.7x  speedups on the four sparse applications, including all of the overheads. This opens up an easy avenue to reuse existing MPI libraries in Spark with little effort. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Intel Labs", 
        "name": "Michael Anderson"
      }, 
      {
        "affiliation": "University of Minnesota", 
        "name": "Shaden Smith"
      }, 
      {
        "affiliation": "Intel", 
        "name": "Narayanan Sundaram"
      }, 
      {
        "affiliation": "Intel Labs", 
        "name": "Mihai Capot\u00c4\u0083"
      }, 
      {
        "affiliation": "Brown University", 
        "name": "Zheguang Zhao"
      }, 
      {
        "affiliation": "Intel Labs", 
        "name": "Subramanya Dulloor"
      }, 
      {
        "affiliation": "Intel Labs", 
        "name": "Nadathur Satish"
      }, 
      {
        "affiliation": "Intel Labs", 
        "name": "Theodore Willke"
      }
    ], 
    "type": "research", 
    "id": "research682"
  }, 
  "industrial1097": {
    "title": "CarStream: An Industrial System of Big Data Processing for Internet-of-Vehicles", 
    "abstract": "As the Internet-of-Vehicles (IoV) technology becomes an increasingly important trend for future transportation, designing large-scale IoV systems has become a critical task that aims to process big data uploaded by fleet and to provide data-driven services. The IoV data, especially high-frequency vehicle statuses (e.g., location, engine parameters), are characterized as large volume with a low density of value and low data quality. Such characteristics pose challenges for developing real-time applications based on such data. In this paper, we address the challenges in designing a scalable IoV system by describing CarStream, an industrial system of big data processing for chauffeured car services. Connected with over 30,000 vehicles, CarStream collects and processes multiple types of driving data including vehicle status, driver activity, and passenger-trip information. Multiple services are provided based on the collected data. CarStream has been deployed and maintained for three years in industrial usage, collecting over 40 terabytes driving data. This paper shares our experiences about designing CarStream based on large-scale driving-data streams, and the lessons learned from the process of addressing the challenges in designing and maintaining CarStream. ", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "Beihang University", 
        "name": "Mingming Zhang"
      }, 
      {
        "affiliation": "Beihang University", 
        "name": "Tianyu Wo"
      }, 
      {
        "affiliation": "Beihang University", 
        "name": "Xuelian Lin"
      }, 
      {
        "affiliation": "University of Illinois.edu", 
        "name": "Tao Xie"
      }, 
      {
        "affiliation": "CAR Inc.", 
        "name": "Yaxiao Liu"
      }
    ], 
    "type": "industrial", 
    "id": "industrial1097"
  }, 
  "research1286": {
    "title": "Caribou: Intelligent Distributed Storage [Innovative Systems and Applications]", 
    "abstract": "Storage in data centers is evolving very quickly: several technologies are used simultaneously, the boundaries between main-memory and persistent storage are becoming blurred, persistent storage no longer has a block-based interface, it can handle random access at high rates, and is often exposed over the network. Yet, the ever increasing amount of data being handled causes an intrinsic inefficiency: moving data around is expensive in terms of bandwidth, latency, and power consumption, especially given the low computational complexity of many database operations.   In this paper we explore near-data processing in database engines, i.e., the option of offloading part of the computation directly to the storage nodes.  We implement our ideas in Caribou, an intelligent distributed storage layer incorporating many of the lessons learned while building systems with specialized hardware. Caribou provides access to DRAM/NVRAM storage over the network through a simple key-value store interface, with each storage node providing high-bandwidth near-data processing at line rate and fault tolerance through transparent replication. The result is a highly efficient data processing component that can be used not only to boost performance but also to reduce power consumption and real estate usage in the data center thanks to the micro-server architecture adopted.  ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "ETH Zurich", 
        "name": "Zsolt Istvan"
      }, 
      {
        "affiliation": "ETH Zurich", 
        "name": "David Sidler"
      }, 
      {
        "affiliation": "ETH", 
        "name": "Gustavo Alonso"
      }
    ], 
    "type": "research", 
    "id": "research1286"
  }, 
  "research433": {
    "title": "Clay: Fine-Grained Adaptive Partitioning for General Database Schemas", 
    "abstract": "Transaction processing database management systems (DBMSs) are critical for today's data-intensive applications because they enable an organization to quickly ingest and query new information. Many of these applications exceed the capabilities of a single server, and thus their database has to be deployed in a distributed DBMS. The key factor affecting such a system's performance is how the database is partitioned. If the database is partitioned incorrectly, the number of distributed transactions can be high. These transactions have to synchronize their operations over the network, which is considerably slower and leads to poor performance. Previous work on elastic database repartitioning has focused on a certain class of applications whose database schema can be represented in a hierarchical tree structure. But many applications cannot be partitioned in this manner, and thus are subject to distributed transactions that impede their performance and scalability.  In this paper, we present a new on-line partitioning approach, called Clay, that supports both tree-based schemas and more complex general\" schemas with arbitrary foreign key relationships. Clay dynamically creates blocks of tuples to migrate among servers during repartitioning", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Qatar Computing Research Institute", 
        "name": "Marco Serafini"
      }, 
      {
        "affiliation": "MIT", 
        "name": "Rebecca Taft"
      }, 
      {
        "affiliation": "University of Chicago", 
        "name": "Aaron Elmore"
      }, 
      {
        "affiliation": "CMU", 
        "name": "Andrew Pavlo"
      }, 
      {
        "affiliation": "Qatar Computing Research Institute", 
        "name": "Ashraf Aboulnaga"
      }, 
      {
        "affiliation": "MIT", 
        "name": "Michael Stonebraker"
      }
    ], 
    "type": "research", 
    "id": "research433"
  }, 
  "research507": {
    "title": "Clue-based Spatio-textual Query", 
    "abstract": "Along with the proliferation of online digital map and location-based service, very large POI (point of interest) databases have been constructed where a record corresponds to a POI with information including name, category, address, geographical location and other features. A basic spatial query in POI database is POI retrieval. In many scenarios, a user cannot provide enough information to pinpoint the POI except some clue. For example, a user wants to identify a cafe in a city visited many years ago. He cannot remember the name and address but he still recalls that \u00e2\u0080\u009cthe cafe is about 200 meters away from a restaurant; and turning left at the restaurant there is a bakery 500 meters away, etc.\u00e2\u0080\u009d. Intuitively, the clue, even partial and approximate, describes the spatio-textual context around the targeted POI. Motivated by this observation, this work investigates clue-based spatio-textual query which allows user providing clue, i.e., some nearby POIs and the spatial relationships between them, in POI retrieval. The objective is to retrieve k POIs from a POI database with the highest spatio-textual context similarities against the clue. This work has deliberately designed data-quality-tolerant spatio-textual context similarity metric to cope with various data quality problems in both the clue and the POI database. Through crossing valuation, the query accuracy is further enhanced by ensemble method. Also, this work has developed an index called roll-out-star R-tree (RSR-tree) to dramatically improve the query processing efficiency. The extensive tests on data sets from the real world have verified the superiority of our methods in all aspects.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "", 
        "name": "Junling Liu"
      }, 
      {
        "affiliation": "RMIT University", 
        "name": "Ke Deng"
      }, 
      {
        "affiliation": "", 
        "name": "Huanliang Sun"
      }, 
      {
        "affiliation": "", 
        "name": "Yu Ge"
      }, 
      {
        "affiliation": "University of Queensland", 
        "name": "Xiaofang Zhou"
      }, 
      {
        "affiliation": "Aalborg University", 
        "name": "Christian Jensen"
      }
    ], 
    "type": "research", 
    "id": "research507"
  }, 
  "research116": {
    "title": "Cohort Query Processing", 
    "abstract": "Modern Internet applications often produce a large volume of user activity records. Data analysts are interested in cohort analysis, or finding unusual user behavioral trends, in these large tables of activity records. In a traditional database system, cohort analysis queries are both painful to specify and expensive to evaluate. We propose to extend database systems to support cohort analysis. We do so by extending SQL with three new operators. We devise three different evaluation schemes for cohort query processing. Two of them adopt a non-intrusive approach. The third approach employs a columnar based evaluation scheme with optimizations specifically designed for cohort query processing. Our experimental results confirm the performance benefits of our proposed columnar database system, compared against the two non-intrusive approaches that implement cohort queries on top of regular relational databases.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "", 
        "name": "Dawei Jiang"
      }, 
      {
        "affiliation": "NUS", 
        "name": "Qingchao Cai"
      }, 
      {
        "affiliation": "", 
        "name": "Gang Chen"
      }, 
      {
        "affiliation": "University of Michigan", 
        "name": "H. Jagadish"
      }, 
      {
        "affiliation": "", 
        "name": "Beng Chin Ooi"
      }, 
      {
        "affiliation": "NUS", 
        "name": "Kian-Lee Tan"
      }, 
      {
        "affiliation": "NUS Singapore", 
        "name": "Anthony Tung"
      }
    ], 
    "type": "research", 
    "id": "research116"
  }, 
  "industrial1190": {
    "title": "Colt: Concept Lineage Tool for Data Flow Metadata Capture and Analysis", 
    "abstract": "Most organizations are becoming increasingly data-driven, often processing data from many different sources to enable critical business operations. Beyond the well-addressed challenge of storing and processing large volumes of data, financial institutions in particular are increasingly subject to federal regulations requiring high levels of accountability for the accuracy and lineage of this data. For companies like GE Capital, which maintain data across a globally interconnected network of thousands of systems, it is becoming increasingly challenging to capture an accurate understanding of the data flowing between those systems. To address this problem, we designed and developed a concept lineage tool allowing organizational data flows to be modeled, visualized and interactively explored. This tool has novel features that allow a data flow network to be contextualized in terms of business-specific metadata such as the concept, business, and product for which it applies. Key analysis features have been implemented, including the ability to trace the origination of particular datasets, and to discover all systems where data is found that meets some user-defined criteria. This tool has been readily adopted by users at GE Capital and in a short time has already become a business-critical application, with over 2,200 data systems and over 1,000 data flows captured.", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "GE Global Research", 
        "name": "Kareem Aggour"
      }, 
      {
        "affiliation": "GE Global Research", 
        "name": "Jenny Weisenberg Williams"
      }, 
      {
        "affiliation": "GE Global Research", 
        "name": "Justin McHugh"
      }, 
      {
        "affiliation": "GE Global Research", 
        "name": "Vijay Kumar"
      }
    ], 
    "type": "industrial", 
    "id": "industrial1190"
  }, 
  "research1290": {
    "title": "Comparative Evaluation of Big-Data Systems on Scientific Image Analytics Workloads[Experiments and Analysis]", 
    "abstract": "Scientific discoveries are increasingly driven by analyzing large volumes of image data. Many new libraries and specialized database management systems (DBMSs) have emerged to support such tasks. It is unclear, however, how well these systems support real-world image analysis use cases, and how performant are the image analytics tasks implemented on top of such systems. In this paper, we present the first comprehensive evaluation of large-scale image analysis systems using two real-world scientific image data processing use cases. We evaluate five representative systems (SciDB, Myria, Spark, Dask, and TensorFlow) and find that each of them has shortcomings that complicate implementation or hurt performance. Such shortcomings lead to new research opportunities in making large-scale image analysis both efficient and easy to use.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Washington", 
        "name": "Parmita Mehta"
      }, 
      {
        "affiliation": "", 
        "name": "Sven Dorkenwald"
      }, 
      {
        "affiliation": "University of Washington", 
        "name": "Dongfang Zhao"
      }, 
      {
        "affiliation": "University of Washington", 
        "name": "Tomer Kaftan"
      }, 
      {
        "affiliation": "University of Washington", 
        "name": "Alvin Cheung"
      }, 
      {
        "affiliation": "University of Washington", 
        "name": "Magdalena  Balazinska"
      }, 
      {
        "affiliation": "", 
        "name": "Ariel Rokem"
      }, 
      {
        "affiliation": "", 
        "name": "Andrew Connolly"
      }, 
      {
        "affiliation": "", 
        "name": "Jacob Vanderplas"
      }, 
      {
        "affiliation": "", 
        "name": "Yusra  AlSayyad"
      }
    ], 
    "type": "research", 
    "id": "research1290"
  }, 
  "research321": {
    "title": "Computing Longest Increasing Subsequences over Sequential Data Streams", 
    "abstract": "In this paper, we propose a data structure, a quadruple neighbor list (QN-list, for short), to support real time queries of all \\underline{l}ongest \\underline{i}ncreasing \\underline{s}ubsequence (LIS) and LIS with constraints over sequential data streams. The data structure built by our algorithm requires $O(w)$ space, where $w$ is the time window size. The running time for building the initial data structure takes $O(w\\log w)$ time. Applying the data structure, insertion of the new item takes $O(\\log w)$ time and deletion of the first item takes $O(w)$ time. To the best of our knowledge, this is the first work to support both LIS enumeration and LIS with constraints computation by using a single uniform data structure for real time sequential data streams. Our method outperforms the state-of-the-art methods in both time and space cost, not only theoretically, but also empirically.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Peking University", 
        "name": "Youhuan Li"
      }, 
      {
        "affiliation": "Peking University", 
        "name": "Lei Zou"
      }, 
      {
        "affiliation": "University of Alabama in Huntsville", 
        "name": "Huaming Zhang"
      }, 
      {
        "affiliation": "", 
        "name": "Dongyan Zhao"
      }
    ], 
    "type": "research", 
    "id": "research321"
  }, 
  "research678": {
    "title": "C\u00c3\u00bcm\u00c3\u00bcl\u00c3\u00b6n-D: Data Analytics in a Dynamic Spot Market", 
    "abstract": "We present a system called C\u00c3\u00bcm\u00c3\u00bcl\u00c3\u00b6n-D for matrix-based data analysis in a spot market of a public cloud.  Prices in such markets fluctuate over time: while users can acquire machines usually at a very low bid price, the cloud can terminate these machines as soon as the market price exceeds their bid price.  The distinguishing features of C\u00c3\u00bcm\u00c3\u00bcl\u00c3\u00b6n-D include its continuous, proactive adaptation to the changing market, and its ability to quantify and control the monetary risk involved in paying for a workflow execution.  We solve the dynamic optimization problem in a principled manner with a Markov decision process, and account for practical details that are often ignored previously but nonetheless important to performance.  We evaluate C\u00c3\u00bcm\u00c3\u00bcl\u00c3\u00b6n-D's effectiveness and advantages over previous approaches with experiments on Amazon EC2.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Microsoft", 
        "name": "Botong Huang"
      }, 
      {
        "affiliation": "Duke University", 
        "name": "Jun Yang"
      }
    ], 
    "type": "research", 
    "id": "research678"
  }, 
  "research426": {
    "title": "DOCS: A Domain-Aware Crowdsourcing System Using Knowledge Bases", 
    "abstract": "Crowdsourcing is a new computing paradigm that harnesses human effort to solve computer-hard problems, such as entity resolution and photo tagging. The crowd (or workers) have diverse qualities and it is important to effectively model a worker's quality. Most of existing worker models assume that workers have the same quality on different tasks. In practice, however, tasks belong to a variety of diverse domains, and workers have different qualities on different domains. For example, a worker who is a basketball fan should have better quality for the task of labeling a photo related to 'Stephen Curry' than the one related to 'Leonardo DiCaprio'. In this paper, we study how to leverage domain knowledge to accurately model a worker's quality. We examine using knowledge base (KB), e.g., Wikipedia and Freebase, to detect the domains of tasks and workers. We develop Domain Vector Estimation, which analyzes the domains of a task with respect to the KB. We also study Truth Inference, which utilizes the domain-sensitive worker model to accurately infer the true answer of a task. We design an Online Task Assignment algorithm, which judiciously and efficiently assigns tasks to appropriate workers. To implement these solutions, we have built DOCS, a system deployed on the Amazon Mechanical Turk. Experiments show that DOCS performs much better than the state-of-the-art approaches. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "The University of Hong Kong", 
        "name": "Yudian Zheng"
      }, 
      {
        "affiliation": "Tsinghua University", 
        "name": "Guoliang Li"
      }, 
      {
        "affiliation": "Hong Kong University", 
        "name": "Reynold Cheng"
      }
    ], 
    "type": "research", 
    "id": "research426"
  }, 
  "research754": {
    "title": "Data Driven Approximation with Bounded Resources", 
    "abstract": "This paper proposes BEAS, a resource-bounded scheme for querying relations.  It is parameterized with a resource ratio \\alpha \\in (0, 1], indicating that given a big dataset D, we can only afford to access an \\alpha-fraction of D with limited resources.  For a query Q posed on D, BEAS computes exact answers Q(D) if it is doable with bounded resources; otherwise it returns approximate answers with an accuracy bound. In the entire process it accesses at most \\alpha*|D| amount of data.  Underlying BEAS are  (1) an access schema, which helps us identify and fetch the part of data needed to answer Q,  (2) an accuracy measure to assess approximate answers, in terms of their relevance and coverage w.r.t. exact answers Q(D),  (3) an Approximability Theorem for the feasibility of resource-bounded approximation, and  (4) algorithms for query evaluation with bounded resources.  A unique feature of BEAS is its ability to answer unpredictable queries, aggregate or not, using bounded resources and assuring a deterministic accuracy lower bound.  Using real-life and synthetic data, we experimentally verify the effectiveness and efficiency of BEAS. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Edinburgh", 
        "name": "Yang Cao"
      }, 
      {
        "affiliation": "University of Edinburgh", 
        "name": "Wenfei Fan"
      }
    ], 
    "type": "research", 
    "id": "research754"
  }, 
  "research570": {
    "title": "Data Tweening: Incremental Visualization of Data Transforms", 
    "abstract": "In the context of interactive query sessions, it is common to issue a succession of queries, transforming a dataset to the desired result. It is often difficult to comprehend a suc- cession of transformations, especially for complex queries. Thus, to facilitate understanding of each data transforma- tion and to provide continuous feedback, we introduce the concept of \u00e2\u0080\u009cdata tweening\u00e2\u0080\u009d, i.e., interpolating between re- sultsets, presenting to the user a series of incremental visual representations of a resultset transformation. We present tweening methods that consider not just the changes in the result, but also the changes in the query. Through user stud- ies, we show that data tweening allows users to efficiently comprehend data transforms, and also enables them to gain a better understanding of the underlying query operations.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "The Ohio State University", 
        "name": "Meraj Ahmed Khan"
      }, 
      {
        "affiliation": "", 
        "name": "Larry Xu"
      }, 
      {
        "affiliation": "Ohio State University", 
        "name": "Arnab Nandi"
      }, 
      {
        "affiliation": "UC Berkeley", 
        "name": "joseph Hellerstein"
      }
    ], 
    "type": "research", 
    "id": "research570"
  }, 
  "industrial825": {
    "title": "Developing a Low Dimensional Patient Class Profile in Accordance to Their Respiration-Induced Tumor Motion", 
    "abstract": "Tumor location displacement caused by respiration-induced motion reduces the efficacy of radiation therapy. Three medically relevant patterns are often observed in the respiration- induced motion signal: baseline shift, ES-Range shift, and D-Range shift. In this paper, for patients with lower body cancer, we develop class profiles (a low dimensional pattern frequency structure) that characterize them in terms of these three medically relevant patterns. We propose an adaptive segmentation technique that turns each respiration-induced motion signal into a multi-set of segments based on persistent variations within the signal. These multi-sets of segments is then probed for base behaviors. These base behaviors are then used to develop the group/class profiles using a modified version of the clustering technique described in [1]. Finally, via quantitative analysis, we provide a medical characterization for the class profiles, which can be used to explore breathing intervention technique. We show that, with i) carefully designed feature sets, ii) the proposed adaptive segmentation technique, iii) the reasonable modifications to an existing clustering algorithm for multi-sets, and iv) the proposed medical characterization methodology, it is possible to reduce the time series respiration-induced motion signals into a compact class profile. One of our co-authors is a medical physician and we used his expert opinion to verify the results.", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "University of Texas at Dallas", 
        "name": "Rittika Shamsuddin"
      }, 
      {
        "affiliation": "University of Maryland", 
        "name": "Amit Sawant"
      }, 
      {
        "affiliation": "University of Texas at Dallas", 
        "name": "Balakrishnan Prabhakaran"
      }
    ], 
    "type": "industrial", 
    "id": "industrial825"
  }, 
  "industrial1218": {
    "title": "Dhalion:Self-Regulating Stream Processing in Heron", 
    "abstract": "In recent years, there has been an explosion of large-scale real-time analytics needs and a plethora of streaming systems have been developed to support such applications. These systems are able to continue stream processing even when faced with hardware and software failures. However, these systems do not address some crucial challenges facing their operators: the manual, time-consuming and error-prone tasks of tuning various configuration knobs to achieve service level objectives (SLO) as well as the maintenance of SLOs in the face of sudden, unpredictable load variation and hardware or software performance degradation.  In this paper, we introduce the notion of self-regulating streaming systems and the key properties that they must satisfy. We then present the design and evaluation of Dhalion, a system that provides self-regulation capabilities to underlying streaming systems. We describe our implementation of the Dhalion framework on top of Twitter Heron, as well as a number of policies that automatically reconfigure Heron topologies to meet throughput SLOs, scaling resource consumption up and down as needed. We experimentally evaluate our Dhalion policies in a cloud environment and demonstrate their effectiveness. We are in the process of open-sourcing our Dhalion policies as part of the Heron project.", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "Microsoft", 
        "name": "Avrilia Floratou"
      }, 
      {
        "affiliation": "", 
        "name": "Ashvin Agrawal"
      }, 
      {
        "affiliation": "", 
        "name": "Bill  Graham"
      }, 
      {
        "affiliation": "", 
        "name": "Sriram Rao"
      }, 
      {
        "affiliation": "", 
        "name": "Karthik Ramasamy"
      }
    ], 
    "type": "industrial", 
    "id": "industrial1218"
  }, 
  "research1377": {
    "title": "DigitHist: a Histogram-Based Data Summary with Tight Error Bounds", 
    "abstract": "We propose DigitHist, a histogram summary for selectivity estimation on multidimensional data with tight error bounds. By combining multidimensional and one-dimensional histograms along regular grids of different resolutions, DigitHist provides an accurate and reliable histogram approach for multidimensional data. To achieve a compact summary, we use a sparse representation combined with a novel histogram compression technique that chooses a higher resolution in dense regions and a lower resolution elsewhere. For the construction of DigitHist, we propose a new error measure, termed u-error, which minimizes the width between the guaranteed upper and lower bounds of the selectivity estimate. The construction algorithm performs a single data scan and has linear time complexity. An in-depth experimental evaluation shows that DigitHist delivers superior precision and error bounds than state-of-the-art competitors at a comparable query time.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Free University of Bolzano", 
        "name": "Michael Shekelyan"
      }, 
      {
        "affiliation": "Free University of Bolzano", 
        "name": "Anton Dign\u00c3\u00b6s"
      }, 
      {
        "affiliation": "Free University of Bozen-Bolzano", 
        "name": "Johann Gamper"
      }
    ], 
    "type": "research", 
    "id": "research1377"
  }, 
  "research607": {
    "title": "Dimensional Testing for Reverse k-Nearest Neighbor Search", 
    "abstract": "Given a query object q, reverse k-nearest neighbor (RkNN) search aims to locate those objects of the database that have q among their k-nearest neighbors. In this paper, we propose an approximation method for solving RkNN queries, where the pruning operations and termination tests are guided by a characterization of the intrinsic dimensionality of the data. The method can accommodate any index structure supporting incremental (forward) nearest-neighbor search for the generation and verification of candidates, while avoiding impractically-high preprocessing costs. We also provide experimental evidence that our method significantly outperforms its competitors in terms of the tradeoff between execution time and the quality of the approximation. Our approach thus addresses many of the scalability issues surrounding the use of previous methods in data mining.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "ONERA-DCSD", 
        "name": "Guillaume Casanova"
      }, 
      {
        "affiliation": "Ludwig-Maximilians-Universit\u00c3\u00a4t M\u00c3\u00bcnchen", 
        "name": "Elias Englmeier"
      }, 
      {
        "affiliation": "NII", 
        "name": "Michael Houle"
      }, 
      {
        "affiliation": "LMU", 
        "name": "Peer Kroeger"
      }, 
      {
        "affiliation": "Google Japan", 
        "name": "Michael Nett"
      }, 
      {
        "affiliation": "LMU Munich", 
        "name": "Erich Schubert"
      }, 
      {
        "affiliation": "SDU", 
        "name": "Arthur Zimek"
      }
    ], 
    "type": "research", 
    "id": "research607"
  }, 
  "industrial843": {
    "title": "Dimensions Based Data Clustering and Zone Maps", 
    "abstract": "In recent years, the data warehouse industry has witnessed decreased use of indexing but increased use of compression and clustering of data facilitating efficient data access and data pruning in the query processing area. A classic example of data pruning is the partition pruning, which is used when table data is range or list partitioned. But lately, techniques have been developed to prune data at a lower granularity than a table partition or sub-partition. A good example is the use of data pruning structure called zone map. A zone map prunes zones of data from a table on which it is defined. Data pruning via zone map is very effective when the table data is clustered by the filtering columns.  The database industry has offered support to cluster data in tables by its local columns, and to define zone maps on clustering columns of such tables. This has helped improve the performance of queries that contain filter predicates on local columns. However, queries in data warehouses are typically based on star/snowflake schema with filter predicates usually on columns of the dimension tables joined to a fact table. Given this, the performance of data warehouse queries can be significantly improved if the fact table data is clustered by columns of dimension tables together with zone maps that maintain min/max value ranges of these clustering columns over zones of fact table data. In recognition of this opportunity of significantly improving the performance of data warehouse queries, Oracle 12c release 1 has introduced the support for dimension based clustering of fact tables together with data pruning of the fact tables via dimension based zone maps.", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "1957", 
        "name": "Mohamed Ziauddin"
      }, 
      {
        "affiliation": "", 
        "name": "Andrew Witkowski"
      }, 
      {
        "affiliation": "", 
        "name": "You Jung Kim"
      }, 
      {
        "affiliation": "", 
        "name": "Janaki Lahorani"
      }, 
      {
        "affiliation": "", 
        "name": "Dmitry Potapov"
      }, 
      {
        "affiliation": "", 
        "name": "Murali Krishna"
      }
    ], 
    "type": "industrial", 
    "id": "industrial843"
  }, 
  "research506": {
    "title": "Distributed Join Algorithms on Thousands of Cores", 
    "abstract": "Traditional database operators such as joins are relevant not only in the context of database engines but also as a building block in many computational and machine learning algorithms. With the advent of big data, there is an increasing demand for efficient join algorithms that can scale with the input data size and the available hardware resources.  In this paper, we explore the implementation of distributed join algorithms in systems with several thousand cores connected by a low-latency network as used in high performance computing systems or data centers. We compare radix hash join to sort-merge join algorithms and discuss their implementation at this scale. In the paper, we explain how to use MPI to implement joins, show the impact and advantages of RDMA, discuss the importance of network scheduling, and study the relative performance of sorting vs. hashing. The experimental results show that the algorithms we present scale well with the number of cores, reaching a throughput of 48.7 billion input tuples per second on 4096 cores.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "ETH Zurich", 
        "name": "Claude Barthels"
      }, 
      {
        "affiliation": "ETH Zurich", 
        "name": "Ingo M\u00c3\u00bcller"
      }, 
      {
        "affiliation": "ETH Zurich", 
        "name": "Timo Schneider"
      }, 
      {
        "affiliation": "ETH", 
        "name": "Gustavo Alonso"
      }, 
      {
        "affiliation": "ETH Zurich", 
        "name": "Torsten Hoefler"
      }
    ], 
    "type": "research", 
    "id": "research506"
  }, 
  "research1374": {
    "title": "Distributed Trajectory Similarity Search", 
    "abstract": "Mobile and sensing devices have already become ubiquitous. They have made tracking moving objects an easy task.  As a result, mobile applications like Uber and many IoT projects have generated massive amounts of trajectory data that can no longer be processed by a single machine efficiently. Among the typical query operations over trajectories, similarity search is a common yet expensive operator in querying trajectory data. It is useful for applications in different domains such as traffic and transportation optimizations, weather forecast and modeling, and sports analytics. It is also a fundamental operator for many important mining operations such as clustering and classification of trajectories. In this paper, we propose a distributed query framework to process trajectory similarity search over a large set of trajectories. We have implemented the proposed framework in Spark, a popular distributed data processing engine, by carefully considering different design choices.  Our query framework supports both the Hausdorff distance the Fr\u00c3\u00a9chet distance. Extensive experiments have demonstrated the excellent scalability and query efficiency achieved by our design, compared to other methods and design alternatives.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Utah", 
        "name": "Dong Xie"
      }, 
      {
        "affiliation": "University of Utah", 
        "name": "Feifei Li"
      }, 
      {
        "affiliation": "University of Utah", 
        "name": "Jeff Phillips"
      }
    ], 
    "type": "research", 
    "id": "research1374"
  }, 
  "research852": {
    "title": "Don't Hold My Data Hostage - A Case For Client Protocol Redesign", 
    "abstract": "Transferring a large amount of data from a database to a client program is a surprisingly expensive operation. The time this requires can easily dominate the query execution time for large result sets. This represents a significant hurdle for external data analysis, for example when using statistical software. In this paper, we explore and analyse the result set serialization design space. We present experimental results from a large chunk of the database market and show the inefficiencies of current approaches. We then propose a columnar serialization method that improves transmission performance by an order of magnitude. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "CWI", 
        "name": "Mark Raasveldt"
      }, 
      {
        "affiliation": "CWI", 
        "name": "Hannes M\u00c3\u00bchleisen"
      }
    ], 
    "type": "research", 
    "id": "research852"
  }, 
  "research1400": {
    "title": "Dscaler: Synthetically Scaling A Given Relational Database", 
    "abstract": "The Dataset Scaling Problem (DSP) defined in previous work states: Given an empirical set of relational tables D and a scale factor s, generate a database state D'  that is similar to D but s times its size. A DSP solution is useful for application development (s < 1), scalability testing (s > 1) and anonymization (s = 1). Current solutions assume all table sizes scale by the same ratio s.  However, a real database tends to have tables that grow at different rates. This paper therefore considers non-uniform scaling (nuDSP), a DSP generalization where, instead of a single scale factor s, tables can scale by different factors.  Dscaler is the first solution for nuDSP. It follows previous work in achieving similarity by reproducing correlation among the primary and foreign keys. However, it introduces the concept of a correlation database that captures fine-grained, per-tuple correlation.  Experiments with well-known real and synthetic datasets D show that Dscaler produces D'  with greater similarity to D than state-of-the-art techniques. Here, similarity is measured by number of tuples, frequency distribution of foreign key references, and multi-join aggregate queries. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "National University of Singapo", 
        "name": "Jiangwei Zhang"
      }, 
      {
        "affiliation": "", 
        "name": "Y.C. Tay"
      }
    ], 
    "type": "research", 
    "id": "research1400"
  }, 
  "research1332": {
    "title": "Dynamic Query Refinements for Interactive Data Exploration", 
    "abstract": "Queries that can navigate large search spaces to identify complex objects of interest cannot be efficiently supported by traditional DBMSs. Searchlight is a recent system that aims to address this fundamental shortcoming by deeply yet transparently integrating Constraint Programming (CP) logic into the query engine of an array DBMS. This hybrid model enables exploration of large multi-dimensional data sets progressively and quickly.   Fast query execution is only one of the requirements of effective data-exploration support. Finding the right questions to ask is another notoriously challenging problem, given the users' lack of familiarity with the structure and contents of the underlying data sets, as well as the inherently fuzzy goals in many exploration-oriented tasks. To this end, in the context of Searchlight, we study the modification of initial query parameters at run-time. We describe how to dynamically refine (i.e., relax or tighten) the parameters of a query, based on the result cardinality desired by the user and the live query progress. This feature allows users to iterate over the datasets faster and without having to make accurate guesses on what parameters to use. Our experimental results show that the proposed techniques introduce little or no overhead while yielding considerable time savings compared to user-driven, manual query refinements. The result is a system that not only optimizes machine resource usage but also reduces user effort. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Brown University", 
        "name": "Alexander Kalinin"
      }, 
      {
        "affiliation": "Brown University", 
        "name": "Ugur Cetintemel"
      }, 
      {
        "affiliation": "Brown University", 
        "name": "Stan Zdonik"
      }
    ], 
    "type": "research", 
    "id": "research1332"
  }, 
  "research574": {
    "title": "Effective Community Search over Large Spatial Graphs", 
    "abstract": "Communities are prevalent in social networks, knowledge graphs, and biological networks. Recently, the topic of community search (CS) has received plenty of attention. Given a query vertex, CS looks for a dense subgraph that contains it. Existing CS solutions do not consider the spatial extent of a community. They can yield communities whose locations of vertices span large areas. In applications that facilitate the creation of social events (e.g., finding conference attendees to join a dinner), it is important to find groups of peoples who are physically close to each other. In this situation, it is desirable to have a spatial-aware community (or SAC), whose vertices are close structurally and spatially. Given a graph G and a query vertex q, we develop exact solutions for finding an SAC that contains q. Since these solutions cannot scale to large datasets, we have further designed three approximation algorithms to compute an SAC. We have performed an experimental evaluation for these solutions on several large real datasets. Experimental results show that SAC is better than the communities returned by existing solutions. Moreover, our approximation solutions can find SACs accurately and efficiently.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "The University of Hong Kong", 
        "name": "Yixiang Fang"
      }, 
      {
        "affiliation": "Hong Kong University", 
        "name": "Reynold Cheng"
      }, 
      {
        "affiliation": "", 
        "name": "Xiaodong Li"
      }, 
      {
        "affiliation": "", 
        "name": "Siqiang Luo"
      }, 
      {
        "affiliation": "The University of Hong Kong", 
        "name": "Jiafeng Hu"
      }
    ], 
    "type": "research", 
    "id": "research574"
  }, 
  "research144": {
    "title": "Effective Indexing for Approximate Constrained Shortest Path Queries on Large Road Networks", 
    "abstract": " In a constrained shortest path (CSP) query, each edge in the road network is associated with both a length and a cost. Given an origin s, a destination t, and a cost constraint theta, the goal is to find the shortest path from s to t whose total cost does not exceed theta. Because exact CSP is NP-hard, previous work mostly focuses on approximate solutions. Even so, existing methods are still prohibitively expensive for large road networks. Two main reasons are (i) that they fail to utilize the special properties of road networks and (ii) that most of them process queries without indices; the few existing indices consume large amounts of memory and yet have limited effectiveness in reducing query costs.  Motivated by this, we propose COLA, the first practical solution for approximate CSP processing on large road networks. COLA exploits the facts that a road network can be effectively partitioned, and that there exists a relatively small set of landmark vertices that commonly appear in CSP results. Accordingly, COLA indexes the vertices lying on partition boundaries, and applies an on-the-fly algorithm called alpha-Dijk for path computation within a partition, which effectively prunes paths based on landmarks. Extensive experiments demonstrate that on continent-sized road networks, COLA answers an approximate CSP query in sub-second time, whereas existing methods take hours. Interestingly, even without an index, the alpha-Dijk algorithm in COLA still outperforms previous solutions by more than an order of magnitude.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Nanyang Technological University", 
        "name": "Sibo Wang"
      }, 
      {
        "affiliation": "Nanyang Technological University", 
        "name": "Xiaokui Xiao"
      }, 
      {
        "affiliation": "Hamad Bin Khalifa University", 
        "name": "Yin Yang"
      }, 
      {
        "affiliation": "Qatar Computing Research Institute", 
        "name": "Wenqing Lin"
      }
    ], 
    "type": "research", 
    "id": "research144"
  }, 
  "research600": {
    "title": "Effective and Complete Discovery of Order Dependencies via Set-based Axiomatization", 
    "abstract": "Integrity constraints (ICs) are useful for query optimization and for expressing and enforcing application semantics. However, formulating constraints manually requires domain expertise, is prone to human errors, and may be excessively time consuming, especially on large datasets. Hence, proposals for automatic discovery have been made for some classes of ICs, such as functional dependencies (FDs), and recently, order dependencies (ODs). ODs properly subsume FDs, as they can additionally express business rules involving order; e.g., an employee never has a higher salary while paying lower taxes than another employee.   We present a new OD discovery algorithm enabled by a novel polynomial mapping to a canonical form of ODs, and a sound and complete set of axioms (inference rules) for canonical ODs. Our algorithm has exponential worst-case time complexity, O(2^{|R|}), in the number of attributes |R| and linear complexity in the number of tuples. We prove that it produces a complete and minimal set of ODs. Using real and synthetic datasets, we experimentally show orders-of-magnitude performance improvements over the prior state-of-the-art.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "UOIT", 
        "name": "Jaroslaw Szlichta"
      }, 
      {
        "affiliation": "York University", 
        "name": "Parke Godfrey"
      }, 
      {
        "affiliation": "University of Waterloo", 
        "name": "Lukasz Golab"
      }, 
      {
        "affiliation": "University of Windsor", 
        "name": "Mehdi Kargar"
      }, 
      {
        "affiliation": "AT&T", 
        "name": "Divesh Srivastava"
      }
    ], 
    "type": "research", 
    "id": "research600"
  }, 
  "research306": {
    "title": "Efficient Computation of Feedback Arc Set at Web-Scale", 
    "abstract": "The minimum feedback arc set problem is an NP-hard problem on graphs that seeks a minimum set of arcs which, when removed from the graph, leave it acyclic. In this work, we investigate several approximations for computing a minimum feedback arc set with the goal of comparing the quality of the solutions and the running times. Our investigation is motivated by applications in Social Network Analysis such as misinformation removal and label propagation. We present careful algorithmic engineering for multiple algorithms to improve the scalability of each approach. In particular, two approaches we optimize (one greedy and one randomized) provide a nice balance between feedback arc set size and running time complexity. We experimentally compare the performance of a wide range of algorithms on a broad selection of large online networks including Twitter, LiveJournal, and the Clueweb12 dataset. The experiments reveal that our greedy and randomized implementations outperform the other approaches by simultaneously computing a feedback arc set of competitive size and scaling to web-scale graphs with billions of vertices and tens of billions of arcs. Finally, we extend the algorithms considered to the probabilistic case in which arcs are realized with some fixed probability and provide detailed experimental comparisons.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Victoria", 
        "name": "Michael Simpson"
      }, 
      {
        "affiliation": "University of Victoria", 
        "name": "Venkatesh Srinivasan"
      }, 
      {
        "affiliation": "University of Victoria", 
        "name": "Alex Thomo"
      }
    ], 
    "type": "research", 
    "id": "research306"
  }, 
  "research435": {
    "title": "Effortless Data Exploration with zenvisage: An Expressive and Interactive Visual Analytics System", 
    "abstract": "Data visualization is by far the most commonly used mechanism to explore and extract insights from datasets, especially by novice data scientists.  And yet, current visual analytics tools are rather limited in their ability to operate on collections of visualizations - by composing, filtering, comparing, and sorting them - to find those that depict desired trends or patterns.  The process of visual data exploration remains a tedious process of trial-and-error.  We propose zenvisage, a visual analytics platform for effortlessly finding desired visual patterns from large datasets. We introduce general purpose visual exploration language, ZQL (zee-quel\") for specifying the desired visual patterns", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "UIUC", 
        "name": "Tarique Ashraf Siddiqui"
      }, 
      {
        "affiliation": "MIT", 
        "name": "Albert Kim"
      }, 
      {
        "affiliation": "UIUC", 
        "name": "John Lee"
      }, 
      {
        "affiliation": "UIUC", 
        "name": "Karrie Karahalios"
      }, 
      {
        "affiliation": "UIUC", 
        "name": "Aditya Parameswaran"
      }
    ], 
    "type": "research", 
    "id": "research435"
  }, 
  "research432": {
    "title": "Estimating Quantiles from the Union of Historical and Streaming Data", 
    "abstract": "Modern enterprises generate huge amounts of streaming data, for example, micro-blog feeds, financial data, network monitoring and industrial application monitoring. While Data Stream Management Systems have proven successful in providing support for real-time alerting, many applications, such as network monitoring for intrusion detection and real-time bidding, require complex analytics over historical and real-time data over the data streams. We present a new method to process one of the most fundamental analytical primitives, quantile queries, on the union of historical and streaming data. Our method combines an index on historical data with a memory-efficient sketch on streaming data to answer quantile queries with accuracy-resource tradeoffs that are significantly better than current solutions that are based solely on disk-resident indexes or solely on streaming algorithms.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "", 
        "name": "Sneha Singh"
      }, 
      {
        "affiliation": "AT&T", 
        "name": "Divesh Srivastava"
      }, 
      {
        "affiliation": "", 
        "name": "Srikanta Tirthapura"
      }
    ], 
    "type": "research", 
    "id": "research432"
  }, 
  "industrial1028": {
    "title": "ExtraV: Boosting Graph Processing Near Storage with a Coherent Accelerator", 
    "abstract": "In this paper, we propose ExtraV, a framework for near-storage graph processing.  It is based on the novel concept of graph virtualization, which efficiently utilizes a cache-coherent hardware accelerator at the storage side to achieve performance and flexibility at the same time.  ExtraV consists of four main components: 1) host processor, 2) main memory, 3) AFU (Accelerator Function Unit) and 4) storage.  The AFU, a hardware accelerator, sits between the host processor and storage.  Using a coherent interface that allows main memory accesses, it performs graph traversal functions that are common to various algorithms while the program running on the host processor (called the host program) manages the overall execution along with more application-specific tasks.  Graph virtualization is a high-level programming model of graph processing that allows designers to focus on algorithm-specific functions.  Realized by the accelerator, graph virtualization gives the host programs an illusion that the graph data reside on the main memory in a layout that fits with the memory access behavior of host programs even though the graph data are actually stored in a multi-level, compressed form in storage.  We prototyped ExtraV on a Power8 machine with a CAPI-enabled FPGA.  Our experiments on a real system prototype offer significant speedup compared to state-of-the-art software only implementations.", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "IBM Research", 
        "name": "Jinho Lee"
      }, 
      {
        "affiliation": "Seoul National University", 
        "name": "Heesu Kim"
      }, 
      {
        "affiliation": "Seoul National University", 
        "name": "Sungjoo Yoo"
      }, 
      {
        "affiliation": "Seoul National University", 
        "name": "Kiyoung Choi"
      }, 
      {
        "affiliation": "IBM Research", 
        "name": "Peter Hofstee"
      }, 
      {
        "affiliation": "IBM Research", 
        "name": "GiJoon Nam"
      }, 
      {
        "affiliation": "IBM Research", 
        "name": "Mark Nutter"
      }, 
      {
        "affiliation": "IBM Research", 
        "name": "Damir Jamsek"
      }
    ], 
    "type": "industrial", 
    "id": "industrial1028"
  }, 
  "industrial1131": {
    "title": "FAD.js: Fast JSON Data Access Using JIT-based Speculative Optimizations", 
    "abstract": "JSON is one of the most popular data encoding formats, with wide adoption in Databases and BigData frameworks as well as native support in popular programming languages such as JavaScript/Node.js, Python, and R. Nevertheless, JSON data processing can easily become a performance bottleneck in data-intensive applications because of parse and serialization overhead. In this paper, we introduce Fad.js, a runtime system for efficient processing of JSON objects in data-intensive applications. Fad.js is based on (1) speculative just-in-time (JIT) compilation and (2) selective access to data. Experiments show that applications using Fad.js achieve speedups up to 2.7x for encoding and 9.9x for decoding JSON data when compared to state-of-the art JSON processing libraries.", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "Oracle Labs", 
        "name": "Daniele Bonetta"
      }, 
      {
        "affiliation": "Oracle Labs", 
        "name": "Matthias Brantner"
      }
    ], 
    "type": "industrial", 
    "id": "industrial1131"
  }, 
  "research331": {
    "title": "Fast Algorithm for the Lasso based L1-Graph Construction", 
    "abstract": "The lasso-based L_1-graph is used in many applications since it can effectively model a set of data points as a graph.  The lasso is a popular regression approach and the L_1-graph represents data points as nodes by using the regression result.  More specifically, by solving the L_1-optimization problem of the lasso, the sparse regression coefficients are used to obtain the weights of the edges in the graph.  Conventional graph structures such as k-NN graph use two steps, adjacency searching and weight selection, for constructing the graph whereas the lasso-based L_1-graph derives the adjacency structure as well as the edge weights simultaneously by using a coordinate descent.  However, the construction cost of the lasso-based L_1-graph is impractical for large data sets since the coordinate descent iteratively updates the weights of all edges until convergence.  Our proposal, Castnet, can efficiently construct the lasso-based L_1-graph.  In order to avoid updating the weights of all edges, we prune edges that cannot have nonzero weights before entering the iterations.  In addition, we update edge weights only if they are nonzero in the iterations.  Experiments show that Castnet is significantly faster than existing approaches. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "NTT", 
        "name": "Yasuhiro Fujiwara"
      }, 
      {
        "affiliation": "NTT", 
        "name": "Yasutoshi Ida"
      }, 
      {
        "affiliation": "NTT", 
        "name": "Junya Arai"
      }, 
      {
        "affiliation": "NTT", 
        "name": "Mai Nishimura"
      }, 
      {
        "affiliation": "NTT", 
        "name": "Sekitoshi Kanai"
      }, 
      {
        "affiliation": "NTT", 
        "name": "Sotetsu Iwamura"
      }
    ], 
    "type": "research", 
    "id": "research331"
  }, 
  "research219": {
    "title": "Fast Hierarchy Construction for Dense Subgraphs", 
    "abstract": "Discovering dense subgraphs and understanding the relations among them is a fundamental problem in graph mining. We want to not only identify dense subgraphs, but also build a hierarchy among them (e.g., larger but sparser subgraphs formed by two smaller dense subgraphs). Peeling algorithms (k-core, k-truss, and nucleus decomposition) have been effective to locate many dense subgraphs. However, constructing a hierarchical representation of density structure, even correctly computing the connected k-cores and k-trusses, have been mostly overlooked. Keeping track of connected components during peeling requires an additional traversal operation, which is as expensive as the peeling process. In this paper, we start with a thorough survey and point to nuances in problem formulations that lead to significant differences in runtimes. We then propose efficient and generic algorithms to construct the hierarchy of dense subgraphs for k-core, k-truss, or any nucleus decomposition. Our algorithms leverage the disjoint-set forest data structure to efficiently construct the hierarchy during traversal. Furthermore, we introduce a new idea to avoid traversal. We construct the subgraphs while visiting neighborhoods in the peeling process, and build the relations to previously constructed subgraphs. We also consider an existing idea to find the k-core hierarchy and adapt for our objectives efficiently. Experiments on different types of large scale real-world networks show significant speedups over naive algorithms and existing alternatives. Our algorithms also outperform the hypothetical limits of any possible traversal-based solution.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Sandia National Laboratories", 
        "name": "Ahmet Erdem Sariyuce"
      }, 
      {
        "affiliation": "Sandia National Laboratories", 
        "name": "Ali  Pinar"
      }
    ], 
    "type": "research", 
    "id": "research219"
  }, 
  "research335": {
    "title": "Fast In-Memory SQL Analytics on Typed Graphs", 
    "abstract": "We study a class of graph analytics SQL queries, which we call relationship queries. These queries involving aggregation, join, semijoin, intersection and selection are a wide superset of fixed-length graph reachability queries and of tree pattern queries. We present real-world OLAP scenarios, where efficient relationship queries are needed. However, row stores, column stores and graph databases are unacceptably slow in such OLAP scenarios.  We propose a GQ-Fast database, which is an indexed database that roughly corresponds to efficient encoding of annotated adjacency lists that combines salient features of column-based organization, indexing and compression. GQ-Fast uses a bottom-up fully pipelined query execution model, which enables (a) aggressive compression (e.g., compressed bitmaps and Huffman) and (b) avoids intermediate results that consist of row IDs (which are typical in column databases). GQ-Fast compiles query plans into executable C++ source code. Besides achieving runtime efficiency, GQ-Fast also reduces main memory requirements because, unlike column databases, GQ-Fast selectively allows dense forms of compression including heavy-weight compressions, which do not support random access.  We used GQ-Fast to accelerate queries for two OLAP dashboards in the biomedical field. GQ-Fast outperforms PostgreSQL by 2\u00e2\u0080\u00934 orders of magnitude and MonetDB, Vertica and Neo4j by 1\u00e2\u0080\u00933 orders of magnitude when all of them are running on RAM. Our experiments dissect GQ-Fast\u00e2\u0080\u0099s advantage between (i) the use of compiled code, (ii) the bottom-up pipelining execution strategy, and (iii) the use of dense structures. Other analysis and experiments show the space savings of GQ-Fast due to the appropriate use of compression methods. We also show that the runtime penalty incurred by the dense compression methods decreases as the number of CPU cores increases.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "UCSD", 
        "name": "Chunbin Lin"
      }, 
      {
        "affiliation": "", 
        "name": "Benjamin Mandel"
      }, 
      {
        "affiliation": "UCSD", 
        "name": "Yannis Papakonstantinou"
      }, 
      {
        "affiliation": "UCSD", 
        "name": "Matthias Springer"
      }
    ], 
    "type": "research", 
    "id": "research335"
  }, 
  "research1380": {
    "title": "Fast Scans on Key-Value Stores", 
    "abstract": "Key-Value Stores (KVS) are becoming increasingly popular because they scale up and down elastically, sustain high throughputs for get/put workloads and have low latencies. KVS owe these advantages to their simplicity. This simplicity, however, comes at a cost: It is expensive to process complex, analytical queries on top of a KVS because today's generation of KVS does not support an efficient way to scan the data. The problem is that there are conflicting goals when designing a KVS for analytical queries and for simple get/put workloads: Analytical queries require high locality and a compact representation of data whereas elastic get/put workloads require sparse indexes.  This paper shows that it is possible to have it all, with reasonable compromises.  We studied the KVS design space and built TellStore, a distributed KVS, that performs almost as well as state-of-the-art KVS for get/put workloads and orders of magnitude better for analytical and mixed workloads. This paper presents the results of comprehensive experiments with an extended version of the YCSB benchmark and a workload from the telecommunication industry.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "ETH Zurich", 
        "name": "Markus Pilman"
      }, 
      {
        "affiliation": "Microsoft", 
        "name": "Kevin Bocksrocker"
      }, 
      {
        "affiliation": "ETH Zurich", 
        "name": "Lucas Braun"
      }, 
      {
        "affiliation": "ETH Zurich", 
        "name": "Renato Marroqu\u00c3\u00adn"
      }, 
      {
        "affiliation": "ETH Zurich", 
        "name": "Donald Kossmann"
      }
    ], 
    "type": "research", 
    "id": "research1380"
  }, 
  "research1401": {
    "title": "Fast and Adaptive Indexing of Multi-Dimensional Observational Data", 
    "abstract": " Tremendous amounts of data are being generated by sensing devices each day, which include large quantities of multi-dimensional measurements. These data are expected to be immediately available for real-time analytics as they are streamed into storage. Such scenarios pose challenges to state-of-the-art indexing methods, as they must not only support efficient queries but also frequent updates. In this paper, we propose a novel indexing method that ingests multi-dimensional observational data in real time. This method primarily guarantees extremely high throughput for data ingestion, while it can be continuously refined in the background to improve query efficiency. Instead of representing collections of points using Minimal Bounding Boxes as in conventional indexes, we model sets of successive points as line segments in hyperspaces, by exploiting the intrinsic value continuity in observational data. Such a representation reduces the number of index entries and drastically reduces \u00e2\u0080\u009cover-coverage\u00e2\u0080\u009d by entries. Our experimental results show that our proposed approach handles real-world workloads gracefully, providing both low-overhead indexing and excellent query efficiency.  ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "", 
        "name": "Sheng Wang"
      }, 
      {
        "affiliation": "", 
        "name": "David Maier"
      }, 
      {
        "affiliation": "", 
        "name": "Beng Chin Ooi"
      }
    ], 
    "type": "research", 
    "id": "research1401"
  }, 
  "industrial927": {
    "title": "Fiber-based architecture for NFV cloud databases", 
    "abstract": "The Telecom industry is gradually shifting from using monolithic software packages deployed on custom hardware, to using modular virtualized software functions deployed on cloudified datacenters of commodity hardware. This transformation is referred to as Network Function Virtualization (NFV). The scalability of the databases (DBs) underlying the virtual network functions is the cornerstone for ripping the benefits from the NFV transformation. This paper presents an industrial experience of applying shared-nothing techniques in order to achieve scalability for a DB in an NFV setup. The special combination of requirements in NFV DBs are not easily met with conventional execution models. Therefore, we designed a special shared-nothing architecture that is based on cooperative multi-tasking on top of user-level threads (fibers). We further show that the fiber-based approach outperforms the approach built using conventional multi-threading and answers the variable deployment needs of the NFV transformation. Furthermore, the fibers yield a simpler-to-maintain software and enable controlling a trade-off between long-duration computations and real-time requests.", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "Huawei", 
        "name": "Vaidas Gasiunas"
      }, 
      {
        "affiliation": "Huawei", 
        "name": "David Dominguez-Sal"
      }, 
      {
        "affiliation": "Huawei", 
        "name": "Ralph Acker"
      }, 
      {
        "affiliation": "Huawei", 
        "name": "Aharon Avitzur"
      }, 
      {
        "affiliation": "Huawei", 
        "name": "Ilan Bronshtein"
      }, 
      {
        "affiliation": "", 
        "name": "Rushan Chen"
      }, 
      {
        "affiliation": "Huawei", 
        "name": "Eli Ginot"
      }, 
      {
        "affiliation": "Huawei", 
        "name": "Norbert Martinez"
      }, 
      {
        "affiliation": "Huawei", 
        "name": "Michael M\u00c3\u00bcller"
      }, 
      {
        "affiliation": "Huawei", 
        "name": "Alexander Nozdrin"
      }, 
      {
        "affiliation": "Huawei", 
        "name": "Weijie Ou"
      }, 
      {
        "affiliation": "Huawei", 
        "name": "Nir Pachter"
      }, 
      {
        "affiliation": "Huawei", 
        "name": "Dima Sivov"
      }, 
      {
        "affiliation": "Huawei", 
        "name": "Eliezer Levy"
      }
    ], 
    "type": "industrial", 
    "id": "industrial927"
  }, 
  "research610": {
    "title": "Finding Diverse, High-Value Representatives on a Surface of Answers", 
    "abstract": "In many applications, the system needs to selectively present a small subset of answers to users.  The set of all possible answers can be seen as an elevation surface over a domain, where the elevation measures the quality of each answer, and the dimensions of the domain correspond to attributes of the answers with which similarity between answers can be measured.  This paper considers the problem of finding a diverse set of k high-quality representatives for such a surface.  We show that existing methods for diversified top-k and weighted clustering problems are inadequate for this problem.  We propose k-DHR as a better formulation for the problem.  We show that k-DHR has a submodular and monotone objective function, and we develop efficient algorithms for solving k-DHR with provable guarantees.  We conduct extensive experiments to demonstrate the usefulness of the results produced by k-DHR for applications in computational lead-finding and fact-checking, as well as the efficiency and effectiveness of our algorithms.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Google Research", 
        "name": "You Wu"
      }, 
      {
        "affiliation": "Duke University", 
        "name": "Junyang Gao"
      }, 
      {
        "affiliation": "Duke University", 
        "name": "Pankaj Agarwal"
      }, 
      {
        "affiliation": "Duke University", 
        "name": "Jun Yang"
      }
    ], 
    "type": "research", 
    "id": "research610"
  }, 
  "research384": {
    "title": "Finding Persistent Items in Data Streams", 
    "abstract": "Frequent item mining, which deals with finding items that occur frequently in a given data stream over a period of time, is one of the heavily studied problems in data stream mining. A generalized version of frequent item mining is the persistent item mining, where a persistent item, unlike a frequent item, does not necessarily occur more frequently compared to other items over a short period of time, rather persists and occurs more frequently over a long period of time. To the best of our knowledge, there is no prior work on mining persistent items in a data stream. In this paper, we address the fundamental problem of finding persistent items in a given data stream during a given period of time at any given observation point. We propose a novel scheme, PIE, that can accurately identify each persistent item with a probability greater than any desired false negative rate (FNR) while using a very small amount of memory. The key idea of PIE is that it uses Raptor codes to encode the ID of each item that appears at the observation point during a measurement period and stores only a few bits of the encoded ID in the memory of that observation point during that measurement period. The item that is persistent occurs in enough measurement period- s that enough encoded bits for the ID can be retrieved from the observation point to decode them correctly and get the ID of the persistent item. We implemented and extensively evaluated PIE using three real network traffic traces and compared its performance with two prior adapted schemes. Our results show that not only PIE achieves the desired FNR in every scenario, its FNR, on average, is 19.5 times smaller than the FNR of the best adapted prior art.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Nanjing University", 
        "name": "Haipeng Dai"
      }, 
      {
        "affiliation": "North Carolina State University", 
        "name": "Muhammad Shahzad"
      }, 
      {
        "affiliation": "", 
        "name": "Alex X. Liu"
      }, 
      {
        "affiliation": "Nanjing University", 
        "name": "Yuankun Zhong"
      }
    ], 
    "type": "research", 
    "id": "research384"
  }, 
  "research1381": {
    "title": "Finding the maximum clique in massive graphs", 
    "abstract": "Cliques refer to subgraphs in an undirected graph such that vertices in each subgraph are pairwise adjacent. The maximum clique problem, to find the clique with most vertices in a given graph, has been extensively studied by researchers. Besides its theoretical value as an NP-hard problem, the maximum clique problem is known to have direct applications in various fields, such as community search in social networks and social media, team formation in expert networks, gene expression and motif discovery in bioinformatics and anomaly detection in complex networks, revealing the structure and function of networks. However, algorithms designed for the maximum clique problem are too expensive to deal with real-world networks, which are massive, sparse and associated with various characteristics.  In this paper, we devise a randomized algorithm for the maximum clique problem. Different from previous algorithms that search from each vertex one after another, our approach RMC, for the randomized maximum clique problem, employs a binary search while maintains a lower bound CL and an upper bound CU of omega(G). In each iteration, RMC attempts to find a CT-clique where CT=(CL+CU)/2. As finding CT in each iteration is NP-complete, we extract a seed set S such that the problem of finding a CT-clique in G is equivalent to finding a CT-clique in S with robability guarantees (>= 1-n^{-c}). We propose a novel iterative algorithm to determine the maximum clique by searching a k-clique in S starting from k=CL+1 until S becomes empty, when more iterations benefit marginally. As confirmed by the experiments, our approach is much more efficient and robust than previous solutions and can always find the exact maximum clique. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "CUHK", 
        "name": "Can Lu"
      }, 
      {
        "affiliation": "Chinese University of Hong Kong", 
        "name": "Jeffrey  Yu"
      }, 
      {
        "affiliation": "The Chinese University of HK", 
        "name": "Hao Wei"
      }, 
      {
        "affiliation": "Chinese University of Hong Kong", 
        "name": "Yikai Zhang"
      }
    ], 
    "type": "research", 
    "id": "research1381"
  }, 
  "research1319": {
    "title": "Flexible Online Task Assignment in Real-Time Spatial Data", 
    "abstract": "The popularity of Online To Offline (O2O) service platforms has spurred the need for effective online task assignment in real-time spatial data, where streams of spatially distributed tasks and workers are matched in real time such that the total number of assigned pairs is maximized. Existing online task assignment models assume that each worker is either assigned a task immediately or waits for a subsequent task at a fixed location once she/he appears on the platform. However, in practice, a worker may actively move around rather than passively wait at the same location if no task is assigned. If the workers guided effectively, the platform can increase the total number of assigned worker-task pairs, which is overlooked in previous works. In this paper, we define a new problem Flexible Two-sided Online task Assignment (FTOA). To address the FTOA problem, we face two challenges: (i) How to leverage the prediction of the spatiotemporal distribution of tasks and workers to effectively generate guidance for idle workers? (ii) How to leverage the guide of worker\u00e2\u0080\u0099s movement to optimize the online task assignment? To this end, we propose a novel two-step framework, which integrates offline prediction and online task assignment. Specifically, the offline prediction component estimates the distributions of tasks and workers per time slot and per unit area, based on which we develop an online task assignment algorithm, Prediction-oriented Online task Assignment in Real-time spatial data (POLAR). POLAR-OP not only yields a 0.5-competitive ratio,  which is nearly twice better than that of the state-of-the-art, but also makes the time complexity of processing each newly-arrived task/worker to O(1). We verify the effectiveness and efficiency of our methods through extensive experiments on both synthetic datasets and a large-scale real-time taxi-calling platform.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Beihang University", 
        "name": "Yongxin Tong"
      }, 
      {
        "affiliation": "Beihang University", 
        "name": "Libin Wang"
      }, 
      {
        "affiliation": "ETH", 
        "name": "Zimu Zhou"
      }, 
      {
        "affiliation": "Microsoft Research", 
        "name": "Bolin Ding"
      }, 
      {
        "affiliation": "HKUST", 
        "name": "Lei Chen"
      }, 
      {
        "affiliation": "Didi Research", 
        "name": "Jieping Ye"
      }, 
      {
        "affiliation": "Beihang University", 
        "name": "Ke Xu"
      }
    ], 
    "type": "research", 
    "id": "research1319"
  }, 
  "research612": {
    "title": "From Community Detection to Community Profiling", 
    "abstract": "Most existing community-related studies focus on detection, which aim to find the community membership for each user from user friendship links. However, membership alone, without a complete profile of what a community is and how it interacts with other communities, has limited applications. This motivates us to consider systematically profiling the communities and thereby developing useful community-level applications. In this paper, we for the first time formalize the concept of community profiling. With rich user information on the network, such as user published content and user diffusion links, we characterize a community in terms of both its internal content profile and external diffusion profile. The difficulty of community profiling is often underestimated. We novelly identify three unique challenges and propose a joint Community Profiling and Detection (CPD) model to address them accordingly. We also contribute a scalable inference algorithm, which scales linearly with the data size and it is easily parallelizable. We evaluate CPD on large-scale real-world data sets, and show that it is significantly better than the state-of-the-art baselines in various tasks. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "ADSC", 
        "name": "Hongyun Cai"
      }, 
      {
        "affiliation": "Advanced Digital Sciences Cent", 
        "name": "Vincent W. Zheng"
      }, 
      {
        "affiliation": "Zhejiang University City College", 
        "name": "Fanwei Zhu"
      }, 
      {
        "affiliation": "University of Illinois at Urbana-Champaign", 
        "name": "Kevin Chen-Chuan Chang"
      }, 
      {
        "affiliation": "The University of Queensland", 
        "name": "Zi Huang"
      }
    ], 
    "type": "research", 
    "id": "research612"
  }, 
  "research857": {
    "title": "Heterogeneous Recommendations: What You Might Like To Read After Watching Interstellar", 
    "abstract": "Recommenders, as widely implemented nowadays by major e-commerce players like Netflix or Amazon, use collaborative filtering to suggest the most relevant items to their users. Clearly, the effectiveness of recommenders depends on the data they can exploit, i.e., the feedback of users conveying their preferences, typically based on their past ratings.  As of today, most recommenders are homogeneous in the sense that they utilize one specific application at a time. In short, Alice will only get recommended a movie if she has been rating movies. But what if she has been only rating books and would like to get recommendations for a movie? Clearly, the multiplicity of web applications is calling for heterogeneous recommenders that could utilize ratings in one application to provide recommendations in another one.  This paper presents X-MAP, a heterogeneous recommender. X-MAP leverages meta-paths between heterogeneous items over several application domains, based on users who rated across these domains. These meta-paths are then used in X-MAP to generate, for every user, a profile (AlterEgo) in a domain where the user might not have rated any item yet. Not surprisingly, leveraging meta-paths poses non-trivial issues of (a) meta-path-based inter-item similarity, in order to enable accurate predictions, (b) scalability, given the amount of computation required, as well as (c) privacy, given the need to aggregate information across multiple applications.  We show in this paper how X-MAP addresses the above-mentioned issues to achieve accuracy, scalability and differential privacy. In short, X-MAP weights the meta-paths based on several factors to compute inter-item similarities, and ensures scalability through a layer-based pruning technique. X-MAP guarantees differential privacy using an exponential scheme that leverages the meta-path-based similarities while determining the probability of item selection to construct the AlterEgos. We present an exhaustive experimental evaluation of X-MAP using real traces from Amazon. We show that, in terms of accuracy, X-MAP outperforms alternative heterogeneous recommenders and, in terms of throughput, X-MAP achieves a linear speedup with an increasing number of machines.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "EPFL", 
        "name": "Rachid Guerraoui"
      }, 
      {
        "affiliation": "Inria", 
        "name": "Anne-Marie Kermarrec"
      }, 
      {
        "affiliation": "EPFL", 
        "name": "Tao Lin"
      }, 
      {
        "affiliation": "EPFL", 
        "name": "Rhicheek Patra"
      }
    ], 
    "type": "research", 
    "id": "research857"
  }, 
  "research514": {
    "title": "High Performance Transactions via Early Write Visibility", 
    "abstract": "The overwhelming majority of existing concurrency control protocols make transactions' writes visible at the end of their execution. This delayed write visibility can significantly impact the performance of serializable protocols by reducing concurrency among conflicting transactions. This paper makes the observation that this delayed write visibility characteristic of existing protocols stem from an assumption that the database system within which they run may arbitrarily abort transactions at any point during their execution. Based on this observation, we propose a new serializable concurrency control protocol, piece-wise visibility (PWV), that is designed to run in a new class of database systems whose ability to abort transactions is deliberately constrained. PWV makes transactions' writes visible prior to the end of their execution, and consequently obtains significantly more concurrency than conventional serializable protocols. We evaluate PWV against state-of-the-art serializable protocols and a highly optimized implementation of read committed, and found that PWV can outperform serializable protocols by an order of magnitude and read committed by 3X on contentious workloads. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Yale University", 
        "name": "Jose Faleiro"
      }, 
      {
        "affiliation": "Yale University", 
        "name": "Daniel Abadi"
      }, 
      {
        "affiliation": "UC Berkeley", 
        "name": "joseph Hellerstein"
      }
    ], 
    "type": "research", 
    "id": "research514"
  }, 
  "research1398": {
    "title": "HippogriffDB: Balancing I/O and GPU Bandwidth in Big Data Analytics", 
    "abstract": "As data sets grow and conventional processor performance scaling slows, data analytics move towards heterogeneous architectures that incorporate hardware accelerators (notably GPUs) to continue scaling performance. However, existing GPU-based databases fail to deal with big data applications efficiently: the limited memory capacity of GPUs hinders the scalability of existing GPU-accelerated databases; existing systems fail to consider the discrepancy between fast GPUs and slow storage, which can counteract the benefit of GPU accelerators.  In this paper, we propose HippogriffDB, an efficient, scalable GPU-accelerated OLAP system. It tackles the bandwidth discrepancy using data compression and direct data transfer path. HippogriffDB stores tables in compressed format and uses the GPU for decompression, trading GPU cycles for improved I/O bandwidth. To improve data transfer efficiency, HippogriffDB introduces a peer-to-peer, multi-threaded data transfer mechanism, directly transferring data from the SSD to the GPU. Focusing on star schema queries, HippogriffDB adopts a query-over-block execution model that provides scalability using a stream-based query execution model. The model helps increase kernel efficiency with operator fusion and double buffering mechanism.  We have implemented HippogriffDB using an NVMe SSD, which connects directly to a commercial GPU. Results on two popular benchmarks demonstrate the scalability and efficiency of HippogriffDB. HippogriffDB outperforms existing GPU-based databases (YDB) and in-memory data analytics (MonetDB) by 1-2 orders of magnitude.   ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "", 
        "name": "Jing Li"
      }, 
      {
        "affiliation": "", 
        "name": "Hung-Wei Tseng"
      }, 
      {
        "affiliation": "UCSD", 
        "name": "Chunbin Lin"
      }, 
      {
        "affiliation": "UCSD", 
        "name": "Steven Swanson"
      }, 
      {
        "affiliation": "UCSD", 
        "name": "Yannis Papakonstantinou"
      }
    ], 
    "type": "research", 
    "id": "research1398"
  }, 
  "research429": {
    "title": "History is a mirror to the future: Best-effort approximate complex event matching with insufficient resources", 
    "abstract": "Complex event processing (CEP) has proven to be a highly relevant topic in practice. As it is sensitive to both errors in the stream and uncertainty in the pattern, approximate complex event processing (ACEP) is an important direction but has not been adequately studied before. ACEP is costly, and is often performed under insufficient computing resources. We propose an algorithm that learns from the past behavior of ACEP runs, and makes decisions on what to process first in an online manner, so as to maximize the number of full matches found. In addition, we devise effective optimization techniques. Finally, we propose a mechanism that uses reinforcement learning to dynamically update the history structure without incurring much overhead. Put together, these techniques drastically improve the fraction of full matches found in resource constrained environments.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Oracle Corporation", 
        "name": "Zheng Li"
      }, 
      {
        "affiliation": "UMass Lowell", 
        "name": "Tingjian Ge"
      }
    ], 
    "type": "research", 
    "id": "research429"
  }, 
  "research1283": {
    "title": "HoloClean: Holistic Data Repairs with Probabilistic Inference", 
    "abstract": "We introduce HoloClean, a framework for holistic data repairing driven by probabilistic inference. HoloClean unifies existing qualitative data repairing approaches, which rely on integrity constraints or external data sources, with quantitative data repairing methods, which leverage statistical properties of the input data. Given an inconsistent dataset as input, HoloClean automatically generates a probabilistic program that performs data repairing. Inspired by recent theoretical advances in probabilistic inference, we introduce a series of optimizations which ensure that inference over HoloClean's probabilistic model scales data instances with millions of tuples. We show that HoloClean scales to instances with millions of tuples and find data repairs with an average precision of ~90% and an average recall of above ~76% across a diverse array of datasets exhibiting different types of errors. This yields an average F1 improvement of more than 2x against state-of-the-art methods.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Stanford University", 
        "name": "Theodoros Rekatsinas"
      }, 
      {
        "affiliation": "University of Waterloo", 
        "name": "Xu Chu"
      }, 
      {
        "affiliation": "University of Waterloo", 
        "name": "Ihab Ilyas"
      }, 
      {
        "affiliation": "Stanford University", 
        "name": "Chris Re"
      }
    ], 
    "type": "research", 
    "id": "research1283"
  }, 
  "research329": {
    "title": "HubPPR: Effective Indexing for Approximate Personalized PageRank", 
    "abstract": "{\\em Personalized PageRank (PPR)} computation is a fundamental operation in web search, social networks, and graph analysis. Given a graph $G$, a source $s$, and a target $t$, the PPR query $\\pi(s, t)$ returns the probability that a random walk on $G$ starting from $s$ terminates at $t$. Unlike global PageRank which can be effectively pre-computed and materialized, the PPR result depends on both the source and the target, rendering results materialization infeasible for large graphs. Existing indexing techniques have rather limited effectiveness; in fact, the current state-of-the-art solution, {\\em BiPPR}, answers individual PPR queries without pre-computation or indexing, and yet it outperforms all previous index-based solutions.  Motivated by this, we propose {\\em HubPPR}, an effective indexing scheme for PPR computation with controllable tradeoffs for accuracy, query time, and memory consumption. The main idea is to pre-compute and index auxiliary information for selected hub nodes that are often involved in PPR processing. Going one step further, we extend HubPPR to answer top-$k$ PPR queries, which returns the $k$ nodes with the highest PPR values with respect to a source $s$, among a given set $T$ of target nodes. Extensive experiments demonstrate that compared to the current best solution {\\em BiPPR}, {\\em HubPPR} achieves up to 10x and 220x speedup for PPR and top-$k$ PPR processing, respectively, with moderate memory consumption. Notably, with a single commodity server, {\\em HubPPR} answers a top-$k$ PPR query in seconds on graphs with billions of edges, with high accuracy and strong result quality guarantees.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Nanyang Technological University", 
        "name": "Sibo Wang"
      }, 
      {
        "affiliation": "Nanyang Technological University", 
        "name": "Youze Tang"
      }, 
      {
        "affiliation": "Nanyang Technological University", 
        "name": "Xiaokui Xiao"
      }, 
      {
        "affiliation": "Hamad Bin Khalifa University", 
        "name": "Yin Yang"
      }, 
      {
        "affiliation": "Institute of High Performance Computing", 
        "name": "Zengxiang Li"
      }
    ], 
    "type": "research", 
    "id": "research329"
  }, 
  "research130": {
    "title": "IL-Miner: Instance-Level Discovery of Complex Event Patterns", 
    "abstract": "Complex event processing (CEP) matches patterns over a continuous stream of events to detect situations of interest. Yet, the definition of an event pattern that precisely characterises a particular situation is challenging: there are manifold dimensions to correlate events, including time windows and value predicates. In the presence of historic event data that is labelled with the situation to detect, event patterns can be learned automatically. To cope with the combinatorial explosion of pattern candidates, existing approaches work on a type-level and discover patterns based on predefined event abstractions, aka event types. Hence, discovery is limited to patterns of a fixed granularity and users face the burden to manually select appropriate event abstractions.  We present IL-Miner, a system that discovers event patterns by genuinely working on the instance-level, not assuming a priori knowledge on event abstractions. In a multi-phase process, IL-Miner first identifies relevant abstractions for the construction of event patterns.  The set of events explored for pattern discovery is thereby reduced, while still providing formal guarantees on correctness, minimality, and completeness of the discovery result. Experiments using real-world datasets from diverse domains show that IL-Miner discovers a much broader range of event patterns compared to the state-of-the-art in the field.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Humboldt-Universit\u00c3\u00a4t zu Berlin", 
        "name": "Lars George"
      }, 
      {
        "affiliation": "", 
        "name": "Bruno Cadonna"
      }, 
      {
        "affiliation": "Humboldt-Universit\u00c3\u00a4t zu Berlin", 
        "name": "Matthias Weidlich"
      }
    ], 
    "type": "research", 
    "id": "research130"
  }, 
  "research1317": {
    "title": "In Search of an Entity Resolution OASIS: Optimal Asymptotic Sequential Importance Sampling", 
    "abstract": "Entity resolution (ER) presents unique challenges for evaluation methodology. While crowdsourcing platforms acquire ground truth, sound approaches to sampling must drive labelling efforts. In ER, extreme class imbalance between matching and non-matching records can lead to enormous labelling requirements when seeking statistically consistent estimates for rigorous evaluation. This paper addresses this important challenge with the OASIS algorithm: a sampler and F-measure estimator for ER evaluation. OASIS draws samples from a (biased) instrumental distribution, chosen to ensure estimators with optimal asymptotic variance. As new labels are collected OASIS updates this instrumental distribution via a Bayesian latent variable model of the annotator oracle, to quickly focus on unlabelled items providing more information. We prove that resulting estimates of F-measure, precision, recall converge to the true population values. Thorough comparisons of sampling methods on a variety of ER datasets demonstrate significant labelling reductions of up to 83% without loss to estimate accuracy.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Melbourne", 
        "name": "Neil Marchant"
      }, 
      {
        "affiliation": "University of Melbourne", 
        "name": "Benjamin Rubinstein"
      }
    ], 
    "type": "research", 
    "id": "research1317"
  }, 
  "research319": {
    "title": "Interactive Time Series Exploration Powered by the Marriage of Similarity Distances", 
    "abstract": "Finding similar trends among time series data is critical for applications ranging from financial planning to policy making. The detection of these multifaceted relationships, especially time warped matching of time series of different lengths and alignments is prohibitively expensive to compute. To achieve real time responsiveness on large time series datasets, we propose a novel paradigm called Online Exploration of Time Series (ONEX) employing a powerful one-time preprocessing step that encodes critical similarity relationships to support subsequent rapid data exploration. Since the encoding of a huge number of pairwise similarity relationships for all variable lengths time series segments is not feasible, our work rests on the important insight that clustering with inexpensive point-to-point distances such as the Euclidean Distance can support subsequent time warped matching. Our ONEX framework overcomes the prohibitive computational costs associated with a more robust elastic distance namely the Dynamic Time Warping distance by applying it over the surprisingly compact knowledge base instead of the raw data.  Our comparative study reveals that ONEX is up to 19% more accurate and several times faster than the state-of-the-art. Beyond being a highly accurate and fast domain independent solution, ONEX offers a truly interactive exploration experience supporting novel time series operations.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Worecster Polytechnic Institut", 
        "name": "Rodica Neamtu"
      }, 
      {
        "affiliation": "WPI", 
        "name": "Ramoza Ahsan"
      }, 
      {
        "affiliation": "WPI", 
        "name": "Elke Rundensteiner"
      }, 
      {
        "affiliation": "WPI", 
        "name": "Gabor Sarkozy"
      }
    ], 
    "type": "research", 
    "id": "research319"
  }, 
  "research1293": {
    "title": "I\u00e2\u0080\u0099ve Seen \u00e2\u0080\u009cEnough\u00e2\u0080\u009d: Incrementally Improving Visualizations to Support Rapid Decision Making", 
    "abstract": "Data visualization is an effective mechanism for identifying trends, insights, and anomalies in data. On large datasets, however, generating visualizations can take a long time, delaying the extraction of insights, hampering decision making, and reducing exploration time. One solution is to use online sampling-based schemes to generate visualizations faster while improving the displayed estimates incrementally, eventually converging to the exact visualization computed on the entire data. However, the intermediate visualizations are approximate, and often fluctuate drastically, leading to potentially incorrect decisions. In this paper, we propose sampling-based incremental visualization algorithms that reveal the \u00e2\u0080\u009csalient\u00e2\u0080\u009d features of the eventual visualization quickly\u00e2\u0080\u0094with a 46\u00c3\u0097 speedup relative to baselines\u00e2\u0080\u0094while minimizing error, thus enabling rapid and error-free decision making. We demonstrate that these algorithms are optimal in terms of sample complexity, in that given the level of interactivity, they generate approximations that take as few samples as possible to incrementally improve the accuracy. We have developed the algorithms in the context of an incremental visualization tool, titled INCVISAGE, for trendline and heatmap visualizations. We evaluate the usability of INCVISAGE via a full- fledged user study and demonstrate that users are able to make effective decisions with incrementally improving visualizations.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "UIUC", 
        "name": "Sajjadur Rahman"
      }, 
      {
        "affiliation": "", 
        "name": "Maryam Aliakbarpour"
      }, 
      {
        "affiliation": "", 
        "name": "Hidy Kong"
      }, 
      {
        "affiliation": "", 
        "name": "Eric Blais"
      }, 
      {
        "affiliation": "UIUC", 
        "name": "Karrie Karahalios"
      }, 
      {
        "affiliation": "UIUC", 
        "name": "Aditya Parameswaran"
      }, 
      {
        "affiliation": "", 
        "name": "Ronitt Rubinfeld"
      }
    ], 
    "type": "research", 
    "id": "research1293"
  }, 
  "research510": {
    "title": "KBQA: Learning Question Answering over QA Corpora and Knowledge Bases", 
    "abstract": "Question answering (QA) has become a popular way for humans to access billion-scale knowledge bases. Unlike web search, QA over a knowledge base gives out accurate and concise results, provided that natural language questions can be understood and mapped precisely to structured queries over the knowledge base. The challenge, however, is that a human can ask one question in many different ways. Previous approaches have natural limits due to their representations: rule based approaches only understand a small set of ``canned'' questions, while keyword based or synonym based approaches cannot fully understand the questions. In this paper, we design a new kind of question representation: templates, over a billion scale knowledge base and a million scale QA corpora. For example, for questions about a city's population, we learn templates such as What's the population of $city?, How many people are there in $city?. We learned 27 million templates for 2782 relations. Based on these templates, our QA system KBQA effectively supports binary factoid questions, as well as complex questions which are composed of a series of binary factoid questions. Furthermore, we expand predicates in RDF knowledge base, which boosts the coverage of knowledge base by 57 times. Our QA system beats all other state-of-art works on both effectiveness and efficiency over QALD benchmarks.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Fudan University", 
        "name": "Wanyun Cui"
      }, 
      {
        "affiliation": "Fudan University", 
        "name": "Yanghua Xiao"
      }, 
      {
        "affiliation": "Facebook", 
        "name": "Haixun Wang"
      }, 
      {
        "affiliation": "West Virginia University", 
        "name": "Yangqiu Song"
      }, 
      {
        "affiliation": "Yonsei University", 
        "name": "Seung-won Hwang"
      }, 
      {
        "affiliation": "Fudan University", 
        "name": "Wei Wang"
      }
    ], 
    "type": "research", 
    "id": "research510"
  }, 
  "research323": {
    "title": "Knowledge Exploration Using Tables on the Web", 
    "abstract": "The increasing popularity of mobile device usage has ushered in many features in modern search engines that help users with various information needs. One of those needs is Knowledge Exploration, where related documents are returned in response to a user query, either directly through right-hand side knowledge panels or indirectly through navigable sections underneath individual search results. Existing knowledge exploration features have relied on a combination of Knowledge Bases and query logs. In this paper, we propose Knowledge Carousels of two modalities, namely sideways and downwards, that facilitate exploration of IS-A and HAS-A relationships, respectively, with regard to an entity-seeking query, based on leveraging the large corpus of tables on the Web. This brings many technical challenges, including associating correct carousels with the search entity, selecting the best carousel from the candidates, and finding titles that best describe the carousel. We describe how we address these challenges and also experimentally demonstrate through user studies that our approach produces better result sets than baseline approaches.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "NYU", 
        "name": "Fernando Chirigati"
      }, 
      {
        "affiliation": "Google Research NYC", 
        "name": "Jialu Liu"
      }, 
      {
        "affiliation": "Google Research", 
        "name": "Flip Korn"
      }, 
      {
        "affiliation": "Google Research", 
        "name": "You Wu"
      }, 
      {
        "affiliation": "Google", 
        "name": "Cong Yu"
      }, 
      {
        "affiliation": "Google Research NYC", 
        "name": "Hao Zhang"
      }
    ], 
    "type": "research", 
    "id": "research323"
  }, 
  "research1327": {
    "title": "Knowledge Verification for LongTail Verticals [Experiments and Analyses]", 
    "abstract": "Collecting structured knowledge for real-world entities has become a critical task for many applications. A big gap between the knowledge in existing knowledge repositories and the knowledge in the real world is the knowledge on tail verticals (i.e., less popular domains). Such knowledge, though not necessarily globally popular, can be personal hobbies to many people and thus collectively impactful. This paper studies the problem of knowledge verification for tail verticals; that is, deciding the correctness of a given triple.   Through comprehensive experimental study we answer the following questions. 1) Can we find evidence for tail knowledge from an extensive set of sources, including knowledge bases, the web, and query logs? 2) Can we judge correctness of the triples based on the collected evidence? 3) How can we further improve knowledge verification on tail verticals? Our empirical study suggests a new knowledge-verification framework, which we call FACTY, that applies various kinds of evidence collection techniques followed by knowledge fusion. FACTY can verify 50% of the (correct) tail knowledge with a precision of 84%, and it significantly outperforms state-of-the-art methods. Detailed error analysis on the obtained results suggests future research directions.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "National University of Singapo", 
        "name": "Furong Li"
      }, 
      {
        "affiliation": "Amazon", 
        "name": "Xin Luna Dong"
      }, 
      {
        "affiliation": "", 
        "name": "Anno Langen"
      }, 
      {
        "affiliation": "Google Inc.", 
        "name": "Yang Li"
      }
    ], 
    "type": "research", 
    "id": "research1327"
  }, 
  "research1363": {
    "title": "LDA*: A Robust and Large-scale Topic Modeling System", 
    "abstract": "We present LDA*, a system that has been deployed in one of the largest Internet companies to fulfil their requirements of \u00e2\u0080\u009ctopic modeling as an internal service\u00e2\u0080\u009d\u00e2\u0080\u0094relying on thousands of machines,engineers in different sectors submit their data, some are as large as 1.8TB, to LDA* and get results back in hours. LDA* is motivated by the observation that none of the existing topic modeling systems is robust enough\u00e2\u0080\u0094Each of these existing systems is designed for a specific point in the trade-off space that can be suboptimal, sometimes by up to 10\u00c3\u0097, across workloads.  Our first contribution is a systematic study of all recently proposed samplers: AliasLDA, F+LDA, LightLDA, and WarpLDA. We discovered a novel system trade-off among these samplers. Each sampler has different sampling complexity and performs differently, sometimes by 5\u00c3\u0097, on documents with different lengths. Based on this trade-off, we further developed a hybrid sampler that uses different samplers for different types of documents. This hybrid approach works across a wide range of workloads and outperforms the existing samplers by up to 2\u00c3\u0097. We then focused on distributed environments in which thousands of workers, each with different performance (due to virtualization and resource sharing), coordinate to train a topic model. Our second contribution is an asymmetric parameter server architecture that pushes some computation to the parameter server side. This architecture is motivated by the skew of the word frequency distribution and a novel trade-off we discovered between communication and computation. With this architecture, we outperform the traditional, symmetric architecture by up to 2\u00c3\u0097.  With these two contributions, together with a carefully engineered implementation, our system is able to outperform existing systems by up to 10\u00c3\u0097 and has already been running to provide topic modeling services for more than six months.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Peking University", 
        "name": "Lele Yu"
      }, 
      {
        "affiliation": "Peking University", 
        "name": "Bin Cui"
      }, 
      {
        "affiliation": "ETH", 
        "name": "Ce Zhang"
      }, 
      {
        "affiliation": "PKU", 
        "name": "Yingxia Shao"
      }
    ], 
    "type": "research", 
    "id": "research1363"
  }, 
  "research603": {
    "title": "LFTF: A Framework for Efficient Tensor Analytics at Scale", 
    "abstract": "Tensors are higher order generalizations of matrices to model multi-aspect data, e.g., a set of purchase records with the schema (user_id, product_id, timestamp, feedback). Tensor factorization is a powerful technique for generating a model from a tensor, just like matrix factorization generates a model from a matrix, but with higher accuracy and richer information as more attributes are available in a higher-order tensor than a matrix. The data model obtained by tensor factorization can be used for classification, recommendation, anomaly detection, and so on. Though having a broad range of applications, tensor factorization has not been popularly applied compared with matrix factorization that has been widely used in recommender systems, mainly due to the high computational cost and poor scalability of existing tensor factorization methods. Efficient and scalable tensor factorization is particularly challenging because real world tensor data are mostly sparse and massive. In this paper, we propose a novel distributed algorithm, called Lock-Free Tensor Factorization (LFTF), which significantly improves the efficiency and scalability of distributed tensor factorization by exploiting asynchronous execution in a re-formulated problem. Our experiments show that LFTF achieves much higher CPU and network throughput than existing methods, converges at least 17 times faster and scales to much larger datasets.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "CUHK", 
        "name": "Fan Yang"
      }, 
      {
        "affiliation": "CUHK", 
        "name": "Fanhua Shang"
      }, 
      {
        "affiliation": "", 
        "name": "Yuzhen Huang"
      }, 
      {
        "affiliation": "The Chinese University of Hong Kong", 
        "name": "James Cheng"
      }, 
      {
        "affiliation": "The Chinese University of Hong Kong", 
        "name": "Jinfeng Li"
      }, 
      {
        "affiliation": "CUHK", 
        "name": "Yunjian Zhao"
      }, 
      {
        "affiliation": "CUHK", 
        "name": "Ruihao Zhao"
      }
    ], 
    "type": "research", 
    "id": "research603"
  }, 
  "research738": {
    "title": "Leveraging Set Relations in Exact Set Similarity Join", 
    "abstract": "Exact set similarity join, which finds all the similar set pairs from two collections of sets, is a fundamental problem with a wide range of applications. The existing solutions for set similarity join follow a filtering-verification framework, which generates a list of candidate pairs through scanning indexes in the filtering phase, and reports those similar pairs in the verification phase. Though much research has been conducted on this problem, set relations, which we find out is quite effective on improving the algorithm efficiency through computational cost sharing, have never been studied. Therefore, in this paper, instead of considering each set individually, we explore the set relations in different levels to reduce the overall computational costs. First, it has been shown that most of the computational time is spent on the filtering phase, which can be quadratic to the number of sets in the worst case for the existing solutions. Thus we explore index-level set relations to reduce the filtering cost to be linear to the size of the input while keeping the same filtering power. We achieve this by grouping related sets into blocks in the index and skipping useless index probes in joins. Second, we explore answer-level set relations to further improve the algorithm based on the intuition that if two sets are similar, their answers may have a large overlap. We derive an algorithm which incrementally generates the answer of one set from an already computed answer of another similar set rather than compute the answer from scratch to reduce the computational cost. Finally, we conduct extensive performance studies using 21 real datasets with various data properties from a wide range of domains. The experimental results demonstrate that our algorithm outperforms all the existing algorithms across all datasets and can achieve more than an order of magnitude speedup against the state-of-the-art algorithms.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "CSE", 
        "name": "Xubo Wang"
      }, 
      {
        "affiliation": "QCIS", 
        "name": "Lu Qin"
      }, 
      {
        "affiliation": "CSE", 
        "name": "Xuemin Lin"
      }, 
      {
        "affiliation": "QCIS", 
        "name": "Ying Zhang"
      }, 
      {
        "affiliation": "CSE", 
        "name": "Lijun Chang"
      }
    ], 
    "type": "research", 
    "id": "research738"
  }, 
  "research427": {
    "title": "Lifting the Haze off the Cloud: A Consumer-Centric Market for Database Computation in the Cloud", 
    "abstract": "The availability of public computing resources in the cloud has revolutionized data analysis, but requesting cloud resources often involves complex decisions for consumers. Estimating the completion time and cost of a computation and requesting the appropriate cloud resources are challenging tasks even for an expert user. We propose a new market-based framework for pricing computational tasks in the cloud. Our framework introduces an agent between consumers and cloud providers. The agent takes data and computational tasks from users, estimates time and cost for evaluating the tasks, and returns to consumers contracts that specify the price and completion time. Our framework can be applied directly to existing cloud markets without altering the way cloud providers offer and price services. In addition, it simplifies cloud use for consumers by allowing them to compare contracts, rather than choose resources directly. We present design, analytical, and algorithmic contributions focusing on pricing computation contracts, analyzing their properties, and optimizing them in complex workflows. We conduct an experimental evaluation of our market framework over a real-world cloud service and demonstrate empirically that our market ensures three key properties: (a) that consumers benefit from using the market due to competitiveness among agents, (b) that agents have incentive to price contracts fairly, and (c) that inaccuracies in estimates do not pose a significant risk to agents\u00e2\u0080\u0099 profits. Finally, we present a fine-grained pricing mechanism for complex workflows and show that it can increase agent profits by more than an order of magnitude in some cases.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "U of Massachusetts Amherst", 
        "name": "Yue Wang"
      }, 
      {
        "affiliation": "University of Massachusetts Amherst", 
        "name": "Alexandra Meliou"
      }, 
      {
        "affiliation": "U. Mass", 
        "name": "Gerome Miklau"
      }
    ], 
    "type": "research", 
    "id": "research427"
  }, 
  "research606": {
    "title": "Local Search Methods for k-Means with Outliers", 
    "abstract": " We study the problem of $k$-means clustering in the presence of  outliers.  The goal is to cluster a given set of data points to  minimize the variance of the points assigned to the same cluster,  with the freedom of ignoring a small set of data points that can be  labeled as outliers.  Clustering with outliers has received a lot of attention in the data processing community, but practical,  efficient, and provably good algorithms remain unknown for the most popular $k$-means objective.  Our work proposes a simple local search-based algorithm for $k$-means clustering with outliers.  We prove that this algorithm   achieves constant-factor approximate solutions and can be combined with known sketching techniques to scale to large data sets.  Using empirical evaluation on both synthetic and real-world data, we  demonstrate that the algorithm dominates recently proposed heuristic approaches for the problem.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "UIUC", 
        "name": "Shalmoli Gupta"
      }, 
      {
        "affiliation": "Google", 
        "name": "Ravi Kumar"
      }, 
      {
        "affiliation": "Washington University St. Louis", 
        "name": "Kefu Lu"
      }, 
      {
        "affiliation": "", 
        "name": "Benjamin Moseley"
      }, 
      {
        "affiliation": "Google", 
        "name": "Sergei Vassilvitskii"
      }
    ], 
    "type": "research", 
    "id": "research606"
  }, 
  "research680": {
    "title": "Looking Ahead Makes Query Plans Robust", 
    "abstract": "Query optimizers and query execution engines cooperate to deliver high performance on complex analytic queries. Typically, the optimizer searches through the plan space and sends a selected plan to the execution engine. However, optimizers may at times miss the optimal plan, with sometimes disastrous impact on performance. In this paper, we develop the notion of robustness of a query evaluation strategy with respect to a space of query plans. We also propose a novel query execution strategy called Lookahead Information Passing (LIP) that is robust with respect to the space of (fully pipeline-able) left-deep query plan trees for in-memory star schema data warehouses. LIP ensures that execution times for the best and the worst case plans are far closer than without LIP. In fact, under certain assumptions of independent and uniform distributions, any plan in that space is theoretically guaranteed to execute in near-optimal time. LIP ensures that the execution time for every plan in the space is nearly-optimal. In this paper, we also evaluate these claims using workloads that include skew and correlation. With LIP we make an initial foray into a novel way of thinking about robustness from the perspective of query evaluation, where we develop strategies (like LIP) that collapse plan sub-spaces in the overall global plan space.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "UW-Madison", 
        "name": "Jianqiao Zhu"
      }, 
      {
        "affiliation": "UW-Madison", 
        "name": "Navneet Potti"
      }, 
      {
        "affiliation": "UW-Madison", 
        "name": "Saket Saurabh"
      }, 
      {
        "affiliation": "University of Wisconsin", 
        "name": "Jignesh Patel"
      }
    ], 
    "type": "research", 
    "id": "research680"
  }, 
  "research1387": {
    "title": "MGTag: a Multi-Dimensional Graph Labeling Scheme for Fast Reachability Queries", 
    "abstract": "Reachability queries ask whether a vertex can reach another vertex on large directed graphs. It is one of the most fundamental graph operators and has attracted researchers in both academics and industry to study it. The main technical challenge is to support fast reachability queries by efficient managing the three main costs: the index construction time, the index size and the query processing time on large and dense graphs. As real world graphs grow bigger in size, these problems remain open challenges that demand high performance solutions. In this paper, we propose a Multi-Dimensional Graph Labeling approach (called MGTag) to supporting fast reachability queries. Our MGTag approach is novel in three aspects. First, it iteratively partitions a graph into multiple subgraphs with disjoint vertex sets, called non-shared graphs, and several inter-partition edges, called cross-edges. Second, we build a four-dimensional label for each vertex, which helps tagging each vertex by its layer, the non-shared graph it belongs to, and the source dimension and destination dimension interval for each vertex. With the layer label and the non-shared graph label, we can determine the topological positions in terms of non-shared graphs for any two vertices in the input graph. With the two dimensional interval label, we can efficiently answer the reachability queries for vertex pairs in the same non-shared graph. Finally, we design the MGTag algorithms to answer reachability queries efficiently and we build two directional labels - up and down labels to quickly filter irrelevant vertices and further speed up the processing of reachability queries. We evaluate MGTag by conducting extensive experiments on 28 large/small and dense/sparse graphs. We show that MGTag can build the multi-dimensional graph label index efficiently with the competitive index size, compared with most of the state-of-the-art approaches, and MGTag is highly scalable and more efficient than the state-of-the-art approaches in answering reachability queries.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "HUST", 
        "name": "Shuang Zhou"
      }, 
      {
        "affiliation": "HUST", 
        "name": "Pingpeng Yuan"
      }, 
      {
        "affiliation": "Georgia Institute of Technology", 
        "name": "Ling Liu"
      }, 
      {
        "affiliation": "", 
        "name": "Hai Jin"
      }
    ], 
    "type": "research", 
    "id": "research1387"
  }, 
  "research677": {
    "title": "MILC: Inverted List Compression in Memory", 
    "abstract": "Inverted list compression is a topic that has been studied for 50 years due to its fundamental importance in numerous applications including information retrieval, databases, and graph analytics. Typically, an inverted list compression algorithm is evaluated on its space overhead and query processing time. Earlier list compression designs mainly focused on minimizing the space overhead to reduce expensive disk I/O time in disk-oriented systems. But the recent trend is shifted towards reducing query processing time because the underlying systems tend to be memory-resident. Although there are many highly optimized compression approaches in main memory, there is still a considerable performance gap between query processing over compressed lists and uncompressed lists, which motivates this work. In this work, we set out to bridge this performance gap for the first time by proposing a new compression scheme, namely, MILC (memory inverted list compression). MILC relies on a series of techniques including offset-oriented fixed-bit encoding, dynamic partitioning, in-block compression, cache-aware optimization, and SIMD acceleration. We conduct experiments on three real-world datasets in information retrieval, databases, and graph analytics to demonstrate the high performance and low space overhead of MILC. We compare MILC with 12 recent compression algorithms and experimentally show that MILC improves the query performance by up to 13.2X and reduces the space overhead by up to 4.7X.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "UC San Diego", 
        "name": "Jianguo Wang"
      }, 
      {
        "affiliation": "UCSD", 
        "name": "Chunbin Lin"
      }, 
      {
        "affiliation": "UCSD", 
        "name": "Ruining He"
      }, 
      {
        "affiliation": "UCSD", 
        "name": "Moojin Chae"
      }, 
      {
        "affiliation": "UCSD", 
        "name": "Yannis Papakonstantinou"
      }, 
      {
        "affiliation": "UCSD", 
        "name": "Steven Swanson"
      }
    ], 
    "type": "research", 
    "id": "research677"
  }, 
  "research489": {
    "title": "MapReduce and Streaming Algorithms for Diversity Maximization in Metric Spaces of Bounded Doubling Dimension", 
    "abstract": "Given a dataset of points in a metric space and an integer $k$, a diversity maximization problem requires determining a subset of $k$ points maximizing some diversity objective measure, e.g., the minimum or the average distance between a pair of points in the subset. Diversity maximization is computationally hard, hence only approximate solutions can be hoped for. Although its applications are mainly in massive data analysis, most of the past research on diversity maximization has concentrated on the standard sequential setting.  In this work we present space and pass/round-efficient  diversity maximization algorithms for the Streaming and MapReduce models and analyze their approximation guarantees for the relevant class of metric spaces of bounded doubling dimension. Similarly to other approaches in the literature, our algorithms revolve upon the determination of high-quality core-sets, that is, (much) smaller subsets of the input which contain good approximations to the optimal solution for the whole input. For a variety of diversity objective functions, our algorithms attain an $(\\alpha+\\epsilon)$-approximation ratio, for any constant $\\epsilon > 0$, where $\\alpha$ is the best approximation ratio achieved by a polynomial-time, linear-space sequential algorithm for the same diversity objective.  This is a substantial improvement compared to the approximation ratios attainable in the Streaming and MapReduce models by state-of-the-art algorithms for the case of general metric spaces. We also provide extensive experimental evidence of the practical relevance of our algorithms. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Padova", 
        "name": "Matteo Ceccarello"
      }, 
      {
        "affiliation": "", 
        "name": "Andrea Pietracaprina"
      }, 
      {
        "affiliation": "University of Padova", 
        "name": "Geppino Pucci"
      }, 
      {
        "affiliation": "Brown University", 
        "name": "Eli Upfal"
      }
    ], 
    "type": "research", 
    "id": "research489"
  }, 
  "industrial1203": {
    "title": "Matrix Profile IV: Using Weakly Labeled Time Series to Predict Outcomes", 
    "abstract": "In academic settings over the last decade, there has been significant progress in time series classification. However, much of this work makes assumptions that are simply unrealistic for deployed industrial applications. Examples of these unrealistic assumptions include the following: assuming that data subsequences have a single fixed-length, are precisely extracted from the data, and are correctly labeled according to their membership in a set of equalsize classes. In real-world industrial settings, these patterns can be of different lengths, the class annotations may only belong to a general region of the data, may contain errors, and finally, the class distribution is typically highly skewed. Can we learn from such weakly labeled data? In this work, we introduce SDTS, a scalable algorithm that can learn in such challenging settings. We demonstrate the utility of our ideas by learning from diverse datasets with millions of datapoints. As we shall demonstrate, our domain-agnostic parameter-free algorithm can be competitive with domain-specific algorithms used in neuroscience and entomology, even when those algorithms have been tuned by domain experts to incorporate domain knowledge.", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "UC Riverside", 
        "name": "Chin-Chia Michael Yeh"
      }, 
      {
        "affiliation": "Oracle Corporation", 
        "name": "Nickolas Kavantzas"
      }, 
      {
        "affiliation": "UC Riverside", 
        "name": "Eamonn Keogh"
      }
    ], 
    "type": "industrial", 
    "id": "industrial1203"
  }, 
  "research1278": {
    "title": "Memory Management Techniques for Large-Scale Persistent-Main-Memory Systems", 
    "abstract": "Storage Class Memory (SCM) is a novel class of memory technologies that promise to revolutionize database architectures. SCMis byte-addressable and exhibits latencies similar to those of DRAM, while being non-volatile. Hence, SCM could replace both main memory and storage, enabling a novel single-level database architecture without the traditional I/O bottleneck. Fail-safe persistent SCM allocation can be considered conditio sine qua non for enabling this novel architecture paradigm for database management systems. In this paper we present  PAllocator, a fail-safe persistent SCM allocator whose design emphasizes high concurrency and capacity scalability. Contrary to previous works, PAllocator thoroughly addresses the important challenge of persistent memory fragmentation by implementing an efficient defragmentation algorithm. We show that PAllocator outperforms state-of-the-art persistent allocators by up to one order of magnitude, both in operation throughput and recovery time, and enables up to 2.39x higher operation throughput on a persistent B-Tree.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "TU Dresden & SAP SE", 
        "name": "Ismail Oukid"
      }, 
      {
        "affiliation": "SAP SE", 
        "name": "Daniel Booss"
      }, 
      {
        "affiliation": "Independent", 
        "name": "Adrien Lespinasse"
      }, 
      {
        "affiliation": "Technische Universit\u00c3\u00a4t Dresden", 
        "name": "Wolfgang Lehner"
      }, 
      {
        "affiliation": "Intel Deutschland GmbH", 
        "name": "Thomas Willhalm"
      }, 
      {
        "affiliation": "Grenoble INP - Ensimag", 
        "name": "Gr\u00c3\u00a9goire Gomes"
      }
    ], 
    "type": "research", 
    "id": "research1278"
  }, 
  "research1297": {
    "title": "Minimal OnRoad Time Route Scheduling on Time-Dependent Graph", 
    "abstract": "On time-dependent graph, fastest path query is an important problem and has been well studied. It focuses on minimizing the total travel time(waiting time + on-road time) but does not allow waiting on any intermediate vertex if FIFO property is applied. However, in practice, waiting on a vertex has the ability to reduce the time spent on road (for example, resuming traveling after a traffic jam). In this paper, we study how to find a path with the minimal on-road time on time-dependent graph by allowing waiting on some predefined parking vertices. The existing works are based on the following fact: the arrival time of a vertex v is determined by the arrival time of its in-neighbor u, which does not hold in our scenario since we also consider the waiting time on u if u allowing waiting. Thus, determining the waiting time on each parking vertex to achieve the minimal on-road time becomes a big challenge, which further breaks FIFO property. To cope with this challenging problem, we propose two efficient algorithms using minimum on-road travel cost function to answer the query. The evaluations on real-world road networks show that the proposed algorithms are more accurate and efficient than the extensions of existing algorithms. In addition to that, the results further indicate, if the parking facilities are enabled in the route scheduling algorithms, the on-road time will reduce significantly compared to the fastest path algorithm.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Queensland", 
        "name": "Lei Li"
      }, 
      {
        "affiliation": "University of Queensland", 
        "name": "Wen Hua"
      }, 
      {
        "affiliation": "University of Queensland", 
        "name": "Xingzhong Du"
      }, 
      {
        "affiliation": "University of Queensland", 
        "name": "Xiaofang Zhou"
      }
    ], 
    "type": "research", 
    "id": "research1297"
  }, 
  "research1053": {
    "title": "Mison: A Fast JSON Parser for Data Analytics", 
    "abstract": "The growing popularity of the JSON format has fueled increased interest in loading and processing JSON data within analytical data processing systems. However, in many applications, JSON parsing dominates performance and cost. In this paper, we present a new JSON parser called Mison that is particularly tailored to this class of applications, by pushing down both projection and filter operators of analytical queries into the parser. To achieve these features, we propose to deviate from the traditional approach of building parsers using finite state machines (FSMs).  Instead, we follow a two-level approach that enables the parser to jump directly to the correct position of a queried field without having to perform expensive tokenizing steps to find the field. At the upper level, Mison speculatively predicts the logical locations of queried fields based on previously seen patterns in a dataset. At the lower level, Mison builds structural indices on JSON data to map logical locations to physical locations. Unlike all existing FSM-based parsers, building structural indices converts control flow into data flow, thereby largely eliminating inherently unpredictable branches in the program and exploiting the parallelism available in modern processors. We experimentally evaluate Mison using representative real-world JSON datasets and the TPC-H benchmark, and show that Mison produces significant performance benefits over the best existing JSON parsers; in some cases, the performance improvement is over one order of magnitude.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Microsoft Research", 
        "name": "Yinan Li"
      }, 
      {
        "affiliation": "University of Pittsburgh", 
        "name": "Nikos R. Katsipoulakis"
      }, 
      {
        "affiliation": "Microsoft Research", 
        "name": "Badrish Chandramouli"
      }, 
      {
        "affiliation": "Microsoft Research", 
        "name": "Jonathan Goldstein"
      }, 
      {
        "affiliation": "ETH Zuerich", 
        "name": "Donald Kossmann"
      }
    ], 
    "type": "research", 
    "id": "research1053"
  }, 
  "research139": {
    "title": "Mostly-Optimistic Concurrency Control for Highly Contended Dynamic Workloads on a Thousand Cores", 
    "abstract": "Future servers will be equipped with thousands of CPU cores and deep memory hierarchies. Traditional concurrency control (CC) schemes\u00e2\u0080\u0094both optimistic and pessimistic\u00e2\u0080\u0094slow down orders of magnitude in such environments for highly contended workloads. Optimistic CC (OCC) scales the best for workloads with few conflicts, but suffers from clobbered reads for high conflict workloads. Although pessimistic locking can protect reads, it floods cache-coherence backbones in deep memory hierarchies and can also cause numerous deadlock aborts.  This paper proposes a new CC scheme, mostly-optimistic concurrency control (MOCC), to address these problems. MOCC achieves orders of magnitude higher performance for dynamic workloads on modern servers. The key objective of MOCC is to avoid clobbered reads for high conflict workloads, without any centralized mechanisms or heavyweight interthread communication. To satisfy such needs, we devise a native, cancellable reader-writer spinlock and a serializable protocol that can acquire, release and re-acquire locks in any order without expensive interthread communication. For low conflict workloads, MOCC maintains OCC\u00e2\u0080\u0099s high performance without taking read locks.  Our experiments with high conflict YCSB workloads on a 288-core server reveal that MOCC performs 8\u00c3\u0097 and 23\u00c3\u0097 faster than OCC and pessimistic locking, respectively. It achieves 17 million TPS for TPC-C and more than 110 million TPS for YCSB without conflicts, 170\u00c3\u0097 faster than pessimistic methods.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Toronto", 
        "name": "Tianzheng Wang"
      }, 
      {
        "affiliation": "Hewlett Packard Enterprise", 
        "name": "Hideaki Kimura"
      }
    ], 
    "type": "research", 
    "id": "research139"
  }, 
  "research235": {
    "title": "Multi-Query Optimization for Subgraph Isomorphism Search", 
    "abstract": "Existing work on subgraph isomorphism search mainly focuses on  a-query-at-a-time approaches: optimizing and answering each query separately and sequentially. When multiple queries arrive in a batch, sequential processing is not always the most efficient. In this paper, we study multi-query optimization for subgraph isomorphism search. We first propose a novel method for efficiently detecting useful common subgraphs and a data structure to organize them. We propose a heuristic algorithm based on the data structure to compute a query execution order so that cached intermediate results can be effectively utilized. To balance memory usage and time for cached result retrieval, we present a novel structure for caching the intermediate results. We then provide strategies to revise existing single-query subgraph isomorphism algorithms to seamlessly utilize the cached results, which leads to significant performance improvement. Extensive experiments verified the effectiveness of our solution.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Griffith University", 
        "name": "Xuguang  Ren"
      }, 
      {
        "affiliation": "Griffith University", 
        "name": "Junhu Wang"
      }
    ], 
    "type": "research", 
    "id": "research235"
  }, 
  "research573": {
    "title": "NED: An Inter-Graph Node Metric Based On Edit Distance", 
    "abstract": "Node similarity is fundamental in graph analytics. However, node similarity between nodes in different graphs (inter-graph nodes) has not received enough attention yet. The inter-graph node similarity is important in learning a new graph based on the knowledge extracted from an existing graph (transfer learning on graphs) and has applications in biological, communication, and social networks. In this paper, we propose a novel distance function for measuring inter-graph node similarity with edit distance, called NED. In NED, two nodes are compared according to their local neighborhood topologies which are represented as unordered k-adjacent trees, without relying on any extra information. Due to the hardness of computing tree edit distance on unordered trees which is NP-Complete, we propose a modified tree edit distance, called TED, for comparing unordered and unlabeled k-adjacent trees. TED* is a metric distance, as the original tree edit distance, but more importantly, TED* is polynomially computable. As a metric distance, NED admits efficient indexing, provides interpretable results, and shows to perform better than existing approaches on a number of data analysis tasks, including graph de-anonymization. Finally, the efficiency and effectiveness of NED are empirically demonstrated using real-world graphs.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Facebook", 
        "name": "Haohan Zhu"
      }, 
      {
        "affiliation": "Apple Inc.", 
        "name": "Xianrui Meng"
      }, 
      {
        "affiliation": "Boston U.", 
        "name": "George Kollios"
      }
    ], 
    "type": "research", 
    "id": "research573"
  }, 
  "research318": {
    "title": "NG-DBSCAN: Scalable Density-Based Clustering for Arbitrary Data", 
    "abstract": "We present NG-DBSCAN, an approximate density-based clustering algorithm that operates on arbitrary data and any symmetric distance measure. The distributed design of our algorithm makes it scalable to very large datasets; its approximate nature makes it fast, yet capable of producing high quality clustering results. We provide a detailed overview of the steps of NG-DBSCAN, together with their analysis. Our results, obtained through an extensive experimental campaign with real and synthetic data, substantiate our claims about NG-DBSCAN\u00e2\u0080\u0099s performance and scalability.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Pisa", 
        "name": "Alessandro Lulli"
      }, 
      {
        "affiliation": "", 
        "name": "Matteo Dell'Amico"
      }, 
      {
        "affiliation": "", 
        "name": "Pietro Michiardi"
      }, 
      {
        "affiliation": "", 
        "name": "Laura Ricci"
      }
    ], 
    "type": "research", 
    "id": "research318"
  }, 
  "research1399": {
    "title": "Non-Invasive Progressive Optimization for In-Memory Databases", 
    "abstract": "Progressive optimization introduces robustness for database workloads against wrong estimates, skewed data, correlated attributes, or outdated statistics. Previous work focuses on cardinality estimates and rely on expensive counting methods as well as complex learning algorithms.    In this paper, we utilize performance counters to drive progressive optimization during query execution. The main advantages are that performance counters introduce virtually no costs on modern CPUs and their usage enables a non-invasive monitoring. We present fine-grained cost models to detect divergences between estimates and actual costs to kick-start the reoptimization process. Based on these cost models, we implement an optimization approach that estimates the individual selectivities of a multi-selection query efficiently. Furthermore, we are able to learn properties like sortedness, skew, or correlation.   In our evaluation we show, that the overhead of our approach is negligible but the performance improvements are convincing. Using progressive optimization, we improve runtime up to a factor of three compared to average runtime and up to a factor of 4,5 compared to worst case runtime. As a result, we avoid costly operator execution orders and thus make query execution highly robust.    ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "", 
        "name": "Steffen Zeug"
      }, 
      {
        "affiliation": "MIT", 
        "name": "Holger Pirk"
      }, 
      {
        "affiliation": "", 
        "name": "Johann-Christoph Freytag"
      }
    ], 
    "type": "research", 
    "id": "research1399"
  }, 
  "research565": {
    "title": "OLAK: An Efficient Algorithm to Prevent Unraveling in Social Networks", 
    "abstract": "In this paper, we study the problem of anchored k-core. Given a graph G, an integer k and a budget b, we aim to identify b vertices in G so that we can determine the largest induced subgraph J in which every vertex, except the b vertices, has at least k neighbors in J. This problem was introduced by Bhawalkar and Kleinberg et al. in the context of user engagement in social networks, where a user may leave a community if he/she has less than k friends engaged. The problem has been shown to be NP-hard and inapproximable. A polynomial-time algorithm for graphs with bounded tree-width has been proposed. However, this assumption usually does not hold in real-life graphs, and their techniques cannot be extended to handle general graphs.  Motivated by this, we propose an efficient algorithm, namely onion-layer based anchored k-core (OLAK), for the anchored k-core problem on large scale graphs. To facilitate computation of the anchored k-core, we design an onion layer structure, which is generated by a simple onion-peeling-like algorithm against a small set of vertices in the graph. We show that the computation of the best anchor can simply be conducted upon the vertices on the onion layers, which significantly reduces the search space. Based on the well-organized layer structure, we develop efficient candidates exploration, early termination and pruning techniques to further speed up computation. Comprehensive experiments on 10 real-life graphs demonstrate the effectiveness and efficiency of our proposed methods.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "UTS", 
        "name": "Fan Zhang"
      }, 
      {
        "affiliation": "CSE", 
        "name": "Wenjie Zhang"
      }, 
      {
        "affiliation": "QCIS", 
        "name": "Ying Zhang"
      }, 
      {
        "affiliation": "QCIS", 
        "name": "Lu Qin"
      }, 
      {
        "affiliation": "CSE", 
        "name": "Xuemin Lin"
      }
    ], 
    "type": "research", 
    "id": "research565"
  }, 
  "research1365": {
    "title": "On Sampling from Massive Graph Streams", 
    "abstract": "We propose Graph Priority Sampling (GPS), a new paradigm for order-based reservoir sampling from massive streams of graph edges. GPS provides a general way to weight edge sampling according to auxiliary and/or size variables so as to accomplish various estimation goals of graph properties. In the context of subgraph counting, we show how edge sampling weights can be chosen so as to minimize the estimation variance of counts of specified sets of subgraphs. In distinction with many prior graph sampling schemes, GPS separates the functions of edge sampling and subgraph estimation. We propose two estimation frameworks: (1) Post-Stream estimation, to allow GPS to construct a reference sample of edges to support retrospective graph queries, and (2) In-Stream estimation, to allow GPS to obtain lower variance estimates by incrementally updating the subgraph count estimates during stream processing. Unbiasedness of subgraph estimators is established through a new Martingale formulation of graph stream order sampling, in which subgraph estimators, written as a product of constituent edge estimators, are unbiased, even when computed at different points in the stream. The separation of estimation and sampling enables significant resource savings relative to previous work. We illustrate our framework with applications to triangle and wedge counting. We perform a large-scale experimental study on real-world graphs from various domains and types. GPS achieves high accuracy with less than 1% error for triangle and wedge counting, while storing a small fraction of the graph with average update times of a few microseconds per edge. Notably, for billion-scale graphs, GPS accurately estimates triangle and wedge counts with less than 1% error, while storing a small fraction of less than 0.01% of the total edges in the graph.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Intel Labs", 
        "name": "Nesreen Ahmed"
      }, 
      {
        "affiliation": "Texas A&M University", 
        "name": "Nick Duffield"
      }, 
      {
        "affiliation": "Intel Labs", 
        "name": "Theodore Willke"
      }, 
      {
        "affiliation": "PARC", 
        "name": "Ryan Rossi"
      }
    ], 
    "type": "research", 
    "id": "research1365"
  }, 
  "research616": {
    "title": "One-Pass Error Bounded Trajectory Simplification", 
    "abstract": "Nowadays, various sensors are collecting, storing and transmitting tremendous trajectory data, and it is known that raw trajectory data seriously wastes the storage, network band and computing resource. Line simplification (LS) algorithms are an effective approach to attacking this issue by compressing data points in a trajectory to a set of continuous line segments, and are commonly used in practice. However, existing  LS algorithms are not sufficient for the needs of sensors in mobile devices. In this study, we first develop a one-pass error bounded trajectory simplification algorithm (OPERB), which scans each data point in a trajectory once and only once. We then propose an aggressive one-pass error bounded trajectory simplification algorithm (OPERB-A), which allows interpolating new data points into a trajectory under certain conditions. Finally, we experimentally verify that our approaches (OPERB and OPERB-A) are both efficient and effective, using four real-life trajectory datasets. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Beihang University", 
        "name": "Xuelian Lin"
      }, 
      {
        "affiliation": "Beihang University", 
        "name": "Shuai Ma"
      }, 
      {
        "affiliation": "Beihang University", 
        "name": "Han  Zhang"
      }, 
      {
        "affiliation": "Beihang University", 
        "name": "Tianyu Wo"
      }, 
      {
        "affiliation": "Beihang University", 
        "name": "Jinpeng Huai"
      }
    ], 
    "type": "research", 
    "id": "research616"
  }, 
  "research1386": {
    "title": "Optimizing Deep CNN-Based Queries over Video Streams at Scale", 
    "abstract": "  Video is one of the fastest-growing sources of data and is rich with   interesting semantic information. Furthermore, recent advances in   computer vision, in the form of deep convolutional neural networks   (CNNs), have made it possible to query this semantic information   with near-human accuracy (in the form of image tagging).  However,   performing inference with state-of-the-art CNNs is computationally   expensive: analyzing videos in real time (at 30 frames/sec) requires   a \\$1200 GPU per video stream, posing a serious computational   barrier to CNN adoption in large-scale video data management   systems. In response, we present \\sn, a system that uses cost-based   optimization to assemble a specialized video processing pipeline for   each input video stream, greatly accelerating subsequent CNN-based   queries on the video.  As \\sn observes a video, it trains two types   of pipeline components (which we call filters) to exploit the   locality in the video stream: \\emph{difference detectors} that   exploit temporal locality between frames, and \\emph{specialized     models} that are tailored to a specific scene and query (i.e.,   exploit environmental and query-specific locality).  We show that   the optimal set of filters and their parameters depends   significantly on the video stream and query in question, so \\sn   introduces an efficient cost-based optimizer for this problem to   select them.  With this approach, our \\sn prototype achieves up to   120-3,200$\\times$ speed-ups (318-8,500$\\times$ real-time) on binary   classification tasks over real-world webcam and surveillance video   while maintaining accuracy within 1-5\\% of a state-of-the-art CNN.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "", 
        "name": "Daniel Kang"
      }, 
      {
        "affiliation": "Stanford University", 
        "name": "John Emmons"
      }, 
      {
        "affiliation": "Stanford University", 
        "name": "Firas Abuzaid"
      }, 
      {
        "affiliation": "Stanford University", 
        "name": "Peter Bailis"
      }, 
      {
        "affiliation": "Stanford University", 
        "name": "Matei Zaharia"
      }
    ], 
    "type": "research", 
    "id": "research1386"
  }, 
  "research1385": {
    "title": "Optimizing Voice-Based Output of Relational Data", 
    "abstract": "Recent research on data visualization aims at finding the best way to present structured data via visual interfaces. We introduce and study the complementary problem of ``data audiolization''. Our goal is to present relational data in the most efficient way via voice audio output. The application domains for the presented techniques include (but are not limited to) database interfaces that interact via speech input and output with their users.   Current systems (e.g., EchoQuery) read out one result row after the other. This effectively limits their applicability to queries with extremely small results. We treat voice output generation as an optimization problem. The goal is to minimize speaking time while transmitting an approximation of a relational table to the user. We formalize voice output optimization and show that it is NP-hard. We present three approaches to solve that problem. First, we show how the problem can be translated into an integer linear program which enables us to apply corresponding solvers. Second, we present a hybrid approach that forms groups of similar rows in a pre-processing step, using a variant of the apriori algorithm. Then, we select row groups via integer programming. Finally, we present a greedy algorithm that runs in polynomial time. Under simplifying assumptions, we prove that it generates near-optimal output by leveraging the sub-modularity property of our cost functions. We compare our algorithms experimentally and analyze their complexity.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Cornell University", 
        "name": "Immanuel Trummer"
      }, 
      {
        "affiliation": "", 
        "name": "Jiancheng Zhu"
      }, 
      {
        "affiliation": "", 
        "name": "Mark Bryan"
      }
    ], 
    "type": "research", 
    "id": "research1385"
  }, 
  "research1069": {
    "title": "OrpheusDB: Bolt-on Versioning for Relational Databases", 
    "abstract": "Data science teams often collaboratively analyze datasets, generating dataset versions at each stage of iterative exploration and analysis. There is a pressing need for a system that can support dataset versioning, enabling such teams to efficiently store, track, and query across dataset versions. We introduce OrpheusDB, a dataset version control system that \u00e2\u0080\u009cbolts on\u00e2\u0080\u009d versioning capabilities to a traditional relational database system, thereby gaining the analytics capabilities of the database \u00e2\u0080\u009cfor free\u00e2\u0080\u009d. We develop and evaluate multiple data models for representing versioned data, as well as a light-weight partitioning scheme, LyreSplit, to further optimize the models for reduced query latencies. With LyreSplit, OrpheusDB is on average 1000x faster in finding effective (and better) partitionings than competing approaches, while also reducing the latency of version retrieval by up to 20x relative to schemes without partitioning. LyreSplit can be applied in an online fashion as new versions are added, alongside an intelligent migration scheme that reduces migration time by 10x on average.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "UIUC", 
        "name": "Aditya Parameswaran"
      }, 
      {
        "affiliation": "University of Illinois at Urbana Champaign", 
        "name": "Silu Huang"
      }, 
      {
        "affiliation": "U. of Illinois at Urbana Champ", 
        "name": "Liqi Xu"
      }, 
      {
        "affiliation": "Peking University", 
        "name": "Jialin Liu"
      }, 
      {
        "affiliation": "University of Chicago", 
        "name": "Aaron Elmore"
      }
    ], 
    "type": "research", 
    "id": "research1069"
  }, 
  "industrial778": {
    "title": "Parallel Replication across Formats in SAP HANA for Scaling Out Mixed OLTP/OLAP Workloads", 
    "abstract": "Modern in-memory database systems are facing the need of efficiently supporting mixed workloads of OLTP and OLAP. A conventional approach to this requirement is to rely on ETL-style, application-driven data replication between two very different OLTP and OLAP systems, sacrificing real-time reporting on operational data. An alternative approach is to run OLTP and OLAP workloads in a single machine, which eventually limits the maximum scalability of OLAP query performance. In order to tackle this challenging problem, we propose a novel database replication architecture called HANA Asynchronous Parallel Table Replication (ATR). ATR supports OLTP workloads in one primary machine, while it supports heavy OLAP workloads in replicas. Here, row-store formats can be used for OLTP transactions at the primary, while column-store formats are used for OLAP analytical queries at the replicas. ATR is designed to support elastic scalability of OLAP query performance while it minimizes the overhead for transaction processing at the primary and minimizes CPU consumption for replayed transactions at the replicas. ATR employs a novel optimistic lock-free parallel log replay scheme which exploits characteristics of multi-version concurrency control (MVCC) in order to enable real-time reporting by minimizing the propagation delay between the primary and replicas. It also supports adaptive query routing depending on its predefined max acceptable staleness range. Through extensive experiments with a concrete implementation available in a commercial product, we demonstrate that ATR achieves sub-second visibility delay even for update-intensive workloads, providing scalable OLAP performance without notable overhead to the primary.", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "SAP Labs Korea", 
        "name": "Juchang Lee"
      }, 
      {
        "affiliation": "Pohang University of Science and Technology (POSTECH)", 
        "name": "SeungHyun Moon"
      }, 
      {
        "affiliation": "SAP Labs Korea", 
        "name": "Kyu Hwan Kim"
      }, 
      {
        "affiliation": "SAP Labs Korea", 
        "name": "Deok Hoe Kim"
      }, 
      {
        "affiliation": "Seoul National University", 
        "name": "Sang Kyun Cha"
      }, 
      {
        "affiliation": "POSTECH", 
        "name": "Wook-Shin Han"
      }, 
      {
        "affiliation": "SAP Labs Korea", 
        "name": "Chang Gyoo Park"
      }, 
      {
        "affiliation": "SAP Labs Korea", 
        "name": "Hyoung Jun Na"
      }, 
      {
        "affiliation": "SAP Labs Korea", 
        "name": "Joo Yeon Lee"
      }
    ], 
    "type": "industrial", 
    "id": "industrial778"
  }, 
  "research218": {
    "title": "Path Cost Distribution Estimation Using Trajectory Data", 
    "abstract": "With the growing volumes of vehicle trajectory data, it becomes increasingly possible to capture time-varying and uncertain travel costs in a road network, including travel time and fuel consumption. The current paradigm represents a road network as a weighted graph, blasts trajectories into small fragments that fit the underlying edges to assign weights to edges, and then applies a routing algorithm to the resulting graph. We propose a new paradigm, the hybrid graph, that targets more accurate and more efficient cost distribution estimation. The new paradigm avoids blasting trajectories into small fragments and instead assigns weights to paths rather than simple to the edges.  We show how to compute path weights using trajectory data while taking into account the travel cost dependencies among the edges in the paths. Next, given a departure time and a path, we show how to select an optimal set of weights with associated paths that cover the query path and such that the weights enable the most accurate joint cost distribution estimation for the query path. The cost distribution of the path is then computed accurately using the joint distribution. Finally, we show how the resulting method for computing cost distributions of paths can be integrated into existing routing algorithms. Empirical studies with substantial trajectory data from two different cities offer insight into the design properties of the proposed method and confirm that the method is effective in real-world settings.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "NUS", 
        "name": "Jian Dai"
      }, 
      {
        "affiliation": "AAU", 
        "name": "Bin Yang"
      }, 
      {
        "affiliation": "AAU", 
        "name": "Chenjuan Guo"
      }, 
      {
        "affiliation": "Aalborg University", 
        "name": "Christian Jensen"
      }, 
      {
        "affiliation": "AAU", 
        "name": "Jilin Hu"
      }
    ], 
    "type": "research", 
    "id": "research218"
  }, 
  "industrial1072": {
    "title": "PaxosStore: High-availability Storage Made Practical in WeChat", 
    "abstract": "In this paper, we present PaxosStore, a high-availability storage system developed to support the comprehensive business of WeChat. It employs a combinational design in the storage layer to engage multiple storage engines constructed for different storage models. PaxosStore is characteristic of extracting the Paxos-based distributed consensus protocol as a middleware that is universally accessible to the underlying multi-model storage engines. This facilitates tuning, maintaining, scaling and extending the storage engines. According to our experience in engineering practice, to achieve a practical consistent read/write protocol is far more complex than its theory. To tackle such engineering complexity, we propose a layered design of the Paxos-based storage protocol stack, where PaxosLog, the key data structure used in the protocol, is devised to bridge the programming-oriented consistent read/write to the storage-oriented Paxos procedure. Additionally, we present optimizations based on Paxos that made fault-tolerance more efficient. Discussion throughout the paper primarily focuses on pragmatic solutions that could be insightful for building practical distributed storage systems.", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "Tencent Inc.", 
        "name": "Jianjun Zheng"
      }, 
      {
        "affiliation": "National University of Singapore", 
        "name": "Qian Lin"
      }, 
      {
        "affiliation": "Tencent Inc.", 
        "name": "Jiatao Xu"
      }, 
      {
        "affiliation": "Tencent Inc.", 
        "name": "Cheng Wei"
      }, 
      {
        "affiliation": "Tencent Inc.", 
        "name": "Chuwei Zeng"
      }, 
      {
        "affiliation": "Tencent Inc.", 
        "name": "Pingan Yang"
      }, 
      {
        "affiliation": "Tencent Inc.", 
        "name": "Yunfan Zhang"
      }
    ], 
    "type": "industrial", 
    "id": "industrial1072"
  }, 
  "research430": {
    "title": "Persistent Hybrid Transactional Memory for Databases", 
    "abstract": "Processors with hardware support for transactional memory (HTM) are rapidly becoming commonplace, and processor manufacturers are currently working on implementing support for upcoming non-volatile memory (NVM) technologies. The combination of HTM and NVM promises to be a natural choice for in-memory database synchronization. However, limitations on the size of hardware transactions and the lack of progress guarantees by modern HTM implementations prevent some applications from obtaining the full benefit of hardware transactional memory. In this paper, we propose a persistent hybrid TM algorithm called PHyTM for systems that support NVM and HTM. PHyTM allows hardware assisted ACID transactions to execute concurrently with pure software transactions, which allows applications to gain the benefit of persistent HTM while simultaneously accommodating unbounded transactions (with a high degree of concurrency). Experimental simulations demonstrate that PHyTM is fast and scalable for realistic workloads.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Huawei", 
        "name": "Hillel Avni"
      }, 
      {
        "affiliation": "University of Toronto", 
        "name": "Trevor Brown"
      }
    ], 
    "type": "research", 
    "id": "research430"
  }, 
  "research1397": {
    "title": "Perturbation Analysis of Database Queries", 
    "abstract": "We present a system, Perada, for parallel perturbation   analysis of database queries.  Perturbation analysis considers the results of a query evaluated with (a typically large number of) different parameter settings, to help discover leads and evaluate claims from data.  Perada simplifies the development of general, ad hoc perturbation analysis by providing a flexible API to support a variety of optimizations such as grouping, memoization, and pruning; by automatically optimizing performance through run-time observation, learning, and adaptation; and by hiding the complexity of concurrency and failures from its developers.  We demonstrate Perada's efficacy and efficiency with real workloads applying perturbation analysis to computational journalism.  ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "", 
        "name": "Brett Walenz"
      }, 
      {
        "affiliation": "Duke University", 
        "name": "Jun Yang"
      }
    ], 
    "type": "research", 
    "id": "research1397"
  }, 
  "research856": {
    "title": "Pivot-based Metric Indexing: Experiments and Analyses", 
    "abstract": "The general notion of a metric space encompasses a diverse range of data types and accompanying similarity measures. Hence, met-ric search plays an important role in a wide range of settings, in-cluding multimedia retrieval, data mining, and data integration. With the aim of accelerating metric search, a collection of pivot-based indexing techniques for metric data has been proposed, which reduces the number of potentially expensive similarity comparisons by exploiting the triangle inequality for pruning and validation. However, no comprehensive empirical study of those techniques exists. Existing studies each offers only a narrower coverage, and they use different pivot selection strategies that af-fect performance substantially and thus render cross-study com-parisons difficult or impossible. We offer a survey of existing piv-ot-based indexing techniques, and report a comprehensive empiri-cal comparison of their construction costs, update efficiency, stor-age sizes, and similarity search performance. As part of the study, we provide modifications for two existing indexing techniques to make them more competitive. The findings and insights obtained from the study reveal different strengths and weaknesses of dif-ferent indexing techniques, and offer guidance on selecting an ap-propriate indexing technique for a given setting.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Zhejiang University", 
        "name": "Lu Chen"
      }, 
      {
        "affiliation": "Zhejiang University", 
        "name": "Yunjun Gao"
      }, 
      {
        "affiliation": "Singapore Management University", 
        "name": "Baihua Zheng"
      }, 
      {
        "affiliation": "Aalborg University", 
        "name": "Christian Jensen"
      }, 
      {
        "affiliation": "Zhejiang University", 
        "name": "Hanyu Yang"
      }, 
      {
        "affiliation": "Zhejiang University", 
        "name": "Keyu Yang"
      }
    ], 
    "type": "research", 
    "id": "research856"
  }, 
  "research499": {
    "title": "Plausible Deniability for Privacy-Preserving Data Synthesis", 
    "abstract": "Releasing full data records is one of the most challenging problems in data privacy. On the one hand, many of the popular techniques such as data de-identification are problematic because of their dependence on the background knowledge of adversaries. On the other hand, rigorous methods such as the exponential mechanism for differential privacy are often computationally impractical to use for releasing high dimensional data or cannot preserve high utility of original data due to their extensive data perturbation.  This paper presents a criterion called plausible deniability that provides a formal privacy guarantee, notably for releasing sensitive datasets: an output record can be released only if a certain amount of input records are indistinguishable, up to a privacy parameter. This notion does not depend on the background knowledge of an adversary. Also, it can efficiently be checked by privacy tests. We present mechanisms to generate synthetic datasets with similar statistical properties to the input data and the same format. We study this technique both theoretically and experimentally. A key theoretical result shows that, with proper randomization, the plausible deniability mechanism generates differentially private synthetic data. We demonstrate the efficiency of this generative technique on a large dataset; it is shown to preserve the utility of original data with respect to various statistical analysis and machine learning measures.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "UIUC", 
        "name": "Vincent Bindschaedler"
      }, 
      {
        "affiliation": "CornellTech", 
        "name": "Reza Shokri"
      }, 
      {
        "affiliation": "UIUC", 
        "name": "Carl Gunter"
      }
    ], 
    "type": "research", 
    "id": "research499"
  }, 
  "research1402": {
    "title": "Price-Optimal Querying with Data APIs", 
    "abstract": "Data is increasingly being purchased online in data markets and REST APIs have emerged as a favored method to acquire such data. Typically, sellers charge buyers based on how much data they purchase. In many scenarios, buyers need to make repeated calls to the seller\u00e2\u0080\u0099s API. The challenge is then for buyers to keep track of the data they purchase and avoid purchasing the same data twice. In this paper, we propose lightweight modifications to data APIs to achieve optimal history-aware pricing so that buyers are only charged once for data that they have purchased and that has not been updated. The key idea behind our approach is the notion of refunds: buyers buy data as needed but have the ability to ask for refunds of data that they had already purchased before. We show that our techniques can provide significant data cost savings while reducing overheads by two orders of magnitude as compared to the state-of-the-art competing approaches.   ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "", 
        "name": "Prasang Upadhyaya"
      }, 
      {
        "affiliation": "University of Washington", 
        "name": "Magdalena  Balazinska"
      }, 
      {
        "affiliation": "U. Washington", 
        "name": "Dan Suciu"
      }
    ], 
    "type": "research", 
    "id": "research1402"
  }, 
  "research1382": {
    "title": "Privacy-preserving Network Provenance", 
    "abstract": "Network accountability, forensic analysis, and failure diagnosis are becoming increasingly important for network management and security. {\\em Network provenance} significantly aids network administrators in these tasks by explaining system behavior and revealing the dependencies between system states. Although resourceful, network provenance can sometimes be too rich, revealing potentially sensitive information that was involved in system execution. In this paper, we propose a cryptographic approach to preserve the confidentiality of provenance (sub)graphs while allowing users to query and access the parts of the graph for which they are authorized. Our proposed solution is a novel application of searchable symmetric encryption (SSE), first introduced by Curtmola et al. (CCS 2006).  Our SSE-enabled provenance system allows a node to enforce access control policies over its provenance data even after the data has been shipped to remote nodes (e.g., for optimization purposes). We present a prototype of our design and demonstrate its practicality, scalability, and efficiency for both provenance maintenance and querying.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Georgetown University", 
        "name": "Yuankai Zhang"
      }, 
      {
        "affiliation": "", 
        "name": "Adam O'Neill"
      }, 
      {
        "affiliation": "", 
        "name": "Micah Sherr"
      }, 
      {
        "affiliation": "Georgetown University", 
        "name": "Wenchao Zhou"
      }
    ], 
    "type": "research", 
    "id": "research1382"
  }, 
  "research1243": {
    "title": "Probabilistic Database Summarization for Interactive Data Exploration", 
    "abstract": "We present a probabilistic approach to generate a small, query-able summary of a dataset for interactive data exploration. Departing from traditional summarization techniques, we use the Principle of Maximum Entropy to generate a probabilistic representation of the data that can be used to give approximate query answers.  We develop the theoretical framework and formulation of our probabilistic representation and show how to use it to answer queries. We then present solving techniques and give three critical optimizations to improve preprocessing time and query accuracy. Lastly, we experimentally evaluate our work using a 5 GB dataset of flights within the United States and a 210 GB dataset from an astronomy particle simulation. While our current work only supports linear queries, we show that our technique can successfully answer queries faster than sampling while introducing, on average, no more error than sampling and can better distinguish between rare and nonexistent values.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Washington", 
        "name": "Laurel Orr"
      }, 
      {
        "affiliation": "U. Washington", 
        "name": "Dan Suciu"
      }, 
      {
        "affiliation": "University of Washington", 
        "name": "Magdalena  Balazinska"
      }
    ], 
    "type": "research", 
    "id": "research1243"
  }, 
  "industrial975": {
    "title": "Probabilistic Demand Forecasting at Scale", 
    "abstract": "We present a platform built on large-scale, data-centric machine learning (ML) approaches, whose particular focus is demand forecasting in retail. At its core, this platform enables the training and application of probabilistic demand forecasting models, and provides convenient abstractions and support functionality for forecasting problems. The platform comprises of a complex end-to-end machine learning system built on Apache Spark, which includes data preprocessing, feature engineering, distributed learning, as well as evaluation, experimentation and ensembling. Furthermore, it meets the demands of a production system and scales to large catalogues containing millions of items.  We describe the challenges of building such a platform and discuss our design decisions. We detail aspects on several levels of the system, such as a set of general distributed learning schemes, our machinery for ensembling predictions, and a high-level dataflow abstraction for modeling complex ML pipelines. To the best of our knowledge, we are not aware of prior work on real-world demand forecasting systems which rivals our approach in terms of scalability.", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "Amazon", 
        "name": "Joos-Hendrik Boese"
      }, 
      {
        "affiliation": "Amazon", 
        "name": "Valentin Flunkert"
      }, 
      {
        "affiliation": "Amazon", 
        "name": "Jan Gasthaus"
      }, 
      {
        "affiliation": "Amazon", 
        "name": "Tim Januschowski"
      }, 
      {
        "affiliation": "Amazon", 
        "name": "Dustin Lange"
      }, 
      {
        "affiliation": "Amazon", 
        "name": "David Salinas"
      }, 
      {
        "affiliation": "Amazon", 
        "name": "Sebastian Schelter"
      }, 
      {
        "affiliation": "Amazon", 
        "name": "Matthias Seeger"
      }, 
      {
        "affiliation": "Amazon", 
        "name": "Bernie Wang"
      }
    ], 
    "type": "industrial", 
    "id": "industrial975"
  }, 
  "research511": {
    "title": "Provenance for Natural Language Queries", 
    "abstract": "Multiple lines of research have developed Natural Language (NL) interfaces for formulating database queries. We build upon this work, but focus on presenting a highly detailed form of the answers in NL. The answers that we present are importantly based on the  provenance of tuples in the query result, detailing not only the results but also their explanations. We develop a novel method for transforming provenance information to NL, by leveraging the original NL query structure. Furthermore, since provenance information is typically large and complex, we present two solutions for its effective presentation as NL text: one that is based on provenance factorization, with novel desiderata relevant to the NL case, and one that is based on summarization. We have implemented our solution in an end-to-end system supporting questions, answers and provenance, all expressed in NL. Our experiments, including a user study, indicate the quality of our solution and its scalability. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Tel Aviv University", 
        "name": "Daniel Deutch"
      }, 
      {
        "affiliation": "Tel Aviv University", 
        "name": "Nave Frost"
      }, 
      {
        "affiliation": "Tel Aviv University", 
        "name": "Amir Gilad"
      }
    ], 
    "type": "research", 
    "id": "research511"
  }, 
  "research1366": {
    "title": "Pyramid Sketch: a Sketch Framework for Frequency Estimation of Data Streams", 
    "abstract": "Sketch is a probabilistic data structure, and is used to store and query the frequency of any item in a given multiset. Due to its high memory efficiency, it has been applied to various fields in computer science, such as stream database, network traffic measurement, etc. The key metrics of sketches for data streams are accuracy, speed, and memory usage. There are various sketches in the literature, but they cannot achieve both high accuracy and high speed using limited memory, especially for skewed datasets. To address this issue, we propose a sketch framework, the Pyramid sketch, which can significantly improve accuracy as well as update and query speed. To verify the effectiveness and efficiency of our framework, we applied our framework to four typical sketches. Extensive experimental results show that the accuracy is improved up to 3.50 times, while the speed is improved up to 2.10 times. We have released our source code at Github.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Peking University", 
        "name": "Tong Yang"
      }, 
      {
        "affiliation": "Peking University", 
        "name": "Yang Zhou"
      }, 
      {
        "affiliation": "Peking University", 
        "name": "Hao Jin"
      }, 
      {
        "affiliation": "Department of Computer & Information of Science & Engineering", 
        "name": "Shigang Chen"
      }, 
      {
        "affiliation": "Peking University", 
        "name": "Xiaoming Li"
      }
    ], 
    "type": "research", 
    "id": "research1366"
  }, 
  "industrial915": {
    "title": "Quaestor: Query Web Caching for Database-as-a-Service Providers", 
    "abstract": "Today, web performance is primarily governed by round-trip latencies between end devices and cloud services. To improve performance, services need to minimize the delay of accessing data. In this paper, we propose a novel approach to low latency that relies on existing content delivery and web caching infrastructure. The main idea is to enable application-independent caching of query results and records with tunable consistency guarantees, in particular bounded staleness. Quaestor (Query Store) employs two key concepts to incorporate both expiration-based and invalidation-based web caches: (1) an Expiring Bloom Filter data structure to indicate potentially stale data, and (2) statistically derived cache expiration times to maximize cache hit rates. Through a distributed query invalidation pipeline, changes to cached query results are detected in real-time. The proposed caching algorithms offer a new means for data-centric cloud services to trade latency against staleness bounds, e.g. in a database-as-a-service. Quaestor is the core technology of the backend-as-a-service platform Baqend, a cloud service for low-latency websites. We provide empirical evidence for Quaestor's scalability and performance through both simulation and experiments. The results indicate that for read-heavy workloads, up to tenfold speed-ups can be achieved through Quaestor's caching.", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "Baqend GmbH", 
        "name": "Felix Gessert"
      }, 
      {
        "affiliation": "University of Cambridge", 
        "name": "Michael Schaarschmidt"
      }, 
      {
        "affiliation": "Universit\u00c3\u00a4t Hamburg", 
        "name": "Wolfram Wingerath"
      }, 
      {
        "affiliation": "Baqend", 
        "name": "Erik Wiit"
      }, 
      {
        "affiliation": "University of Cambridge", 
        "name": "Eiko Yoneki"
      }, 
      {
        "affiliation": "University of Hamburg", 
        "name": "Norbert Ritter"
      }
    ], 
    "type": "industrial", 
    "id": "industrial915"
  }, 
  "research1310": {
    "title": "Query Optimization for Dynamic Imputation", 
    "abstract": "Missing values are common in data analysis and present a usability challenge. Users are forced to pick between removing tuples with missing values or creating a cleaned version of their data by applying a relatively expensive imputation strategy. Our system, ImputeDB, incorporates imputation into a cost-based query optimizer, performing necessary imputations on-the-fly for each query. This allows users to immediately explore their data, while the system picks the optimal placement of imputation operations. We evaluate this approach on three real-world survey-based datasets. Our experiments show that our query plans execute between 10 and 140 times faster than first imputing the base tables. Furthermore, we show that the query results from on-the-fly imputation differ from the traditional base-table imputation approach by 0\u00e2\u0080\u009320%. Finally, we show that while dropping tuples with missing values that fail query constraints discards 6\u00e2\u0080\u009378% of the data, on-the-fly imputation loses only 0\u00e2\u0080\u009321%.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "MIT", 
        "name": "Jose Cambronero Sanchez"
      }, 
      {
        "affiliation": "MIT", 
        "name": "John Feser"
      }, 
      {
        "affiliation": "MIT", 
        "name": "Micah Smith"
      }, 
      {
        "affiliation": "MIT", 
        "name": "Samuel Madden"
      }
    ], 
    "type": "research", 
    "id": "research1310"
  }, 
  "industrial887": {
    "title": "Query-able Kafka: An agile data analytics pipeline for mobile wireless networks", 
    "abstract": "Due to their promise of delivering real-time network insights, today\u00e2\u0080\u0099s streaming analytics platforms are increasingly be- ing used in the communications networks where the impact of the insights go beyond sentiment and trend analysis to include real-time detection of security attacks and predic- tion of network state (i.e., is the network transitioning to- wards an outage). Current streaming analytics platforms operate under the assumption that arriving traffic is to the order of kilobytes produced at very high frequencies. How- ever, communications networks, especially the telecommu- nication networks, challenge this assumption because some of the arriving traffic in these networks is to the order of gigabytes, but produced at medium to low velocities. Fur- thermore, these large datasets may need to be ingested in their entirety to render network insights in real-time. Our interest is to subject today\u00e2\u0080\u0099s streaming analytics platforms \u00e2\u0080\u0094 constructed from state-of-the art software components (Kafka, Spark, HDFS, ElasticSearch) \u00e2\u0080\u0094 to traffic densities observed in such communications networks. We find that filtering on such large datasets is best done in a common upstream point instead of being pushed to, and repeated, in downstream components. To demonstrate the advantages of such an approach, we modify Apache Kafka to perform limited native data transformation and filtering, relieving the downstream Spark application from doing this. Our approach outperforms four prevalent analytics pipeline ar- chitectures with negligible overhead compared to standard Kafka. (Our modifications to Apache Kafka are publicly available at https://github.com/Esquive/queryable-kafka.git)", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "University of Luxembourg", 
        "name": "Eric Falk"
      }, 
      {
        "affiliation": "Bell Labs", 
        "name": "Vijay Gurbani"
      }, 
      {
        "affiliation": "University of Luxembourg", 
        "name": "Radu State"
      }
    ], 
    "type": "industrial", 
    "id": "industrial887"
  }, 
  "research1396": {
    "title": "Quill: Efficient, Transferable, and Rich Analytics at Scale", 
    "abstract": "This paper introduces Quill (stands for a quadrillion tuples per day), a library and distributed platform for relational and temporal analytics over large datasets in the cloud. Quill exposes a new abstraction for parallel datasets and computation, called ShardedStreamable. This abstraction provides the ability to express efficient distributed physical query plans that are transferable, i.e., movable from offline to real-time and vice versa. ShardedStreamable decouples incremental query logic specification, a small but rich set of data movement operations, and keying; this allows Quill to express a broad space of plans with complex querying functionality, while leveraging existing temporal libraries such as Trill. Quill's layered architecture provides a careful separation of responsibilities with independently useful components, while retaining high performance. We built Quill for the cloud, with a master-less design where a language-integrated client library directly communicates and coordinates with cloud workers using off-the-shelf distributed cloud components such as queues. Experiments on up to 400 cloud machines, and on datasets up to 1TB, find Quill to incur low overheads and outperform SparkSQL by up to orders-of-magnitude for temporal and 6X for relational queries, while supporting a rich space of transferable, programmable, and expressive distributed physical query plans.  ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Microsoft Research", 
        "name": "Badrish Chandramouli"
      }, 
      {
        "affiliation": "MIT", 
        "name": "Raul Castro Fernandez"
      }, 
      {
        "affiliation": "Microsoft Research", 
        "name": "Jonathan Goldstein"
      }, 
      {
        "affiliation": "", 
        "name": "Ahmed Eldawy"
      }, 
      {
        "affiliation": "", 
        "name": "Abdul Quamar"
      }
    ], 
    "type": "research", 
    "id": "research1396"
  }, 
  "research741": {
    "title": "READS: A Random Walk Approach for Efficient and Accurate Dynamic SimRank", 
    "abstract": "Similarity among entities in graphs plays a key role in data analysis and mining. SimRank is a widely used and popular measurement to evaluate the similarity among the vertices. In real-life applications, graphs do not only grow in size, requiring fast and precise SimRank computation for large graphs, but also change and evolve continuously over time, demanding an efficient maintenance process to handle dynamic updates. In this paper, we propose a random walk based indexing scheme to compute SimRank efficiently and accurately over large dynamic graphs. We show that our algorithm outperforms the state-of-the-art static and dynamic SimRank algorithms. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "HKUST", 
        "name": "Minhao Jiang"
      }, 
      {
        "affiliation": "The Chinese University of Hong Kong", 
        "name": "Ada Wai Chee Fu"
      }, 
      {
        "affiliation": "The Hong Kong University of Science and Technology", 
        "name": "Raymond Chi-Wing Wong"
      }, 
      {
        "affiliation": "Simon Fraser University", 
        "name": "Ke Wang"
      }
    ], 
    "type": "research", 
    "id": "research741"
  }, 
  "research611": {
    "title": "Real-Time Influence Maximization on Dynamic Social Streams", 
    "abstract": "Influence maximization (IM), which selects a set of $k$ users (called seeds) to maximize the influence spread over a social network, is a fundamental problem in a wide range of applications such as viral marketing and network monitoring. Existing IM solutions fail to consider the highly dynamic nature of social influence, which results in either poor seed qualities or long processing time when the network evolves. To address this problem, we define a novel IM query named Stream Influence Maximization (SIM) on social streams. Technically, SIM adopts the sliding window model and maintains a set of $k$ seeds with the largest influence value over the most recent social actions. Next, we propose the Influential Checkpoints (IC) framework to facilitate continuous SIM query processing. The IC framework creates a checkpoint for each window shift and ensures an $\\varepsilon$-approximate solution. To improve its efficiency, we further devise a Sparse Influential Checkpoints (SIC) framework which selectively keeps $O(\\frac{\\log{N}}{\\beta})$ checkpoints for a sliding window of size $N$ and maintains an $\\frac{\\varepsilon(1-\\beta)}{2}$ approximate solution. Experimental results on both real-world and synthetic datasets confirm the effectiveness and efficiency of our proposed frameworks against the state-of-the-art IM approaches.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "National University of Singapore", 
        "name": "Yanhao Wang"
      }, 
      {
        "affiliation": "National University of Singapore", 
        "name": "Qi Fan"
      }, 
      {
        "affiliation": "National University of Singapore", 
        "name": "Yuchen Li"
      }, 
      {
        "affiliation": "NUS", 
        "name": "Kian-Lee Tan"
      }
    ], 
    "type": "research", 
    "id": "research611"
  }, 
  "research1370": {
    "title": "Reconciling Skyline and Ranking Queries", 
    "abstract": "Traditionally, skyline and ranking queries have been treated separately as alternative ways of discovering interesting data in potentially large datasets. While ranking queries adopt a specific scoring function to rank tuples, skyline queries return the set of non-dominated tuples and are independent of attribute scales and scoring functions. Ranking queries are thus less general but usually cheaper to compute and widely used in data management systems. We propose a framework to seamlessly integrate these two approaches by introducing the notion of restricted skyline queries (R-skylines). We propose R-skyline operators that generalize both skyline and ranking queries by applying the notion of dominance to a set of scoring functions of interest. Such sets can be characterized, e.g., by imposing constraints on the function\u00e2\u0080\u0099s parameters, such as the weights in a linear scoring function. We discuss the formal properties of these new operators, show how to implement them efficiently, and evaluate them on both synthetic and real datasets.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Universit\u00c3\u00a0 di Bologna", 
        "name": "Paolo Ciaccia"
      }, 
      {
        "affiliation": "Politecnico di Milano", 
        "name": "Davide Martinenghi"
      }
    ], 
    "type": "research", 
    "id": "research1370"
  }, 
  "research77": {
    "title": "Remember Where You Came From: On The Second-Order Random Walk Based Proximity Measures", 
    "abstract": "Measuring the proximity between different nodes is a fundamental problem in graph analysis. Random walk based proximity measures have been shown to be effective and widely used. Most existing random walk measures are based on the first-order Markov model, i.e., they assume that the next step of the random surfer only depends on the current node. However, this assumption neither holds in many real-life applications nor captures the clustering structure in the graph. To address the limitation of the existing first-order measures, in this paper, we study the second-order random walk measures, which take the previously visited node into consideration. While the existing first-order measures are built on node-to-node transition probabilities, in the second-order random walk, we need to consider the edge-to-edge transition probabilities. Using incidence matrices, we develop simple and elegant matrix representations for the second-order proximity measures. A desirable property of the developed measures is that they degenerate to their original first-order forms when the effect of the previous step is zero. We further develop Monte Carlo methods to efficiently compute the second-order measures and provide theoretical performance guarantees. Experimental results show that in a variety of applications, the second-order measures can dramatically improve the performance compared to their first-order counterparts.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Case Western Reserve University", 
        "name": "Yubao Wu"
      }, 
      {
        "affiliation": "Case Western Reserve University", 
        "name": "Yuchen Bian"
      }, 
      {
        "affiliation": "Case Western Reserve University", 
        "name": "Xiang Zhang"
      }
    ], 
    "type": "research", 
    "id": "research77"
  }, 
  "research332": {
    "title": "Resisting Tag Spam by Leveraging Implicit User Behaviors", 
    "abstract": "Tagging systems are vulnerable to tag spam attacks. However, defending against tag spam has been challenging in practice, since adversaries can easily launch spam attacks in various ways and scales. To deeply understand users' tagging behaviors and explore more effective defense, this paper first conducts measurement experiments on public datasets of two representative tagging systems: Del.icio.us and CiteULike. Our key finding is that a significant fraction of correct tag-resource annotations are contributed by a small number of implicit similarity cliques, where users annotate common resources with similar tags. Guided by the above finding, we propose a new service, called Spam-Resistance-as-a-Service (or SRaaS), to effectively defend against heterogeneous tag spam attacks even at very large scales. At the heart of SRaaS is a novel reputation assessment protocol, whose design leverages the implicit similarity cliques coupled with the social networks inherent to typical tagging systems. With such a design, SRaaS manages to offer provable guarantees on diminishing the influence of tag spam attacks. We build an SRaaS prototype and evaluate it using a large-scale spam-oriented research dataset (which is much more polluted by tag spam than Del.icio.us and CiteULike datasets). Our evaluational results demonstrate that SRaaS outperforms existing tag spam defenses deployed in real-world systems, while introducing low overhead.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Yale University", 
        "name": "Ennan Zhai"
      }, 
      {
        "affiliation": "Tsinghua University", 
        "name": "Zhenhua Li"
      }, 
      {
        "affiliation": "ICT", 
        "name": "Zhenyu Li"
      }, 
      {
        "affiliation": "Shanghai Jiaotong University", 
        "name": "Fan Wu"
      }, 
      {
        "affiliation": "Shanghai Jiaotong University", 
        "name": "Guihai Chen"
      }
    ], 
    "type": "research", 
    "id": "research332"
  }, 
  "industrial1085": {
    "title": "Resumable Online Index Rebuild in SQL Server", 
    "abstract": "Azure SQL Database and the upcoming release of SQL Server enhance Online Index Rebuild to provide fault-tolerance and allow index rebuild operations to resume after a system failure or a user-initiated pause. SQL Server is the first commercial DBMS to support pause and resume functionality for index rebuilds. This is achieved by splitting the operation into incremental units of work and persisting the required state so that it can be resumed later with minimal loss of progress. At the same time, the proposed technology minimizes the log space required for the operation to succeed, making it possible to rebuild large indexes using only a small, constant amount of log space. These capabilities are critical to guarantee the reliability of these operations in an environment where a) the database sizes are increasing at a much faster pace compared to the available hardware, b) system failures are frequent in Cloud architectures using commodity hardware, c) software upgrades and other maintenance tasks are automatically handled by the Cloud platforms, introducing further unexpected failures for the users and d) most modern applications need to be available 24/7 and have very tight maintenance windows. This paper describes the design of \u00e2\u0080\u009cResumable Online Index Rebuild\u00e2\u0080\u009d and discusses how this technology can be extended to cover more schema management operations in the future.", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "Microsoft", 
        "name": "Panagiotis Antonopoulos"
      }, 
      {
        "affiliation": "Microsoft", 
        "name": "Hanuma Kodavalla"
      }, 
      {
        "affiliation": "Microsoft", 
        "name": "Alex Tran"
      }, 
      {
        "affiliation": "Microsoft", 
        "name": "Nitish Upreti"
      }, 
      {
        "affiliation": "Microsoft", 
        "name": "Chaitali Shah"
      }, 
      {
        "affiliation": "Microsoft", 
        "name": "Mirek Sztajno"
      }
    ], 
    "type": "industrial", 
    "id": "industrial1085"
  }, 
  "research1291": {
    "title": "Revenue Maximization in Incentivized Social Advertising", 
    "abstract": "Incentivized social advertising, an emerging marketing model, provides monetization opportunities not only to the owners of the social networking  platforms but also to their influential users by offering a ``cut'' on the advertising revenue. More in details, we consider a social network (the host) that sells ad-engagements to advertisers by inserting their ads, in the form of promoted posts, into the feeds of carefully selected ``initial endorsers or seed users: these users receive monetary incentives in exchange for their endorsements. The endorsements help propagate the ads to the feeds of their followers. Whenever any user of the platform engages with an ad", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "ISI Foundation", 
        "name": "Cigdem Aslay"
      }, 
      {
        "affiliation": "ISI Foundation", 
        "name": "Francesco  Bonchi"
      }, 
      {
        "affiliation": "UBC", 
        "name": "Laks Lakshmanan"
      }, 
      {
        "affiliation": "LinkedIn Corporation", 
        "name": "Wei Lu"
      }
    ], 
    "type": "research", 
    "id": "research1291"
  }, 
  "research1331": {
    "title": "Reverse Engineering Aggregation Queries", 
    "abstract": "Query reverse engineering seeks to re-generate the SQL query that produced a given query output table from a given database. In this paper, we propose our solution for this problem for OLAP queries with group-by and aggregation. We develop a novel three-phase algorithm named REGAL for this problem. First, based on a lattice graph structure, we identify a set of group-by candidates for the desired query. Second, we apply a set of aggregation constraints that are derived from the properties of aggregate operators at both the table level and the group level to discover candidate combinations of group-by columns and aggregations that are consistent with the given query output table. Finally, we find a multidimensional filter, i.e., a conjunction of selection predicates over the base table attributes, that is needed to generate the exact query output table. We conduct an extensive experimental study over the TPC-H dataset to examine the effectiveness and efficiency of our proposal.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "SUTD", 
        "name": "Wei Chit  Tan"
      }, 
      {
        "affiliation": "SUTD", 
        "name": "Meihui Zhang"
      }, 
      {
        "affiliation": "Turn Inc.", 
        "name": "Hazem Elmeleegy"
      }, 
      {
        "affiliation": "AT&T", 
        "name": "Divesh Srivastava"
      }
    ], 
    "type": "research", 
    "id": "research1331"
  }, 
  "research1133": {
    "title": "Revisiting Reuse for Approximate Query Processing", 
    "abstract": "Visual data exploration tools allow users to rapidly explore datasets in order to quickly understand trends and gather insights. As dataset sizes continue to increase, new techniques will be necessary to maintain the interactivity guarantees that these tools require. Approximate query processing (AQP) attempts to tackle this problem and allows systems to return query results at ``human speed.'' However, existing techniques start to break down when applied to queries over rare subpopulations, since they fail to leverage the unique properties of interactive data exploration.  We therefore present a novel AQP formulation that can provide low-error approximate results at interactive speeds, even for queries over rare subpopulations. In particular, our formulation treats query results as random variables in order to better take advantage of the ample opportunities for result reuse inherent in interactive data exploration. As part of our approach, we apply a variety of optimization techniques that are based on probability theory, including new query rewrite rules and index structures. We implemented these techniques in a prototype system and show that they can achieve interactive latencies where alternative approaches cannot.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Brown University", 
        "name": "Alex Galakatos"
      }, 
      {
        "affiliation": "Brown University", 
        "name": "Andrew Crotty"
      }, 
      {
        "affiliation": "Brown University", 
        "name": "Emanuel Zgraggen"
      }, 
      {
        "affiliation": "Brown University", 
        "name": "Tim Kraska"
      }, 
      {
        "affiliation": "Brown University", 
        "name": "Carsten Binnig"
      }
    ], 
    "type": "research", 
    "id": "research1133"
  }, 
  "research737": {
    "title": "Revisiting the Stop-and-Stare Algorithms for Influence Maximization [Experiments and Analyses]", 
    "abstract": "Influence maximization is a combinatorial optimization problem that finds important applications in viral marketing, feed recommendation, etc. Recent research has led to a number of scalable approximation algorithms for influence maximization, such as TIM^+ and IMM, and more recently, SSA and D-SSA. The goal of this paper is to conduct a rigorous theoretical and experimental analysis of SSA and D-SSA and compare them against the preceding algorithms. In doing so, we uncover inaccuracies in previously reported technical results on the accuracy and efficiency of SSA and D-SSA, which we set right. We also attempt to reproduce the original experiments on SSA and D-SSA, based on which we provide interesting empirical insights. Our evaluation confirms some results reported from the original experiments, but it also reveals anomalies in some other results and sheds light on the behavior of SSA and D-SSA in some important settings not considered previously. We also report on the performance of SSA-Fix, our modification to SSA in order to restore the approximation guarantee that was claimed for but not enjoyed by SSA. Overall, our study suggests that there exist opportunities for further scaling up influence maximization with approximation guarantees.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Nanyang Technological University", 
        "name": "Keke Huang"
      }, 
      {
        "affiliation": "Nanyang Technological University", 
        "name": "Sibo Wang"
      }, 
      {
        "affiliation": "University of British Columbia", 
        "name": "Glenn Bevilacqua"
      }, 
      {
        "affiliation": "Nanyang Technological University", 
        "name": "Xiaokui Xiao"
      }, 
      {
        "affiliation": "UBC", 
        "name": "Laks Lakshmanan"
      }
    ], 
    "type": "research", 
    "id": "research737"
  }, 
  "research1375": {
    "title": "Runtime Optimization of Join Location in Parallel Data Management Systems", 
    "abstract": "Applications running on parallel systems often need to join a streaming relation or a stored relation with data indexed in a parallel data storage system. Some applications also compute UDFs on the joined tuples. The join can be done at the data storage nodes, corresponding to reduce side joins, or by fetching data from the storage system, corresponding to map side join. Both may be suboptimal: reduce side joins may cause skew, while map side joins may lead to a lot of data being transferred and replicated.   In this paper, we present techniques to make runtime decisions between the two options on a per key basis, in order to improve the throughput of the join, accounting for UDF computation if any. Our techniques are based on an extended ski-rental algorithm and provide worst-case performance guarantees with respect to the optimal point in the space considered by us. Our techniques use load balancing taking into account the CPU, network and I/O costs as well as the load on clients and servers. We have implemented our techniques on Hadoop, Spark and the Muppet stream processing engine. Our experiments show that our optimization techniques provide a significant improvement in throughput over existing techniques.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "IIT Bombay", 
        "name": "Bikash Chandra"
      }, 
      {
        "affiliation": "IIT Bombay", 
        "name": "S. Sudarshan"
      }
    ], 
    "type": "research", 
    "id": "research1375"
  }, 
  "industrial1091": {
    "title": "SAP HANA Adoption of Non-Volatile Memory", 
    "abstract": "Non-Volatile RAM (NVRAM) is a novel class of hardware technology which is an interesting blend of two storage paradigms: byte-addressable DRAM and block-addressable storage (e.g. HDD/SSD). Most of the existing enterprise relational data management systems such as SAP HANA have their internal architecture based on the inherent assumption that memory is volatile and base their persistence on explicit handling of block-oriented storage devices. In this paper, we present the early adoption of Non-Volatile Memory within the SAP HANA Database, from the architectural and technical angles. We discuss our architectural choices, dive deeper into a few challenges of the NVRAM integration and their solutions, and share our experimental results. As we present our solutions for the NVRAM integration, we also give, as a basis, a detailed description of the relevant HANA internals. ", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "SAP", 
        "name": "Mihnea Andrei"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Christian Lemke"
      }, 
      {
        "affiliation": "SAP", 
        "name": "G\u00c3\u00bcnter Radestock"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Robert Schulze"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Carsten Thiel"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Rolando Blanco"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Akanksha Meghlan"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Muhammad Sharique"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Sebastian Seifert"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Surendra Vishnoi"
      }, 
      {
        "affiliation": "SAP SE", 
        "name": "Daniel Booss"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Thomas Peh"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Ivan Schreter"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Werner Thesing"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Mehul Wagle"
      }, 
      {
        "affiliation": "Intel Deutschland GmbH", 
        "name": "Thomas Willhalm"
      }
    ], 
    "type": "industrial", 
    "id": "industrial1091"
  }, 
  "research571": {
    "title": "SMCQL: Secure Querying for Federated Databases", 
    "abstract": "People and machines are collecting data at an unprecedented rate. Despite this newfound abundance of data, progress has been slow in sharing it for open science, business, and other data-intensive endeavors. Many such efforts are stymied by privacy concerns and regulatory compliance issues. For example, many hospitals are interested in pooling their medical records for research, but none may disclose arbitrary patient records to researchers or other healthcare providers. In this context we propose the Private Data Network (PDN), a federated database for querying over the collective data of mutually distrustful parties. In a PDN, each member database does not reveal its tuples to its peers nor to the query writer. Instead, the user submits a query to an honest broker that plans and coordinates its execution over multiple private databases using secure multiparty computation (SMC). Here, each database\u00e2\u0080\u0099s query execution is oblivious, and its program counters and memory traces are agnostic to the inputs of others.  We introduce a framework for executing PDN queries named SMCQL. This system translates SQL statements into SMC primitives to compute query results over the union of its source databases without revealing sensitive information about individual tuples to peer data providers or the honest broker. Only the honest broker and the querier receive the results of a PDN query. For fast secure query evaluation, we explore a heuristics-driven optimizer that minimizes the PDN\u00e2\u0080\u0099s use of secure computation and partitions its query evaluation into scalable slices.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "", 
        "name": "Johes Bater"
      }, 
      {
        "affiliation": "Northwestern University", 
        "name": "Greg Elliott"
      }, 
      {
        "affiliation": "Northwestern University", 
        "name": "Craig Eggen"
      }, 
      {
        "affiliation": "Northwestern University", 
        "name": "Satyender Goel"
      }, 
      {
        "affiliation": "Northwestern University", 
        "name": "Abel Kho"
      }, 
      {
        "affiliation": "", 
        "name": "Jennie Rogers"
      }
    ], 
    "type": "research", 
    "id": "research571"
  }, 
  "industrial875": {
    "title": "Samza: Stateful Scalable Stream Processing at LinkedIn", 
    "abstract": "Distributed stream processing systems need to support stateful processing, recover quickly from failures to resume such processing, and reprocess an entire data stream quickly. We present Apache Samza, a distributed system for stateful and fault-tolerant stream processing. Samza utilizes a partitioned local state along with a low-overhead background changelog mechanism, allowing it to scale to massive state sizes (hundreds of TB) per application. Recovery from failures is sped up by re-scheduling based on Host Affinity. In addition to processing infinite streams of events, Samza supports processing a finite dataset as a stream, from either a streaming source (e.g., Kafka), a database snapshot (e.g., Databus), or a file system (e.g. HDFS), without having to change the application code (unlike the popular Lambda-based architectures which necessitate maintenance of separate code bases for batch and stream path processing).  Samza is currently in use at LinkedIn by hundreds of production applications with more than 10,000 containers. Samza is an open-source Apache project adopted by many top-tier companies (e.g., LinkedIn, Uber, Netflix, TripAdvisor, etc.). Our experiments show that Samza: a) handles state efficiently, improving latency and throughput by more than 100x compared to using a remote storage; b) provides recovery time independent of state size; c) scales performance linearly with number of containers; and d) supports reprocessing of the data stream quickly and with minimal interference on real-time traffic.  ", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "University of Illinois at Urba", 
        "name": "Shadi A Noghabi"
      }, 
      {
        "affiliation": "LinkedIn", 
        "name": "Kartik Paramasivam"
      }, 
      {
        "affiliation": "LinkedIn", 
        "name": "Yi Pan"
      }, 
      {
        "affiliation": "LinkedIn", 
        "name": "Navina Ramesh"
      }, 
      {
        "affiliation": "LinkedIn", 
        "name": "Jon Bringhurst"
      }, 
      {
        "affiliation": "UIUC", 
        "name": "Indranil Gupta"
      }, 
      {
        "affiliation": "University of Illinois at Urbana-Champaign", 
        "name": "Roy Campbell"
      }
    ], 
    "type": "industrial", 
    "id": "industrial875"
  }, 
  "research223": {
    "title": "Sapprox: Enabling Efficient and Accurate Approximations on Sub-datasets with Distribution-aware Online Sampling", 
    "abstract": "In this paper, we aim to enable both efficient and accurate approximations on arbitrary sub-dataset of a large dataset. Due to the prohibitive storage overhead of caching offline samples for each sub-dataset, existing offline sample based systems provide high accuracy results for only a limited number of sub-datasets, such as the popular ones. On the other hand, current online sample based approximation systems, which generate samples at runtime, do not take into account the uneven storage distribution of a sub-dataset. They work well for uniform distribution of a sub-dataset while suffer low sampling efficiency and poor estimation accuracy on unevenly distributed sub-datasets.    To address the problem, we develop a distribution aware method called Sapprox. Our idea is to collect the occurrences of a sub-dataset at each logical partition of a dataset (storage distribution) in the distributed system, and make good use of such information to facilitate online sampling. There are three thrusts in Sapprox. First, we develop a probabilistic map to reduce the exponential number of recorded sub-datasets to a linear one. Second, we apply the cluster sampling with unequal probability theory to implement a distribution-aware sampling method for efficient online sub-dataset sampling. Third, we quantitatively derive the optimal sampling unit size in a distributed file system by associating it with approximation costs and accuracy. %Finally, we leverage the sampling theory to compute error bounds for approximations in MapReduce-like systems. We have implemented Sapprox into Hadoop ecosystem as an example system and open sourced it on GitHub. Our comprehensive experimental results show that Sapprox can achieve a speedup by up to 20x over the precise execution.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of central florida", 
        "name": "Xuhong Zhang"
      }, 
      {
        "affiliation": "university of central florida", 
        "name": "Jun Wang"
      }, 
      {
        "affiliation": "university of central florida", 
        "name": "Jiangling Yin"
      }, 
      {
        "affiliation": "University of Central Florida", 
        "name": "Xunchao Chen"
      }, 
      {
        "affiliation": "Zhejiang University", 
        "name": "Shouling Ji"
      }, 
      {
        "affiliation": "University of Central Florida", 
        "name": "Ruijun Wang"
      }, 
      {
        "affiliation": "University of Central Florida", 
        "name": "Dan Huang"
      }
    ], 
    "type": "research", 
    "id": "research223"
  }, 
  "research845": {
    "title": "Scalable Asynchronous Gradient Descent Optimization for Out-of-Core Models", 
    "abstract": "Existing data analytics systems have approached predictive model training exclusively from a data-parallel perspective. Data examples are partitioned to multiple workers and training is executed concurrently over different partitions, under various synchronization policies that emphasize speedup or convergence. Since models with millions and even billions of features become increasingly common nowadays, model management becomes an equally important task for effective training. In this paper, we present a general framework for parallelizing stochastic optimization algorithms over massive models that cannot fit in memory. We extend the lock-free HOGWILD!-family of algorithms to disk-resident models by vertically partitioning the model offline and asynchronously updating the resulting partitions online. Unlike HOGWILD!, concurrent requests to the common model are minimized by a preemptive push-based sharing mechanism that reduces the number of disk accesses. Experimental results on real and synthetic datasets show that the proposed framework achieves improved convergence over HOGWILD! and is the only solution scalable to massive models.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "UC Merced", 
        "name": "Chengjie Qin"
      }, 
      {
        "affiliation": "UC Merced", 
        "name": "Martin Torres"
      }, 
      {
        "affiliation": "University of California", 
        "name": "Florin Rusu"
      }
    ], 
    "type": "research", 
    "id": "research845"
  }, 
  "research330": {
    "title": "Scalable Distributed Subgraph Enumeration", 
    "abstract": "Subgraph enumeration aims to find all the subgraphs of a large data graph that are isomorphic to a given pattern graph. As the subgraph isomorphism operation is computationally intensive, researchers have recently focused on solving this problem in distributed environments, such as MapReduce and Pregel. Among them, the state-of-the-art algorithm, TwinTwigJoin, is proven to be instance optimal based on a left-deep-join framework. However, it is still not scalable to large graphs because of the constraints in the left-deep-join framework and that each decomposed component (join unit) must be a star. In this paper, we propose SEED - a scalable subgraph enumeration approach in the distributed environment. Compared to TwinTwigJoin, SEED returns optimal solution in a generalized join framework without the constraints in TwinTwigJoin. We use both star and clique as the join units, and design an effective distributed graph storage mechanism to support such an extension. We develop a comprehensive cost model, that evaluates the number of matches of any given pattern graph by considering power-law degree distribution in the data graph. We then generalize the left-deep-join framework and develop a dynamic-programming algorithm to compute an optimal bushy join plan. We also consider overlaps among the join units. Finally, we propose clique compression to further improve the algorithm by reducing the number of the intermediate results.  Extensive performance studies are conducted on several real graphs, one containing billions of edges. The results demonstrate that our algorithm outperforms all other state-of-the-art algorithms by more than one order of magnitude.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "CSE", 
        "name": "Longbin Lai"
      }, 
      {
        "affiliation": "QCIS", 
        "name": "Lu Qin"
      }, 
      {
        "affiliation": "CSE", 
        "name": "Xuemin Lin"
      }, 
      {
        "affiliation": "QCIS", 
        "name": "Ying Zhang"
      }, 
      {
        "affiliation": "CSE", 
        "name": "Lijun Chang"
      }, 
      {
        "affiliation": "", 
        "name": "Shiyu Yang"
      }
    ], 
    "type": "research", 
    "id": "research330"
  }, 
  "research1362": {
    "title": "Scalable and robust set similarity join", 
    "abstract": "Set similarity join is a fundamental and well-studied database operator. It is usually studied in the exact setting where the goal is to compute all pairs of sets that exceed a given level of similarity (measured e.g. as Jaccard similarity). But set similarity join is often used in settings where 100% recall may not be important --- indeed, where the exact set similarity join is itself only an approximation of the desired result set.  We present a new randomized algorithm for set similarity join that can achieve any desired recall up to 100%,  and show theoretically and empirically that it significantly outperforms state-of-the-art implementations of exact methods, and improves on existing approximate methods. Our experiments on benchmark data sets show that speedups of more than 1 order of magnitude is possible on real data, while keeping the recall above 90%. Our algorithm makes use of recent theoretical advances in high-dimensional sketching and indexing that we believe to be of wider relevance to the database community.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "IT University of Copenhagen", 
        "name": "Tobias Christiani"
      }, 
      {
        "affiliation": "IT University of Copenhagen", 
        "name": "Rasmus Pagh"
      }, 
      {
        "affiliation": "IT University of Copenhagen", 
        "name": "Johan Sivertsen"
      }
    ], 
    "type": "research", 
    "id": "research1362"
  }, 
  "research505": {
    "title": "Shrink - Prescribing Resiliency Solutions for Streaming", 
    "abstract": "Streaming query deployments make up a vital part of cloud oriented applications. They vary widely in their data, logic, and statefulness, and are typically executed in multi-tenant distributed environments with varying uptime SLAs. In order to achieve these SLAs, one of a number of proposed resiliency strategies is employed to protect against failure. This paper has introduced the first, comprehensive, cloud friendly comparison between different resiliency techniques for streaming queries. In this paper, we introduce models which capture the costs associated with different resiliency strategies, and through a series of experiments which implement and validate these models, show that (1) there is no single resiliency strategy which efficiently handles most streaming scenarios; (2) the optimization space is too complex for a person to employ a \u00e2\u0080\u009crules of thumb\u00e2\u0080\u009d approach; and (3) there exists a clear generalization of periodic checkpointing that is worth considering in many cases. Finally, the models presented in this paper can be adapted to fit a wide variety of resiliency strategies, and likely have important consequences for cloud services beyond those that are obviously streaming.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Microsoft Research", 
        "name": "Badrish Chandramouli"
      }, 
      {
        "affiliation": "Microsoft Research", 
        "name": "Jonathan Goldstein"
      }
    ], 
    "type": "research", 
    "id": "research505"
  }, 
  "research860": {
    "title": "SilkMoth: An Efficient Method for Finding Related Sets with Maximum Matching Constraints", 
    "abstract": "Determining if two sets are related \u00e2\u0080\u0093 that is, if they have similar values or if one set contains the other \u00e2\u0080\u0093 is an important problem with many applications in data cleaning, data integration, and information retrieval. For example, set relatedness can be a useful tool to discover whether columns from two different databases are join- able; if enough of the values in the columns match, it may make sense to join them. A common metric is to measure the related- ness of two sets by treating the elements as vertices of a bipartite graph and calculating the score of the maximum matching pairing between elements. Compared to other metrics which require ex- act matchings between elements, this metric uses a similarity function to compare elements between the two sets, making it robust to small dissimilarities in elements and more useful for real-world, dirty data. Unfortunately, the metric suffers from expensive computational cost, taking O(n3) time, where n is the number of elements in the sets, for each set-to-set comparison. Thus for applications that try to search for all pairings of related sets in a brute-force manner, the runtime becomes unacceptably large.  To address this challenge, we developed SilkMoth, a system capable of rapidly discovering related set pairs in collections of sets. Internally, SilkMoth creates a signature for each set, with the property that any other set which is related must match the sig- nature. SilkMoth then uses these signatures to prune the search space, so only sets that match the signatures are left as candidates. Finally, SilkMoth applies the maximum matching metric on remaining candidates to verify which of these candidates are truly related sets. An important property of SilkMoth is that it is guaranteed to output exactly the same related set pairings as the brute- force method, unlike approximate techniques. Thus, a contribution of this paper is the characterization of the space of signatures which enable this property. We show that selecting the optimal signature in this space is NP-complete, and based on insights from the characterization of the space, we propose two novel filters which help to prune the candidates further before verification. In addition, we introduce a simple optimization to the calculation of the maximum matching metric itself based on the triangle inequality. Compared to related approaches, SilkMoth is much more general, handling a larger space of similarity functions and relatedness metrics, and is an order of magnitude more efficient on real datasets. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "CSAIL MIT", 
        "name": "Dong Deng"
      }, 
      {
        "affiliation": "MIT", 
        "name": "Albert Kim"
      }, 
      {
        "affiliation": "MIT", 
        "name": "Samuel Madden"
      }, 
      {
        "affiliation": "MIT", 
        "name": "Michael Stonebraker"
      }
    ], 
    "type": "research", 
    "id": "research860"
  }, 
  "research431": {
    "title": "Skipping-oriented Partitioning for Columnar Layouts", 
    "abstract": "As data volumes continue to grow, modern database systems increasingly rely on data skipping mechanisms to improve performance by avoiding access to irrelevant data. Recent work proposed a fine-grained partitioning scheme that was shown to improve the opportunities for data skipping in row-oriented systems. Modern analytics and big data systems increasingly adopt columnar storage schemes, and in such systems, a row-based approach misses important opportunities for further improving data skipping. The flexibility of column-oriented organizations, however, comes with the additional cost of tuple reconstruction. In this paper, we develop Generalized Skipping-Oriented Partitioning (GSOP), a novel hybrid data skipping framework that takes into account these row-based and column-based tradeoffs. In contrast to previous column-oriented physical design work, GSOP considers the tradeoffs between horizontal data skipping and vertical partitioning jointly. Our experiments using two public benchmarks and a real-world workload show that GSOP can significantly reduce the amount of data scanned and improve end-to-end query response times over the state-of-the-art techniques.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "UC Berkeley", 
        "name": "Liwen Sun"
      }, 
      {
        "affiliation": "UC Berkeley", 
        "name": "Michael Franklin"
      }, 
      {
        "affiliation": "Simon Fraser University", 
        "name": "Jiannan Wang"
      }, 
      {
        "affiliation": "Columbia University", 
        "name": "Eugene Wu"
      }
    ], 
    "type": "research", 
    "id": "research431"
  }, 
  "research1330": {
    "title": "SkyGraph: Retrieving Regions of Interest using Skyline Subgraph Queries", 
    "abstract": "Several services today are annotated with \\emph{points of interest (PoIs)}. In this paper, we study the scenario where a user wants to identify the best \\emph{region of interest (RoI)} in a city. An RoI is a neighborhood that contains PoIs relevant to the user. The user expresses relevance through a set of keywords such as ``coffee shop'', ``park'', etc. Ideally, the RoI should be small enough in size so that the user can conveniently explore the PoIs. How does one balance the importance of size versus relevance? To a user exploring the RoI on foot, size is critical for convenience. On the other hand, to a user equipped with a vehicle, relevance is a more important factor. In this paper, we solve this dilemma through \\emph{skyline subgraph queries} on keyword-embedded road networks. Skyline subgraphs subsume the choice of optimization function for an RoI since the optimal RoI for any rational user is necessarily a part of the skyline set.  Our analysis reveals that the problem of computing the skyline set is NP-hard. We overcome this computational bottleneck by proposing a polynomial-time approximation algorithm called \\emph{SkyGraph}. To further expedite the running time, we develop an index structure called \\emph{Partner Index} that drastically prunes the search space and provides up to $3$ orders of magnitude speed-up on real road networks. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "IIT Madras", 
        "name": "Shiladitya Pande"
      }, 
      {
        "affiliation": "IIT Delhi", 
        "name": "Sayan Ranu"
      }, 
      {
        "affiliation": "IIT Kanpur", 
        "name": "Arnab Bhattacharya"
      }
    ], 
    "type": "research", 
    "id": "research1330"
  }, 
  "research932": {
    "title": "Slalom: Coasting Through Raw Data via Adaptive Partitioning and Indexing", 
    "abstract": "The constant flux of data and queries alike has been pushing the boundaries of data analysis systems. The increasing size of raw data files has made data loading an expensive operation that delays the data-to-insight time. Hence, recent in-situ query processing systems operate directly over raw data, alleviating the loading cost. Another trend is the increasing number of queries. Each query, typically, focuses on a constantly shifting, yet small, range of the dataset. Minimizing the cumulative query latency of such a workload requires the benefits of indexing in in-situ query processing.  In this paper we present an online partitioning and indexing scheme, along with a partitioning and indexing tuner tailored for in-situ querying engines. The proposed scheme improves query execution time by taking into account user query patterns, to (i) partition raw data files logically and  (ii) build for each partition lightweight partition-specific indexes.  We build Slalom, an in-situ query engine that follows the state of the art. Slalom accommodates workload shifts by updating partitioning and indexing decisions on-the-fly, based on query access patterns gathered by lightweight monitoring. Using this information Slalom builds non-obtrusive indexes. Due to its lightweight and adaptive nature, Slalom achieves efficient accesses to raw data with minimal memory consumption. Our experimentation with both micro-benchmarks and real-life workloads shows that Slalom outperforms state-of-the-art in-situ engines, and achieves comparable query response times with fully indexed DBMS, offering much lower cumulative query execution times for query workloads with increasing size and unpredictable access patterns.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "EPFL", 
        "name": "Matthaios Olma"
      }, 
      {
        "affiliation": "EPFL", 
        "name": "Manos Karpathiotakis"
      }, 
      {
        "affiliation": "Microsoft", 
        "name": "Ioannis Alagiannis"
      }, 
      {
        "affiliation": "Harvard University", 
        "name": "Manos Athanassoulis"
      }, 
      {
        "affiliation": "EPFL", 
        "name": "Anastasia Ailamaki"
      }
    ], 
    "type": "research", 
    "id": "research932"
  }, 
  "research1364": {
    "title": "Social Hash Partitioner:  A Scalable Distributed Hypergraph Partitioner", 
    "abstract": "We design and implement a distributed algorithm for balanced k-way hypergraph partitioning that minimizes fanout, a fundamental hypergraph quantity also known as the communication volume and (k-1)-cut metric, by optimizing a novel objective we call probabilistic fanout. This choice allows our simple local heuristic algorithm to achieve comparable solution quality to the best existing hypergraph partitioners.       Our algorithm is arbitrarily scalable due to a careful design that controls computational complexity, space complexity, and communication.  In practice, we commonly process hypergraphs with billions of vertices and hyperedges in a few hours. We explain how the algorithm's scalability, both in terms of hypergraph size and bucket count, is limited only by the number of machines available.  We perform an extensive comparison to existing distributed hypergraph partitioners and find that our approach is able to optimize hypergraphs roughly 100 times bigger on the same set of machines.      We call the resulting tool Social Hash Partitioner (SHP), and accompanying this paper, we open-source the most scalable version based on recursive bisection. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Facebook", 
        "name": "Igor Kabiljo"
      }, 
      {
        "affiliation": "Facebook", 
        "name": "Brian Karrer"
      }, 
      {
        "affiliation": "Facebook", 
        "name": "Mayank Pundir"
      }, 
      {
        "affiliation": "Facebook", 
        "name": "Sergey Pupyrev"
      }, 
      {
        "affiliation": "Facebook", 
        "name": "Alon Shalita"
      }, 
      {
        "affiliation": "Karlsruhe Institute of Technology", 
        "name": "Yaroslav Akhremtsev"
      }, 
      {
        "affiliation": "Google", 
        "name": "Alessandro Presta"
      }
    ], 
    "type": "research", 
    "id": "research1364"
  }, 
  "research1292": {
    "title": "SquirrelJoin: Network-Aware Distributed Join Processing with Lazy Partitioning", 
    "abstract": "To execute distributed joins in parallel on compute clusters, systems partition and exchange data records between workers. With large datasets, workers spend a considerable amount of time transferring data over the network. When compute clusters are shared among multiple applications, workers must compete for network bandwidth with other applications. These variances in the available network bandwidth lead to network skew, which causes straggling workers to prolong the join completion time.  We describe SquirrelJoin, a distributed join processing technique that uses lazy partitioning to adapt to transient network skew in clusters. Workers maintain in-memory lazy partitions to withhold a subset of records, i.e. not sending them immediately to other workers for processing. Lazy partitions are then assigned dynamically to other workers based on network conditions: each worker takes periodic throughput measurements to estimate its completion time, and lazy partitions are allocated as to minimise the join completion time. We implement SquirrelJoin as part of the Apache Flink distributed dataflow framework and show that, under transient network contention in a shared compute cluster, SquirrelJoin speeds up join completion times by up to 2.3x with only a small, fixed overhead.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Imperial College London", 
        "name": "Lukas Rupprecht"
      }, 
      {
        "affiliation": "Imperial College London", 
        "name": "William Culhane"
      }, 
      {
        "affiliation": "Imperial College London", 
        "name": "Peter Pietzuch"
      }
    ], 
    "type": "research", 
    "id": "research1292"
  }, 
  "industrial1041": {
    "title": "State Management in Apache Flink\u00c2\u00ae: Consistent Stateful Distributed Stream Processing", 
    "abstract": "Stream processors are emerging in industry as an apparatus that drives analytical but also mission critical services handling the core of persistent application logic. Thus, apart from scalability and low-latency, a rising system need is first-class support for application state together with strong consistency guarantees, and adaptivity to cluster reconfigurations, software patches and partial failures. Although prior systems research has addressed some of these specific problems, the practical challenge lies on how such guarantees can be materialized in a transparent, non-intrusive manner that relieves the user from unnecessary constraints. Such needs served as the main design principles of state management in Apache Flink, an open source, scalable stream processor.   We present Flink's core pipelined, in-flight mechanism which guarantees the creation of lightweight, consistent, distributed snapshots of application state, progressively, without impacting continuous execution. Consistent snapshots cover all needs for system reconfiguration, fault tolerance and version management through coarse grained rollback recovery. Application state is declared explicitly to the system, allowing efficient partitioning and transparent commits to persistent storage. We further present Flink's backend implementations and mechanisms for high availability, external state queries and output commit. Finally, we demonstrate how these mechanisms behave in practice with metrics and large-deployment insights exhibiting the low performance trade-offs of our approach and the general benefits of exploiting asynchrony in continuous, yet sustainable system deployments.", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "KTH Royal Institute of Technol", 
        "name": "Paris Carbone"
      }, 
      {
        "affiliation": "data Artisans", 
        "name": "Stephan Ewen"
      }, 
      {
        "affiliation": "King Digital Entertainment Limited", 
        "name": "Gyula F\u00c3\u00b3ra"
      }, 
      {
        "affiliation": "KTH", 
        "name": "Seif Haridi"
      }, 
      {
        "affiliation": "data Artisans", 
        "name": "Stefan Richter"
      }, 
      {
        "affiliation": "Data Artisans", 
        "name": "Kostas Tzoumas"
      }
    ], 
    "type": "industrial", 
    "id": "industrial1041"
  }, 
  "industrial910": {
    "title": "Statisticum: Data Statistics Management in SAP HANA", 
    "abstract": " We introduce a new concept of leveraging traditional data statistics as dynamic data integrity constraints. These data statistics produce transient database constraints, which are valid as long as they can be proven to be consistent with the current data. We denote this type of data statistics by constraint data statistics, their properties needed for consistency checking by consistency metadata, and their implied integrity constraints by implied data statistics constraints (implied constraints for short). Implied constraints are valid integrity constraints which are powerful query optimization tools employed, just as traditional database constraints, in semantic query transformation (aka query reformulation), partition pruning, runtime optimization, and semi-join reduction, to name a few. To our knowledge, this is the _rst work introducing this novel and powerful concept of deriving implied integrity constraints from data statistics. We discuss theoretical aspects of the constraint data statistics concept and their integration into query processing. We present the current architecture of data statistics management in SAP HANA and detail how constraint data statistics are designed and integrated into this architecture. As an instantiation of this framework, we consider dynamic partition pruning for data aging scenarios. We discuss our current implementation for constraint data statistics objects in SAP HANA which can be used for dynamic partition pruning.  We enumerate their properties and show how consistency checking for implied integrity constraints is supported in the data statistics architecture. Our experimental evaluations on the TPC-H benchmark and a real customer application confirm the effectiveness of the implied integrity constraints; (1) for 59% of TPC-H queries, constraint data statistics utilization results in pruning cold partitions and reducing memory consumption, and (2) we observe up to 3 orders of magnitude speed-up in query processing time, for a real customer running an S/4HANA application. ", 
    "subtype": "industrial", 
    "authors": [
      {
        "affiliation": "SAP", 
        "name": "Anisoara Nica"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Reza Sherkat"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Mihnea Andrei"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Xun Chen"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Martin Heidel"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Christian Bensberg"
      }, 
      {
        "affiliation": "SAP", 
        "name": "Heiko  Gerwens"
      }
    ], 
    "type": "industrial", 
    "id": "industrial910"
  }, 
  "research1376": {
    "title": "Stitching Web Tables for Improving Matching Quality [Experiments and Analyses]", 
    "abstract": "Tables on web pages (\u00e2\u0080\u009cweb tables\u00e2\u0080\u009d) cover a diversity of topics and can be a source of information for different tasks such as knowledge base augmentation or the ad-hoc extension of datasets. However, to use this information, the tables must first be integrated, either with each other or into existing data sources. The challenges that matching methods for this purpose have to overcome are the high heterogeneity and the small size of the tables. Though it is known that the majority of web tables in currently available corpora are very small, most of the existing methods do not recognise this fact as a problem for the matching process. In this experimental paper, we evaluate T2K Match, an existing web table to knowledge base matching method, and COMA, a standard schema matching tool, on a sample of web tables that is more realistic than the gold standards that are often used to compare matching systems, drawn from a corpus of 5 million web tables. We find that the methods fail to produce a correct result for many of the very small tables in this sample. As a remedy, we propose to stitch the many small web tables into larger ones. For this stitching process, we experimentally evaluate several standard schema matching methods in combination with holistic schema matching. Limiting this procedure to web tables from the same web site decreases the heterogeneity and we can perform this stitching at very high precision. Our experiments show that we improve the results for the original task by 0.38 in F1-Measure for T2K Match and by 0.14 for COMA when applying the stitching method. Stitching the tables further allows us to reduce the amount of tables in the corpus from 5 million original web tables to as few as 100,000 stitched tables.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Mannheim", 
        "name": "Oliver Lehmberg"
      }, 
      {
        "affiliation": "University of Mannheim", 
        "name": "Christian Bizer"
      }
    ], 
    "type": "research", 
    "id": "research1376"
  }, 
  "research337": {
    "title": "Stochastic Data Acquisition for Answering Queries as Time Goes by", 
    "abstract": "Data and actions are tightly coupled. On one hand, data analysis results trigger decision making and actions. On the other hand, the action of acquiring data is the very first step in the whole data processing pipeline. Data acquisition almost always has some costs, which could be either monetary costs or computing resource costs such as sensor battery power, network transfers, or I/O costs. Using outdated data to answer queries can avoid the data acquisition costs, but there is a penalty of potentially inaccurate results. Given a sequence of incoming queries over time, we study the problem of sequential decision making on when to acquire data and when to use existing versions to answer each query. We propose two approaches to solve this problem using reinforcement learning and tailored locality-sensitive hashing. A systematic empirical study using two real-world datasets shows that our approaches are effective and efficient.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "UMass Lowell", 
        "name": "Zheng Li"
      }, 
      {
        "affiliation": "UMass Lowell", 
        "name": "Tingjian Ge"
      }
    ], 
    "type": "research", 
    "id": "research337"
  }, 
  "research1372": {
    "title": "TPStream: Low-Latency Temporal Pattern Matching on Event Streams", 
    "abstract": "Complex Event Processing (CEP) has emerged as the state-of-the-art technology for continuously monitoring and analyzing streams of events in time-critical applications. The key feature in CEP is sequential pattern matching to detect a user-defined sequence of conditions on event streams. However, many CEP applications are not restricted to events only, but require native support for situations (aggregated event data lasting periods of time) and expressive temporal pattern matching among these situations. These important requirements regarding situations are not sufficiently addressed in the CEP literature so far.  In this paper we present TPStream, a novel event-processing operator for both deriving situations from event streams and detecting temporal patterns among situations. First, we provide a formal foundation of situations and TPStream. Then, we propose a low-latency algorithm for TPStream that delivers situations and temporal matches at the earliest possible point in time. Furthermore, we utilize a simple, yet effective cost model in order to adapt to changing workloads on the fly and with negligible cost for migrating operator states. The results of our exhaustive experimental evaluation show that TPStream is capable of processing high-volume event streams with low latency and outperforms applicable CEP solutions from academia and industry.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Philipps University Marburg", 
        "name": "Michael K\u00c3\u00b6rber"
      }, 
      {
        "affiliation": "Philipps University Marburg", 
        "name": "Nikolaus Glombiewski"
      }, 
      {
        "affiliation": "University of Marburg", 
        "name": "Bernhard Seeger"
      }
    ], 
    "type": "research", 
    "id": "research1372"
  }, 
  "research572": {
    "title": "The End of a Myth: Distributed Transaction Can Scale", 
    "abstract": "The common wisdom is that distributed transactions do not scale. But what if distributed transactions could be made scalable using the next generation of networks and a redesign of distributed databases? There would no longer be a need for developers to worry about co-partitioning schemes to achieve decent performance. Application development would become easier as data placement would no longer determine how scalable an application is. Hardware provisioning would be simplified as the system administrator can expect a linear scale-out when adding more machines rather than some complex sub-linear function, which is highly application specific.   In this paper, we present the design of our novel scalable database system NAM-DB and show that distributed transactions with the very common Snapshot Isolation guarantee can indeed scale using the next generation of RDMA-enabled network technology without any inherent bottlenecks. Our experiments with the TPC-C benchmark show that our system scales linearly to over 6.5 million new-order (14.5 million total) distributed transactions per second on 56 machines. ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Brown University", 
        "name": "Erfan Zamanian Dolati"
      }, 
      {
        "affiliation": "Brown University", 
        "name": "Carsten Binnig"
      }, 
      {
        "affiliation": "Oracle Labs", 
        "name": "Tim Harris"
      }, 
      {
        "affiliation": "Brown University", 
        "name": "Tim Kraska"
      }
    ], 
    "type": "research", 
    "id": "research572"
  }, 
  "research423": {
    "title": "The TileDB Array Data Storage Manager", 
    "abstract": "We present a novel storage manager for multi-dimensional arrays that arise in scientific applications, which is part of a larger scientific data management system called TileDB. In contrast to existing solutions, TileDB is optimized for both dense and sparse arrays. Its key idea is to organize array elements into ordered collections called fragments. Each fragment is dense or sparse, and groups contiguous array elements into data tiles of fixed capacity. The organization into fragments turns random writes into sequential writes, and, coupled with a novel read algorithm, leads to very efficient reads. TileDB enables parallelization via multi-threading and multi- processing, offering thread-/process-safety and atomicity via lightweight locking. We show that TileDB delivers comparable performance to the HDF5 dense array storage manager, while providing much faster random writes. We also show that TileDB offers substantially faster reads and writes than the SciDB array database system with both dense and sparse arrays. Finally, we demonstrate that TileDB is considerably faster than adaptations of the Vertica relational column-store for dense array storage management, and at least as fast for the case of sparse arrays.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Intel Labs and MIT", 
        "name": "Stavros Papadopoulos"
      }, 
      {
        "affiliation": "Intel Corporation", 
        "name": "Kushal Datta"
      }, 
      {
        "affiliation": "MIT", 
        "name": "Samuel Madden"
      }, 
      {
        "affiliation": "Intel Labs", 
        "name": "Timothy Mattson"
      }
    ], 
    "type": "research", 
    "id": "research423"
  }, 
  "research855": {
    "title": "Time Series Data Cleaning: From Anomaly Detection to Anomaly Repairing", 
    "abstract": "Errors are prevalent in time series data, such as GPS trajec- tories or sensor readings. Existing methods focus more on anomaly detection but not on repairing the detected anoma- lies. By simply filtering out the dirty data via anomaly detection, applications could still be unreliable over the in- complete time series. Instead of simply discarding anoma- lies, we propose to (iteratively) repair them in time series data, by creatively bonding the beauty of temporal nature in anomaly detection with the widely considered minimum change principle in data repairing. Our major contributions include: (1) a novel framework of iterative minimum re- pairing (IMR) over time series data, (2) explicit analysis on convergence of the proposed iterative minimum repairing, and (3) efficient estimation of parameters in each iteration. Remarkably, with incremental computation, we reduce the complexity of parameter estimation from O(n) to O(1). Ex- periments on real datasets demonstrate the superiority of our proposal compared to the state-of-the-art approaches. In particular, we show that (the proposed) repairing indeed improves the time series classification application.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Tsinghua Universtiy", 
        "name": "Aoqian Zhang"
      }, 
      {
        "affiliation": "Tsinghua University", 
        "name": "Shaoxu Song"
      }, 
      {
        "affiliation": "Tsinghua Universtiy", 
        "name": "Jianmin Wang"
      }, 
      {
        "affiliation": "University of Illinois at Chicago", 
        "name": "Philip Yu"
      }
    ], 
    "type": "research", 
    "id": "research855"
  }, 
  "research213": {
    "title": "Toward High-Performance Distributed Stream Processing via Approximate Fault Tolerance", 
    "abstract": "Fault tolerance is critical for distributed stream processing systems, yet achieving error-free fault tolerance often incurs substantial performance overhead. We present AF-Stream, a distributed stream processing system that addresses the trade-off between performance and accuracy in fault tolerance. AF-Stream builds on a notion called approximate fault tolerance, whose idea is to mitigate backup overhead by adaptively issuing backups, while ensuring that the errors upon failures are bounded with theoretical guarantees. Our AF-Stream design provides an extensible programming model for incorporating general streaming algorithms, and also exports only few threshold parameters for configuring approximation fault tolerance. Experiments on Amazon EC2 show that AF-Stream maintains high performance (compared to no fault tolerance) and high accuracy after multiple failures (compared to no failures) under various streaming algorithms.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "The Chinese Univ of Hong Kong", 
        "name": "Qun Huang"
      }, 
      {
        "affiliation": "The Chinese Univ of Hong Kong", 
        "name": "Patrick P. C. Lee"
      }
    ], 
    "type": "research", 
    "id": "research213"
  }, 
  "research1287": {
    "title": "Towards Linear Algebra over Normalized Data", 
    "abstract": "Providing machine learning (ML) over relational data is a mainstream requirement for data analytics systems. While almost all the ML tools require the input data to be presented as a single table, many datasets are multi-table, which forces data scientists to join those tables first, leading to data redundancy and runtime waste. Recent works on factorized\" ML mitigate this issue for a few specific ML algorithms by pushing ML through joins. But their approaches require a manual rewrite of ML implementations. Such piecemeal methods create a massive development overhead when extending such ideas to other ML algorithms. In this paper", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Univ. of Wisconsin-Madison", 
        "name": "Lingjiao Chen"
      }, 
      {
        "affiliation": "Univ. of California", 
        "name": "Arun Kumar"
      }, 
      {
        "affiliation": "Google", 
        "name": "Jeffrey Naughton"
      }, 
      {
        "affiliation": "University of Wisconsin", 
        "name": "Jignesh Patel"
      }
    ], 
    "type": "research", 
    "id": "research1287"
  }, 
  "research1279": {
    "title": "Trajectory Similarity Join in Spatial Networks", 
    "abstract": "The matching of similar pairs of objects, called similarity join, is fundamental functionality in data management. We consider the case of trajectory similarity join (TS-Join), where the objects are trajectories of vehicles moving in road networks. Thus, given two sets of trajectories and a threshold $\\theta$, the TS-Join returns all pairs of trajectories from the two sets with similarity above $\\theta$. This join targets applications such as trajectory duplicate detection, data cleaning, ridesharing/carpooling recommendation, and traffic congestion prediction.   With these applications in mind, we provide a purposeful definition of similarity. To enable efficient TS-Join processing on large sets of trajectories, we develop search space pruning techniques and take into account the parallel processing capabilities of modern processors. Specifically, we present a two-phase divide-and-conquer algorithm. For each trajectory, the algorithm first finds similar trajectories. Then it merges the results to achieve a final result. The algorithm exploits an upper bound on the spatiotemporal similarity and a heuristic scheduling strategy for search space pruning. The algorithm's per-trajectory searches are independent of each other and can be performed in parallel, and the merging has constant cost.  An empirical study with real data offers insight in the performance of the algorithm and demonstrates that is capable of outperforming a well-designed baseline algorithm by an order of magnitude.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "KAUST", 
        "name": "Shuo Shang"
      }, 
      {
        "affiliation": "HKBU", 
        "name": "Lisi Chen"
      }, 
      {
        "affiliation": "RUC", 
        "name": "Zhewei Wei"
      }, 
      {
        "affiliation": "Aalborg University", 
        "name": "Christian Jensen"
      }, 
      {
        "affiliation": "Soochow University", 
        "name": "Kai Zheng"
      }, 
      {
        "affiliation": "KAUST", 
        "name": "Panos Kalnis"
      }
    ], 
    "type": "research", 
    "id": "research1279"
  }, 
  "research1307": {
    "title": "Truss-based Community Search: a Truss-equivalence Based Indexing Approach", 
    "abstract": "We consider the community search problem defined upon a large graph G: given a query vertex q in G, to find as output all the densely connected subgraphs from G, each of which contains the query vertex v. As an online, query-dependent variant of the well-known community detection problem, community search enables personalized community discovery from graphs, and has found a wide range of real-world applications. In this paper, we study the community search problem in the truss-based model aimed at discovering all dense and cohesive k-truss communities to which the query vertex q belongs. We introduce a novel equivalence relation, k-truss equivalence, to model the intrinsic density and cohesiveness of edges in k-truss communities. Consequently, all the edges of G can be partitioned to a series of k-truss equivalence classes that constitute a space-efficient, truss-preserving index structure, EquiTruss. Community search can thus be addressed upon EquiTruss without repeated, time-demanding accesses to the original graph G, which proves to be theoretically optimal. In addition, EquiTruss can be efficiently maintained in a dynamic fashion when G evolves with edge insertion/deletion. Experimental studies validate both the efficiency and effectiveness of EquiTruss in real-world, large-scale graphs, which achieves at least an order of magnitude speedup in community search over the state-of-the-art method, TCP-Index.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Florida State University", 
        "name": "Esra Akbas"
      }, 
      {
        "affiliation": "Florida State University", 
        "name": "Peixiang Zhao"
      }
    ], 
    "type": "research", 
    "id": "research1307"
  }, 
  "research1384": {
    "title": "Truth Discovery for SpatioTemporal Events from Crowdsourced Data", 
    "abstract": "One of the greatest challenges in spatial crowdsourcing is determining the veracity of reports from multiple users about a particular event or phenomenon. In this paper, we address the difficulties of truth discovery in spatio-temporal tasks and present a new method based on recursive Bayesian estimation (BE) from multiple reports of users. Our method incorporates a reliability model for users, which improves as more reports arrive while increasing the accuracy of the model in labeling the state of the event. The model is fur- ther improved by Kalman estimation (BE+KE) that models the spatio-temporal correlations of the events and predicts the next state of an event and is corrected when new reports arrive. The methods are tested in a simulated environment, as well as using real-world data. Experimental results show that our methods are adaptable to the available data, can incorporate previous beliefs, and outperform existing truth discovery methods of spatio-temporal events.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Emory University", 
        "name": "Daniel Garcia Ulloa"
      }, 
      {
        "affiliation": "Emory", 
        "name": "Li Xiong"
      }, 
      {
        "affiliation": "Emory University", 
        "name": "Vaidy Sunderam"
      }
    ], 
    "type": "research", 
    "id": "research1384"
  }, 
  "research508": {
    "title": "Truth Inference in Crowdsourcing: Is the Problem Solved?", 
    "abstract": "Crowdsourcing has emerged as a novel problem-solving paradigm, which facilitates addressing problems that are hard for computers, e.g., entity resolution and sentiment analysis. However, due to the openness of crowdsourcing, workers may yield low-quality answers, and a redundancy-based method is widely employed, which first assigns each task to multiple workers and then infers the correct answer (called truth) for the task based on the answers of the assigned workers. A fundamental problem in this method is Truth Inference, which decides how to effectively infer the truth. Recently, the database community and data mining community independently study this problem and propose various algorithms. However, these algorithms are not compared extensively under the same framework and it is hard for practitioners to select appropriate algorithms. To alleviate this problem, we provide a detailed survey on 17 existing algorithms and perform a comprehensive evaluation using 5 real datasets. We make all codes and datasets public for future research. Through experiments we find that existing algorithms are not stable across different datasets and there is no algorithm that outperforms others consistently. We believe that the truth inference problem is not fully solved, and identify the limitations of existing algorithms and point out promising research directions.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "The University of Hong Kong", 
        "name": "Yudian Zheng"
      }, 
      {
        "affiliation": "Tsinghua University", 
        "name": "Guoliang Li"
      }, 
      {
        "affiliation": "Tsinghua University", 
        "name": "Yuanbing Li"
      }, 
      {
        "affiliation": "The University of Hong Kong", 
        "name": "Caihua Shan"
      }, 
      {
        "affiliation": "Hong Kong University", 
        "name": "Reynold Cheng"
      }
    ], 
    "type": "research", 
    "id": "research508"
  }, 
  "research428": {
    "title": "Two Birds, One Stone: A Fast, yet Lightweight, Indexing Scheme for Modern Database Systems", 
    "abstract": "Classic database indexes (e.g., B+ Tree), though speed up queries, suffer from two main drawbacks: (1) An index usually yields 5 to 15 additional storage overhead which results in non-ignorable dollar cost in big data scenarios especially when deployed on modern storage devices. (2) Maintaining an index incurs high latency because the DBMS has to locate and update those index pages affected by the underlying table changes. This paper proposes Hippo a fast, yet scalable, database indexing approach. It significantly shrinks the index storage and mitigates maintenance overhead without compromising much on the query execution performance. Hippo stores disk page ranges instead of tuple pointers in the indexed table to reduce the storage space occupied by the index. It maintains simplified histograms that represent the data distribution and adopts a page grouping technique that groups contiguous pages into page ranges based on the similarity of their index key attribute distributions. When a query is issued, Hippo leverages the page ranges and histogram-based page summaries to recognize those pages such that their tuples are guaranteed not to satisfy the query predicates and inspects the remaining pages. Experiments based on real and synthetic datasets show that Hippo occupies up to two orders of magnitude less storage space than that of the B+ Tree while still achieving comparable query execution performance to that of the B+ Tree for 0.1 - 1 selectivity factors. Also, the experiments show that Hippo outperforms BRIN (Block Range Index) in executing queries with various selectivity factors. Furthermore, Hippo achieves up to three orders of magnitude less maintenance overhead and up to an order of magnitude higher throughput (for hybrid query/update workloads) than its counterparts.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Arizona State University", 
        "name": "Jia Yu"
      }, 
      {
        "affiliation": "Arizona State University", 
        "name": "Mohamed  Sarwat"
      }
    ], 
    "type": "research", 
    "id": "research428"
  }, 
  "research613": {
    "title": "Understanding Workers, Developing Effective Tasks, and Enhancing Marketplace Dynamics: A Study of a Large Crowdsourcing Marketplace", 
    "abstract": "We conduct an experimental analysis of a dataset comprising over 25 million microtasks performed by over 70,000 workers issued to a large crowdsourcing marketplace between 2012-2016. Using this data---never before analyzed in an academic context---we shed light on three crucial aspects of crowdsourcing: (1) Task design --- helping requesters understand what constitutes an effective task, and how to go about designing one; (2) Marketplace dynamics --- helping marketplace administrators and designers understand the interaction between tasks and workers, and the corresponding marketplace load; and (3) Worker behavior --- understanding worker attention spans, lifetimes, and general behavior, for the improvement of the crowdsourcing ecosystem as a whole.   ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Illinois", 
        "name": "Ayush Jain"
      }, 
      {
        "affiliation": "Stanford", 
        "name": "Akash Das Sarma"
      }, 
      {
        "affiliation": "UIUC", 
        "name": "Aditya Parameswaran"
      }, 
      {
        "affiliation": "Stanford", 
        "name": "Jennifer Widom"
      }
    ], 
    "type": "research", 
    "id": "research613"
  }, 
  "research558": {
    "title": "Understanding the Sparse Vector Technique for Differential Privacy", 
    "abstract": "The Sparse Vector Technique (SVT) is a fundamental technique for satisfying differential privacy and  has the unique quality that one can output some query answers without apparently paying any privacy cost.  SVT has been used in both the interactive setting, where one tries to answer a sequence of queries that are not known ahead of the time, and in the non-interactive setting, where all queries are known.  Because of the potential savings on privacy budget, many variants for SVT have been proposed and employed in privacy-preserving data mining and publishing.  However, most variants of SVT are actually not private.  In this paper, we analyze these errors and identify the misunderstandings that likely contribute to them.  We also propose a new version of SVT that provides better utility, and introduce an effective technique to improve the performance of SVT.  These enhancements can be applied to improve utility in the interactive setting.  Through both analytical and experimental comparisons, we show that, in the non-interactive setting (but not the interactive setting), the SVT technique is unnecessary, as it can be replaced by the Exponential Mechanism (EM) with better accuracy.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "University of Science and Technology of China", 
        "name": "Min Lyu"
      }, 
      {
        "affiliation": "Purdue University", 
        "name": "Dong Su"
      }, 
      {
        "affiliation": "Purdue University", 
        "name": "Ninghui Li"
      }
    ], 
    "type": "research", 
    "id": "research558"
  }, 
  "research413": {
    "title": "VIP-Tree: An Effective Index for Indoor Spatial Queries", 
    "abstract": "Due to the growing popularity of indoor location-based services, indoor data management has received significant research attention in the past few years. However, we observe that the existing indexing and query processing techniques for the indoor space do not fully exploit the properties of the indoor space. Consequently, they provide below par performance which makes them unsuitable for large indoor venues with high query workloads. In this paper, we propose two novel indexes called Indoor Partitioning Tree (IP-Tree) and Vivid IP-Tree (VIP-Tree) that are carefully designed by utilizing the properties of indoor venues. The proposed indexes are lightweight, have small pre-processing cost and provide near- optimal performance for shortest distance and shortest path queries. We also present efficient algorithms for other spatial queries such as k nearest neighbors queries and range queries. Our extensive experimental study on real and synthetic data sets demonstrates that our proposed indexes outperform the existing algorithms by several orders of magnitude.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Monash University", 
        "name": "Zhou Shao"
      }, 
      {
        "affiliation": "Monash University Australia", 
        "name": "Muhammad Cheema"
      }, 
      {
        "affiliation": "Monash University", 
        "name": "David Taniar"
      }, 
      {
        "affiliation": "Aalborg University", 
        "name": "Hua Lu"
      }
    ], 
    "type": "research", 
    "id": "research413"
  }, 
  "research1403": {
    "title": "Voodoo - A Vector Algebra for Portable Database Performance on Modern Hardware", 
    "abstract": "In-memory databases require careful tuning and many engineering tricks to achieve good performance. This kind of database performance engineering is hard: a plethora of both data and hardware-dependent optimization techniques create a design space that is hard for a skilled database engineer \u00e2\u0080\u0093 much less a query compiler \u00e2\u0080\u0093 to navigate. To facilitate performance-oriented design exploration and query plan compilation, we propose and present Voodoo, a declarative intermediate algebra that abstracts the detailed architectural properties of the hardware, such as multi- or many-core architectures, hierarchical caches and SIMD registers, without losing the ability generate highly tuned code. Because consists of a collection of declarative, vector-oriented operations, Voodoo is easier to reason about and tune than low-level C and related hardware-focused extensions (i.e., Vector Intrinsics, OpenCL, CUDA, etc.). This enables our Voodoo compiler (which actually generates OpenCL) to produce code that rivals and even outperforms the fastest state-of-the-art in memory databases for both GPUs and CPUs. In addition, Voodoo makes it possible to to express techniques as diverse as cache-conscious processing, predication and SIMD (again on both GPUs and CPUs) to be expressed with just a few lines of code. Central to our approach is a novel idea we call control vectors, which allow programmers to expose parallelism to the Voodoo compiler in a highly abstracted manner that is largely agnostic to the targeted hardware, enabling portable performance across different hardware platforms. We used Voodoo to build an alternative backend for MonetDB, a popular open-source in-memory database. Our backend allows MonetDB to perform at the same level as highly tuned in-memory databases, including HyPeR and Ocelot. We also demonstrate Voodoo\u00e2\u0080\u0099s usefulness when investigating hardware conscious tuning techniques, assessing their performance on different queries, devices and data.  ", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "MIT", 
        "name": "Holger Pirk"
      }, 
      {
        "affiliation": "", 
        "name": "Oscar Moll"
      }, 
      {
        "affiliation": "", 
        "name": "Matei Zaharia"
      }, 
      {
        "affiliation": "MIT", 
        "name": "Samuel Madden"
      }
    ], 
    "type": "research", 
    "id": "research1403"
  }, 
  "research609": {
    "title": "We Think That You Will Really Enjoy This Empirical Evaluation Paper on In-Memory Multi-Version Concurrency Control", 
    "abstract": "Multi-version concurrency control (MVCC) is currently the most popular transaction management scheme in modern database management systems (DBMSs). Although MVCC was discovered in the late 1970s, it is used in almost every major relational DBMS released in the last decade. Maintaining multiple versions of data potentially increases parallelism without sacrificing serializability when processing transactions. But scaling MVCC in a multi-core and in-memory setting is non-trivial: when there are a large number of threads running in parallel, the synchronization overhead can outweigh the benefits of multi-versioning.  To understand how MVCC perform when processing transactions in modern hardware settings, we conduct an extensive study of the scheme's four key design decisions: concurrency control protocol, version storage, garbage collection, and index management. We implemented state-of-the-art variants of all of these in an in-memory DBMS and evaluated them using OLTP workloads. Our analysis identifies the fundamental bottlenecks of each design choice.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "National University of Singapore", 
        "name": "Yingjun Wu"
      }, 
      {
        "affiliation": "Carnegie Mellon University", 
        "name": "Joy Arulraj"
      }, 
      {
        "affiliation": "Carnegie Mellon University", 
        "name": "Jiexi Lin"
      }, 
      {
        "affiliation": "Carnegie Mellon University", 
        "name": "Ran Xian"
      }, 
      {
        "affiliation": "CMU", 
        "name": "Andrew Pavlo"
      }
    ], 
    "type": "research", 
    "id": "research609"
  }, 
  "research850": {
    "title": "When Engagement Meets Similarity: Efficient (k,r)-Core Computation on Social Networks", 
    "abstract": "In this paper, we investigate the problem of (k,r)-core which intends to find cohesive subgraphs on social networks considering both user engagement and similarity perspectives. In particular, we adopt the popular concept of k-core to guarantee the engagement of the users (vertices) in a group (subgraph) where each vertex in a (k,r)-core connects to at least k other vertices. Meanwhile, we consider the pairwise similarity among users based on their attributes. Efficient algorithms are proposed to enumerate all maximal (k,r)-cores and find the maximum (k,r)-core, where both problems are shown to be NP-hard. Effective pruning techniques substantially reduce the search space of two algorithms. A novel (k,k')-core based (k,r)-core size upper bound enhances performance of the maximum (k,r)-core computation. We also devise effective search orders for two mining algorithms where search priorities for vertices are different. Comprehensive experiments on real-life data demonstrate that the maximal/maximum (k,r)-cores enable us to find interesting cohesive subgraphs, and performance of two mining algorithms is effectively improved by proposed techniques.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "UTS", 
        "name": "Fan Zhang"
      }, 
      {
        "affiliation": "QCIS", 
        "name": "Ying Zhang"
      }, 
      {
        "affiliation": "QCIS", 
        "name": "Lu Qin"
      }, 
      {
        "affiliation": "CSE", 
        "name": "Wenjie Zhang"
      }, 
      {
        "affiliation": "CSE", 
        "name": "Xuemin Lin"
      }
    ], 
    "type": "research", 
    "id": "research850"
  }, 
  "research415": {
    "title": "Write-Behind Logging", 
    "abstract": "The design of the logging and recovery components of database management systems (DBMSs) has always been influenced by the difference in the performance characteristics of volatile (DRAM) and non-volatile storage devices (HDD/SSDs). Until now, the key assumption has been that non-volatile storage is much slower than DRAM and only support block-oriented read/writes. But the arrival of new non-volatile memory (NVM) storage that is almost as fast as DRAM with fine-grained read/writes invalidates these previous design choices.  This paper explores the changes that are required in a DBMS to leverage the unique properties of NVM in systems that still include volatile DRAM. We make the case for a new logging and recovery protocol, called write-behind logging, that enables a DBMS to recover nearly instantaneously from system failures. The key idea of our approach is that the DBMS logs what parts of the database have changed rather than how it was changed. Using this logging method, the DBMS can directly flush the changes to the database even before recording them in the log. Our evaluation using an in-memory DBMS shows that this protocol improves the transactional throughput by 1.3 times, reduces the recovery time by more than two orders of magnitude, and shrinks the storage footprint of the DBMS on NVM by 1.5 times. We also demonstrate that our logging protocol works seamlessly with standard replication schemes.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Carnegie Mellon University", 
        "name": "Joy Arulraj"
      }, 
      {
        "affiliation": "Carnegie Mellon University", 
        "name": "Matthew Perron"
      }, 
      {
        "affiliation": "Carnegie Mellon University", 
        "name": "Garth Gibson"
      }, 
      {
        "affiliation": "CMU", 
        "name": "Andrew Pavlo"
      }
    ], 
    "type": "research", 
    "id": "research415"
  }, 
  "research517": {
    "title": "ZooBP: Belief Propagation for Heterogeneous Networks", 
    "abstract": "Given a heterogeneous network, with nodes of different types -- e.g., products, users and sellers from an online recommendation site like Amazon -- and labels for a few nodes ('honest', 'suspicious', etc), can we find a closed formula for Belief Propagation (BP), exact or approximate? Can we say whether it will converge?  BP, traditionally an inference algorithm for graphical models, exploits the so-called ``network effects'' to perform real-world graph classification tasks; and it has been successful in numerous settings like fraudulent entity detection in online retailers and classification in social networks when we are given the labels for a subset of the nodes.  We propose ZooBP, a method to perform fast BP on undirected heterogeneous graphs with provable convergence guarantees. ZooBP has the following advantages: (1) Generality: It works on heterogeneous graphs with multiple node- and edge-types; (2) Theoretical Guarantees: unlike the traditional loopy BP, ZooBP gives a closed-form solution, as well as  convergence guarantees;  (3) Scalability: ZooBP is linear on the graph size and is up to 600 times faster than BP, running on graphs with 3.3 million edges in a few seconds. (4) Effectiveness: Applied on real data (a Flipkart e-commerce network with users, products and sellers), ZooBP identifies fraudulent users with a near-perfect precision of 92.3% over the top 300 results.", 
    "subtype": "research", 
    "authors": [
      {
        "affiliation": "Carnegie Mellon University", 
        "name": "Dhivya Eswaran"
      }, 
      {
        "affiliation": "Technical University of Munich", 
        "name": "Stephan Guennemann"
      }, 
      {
        "affiliation": "Carnegie Mellon University", 
        "name": "Christos Faloutsos"
      }, 
      {
        "affiliation": "Flipkart", 
        "name": "Disha Makhija"
      }, 
      {
        "affiliation": "Flipkart", 
        "name": "Mohit Kumar"
      }
    ], 
    "type": "research", 
    "id": "research517"
  }
}