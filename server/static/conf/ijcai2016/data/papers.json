entities={
  "10": {
    "abstract": "Formative assessments allow learners to quickly identify knowledge gaps. In traditional educational settings, expert instructors can create assessments, but in informal learning environment, it is difficult for novice learners to self assess because they don\u00e2\u0080\u0099t know what they don\u00e2\u0080\u0099t know. This paper introduces Questimator, an automated system that generates multiple-choice assessment questions for any topic contained within Wikipedia. Given a topic, Questimator traverses the Wikipedia graph to find and rank related topics, and uses article text to form questions, answers and distractor options. In a study with 833 participants from Mechanical Turk, we found that participants\u00e2\u0080\u0099 scores on Questimator-generated quizzes correlated well with their scores on existing online quizzes on topics ranging from philosophy to economics. Also Questimator generates questions with comparable discriminatory power as existing online quizzes. These results suggest that Questimator may be a useful way to assess learning in topics for which there is not an existing quiz.", 
    "authors": [
      {
        "name": "Qi Guo"
      }, 
      {
        "name": "Chinmay Kulkarni"
      }, 
      {
        "name": "Aniket Kittur"
      }, 
      {
        "name": "Jeffrey Bigham"
      }, 
      {
        "name": "Emma Brunskill"
      }
    ], 
    "keywords": "education, knowledge assessment, information extraction, Wikipedia", 
    "title": "Questimator: Generating Knowledge Assessments for Arbitrary Topics", 
    "type": "paper"
  }, 
  "1002": {
    "abstract": "Convex Transductive Experimental Design (CTED) is one of the most representative active learning methods. It utilizes a data reconstruction framework to select informative samples for manual annotation. However, we observe that CTED cannot well handle the diversity of selected samples, and therefore the set of selected samples is redundant. Compared with non-redundant one, the set of redundant samples is less informative because similar samples share similar properties or contain overlapped information. Given limited data labeling budget, it is desirable to exclude highly similar samples and include non-redundant ones with complementary information to construct an informative samples set. To this end, this paper proposes Diversified CTED to reduce such redundancy of the selected samples set. Specifically, we design a novel, effective diversity metric and use it to regularize CTED. The obtained optimization problem is hard to solve due to the complex structure of the diversity metric. Luckily, we find an equivalent optimization problem which is easier to optimize than the original one. We then derive an effective optimization algorithm based on alternating minimization with proved convergence. Extensive experimental results on several benchmark data sets demonstrate that Diversified CTED significantly improves CTED and consistently outperforms the state-of-the-art methods in the literature, verifying the effectiveness and advantages of incorporating the proposed diversity metric into CTED.", 
    "authors": [
      {
        "name": "Lei Shi"
      }, 
      {
        "name": "Yi-Dong Shen"
      }
    ], 
    "keywords": "Convex Transductive Experimental Design, Active Learning, Diversity", 
    "title": "Diversifying Convex Transductive Experimental Design for Active Learning", 
    "type": "paper"
  }, 
  "1004": {
    "abstract": "We study a generalized form of planning under partial observability, in which we have multiple, possibly infinitely many, planning domains with the same actions and observations, and goals expressed over observations, which are possibly temporally extended. By building on work on two-player (non-probabilistic) games with imperfect information in the Verification literature, we devise a general technique, generalizing the belief-state construction, to remove partial observability. This reduces the planning problem to a game of perfect information with a tight correspondence between plans and strategies. Then we instantiate the technique and solve some generalized planning problems.", 
    "authors": [
      {
        "name": "Giuseppe De Giacomo"
      }, 
      {
        "name": "Antonio Di Stasio"
      }, 
      {
        "name": "Aniello Murano"
      }, 
      {
        "name": "Sasha Rubin"
      }
    ], 
    "keywords": "Generalized Planning, Partial Observability, Imperfect Information Games, Temporally Extended Goals, Belief-State Construction, LTL on Infinite Traces", 
    "title": "Imperfect information games and generalized planning", 
    "type": "paper"
  }, 
  "1005": {
    "abstract": "This paper discusses the discovery of the inconsistency in G\u00c3\u00b6del\u00e2\u0080\u0099s ontological argument as a success story for artificial intelligence. Despite the popularity of the argument since the appearance of G\u00c3\u00b6del\u00e2\u0080\u0099s manuscript in 1970, the inconsistency of the axioms used in the argument remained unnoticed until 2013, when it was detected automatically by the higher-order theorem prover Leo-II. Understanding and verifying the refutation generated by the prover turned out to be a time-consuming task, and its completion, as reported here, required the reconstruction of the refutation in the Isabelle proof assistant. The development of an improved syntactical hiding for the embedding technique, utilizing e.g. Isabelle\u00e2\u0080\u0099s binding notation mechanism, allows the refutation to be presented in a human-friendly way, suitable for non-experts in the technicalities of higher-order theorem proving. This brings us a step closer to wider adoption of logic-based artificial intelligence tools by philosophers.  In addition, the paper reports on significant improvements regarding full automation of the ontological argument in a higher-order modal logic S5 with a universal accessibility relation.", 
    "authors": [
      {
        "name": "Christoph Benzm\u00fcller"
      }, 
      {
        "name": "Bruno Woltzenlogel Paleo"
      }
    ], 
    "keywords": "Automated Reasoning, Higher-Order Logic, Modal Logic, Automated Theorem Proving, Interactive Theorem Proving, Proof Assistants, Metaphysics, G\u00f6del's Ontological Argument", 
    "title": "The Inconsistency in G\u00f6del\u2019s Ontological Argument: A Success Story for AI in Metaphysics", 
    "type": "paper"
  }, 
  "1008": {
    "abstract": "In recent years, binary coding techniques, which compress the original high-dimensional data samples into short binary codes, are becoming increasingly popular due to their high efficiency in information retrieval. It has been shown that binary coding techniques which leverage supervised information can significantly enhance the coding quality, and hence improve search performance. However, few methods aims to learn coding functions efficiently such that the precision at the top of Hamming distance ranking list is optimized while the geometric relationship of database examples are well preserved. In this paper, we propose a novel supervised binary coding approach, namely Fast Structural Binary Coding (FSBC), to explicitly optimize the precision at the top of a Hamming distance ranking list and ensure that similar images can be returned as a whole. The key idea is to train disciplined coding functions by optimizing a lower bound of AUC and penalize this objective such that the geometric relationship of database examples in the original Euclidean space can be preserved in the Hamming space. To solve such coding function, we relax the original discrete optimization objective with a continuous surrogate, and then derive a stochastic gradient descent method to optimize the surrogate objective efficiently. Empirical studies based upon two image datasets demonstrate that the proposed binary coding approaches achieve superior image search performance over the state-of-the-arts.", 
    "authors": [
      {
        "name": "Dongjin Song"
      }, 
      {
        "name": "Wei Liu"
      }, 
      {
        "name": "David Meyer"
      }
    ], 
    "keywords": "Binary Coding, Image Retrieval, AUC", 
    "title": "Fast Structural Binary Coding", 
    "type": "paper"
  }, 
  "1015": {
    "abstract": "In social network sites (SNS), propagation histories which record the information diffusion process can be used to explain to users what happened in their networks. However, these histories easily grow in size and complexity, limiting their intuitive understanding by users. To reduce this information overload, in this paper, we present the problem of propagation history ranking. The goal is to rank participant edges/nodes by their contribution to the diffusion. First, we discuss and adapt the ranking criterion Difference of Causal Effects (DCE). Then, to avoid the complex calculation of DCE, we propose a \u00e2\u0080\u009cresp-cap\u00e2\u0080\u009d ranking strategy by adopting two indicators. The first is responsibility which captures the necessary face of causal effects. We further give an approximate algorithm for responsibility calculation. The second is capability which is defined to capture the sufficient face of causal effects. Finally, promising experimental results are presented to verify the feasibility of our method.", 
    "authors": [
      {
        "name": "Zheng Wang"
      }, 
      {
        "name": "Chaokun Wang"
      }, 
      {
        "name": "Jisheng Pei"
      }, 
      {
        "name": "Xiaojun Ye"
      }, 
      {
        "name": "Philip S. Yu"
      }
    ], 
    "keywords": "propagation history ranking, social networks, causality", 
    "title": "Causality based Propagation History Ranking in Social Networks", 
    "type": "paper"
  }, 
  "1017": {
    "abstract": "In this paper we present greedy methods for selecting a subset of heuristic functions for guiding A* search. Our methods are able to optimize various objective functions while selecting a subset from a pool of up to thousands of heuristics. Specifically, our methods minimize approximations of A*'s search tree size, and approximations of A*'s running time. We show empirically that our methods can outperform state-of-the-art planners for deterministic optimal planning.", 
    "authors": [
      {
        "name": "Levi Lelis"
      }, 
      {
        "name": "Santiago Franco"
      }, 
      {
        "name": "Marvin Abisrror"
      }, 
      {
        "name": "Mike Barley"
      }, 
      {
        "name": "Sandra Zilles"
      }, 
      {
        "name": "Robert Holte"
      }
    ], 
    "keywords": "Heuristic Subset Selection, Stratified Sampling, Classical Planning, Pattern Databases", 
    "title": "Heuristic Subset Selection in Classical Planning", 
    "type": "paper"
  }, 
  "1019": {
    "abstract": "The alternating direction method of multipliers (ADMM) is a powerful optimization solver for many machine learning models. Recently, stochastic ADMM has been integrated with variance reduction methods for stochastic gradient, leading to SAG-ADMM and SDCA-ADMM that have fast convergence rates and low iteration complexities. However, their space requirements can still be high. In this paper, we propose an integration of ADMM with the method of stochastic variance reduced gradient (SVRG). This integration is not straightforward. Unlike another recent integration attempt called SCAS-ADMM, the proposed algorithm retains the fast convergence benefits of SAG-ADMM and SDCA-ADMM, but is more advantageous in that its storage requirement is very low,  even independent of the sample size $n$. Experimental results demonstrate that it is as fast as SAG-ADMM and SDCA-ADMM, much faster than SCAS-ADMM, and can be used on much bigger data sets.", 
    "authors": [
      {
        "name": "Shuai Zheng"
      }, 
      {
        "name": "James Kwok"
      }
    ], 
    "keywords": "ADMM, stochastic gradient, variance reduction", 
    "title": "Fast-and-Light Stochastic ADMM", 
    "type": "paper"
  }, 
  "1021": {
    "abstract": "Multi-task clustering improves the clustering performance of each task by transferring knowledge across related tasks. Most existing multi-task clustering methods are based on the ideal assumption that the tasks are completely related. However, in many real applications, the tasks are usually partially related, and brute-force transfer may cause negative effect which degrades the clustering performance. In this paper, we propose a self-adapted multi-task clustering (SAMTC) method which can automatically identify and transfer usable instances among the tasks, thus avoiding negative transfer. SAMTC begins with an initialization by performing single-task clustering on each task, then executes the following three steps: first, it finds the usable instances by measuring related clusters with Jensen-Shannon divergence between each pair of tasks, and obtains a pair of possibly related subtasks; second, it estimates the relatedness between each pair of subtasks with kernel mean matching; third, it constructs the similarity matrix for each task by exploiting useful information from the other tasks through instance transfer, and adopts spectral clustering to get the final clustering result. Experimental results on several real data sets show the superiority of the proposed algorithm over traditional single-task clustering methods and existing multitask clustering methods.", 
    "authors": [
      {
        "name": "Xianchao Zhang"
      }, 
      {
        "name": "Xiaotong Zhang"
      }, 
      {
        "name": "Han Liu"
      }
    ], 
    "keywords": "Unsupervised learning, Multi-task clustering, Negative transfer", 
    "title": "Self-Adapted Multi-task Clustering", 
    "type": "paper"
  }, 
  "1041": {
    "abstract": "Stochastic And-Or grammars (AOG) extend traditional stochastic grammars of language to model other types of data such as images and events. In this paper we propose a representation framework of stochastic AOGs that is agnostic to the type of the data being modeled and thus unifies various domain-specific AOGs. Many existing grammar formalisms and probabilistic models in natural language processing, computer vision, and machine learning can be seen as special cases of this framework. We also propose a domain-independent inference algorithm of stochastic context-free AOGs and show its tractability under a reasonable assumption. Furthermore, we provide two interpretations of stochastic context-free AOGs as a subset of probabilistic logic, which connects stochastic AOGs to the field of statistical relational learning and clarifies their relation with a few existing statistical relational models.", 
    "authors": [
      {
        "name": "Kewei Tu"
      }
    ], 
    "keywords": "stochastic And-Or grammars, parsing, statistical relational models", 
    "title": "Stochastic And-Or Grammars: A Unified Framework and Logic Perspective", 
    "type": "paper"
  }, 
  "1043": {
    "abstract": "Feature-to-feature matching is the key issue in the Bag-of-Features model. The baseline approach employs a coarse feature-to-feature matching, namely, two descriptors are assumed to match if they are assigned the same quantization index. However, this Hard Assignment strategy usually incurs undesirable low precision. To fix it, Multiple Assignment and Soft Assignment are proposed, the two methods reduce the quantization error to some extent, but there are a lot of room for improvement. To further improve retrieval precision, in this paper, we propose an novel feature matching strategy, called Local-Restricted Soft Assignment strategy, in which a new feature matching function is introduced. The local-restricted soft assignment strategy is evaluated through extensive experiments on five benchmark datasets (Ukbench, Holidays, Oxford 5K, Paris 6K and MIR Flickr 1M). Experiments show that the results exceed the current quantization methods retrieval performance on these datasets. Combined with post-processing steps, we have achieved competitive results compared with the state-of-the-art methods. Overall, our strategy shows notable benefit for retrieval with large vocabularies and database size, at a cost of increased storage requirements for the index.", 
    "authors": [
      {
        "name": "Hao Tang"
      }, 
      {
        "name": "Hong Liu"
      }
    ], 
    "keywords": "Image retrieval, Feature quantization, Large scale", 
    "title": "A Novel Feature Matching Strategy for Large Scale Image Retrieval", 
    "type": "paper"
  }, 
  "1047": {
    "abstract": "Answer Set Programming (ASP) has recently been employed to specify and run dynamic programming (DP) algorithms on tree decompositions, a central approach in the field of parameterized complexity, which aims at solving hard problems efficiently for instances of certain structure. This ASP-based method followed the standard DP approach where tables are computed in a bottom-up fashion, yielding good results for several counting or enumeration problems. However, for optimization problems this approach lacks the possibility to report solutions before the optimum is found, and for search problems it often computes a lot of unnecessary rows. In this paper, we present a novel ASP-based system allowing for \"lazy\" DP, which utilizes recent multi-shot ASP technology. Preliminary experimental results show that this approach not only yields better performance for search problems, but also outperforms some state-of-the-art ASP encodings for optimization problems in terms of anytime computation, i.e., measuring the quality of the best solution after a certain timeout.", 
    "authors": [
      {
        "name": "Bernhard Bliem"
      }, 
      {
        "name": "Benjamin Kaufmann"
      }, 
      {
        "name": "Torsten Schaub"
      }, 
      {
        "name": "Stefan Woltran"
      }
    ], 
    "keywords": "answer set programming, tree decomposition, anytime algorithms, dynamic programming, treewidth", 
    "title": "ASP for Anytime Dynamic Programming on Tree Decompositions", 
    "type": "paper"
  }, 
  "1054": {
    "abstract": "Among learning-based hashing methods, supervised hashing tries to find hash codes which preserve semantic similarities of original data. Recent years have witnessed much efforts devoted to design objective functions and optimization methods for supervised hashing learning, in order to improve search accuracy and reduce training cost. In this paper, we propose a very straightforward supervised hashing algorithm and demonstrate its superiority to several state-of-the-art methods. The key idea of our approach is to treat label vectors as binary codes and to learn target codes which have similar structure to label vectors. To circumvent direct optimization on large n by n Gram matrices, we identify an inner-product-preserving transformation and use it to bring close label vectors and hash codes without changing the structure. The optimization process is very efficient and scales well. In our experiment, training $16$-bit and $96$-bit code on NUS-WIDE cost only 3 and 6 minutes respectively.", 
    "authors": [
      {
        "name": "Qi Liu"
      }, 
      {
        "name": "Hongtao Lu"
      }
    ], 
    "keywords": "supervised hashing, semantic similarity, preserve inner product", 
    "title": "Natural Supervised Hashing", 
    "type": "paper"
  }, 
  "1056": {
    "abstract": "In multiagent systems, we often have a set of agents each of which have a preference ordering over a set of items and one would like to know these preference orderings for various tasks, for example, data analysis, preference aggregation, voting etc. However, we often have a large number of items which makes it impractical to ask the agents for their complete preference ordering. In such scenarios, we usually elicit these agents' preferences by asking (a hopefully small number of) comparison queries --- asking an agent to compare two items. Prior works on preference elicitation focus on unrestricted domain and single peaked preference domain and establish the fact that it is indeed possible to reduce the number of queries asked, called query complexity, by restricting the domain where the preference profile belongs. We extend this line of work and study preference elicitation for single peaked preference profiles on trees which is a strict superset of the domain of single peaked preferences. We show that the query complexity for eliciting preference profiles crucially depends on the number of leaves and the path cover number of the underlying single peaked tree, whereas the other natural parameters like maximum degree, diameter, pathwidth, distance from path do not directly influence the query complexity. We then move on to study query complexity for finding a weak Condorcet winner for profiles which are single peaked on trees and show that this task has much less query complexity. Here again we observe that the number of leaves in the underlying single peaked tree and the path cover number of the tree influence the query complexity of the problem.", 
    "authors": [
      {
        "name": "Palash Dey"
      }, 
      {
        "name": "Neeldhara Misra"
      }
    ], 
    "keywords": "voting, elicitation, algorithm, single peaked, tree, preference, theory, social choice", 
    "title": "Elicitation for Preferences Single Peaked on Trees", 
    "type": "paper"
  }, 
  "1061": {
    "abstract": "Eliciting preferences of a set of agents over a set of alternatives is a fundamental problem in social choice theory. Prior work study the query complexity of preference elicitation for unrestricted domain and the domain of single peaked preference profiles. We study the query complexity of preference elicitation for the domain of single crossing preference profiles when an ordering of the voters with respect to which the profile is single crossing is known unknown and the votes can be accessed randomly | sequentially in a single crossing order | sequentially in any given order. We present polynomial time algorithms with low query complexity for preference elicitation for all the above six cases. We also show that the query complexities of our algorithms are optimal up to constant factors for all but one of the above six cases.", 
    "authors": [
      {
        "name": "Palash Dey"
      }, 
      {
        "name": "Neeldhara Misra"
      }
    ], 
    "keywords": "voting, elicitation, algorithm, single crossing, domain restriction, preference, theory, social choice", 
    "title": "Preference Elicitation For Single Crossing Domain", 
    "type": "paper"
  }, 
  "1067": {
    "abstract": "The Coalitional Manipulation problem has been studied extensively in the literature for many voting rules. However, most studies have focused on the complete information setting, wherein the manipulators know the votes of the non-manipulators. While this assumption is reasonable for purposes of showing intractability, it is unrealistic for algorithmic considerations. In most real-world scenarios, either due to the large amounts of data involved or due to pertinent privacy issues, it is impractical for the manipulators to have accurate knowledge of all the other votes. In this paper, we introduce a natural model for manipulation under incomplete information. In our framework, the manipulators know a partial order for each voter that is consistent with the true preference of that voter. We say that an extension of a partial order is viable if there exists a manipulative vote for that extension. In this setting, we formulate three natural notions of manipulation:  -Weak Manipulation (WM), where the manipulators seek to vote in a way that makes their preferred candidate win in at least one extension of the partial votes of the non-manipulators;  -Opportunistic Manipulation (OM),  where the manipulators seek to vote in a way that makes their preferred candidate win in every viable extension of the partial votes of the non-manipulators;  -Strong Manipulation (SM),  where the manipulators seek to vote in a way that makes their preferred candidate win in every extension of the partial votes of the non-manipulators.  We study the computational complexity of the WM, OM, and the SM problems for commonly used voting rules such as plurality, veto, k-approval, k-veto, maximin, Copeland, Bucklin, simplified Bucklin, Fallback, and simplified Fallback. Our key finding is that, barring a few exceptions, manipulation turns out to be a significantly harder problem in the setting of incomplete votes. Our results therefore strengthen the view that manipulation may not be practical if we restrict the information the manipulators have about the votes of other voters.", 
    "authors": [
      {
        "name": "Palash Dey"
      }, 
      {
        "name": "Neeldhara Misra"
      }, 
      {
        "name": "Narahari Yadati"
      }
    ], 
    "keywords": "voting, manipulation, partial information, algorithm, social choice, theory", 
    "title": "Complexity of Manipulation with Partial Information", 
    "type": "paper"
  }, 
  "1069": {
    "abstract": "Approximate matrix multiplication (AMM) becomes increasingly popular because it can  make matrix factorization methods find their usage on large-scale datasets. Most previous work is based on the idea of random selection or random projection. In this paper, we propose a  deterministic algorithm for computing an approximation to the product of two given matrices. Moreover, the algorithm works in a streaming manner. In particular, our approach is based on a recently proposed matrix sketching algorithm called Frequent Directions (FD). It has stronger error bound than both random selection and random projection algorithms with respect to the same space complexity. Our approach also leads to an algorithm for computing the Canonical Correlation Analysis (CCA) of two matrices exactly in a streaming way, which takes less space than the classical method. Experimental results validate the effectiveness of our method.", 
    "authors": [
      {
        "name": "Qiaomin Ye"
      }, 
      {
        "name": "Zhihua Zhang"
      }
    ], 
    "keywords": "Streaming, Matrix multiplication, CCA", 
    "title": "Frequent Direction Algorithms for Approximate Matrix Multiplication with Applications in CCA", 
    "type": "paper"
  }, 
  "1073": {
    "abstract": "We study the multiple-play budgeted multi-armed bandit (MP-BMAB) problem, in which pulling an arm receives both a random reward and a random cost, and a player pulls $L(\\ge1)$ arms at each round. The player targets at maximizing her total expected reward under a budget constraint $B$ for the pulling costs. We present a multiple ratio confidence bound policy: At each round, we first calculate a truncated upper (lower) confidence bound for the expected reward (cost) of each arm, and then pull the $L$ arms with the maximum ratio of the sum of the upper confidence bounds of rewards to the sum of the lower confidence bounds of costs. We design a $0$-$1$ integer linear fractional programming oracle that can pick such the $L$ arms within polynomial time. We prove that the regret of our policy is sublinear in general and is log-linear for certain parameter settings. We further consider two special cases of MP-BMABs: (1) We derive a lower bound for any consistent policy for MP-BMABs with Bernoulli reward and cost distributions. (2) We show that the proposed policy can also solve conventional budgeted MAB problem (a special case of MP-BMABs with $L = 1$) and provides better theoretical results than existing UCB-based pulling policies.", 
    "authors": [
      {
        "name": "Yingce Xia"
      }, 
      {
        "name": "Tao Qin"
      }, 
      {
        "name": "Weidong Ma"
      }, 
      {
        "name": "Tie-Yan Liu"
      }
    ], 
    "keywords": "online learning, budgeted multi-armed bandit, multiple play, upper and lower bounds", 
    "title": "Budgeted Multi-armed Bandits with Multiple Plays", 
    "type": "paper"
  }, 
  "1075": {
    "abstract": "Plan Recognition algorithms require to recognize not only the goal of the agent, but also a complete sequence or hierarchy explaining the agent's actions. While the output of such algorithms is informative, the cost of its calculation is high and such algorithms usually need to pay in either runtime, space, completeness or expressibility. Moreover, performing plan recognition online requires the observing agent to reason about future actions that have not yet been seen and maintain a set of hypotheses to support all possible options. To address this problem, this paper presents a new and efficient algorithm for online plan recognition called SLIM (Semi-Lazy Inference Mechanism). It uses both a bottom-up and top-down parsing processes, which allow it to commit only to the minimum necessary actions in real-time, but still provide complete hypotheses on demand. We show both theoretically and empirically that although this process is still exponential, there is a significant improvement in runtime when compared to the state of the art.", 
    "authors": [
      {
        "name": "Reuth Mirsky"
      }, 
      {
        "name": "Kobi Gal"
      }
    ], 
    "keywords": "plan recognition, activity recognition, Knowledge Representation", 
    "title": "SLIM: Semi-Lazy Inference Mechanism for Plan Recognition", 
    "type": "paper"
  }, 
  "1076": {
    "abstract": "We analyze how complex a heuristic function must be to directly guide a state-space search algorithm towards the goal. As a case study we examine functions that evaluate states with a weighted sum of state features. We measure the complexity of a domain by the complexity of the required features. We analyze conditions under which the search algorithm runs in polynomial time and show complexity results for several classical planning domains.", 
    "authors": [
      {
        "name": "Jendrik Seipp"
      }, 
      {
        "name": "Florian Pommerening"
      }, 
      {
        "name": "Gabriele R\u00f6ger"
      }, 
      {
        "name": "Malte Helmert"
      }
    ], 
    "keywords": "classical planning, heuristic search, potential heuristics, state space topology", 
    "title": "Correlation Complexity of Classical Planning Domains", 
    "type": "paper"
  }, 
  "1080": {
    "abstract": "In this paper, we propose a novel method to improve object recognition accuracies of convolutional neural networks (CNNs) by embedding the proposed Min-Max objective into a high layer of the models during the training process. The Min-Max objective  explicitly enforces the learned object feature maps to have the minimum compactness for each object manifold and the maximum margin between different object manifolds. The Min-Max objective can be universally applied to different CNN models with negligible additional computation cost. Experiments with shallow and deep models on four benchmark datasets including CIFAR-10, CIFAR-100, SVHN and MNIST demonstrate that CNN models trained with the Min-Max objective achieve remarkable performance improvements compared to the corresponding baseline models.", 
    "authors": [
      {
        "name": "Weiwei Shi"
      }, 
      {
        "name": "Yihong Gong"
      }, 
      {
        "name": "Jinjun Wang"
      }
    ], 
    "keywords": "Convolutional Neural Network, Min-Max Objective, Object Recognition, Deep Learning", 
    "title": "Improving CNN Performance with Min-Max Objective", 
    "type": "paper"
  }, 
  "1094": {
    "abstract": "Recently, the long short-term memory neural network (LSTM) has attracted wide interest due to its success in many tasks. The LSTM architecture looks similar to the neuronal networks in the brain, however there still lacks the evidence of the cognitive plausibility of LSTM architecture as well as its working mechanism. In this paper, we study the alignment between the artificial memory vector used by LSTM and the brain activity observed via fMRI when the subjects read a story. Experiment results show that the artificial memory vector in LSTM has the ability to accurately predict the observed sequential brain activities, which also indicates the similar mechanism of working memory shared by the LSTM and the cognitive process of story reading.", 
    "authors": [
      {
        "name": "Peng Qian"
      }, 
      {
        "name": "Xipeng Qiu"
      }, 
      {
        "name": "Xuanjing Huang"
      }
    ], 
    "keywords": "LSTM, Brain activity, Cognitive plausibility", 
    "title": "Bridging LSTM Architecture with the Neural Dynamics of Reading", 
    "type": "paper"
  }, 
  "1096": {
    "abstract": "We introduce a framework for knowledge-based sequence mining based on Answer Set Programming (ASP). We begin by modeling the basic task and refine it in the sequel in several ways. First, we show how easily condensed patterns can be extracted by modular extensions of the basic approach. Second, we illustrate how ASP's preference handling capacities can be exploited for mining patterns of interest. In doing so, we demonstrate the ease of incorporating knowledge into the ASP-based mining process. To assess the trade-off in effectiveness, we provide an empirical study comparing our approach with a related sequence mining mechanism.", 
    "authors": [
      {
        "name": "Martin Gebser"
      }, 
      {
        "name": "Thomas Guyet"
      }, 
      {
        "name": "Ren\u00e9 Quiniou"
      }, 
      {
        "name": "Javier Romero"
      }, 
      {
        "name": "Torsten Schaub"
      }
    ], 
    "keywords": "Answer Set Programming, Sequence Mining, Data Mining", 
    "title": "Knowledge-based Sequence Mining with ASP", 
    "type": "paper"
  }, 
  "1097": {
    "abstract": "Autonomous mobile robots navigate in our spaces by planning and executing routes to destinations. When a mobile robot appears at some user's location, there is no clear way to understand what navigational path the robot planned and experienced just by looking at it. In this work, we address the problem of generating narrations of autonomous mobile robot route experiences. We contribute the concept of verbalization as a parallel to the well-studied concept of visualization. Through verbalizations, robots can describe what they experience as they traverse their paths. For every executed path, we consider many possible verbalizations that could be generated. We introduce the verbalization space that covers the variability of utterances that the robot may use to narrate its actions to different humans. We present an algorithm for segmenting a path and mapping each segment to an utterance, as a function of the desired point in the verbalization space, and demonstrate its application using our mobile service robot moving in our buildings. We believe our verbalization space and algorithm are applicable for many mobile robots, including autonomous cars.", 
    "authors": [
      {
        "name": "Stephanie Rosenthal"
      }, 
      {
        "name": "Sai Pandi Selvaraj"
      }, 
      {
        "name": "Manuela Veloso"
      }
    ], 
    "keywords": "verbalization, explanation, planning, robotics", 
    "title": "Verbalization: Narratives of Autonomous Robot Experience", 
    "type": "paper"
  }, 
  "1098": {
    "abstract": "Understanding the behavior of belief change operators for fragments of classical logic has received increasing interest over the last years. Results in this directions are mainly concerned with adapting representation theorems. However, fragment-driven belief change also leads to novel research questions. In this paper, we propose the concept of belief distribution which can be understood as the reverse task of merging. More specifically, we are interested in the following question: Given an arbitrary knowledge base $K$ and some merging operator $\\Delta$, can we find a profile $E$ and a constraint $\\mu$, both from a given fragment of classical logic, such that $\\Delta_\\mu(E)$ yields a result equivalent to $K$? In other words, can we distribute $K$ into knowledge bases of simpler structure, such that the task of merging allows for a reconstruction of the original knowledge.  Our initial results show that merging based on drastic distance allows for an easy distribution of knowledge, while the power of distribution for operators based on Hamming distance heavily relies on the fragment of choice.", 
    "authors": [
      {
        "name": "Adrian Haret"
      }, 
      {
        "name": "Jean-Guy Mailly"
      }, 
      {
        "name": "Stefan Woltran"
      }
    ], 
    "keywords": "belief change, belief merging, fragments", 
    "title": "Distributing Knowledge into Simple Bases", 
    "type": "paper"
  }, 
  "1099": {
    "abstract": "Big brother is watching but his eyesight is not all that great, since he only has partial observability of the environment. In such a setting agents may be able to preserve their privacy by hiding their true goal, following paths that may lead to multiple goals. In this work we present a framework that supports the offline analysis of goal recognition settings with non-deterministic system sensor models, in which the observer has partial (and possibly noisy) observability of the agent\u00e2\u0080\u0099s actions, while the agent is assumed to have full observability of his environment. In particular, we propose a new variation of worst case distinctiveness (wcd), a measure that assesses the ability to perform goal recognition within a model. We describe a new efficient way to compute this measure via a novel compilation to classical planning. In addition, we offer a decision support tool for agents that wish to keep their goal ambiguous as long as possible to preserve privacy. Our empirical evaluation shows the feasibility of the proposed solution.", 
    "authors": [
      {
        "name": "Sarah Keren"
      }, 
      {
        "name": "Avigdor Gal"
      }, 
      {
        "name": "Erez Karpas"
      }
    ], 
    "keywords": "Goal recognition design, Plan recognition, Compilation to classical planning, Partial observability, Privacy", 
    "title": "Privacy Preserving Plans in Partially Observable Environments", 
    "type": "paper"
  }, 
  "1101": {
    "abstract": "We investigate the problem of conservative rewritability of a TBox T in a  description logic L into a TBox T' in a weaker description logic L'. We focus on model-conservative rewritability (T' entails T and all models of T are expandable to models of T'), subsumption-conservative rewritability (T' entails T and all subsumptions in the signature of T entailed by T' are entailed by T ), and standard description logics between ALC and ALCQI. We give model-theoretic characterizations of conservative rewritability via bisimulations, inverse p-morphisms and generated subinterpretations, and use them to obtain a few rewriting algorithms and complexity results for deciding rewritability.", 
    "authors": [
      {
        "name": "Boris Konev"
      }, 
      {
        "name": "Carsten Lutz"
      }, 
      {
        "name": "Frank Wolter"
      }, 
      {
        "name": "Michael Zakharyaschev"
      }
    ], 
    "keywords": "Description Logic, Ontology, conservative extension", 
    "title": "Conservative Rewritability of Description Logic TBoxes", 
    "type": "paper"
  }, 
  "1102": {
    "abstract": "Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, in most previous works, the models are learned based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we use the multi-task learning framework to jointly learn across multiple related tasks. Based on recurrent neural network, we propose three different mechanisms of sharing information to model text with task-specific and shared layers. The entire network is trained jointly on all these tasks. Experiments on four benchmark text classification tasks show that our pro- posed models can improve the performance of a task with the help of other related tasks.", 
    "authors": [
      {
        "name": "Pengfei Liu"
      }, 
      {
        "name": "Xipeng Qiu"
      }, 
      {
        "name": "Xuanjing Huang"
      }
    ], 
    "keywords": "different shared mechanisms of RNN, Multi-task learning, global gate, visualization", 
    "title": "Recurrent Neural Network for Text Classification with Multi-Task Learning", 
    "type": "paper"
  }, 
  "1109": {
    "abstract": "We investigate the problem whether two ALC knowledge bases are indistinguishable by queries over a given vocabulary. We give model-theoretic criteria and prove that this problem is undecidable for conjunctive queries (CQs) but decidable in 2EXPTIME for unions of rooted CQs. We also consider the problem whether two ALC TBoxes give the same answers to queries over a given vocabulary, for any ABox, and show that this problem is again undecidable for CQs, 2EXPTIME-complete for Horn-ALC, and EXPTIME-complete for Horn-ALC when restricted to (unions of) rooted CQ.", 
    "authors": [
      {
        "name": "Elena Botoeva"
      }, 
      {
        "name": "Carsten Lutz"
      }, 
      {
        "name": "Vladislav Ryzhikov"
      }, 
      {
        "name": "Frank Wolter"
      }, 
      {
        "name": "Michael Zakharyaschev"
      }
    ], 
    "keywords": "Description Logic, conjunctive query, ontology-based data management", 
    "title": "Query Entailment and Inseparability for ALC Ontologies", 
    "type": "paper"
  }, 
  "1114": {
    "abstract": "Participants  in volunteer-based crowdsourcing platforms cannot be motivated with monetary incentives, making it challenging to keep them engaged in the system.  The goal of this paper is to increase the engagement of participants  in volunteer-based crowdsourcing by combining machine learning with intervention strategies.  It describes an approach that generates intervention messages to participants who  are predicted to disengage from the system above a given threshold. The  intervention messages were chosen to   directly address motivational factors that affect participants'  engagement.  We evaluated this approach on  Galaxy Zoo, the largest  citizen science platform on the web, where we traced the behavior and contributions of thousands of participants who received intervention messages over a period of a few months.   We found that a message emphasizing the helpfulness of individual participants was able to significantly increase participants' contributions in the system (as compared to other types of messages and a control condition) when timed  intelligently according to the  prediction model, but not  randomly.  The impact of the message on participants'  contributions was more pronounced as additional data was collected from users and made available to the classifier. We  also found that  an  agnostic  threshold of 0.5 provided the best contributions from participants who received the  helpfulness message.  The contribution of  this work is in showing that both the message type and  intelligent timing  are crucial aspects of intervention design for alleviating disengagement in crowdsourcing.", 
    "authors": [
      {
        "name": "Avi Segal"
      }, 
      {
        "name": "Kobi Gal"
      }, 
      {
        "name": "Ece Kamar"
      }, 
      {
        "name": "Eric Horvitz"
      }, 
      {
        "name": "Alex Bowyer"
      }, 
      {
        "name": "Grant Miller"
      }
    ], 
    "keywords": "crowdsourcing, web-based collaboration, web-based personalization", 
    "title": "Intervention  Strategies for Increasing Engagement in Volunteer-Based Crowdsourcing", 
    "type": "paper"
  }, 
  "1126": {
    "abstract": "We propose a query language LARE for graphs whose edges are labelled by a finite alphabet and nodes store unbounded data  values. LARE is based on a variant of regular expressions with  memory. Queries of LARE can compare quantities of memorised  graph nodes and their neighbourhoods.These features allow us  to express a number of natural properties, while keeping the  data complexity (with a query fixed) in non-deterministic  logarithmic space. We establish an algorithm that works efficiently not only with LARE, but also with a wider language defined using  effective relational conditions, another formalism we propose.", 
    "authors": [
      {
        "name": "Maciej Grabo\u0144"
      }, 
      {
        "name": "Jakub Michaliszyn"
      }, 
      {
        "name": "Jan Otop"
      }, 
      {
        "name": "Piotr Wieczorek"
      }
    ], 
    "keywords": "query-answering, graph databases, graph queries, path queries", 
    "title": "Querying Data Graphs with Arithmetical Regular Expressions", 
    "type": "paper"
  }, 
  "1127": {
    "abstract": "Standard incremental parsing algorithm employs a single scoring function and beam-search to find the best parse tree from exponentially large search space. Inspired by recently proposed HC-search framework, we decompose the incremental parsing algorithm into two steps: first searching a set of high-quality outputs with beam-search, and second selecting the best output from these outputs with a ranking model. We learn our incremental parsing model with a relaxed learning objective. We incorporate arbitrary features in our ranking model and learn the model from fine grain ranking examples. Experimental results on standard English and Chinese dataset show our method significantly outperforms a strong baseline.", 
    "authors": [
      {
        "name": "Yijia Liu"
      }, 
      {
        "name": "Wanxiang Che"
      }, 
      {
        "name": "Bing Qin"
      }, 
      {
        "name": "Ting Liu"
      }
    ], 
    "keywords": "natural language processing, incremental parsing, structured prediction", 
    "title": "HC-search for Incremental Parsing", 
    "type": "paper"
  }, 
  "1128": {
    "abstract": "This paper describes an application of Answer Set Programming (ASP) to crop allocation for generating realistic landscapes. The aim is to cover optimally a bare landscape, represented by its plot graph, with spatial patterns describing local arrangements of crops. This  problem belongs to the hard class of graph packing problems and is modelled in the framework of ASP. The approach provides a compact solution to the basic problem and at the same time allows extensions such as a flexible integration of expert knowledge. Particular attention is paid to the treatment of symmetries, especially due to sub-graph isomorphism issues.  Experiments were conducted on a database of simulated and real landscapes. Currently, the approach can generate graphs of medium size, a size that enables studies on real agricultural practices.", 
    "authors": [
      {
        "name": "Thomas Guyet"
      }, 
      {
        "name": "Yves Moinard"
      }, 
      {
        "name": "Jacques Nicolas"
      }, 
      {
        "name": "Ren\u00e9 Quiniou"
      }
    ], 
    "keywords": "logic programming, graphs processing, environmental application", 
    "title": "Packing graphs with ASP for landscape simulation", 
    "type": "paper"
  }, 
  "1130": {
    "abstract": "Previous learning based hand pose estimation methods suffer from the lacking of prior information in hand model geometry. They usually require a separate model fitting step to generate geometrically valid hand poses. Such post processing is inconvenient and sub-optimal. In this work, we propose a model based deep learning approach that fully exploits the hand geometry constraints. A forward kinematics based layer is adopted in the network training to ensure the validity of estimated poses. For the first time, we show that training a network with a non-linear generative process for hand pose estimation is feasible and efficient. Our approach is verified on challenging public datasets and achieves state-of-the-art performance.", 
    "authors": [
      {
        "name": "Xingyi Zhou"
      }, 
      {
        "name": "Qingfu Wan"
      }, 
      {
        "name": "Wei Zhang"
      }, 
      {
        "name": "Xiangyang Xue"
      }, 
      {
        "name": "Yichen Wei"
      }
    ], 
    "keywords": "Deep learning, Hand pose estimation, Human-computer interaction", 
    "title": "Model-based Deep Hand Pose Estimation", 
    "type": "paper"
  }, 
  "1135": {
    "abstract": "We present an unsupervised approach for recognition of Activities of Daily Living (ADL) in a smart home. Activity recognition is an important enabling technology, for example to tackle the healthcare requirements of elderly people in their homes. The technique applied most often is supervised learning, which relies on expensive labelled data and lacks the flexibility to learn new activities. Building on ideas from text mining, we present a powerful topic model and a segmentation algorithm that can learn from unlabelled sensor data. The model has been evaluated extensively on datasets collected from real smart homes. The results demonstrate that this approach can successfully recognise the ADL of residents, and can be effectively used in a range of applications, such as detection of abnormal activities, monitoring of sleep quality, among many others.", 
    "authors": [
      {
        "name": "Yu Chen"
      }, 
      {
        "name": "Tom Diethe"
      }, 
      {
        "name": "Peter Flach"
      }
    ], 
    "keywords": "Topic Model, Activity of Daily Living, Smart Home, Unsupervised Learning, Sensor Data, Segmentation, Time Series", 
    "title": "ADL\u2122: A Topic Model for Recognition of Activities of Daily Living in a Smart Home", 
    "type": "paper"
  }, 
  "1141": {
    "abstract": "A disjunctive logic program under the answer set semantics can be equivalently translated to a normal logic program by the shifting transformation, if the program is head-cycle-free. In this paper, we provide an answer-set-preserving rewriting of a general disjunctive program to a normal program by first applying the unfolding transformation on atoms that prevent the program to be head-cycle-free, then shifting the resulting program. Different from other transformations that eliminate disjunctions in answer set programming, the new rewriting is efficient for \u00e2\u0080\u009calmost\u00e2\u0080\u009d head-cycle-free programs, i.e., programs that have only a few atoms that prevent them to be head-cycle-free. Based on the new rewriting, we provide an anytime algorithm to compute answer sets of a disjunctive program by calling normal logic programming solvers. We also extend the rewriting to non-ground answer set programs on finite structures.", 
    "authors": [
      {
        "name": "Jianmin Ji"
      }, 
      {
        "name": "Hai Wan"
      }, 
      {
        "name": "Kewen Wang"
      }, 
      {
        "name": "Zhe Wang"
      }, 
      {
        "name": "Chuhan Zhang"
      }, 
      {
        "name": "Jiangtao Xu"
      }
    ], 
    "keywords": "Answer Set Programming, Eliminating Disjunctions, Restricted Unfolding, answer-set-preserving rewriting, head-cycle-free", 
    "title": "Eliminating Disjunctions in Answer Set Programming by Restricted Unfolding", 
    "type": "paper"
  }, 
  "1154": {
    "abstract": "We show how to generate ``really hard'' random instances for subgraph isomorphism problems. For the non-induced variant, we predict and observe a phase transition between satisfiable and unsatisfiable instances, with a corresponding complexity peak seen in three different solvers. For the induced variant, much richer behaviour is observed, and constrainedness gives a better measure of difficulty than does proximity to a phase transition. We also discuss variable and value ordering heuristics, and their relationship to the expected number of solutions.", 
    "authors": [
      {
        "name": "Ciaran McCreesh"
      }, 
      {
        "name": "James Trimble"
      }, 
      {
        "name": "Patrick Prosser"
      }
    ], 
    "keywords": "subgraph isomorphism, phase transitions, really hard problems, variable ordering heuristics, value ordering heuristics, backtracking search algorithms, graph algorithms, random graphs", 
    "title": "Heuristics and Really Hard Instances for Subgraph Isomorphism Problems", 
    "type": "paper"
  }, 
  "1157": {
    "abstract": "In this paper a new dictionary learning algorithm for multidimensional data is proposed. Unlike most conventional dictionary learning methods which are derived for dealing with vectors or matrices, our algorithm, named K-TSVD, learns a multidimensional dictionary directly via a novel algebraic approach for tensor factorization as proposed in [3, 12, 13]. Using this approach one can define a tensor-SVD and we propose to extend K-SVD algorithm used for 1-D data to a K-TSVD algorithm for handling 2-D and 3-D data. Our algorithm, based on the idea of sparse coding (using group-sparsity over multidimensional coefficient vectors), alternates between estimating a compact representation and dictionary learning. We analyze our K-TSVD algorithm and demonstrate its result on video completion and video/multispectral image denoising.", 
    "authors": [
      {
        "name": "Zemin Zhang"
      }, 
      {
        "name": "Shuchin Aeron"
      }
    ], 
    "keywords": "dictionary learning, multidimension, tensor, t-SVD, video completion, video denoting, multispectral image denoising", 
    "title": "Denoising and Completion of 3D Data via Multidimensional Dictionary Learning", 
    "type": "paper"
  }, 
  "1163": {
    "abstract": "We present a novel filtering algorithm for the Cumulative constraint based on a new energetic relaxation. We introduce a general form of the Overload Check and Edge-Finder rules based on a function computing the earliest completion time for a set of tasks. Depending on the relaxation used to compute this function, one obtains different levels of filtering. We present two algorithms that enforce these rules. The algorithms utilize a novel data structure that we call the Profile and that encodes the resource utilization over time. Experiments show that these algorithms are competitive with the state of the art algorithms by doing a greater filtering and having a faster runtime.", 
    "authors": [
      {
        "name": "Vincent Gingras"
      }, 
      {
        "name": "Claude-Guy Quimper"
      }
    ], 
    "keywords": "Constraint Programming, Scheduling, Global Constraint, Edge Finder, Cumulative Constraint, Filtering Algorithms", 
    "title": "Generalizing the Edge-Finder Rule for the Cumulative Constraint", 
    "type": "paper"
  }, 
  "1166": {
    "abstract": "Model-Based Diagnosis is a principled approach to identify the possible causes when a system under observation behaves unexpectedly. In case the number of possible explanations for the unexpected behavior is large, sequential diagnosis approaches can be applied. The strategy of such approaches is to iteratively take additional measurements - either through an automated process or by asking the user - to narrow down the set of alternatives in order to find the true cause of the problem.  In this paper we propose a sound and complete sequential diagnosis approach which does not require any information about the structure of the diagnosed system. The method is based on the new concept of \"partial\" diagnoses and uses the recent MergeXplain algorithm to efficiently determine a small number of minimal conflicts and partial diagnoses in each iteration. An experimental evaluation on different benchmark problems shows that our approach needs significantly less computation time when compared with an existing domain-independent approach to sequential diagnosis.", 
    "authors": [
      {
        "name": "Kostyantyn Shchekotykhin"
      }, 
      {
        "name": "Thomas Schmitz"
      }, 
      {
        "name": "Dietmar Jannach"
      }
    ], 
    "keywords": "Model-Based Diagnosis, Sequential Diagnosis, Interactive Reasoning", 
    "title": "Efficient Sequential Model-Based Fault-Localization with Partial Diagnoses", 
    "type": "paper"
  }, 
  "1170": {
    "abstract": "Molecular robots (nanobots) are being developed for biomedical applications, e.g., to deliver medications without worrying about  side-effects. Future treatments will  require swarms of heterogeneous nanobots  We present a novel approach to generating such swarms  from a treatment program. A compiler translates medications, written in a rule-based language, into specifications of a swarm built by specializing generic nanobot platforms to specific payloads and action-triggering behavior. The mixture of nanobots, when deployed, carries out the treatment program. We describe the medication programming language, and the associated compiler. We prove the validity of the compiler output, and report on in-vitro experiments using generated nanobot swarms.", 
    "authors": [
      {
        "name": "Inbal Wiesel"
      }, 
      {
        "name": "Gal A. Kaminka"
      }, 
      {
        "name": "Noa Agmon"
      }, 
      {
        "name": "Ido Bachelet"
      }, 
      {
        "name": "Guy Hachmon"
      }
    ], 
    "keywords": "nanorobots, molecular robotics, rule-based languages, molecular programming, nanobots", 
    "title": "Rule-Based Programming of Molecular Robot Swarms for Biomedical Applications", 
    "type": "paper"
  }, 
  "1172": {
    "abstract": "Statistical models such as Markov chains have recently started to be studied for the purpose of Procedural Content Generation (PCG) for video games. A major problem with this approach is how to control the sampling process in order to obtain output satisfying some desired constraints. In this paper we describe and compare three approaches to constraining the content generated using multi-dimensional Markov chains (MdMC): (1) a generate and test approach that simply keeps resampling the content until the desired constraints are satisfied. (2) An approach that determines parts of the generated content that if resampled could result in satisfying the constraints. (3) An incremental method that checks for constraint violations during sampling, triggering resampling the last generated section if necessary. We test our approaches by generating maps for two classic video games, \"Super Mario Bros.\" and \"Kid Icarus.\"", 
    "authors": [
      {
        "name": "Sam Snodgrass"
      }, 
      {
        "name": "Santiago Onta\u00f1\u00f3n"
      }
    ], 
    "keywords": "Procedural Content Generation, Markov Chains, Constrained Sampling", 
    "title": "Controllable Procedural Content Generation via Constrained Multi-Dimensional Markov Chain Sampling", 
    "type": "paper"
  }, 
  "1174": {
    "abstract": "We propose an framework for a service robot to behave intelligently in domains that contain incomplete information, underspecified goals and dynamic change. Human robot interaction (HRI), sensing actions and physical actions are uniformly formalized in action language BC. An answer set solver is then called to generate plans that guide the robot to acquire task-oriented knowledge and execute actions to achieve its goal, including interacting with human to gather information and sensing the environment to help motion planning. By continuously interpreting and grounding useful sensing information, robot is able to use contingent knowledge to adapt to unexpected changes and faults. The framework achieves technical soundness, architectural simplicity and high robustness. We evaluate the approach on a real mobile robot that serves drink to guests, a testing benchmark for general purpose service robot proposed by Robocup@Home competition.", 
    "authors": [
      {
        "name": "Kai Chen"
      }, 
      {
        "name": "Fangkai Yang"
      }, 
      {
        "name": "Xiaoping Chen"
      }
    ], 
    "keywords": "service robot, automated planning, human robot interaction, answer set programming, action language", 
    "title": "Planning with Task-oriented Knowledge Acquisition for A Service Robot", 
    "type": "paper"
  }, 
  "1180": {
    "abstract": "Protein secondary structure prediction is an important problem in bioinformatics. Inspired by the recent successes of artificial neural networks, in this paper, we propose an end-to-end neural network model that predicts protein secondary structures from integrated local and global contextual features. Our neural network architecture leverages convolutional neural networks with different kernel sizes to extract multiscale local contextual features. In addition, considering long-range dependencies existing in amino acid sequences, we set up a bidirectional neural network consisting of gated recurrent units to capture global contextual features. Furthermore, multi-task learning is utilized to predict secondary structure labels and amino-acid solvent accessibility simultaneously. The effectiveness of our proposed model is demonstrated by its state-of-the-art performance, i.e., 69.7% Q8 accuracy on the public benchmark CB513.", 
    "authors": [
      {
        "name": "Zhen Li"
      }, 
      {
        "name": "Yizhou Yu"
      }
    ], 
    "keywords": "Multiscale convolutional neural networks (Multiscale CNNs), Bidirectional gated recurrent units (BGRUs), Local and global contexts", 
    "title": "Protein Secondary Structure Prediction Using Cascaded Convolutional and Recurrent Neural Networks", 
    "type": "paper"
  }, 
  "1181": {
    "abstract": "The performance of a recommendation system relies heavily on the feedback of users. Most of the traditional recommendation algorithms based only on historical ratings will encounter severe difficulties given the problem of data sparsity. Users' feedback usually contains rich textual reviews in addition to numeric ratings. In this paper, we exploit textual review information, as well as ratings, to model user preferences and item features in a shared topic space and subsequently introduce them into a matrix factorization model for recommendation. To this end, the data sparsity problem is alleviated and good interpretability of the recommendation results is gained. Another contribution of this work is that we model the item feature distributions with rating-boosted reviews which combine textual reviews with user sentiments.   Experimental results on 26 real-world datasets from Amazon demonstrate that our approach significantly improves the rating prediction accuracy compared with various state-of-art models, such as SVD++, LFM and HFT which also exploits ratings and reviews simultaneously. In particular,  higher improvement is achieved for users who have few ratings, which verifies the effectiveness of the proposed approach for sparse data. Moreover, our method also benefits much from reviews on top-N recommendation tasks.", 
    "authors": [
      {
        "name": "Yunzhi Tan"
      }, 
      {
        "name": "Min Zhang"
      }, 
      {
        "name": "Yiqun Liu"
      }, 
      {
        "name": "Shaoping Ma"
      }
    ], 
    "keywords": "Recommendation System, Latent Topics, Collaborative Filtering, Textual Reviews", 
    "title": "Rating-Boosted Latent Topics: Understanding Users and Items with Ratings and Reviews", 
    "type": "paper"
  }, 
  "1184": {
    "abstract": "The problem of voting outcome manipulation is both a fundamental theoretical problem in social choice, as well as a major practical concern to democratic institutions. Consequently, this issue has received considerable attention, particularly as it pertains to different voting rules. In contrast, the problem of how election control can be prevented or deterred has been largely ignored. We introduce the problem of optimal protection against election control, where manipulation is allowed at the granularity of groups of voters (e.g., voting locations), through a denial-of-service attack, and the defender allocates limited protection resources to prevent control. We show that for plurality voting, election control through group deletion to prevent a candidate from winning is in P, while it is NP-Hard to prevent such control. We then present a double-oracle framework for computing an optimal prevention strategy, developing exact mixed-integer linear programming formulations for both the defender and attacker oracles (both of these subproblems we show to be NP-Hard), as well as heuristic oracles. Experiments conducted on both synthetic and real data demonstrate that the proposed computational framework can scale to realistic problem instances.", 
    "authors": [
      {
        "name": "Yue Yin"
      }, 
      {
        "name": "Yevgeniy Vorobeychik"
      }, 
      {
        "name": "Bo An"
      }, 
      {
        "name": "Noam Hazon"
      }
    ], 
    "keywords": "Plurality voting, Election control, Stackelberg game", 
    "title": "Optimally Protecting Elections", 
    "type": "paper"
  }, 
  "1187": {
    "abstract": "One approach to preference learning, based on linear support vector machines,involves choosing a weight vector whose associated hyperplane has maximum margin with respect to an input set of preference vectors, and using this to compare feature vectors. However, as is well known, the result can be sensitive to how each feature is scaled, so that rescaling can lead to an essentially different vector. This gives rise to a set of possible weight vectors - which we call the rescale-optimal ones - considering all possible rescalings. From this set one can define a more cautious preference relation, in which one vector is preferred to another if it is preferred for all rescaled-optimal weight vectors. In this paper, we analyse which vectors are rescale-optimal,and when there is a unique rescale-optimal vector, and consider how to compute the induced preference relation.", 
    "authors": [
      {
        "name": "Nic Wilson"
      }, 
      {
        "name": "Mojtaba Montazery"
      }
    ], 
    "keywords": "Preference inference, preference learning, computation of preference", 
    "title": "Preference Inference Through Rescaling Preference Learning", 
    "type": "paper"
  }, 
  "1192": {
    "abstract": "Most of human mobility big data available by now, for example call detail records or twitter data with geotag, are always sparse and heavily biased, as a result, using such kind of data directly to represent real-world human mobility is unreliable and problematic. However, difficult though it is, a completion of human mobility turns out to be a promising way to minimize the issues of sparsity and bias. In this paper, we model the completion problem as a recommender system and therefore solve this problem in a collaborative filtering (CF) framework. We propose a spatio-temporal CF that simultaneously infers the topic distribution over users, time-of-days, days as well as locations, and then use the topic distributions to estimate a posterior over locations and infer the optimal location sequence in a Hidden Markov Model considering the spatio-temporal continuity. We apply and evaluate our algorithm using a real-world Call Detail Records dataset from Bangladesh and give an application, namely Dynamic Census using our algorithm.", 
    "authors": [
      {
        "name": "Zipei Fan"
      }, 
      {
        "name": "Ayumi Arai"
      }, 
      {
        "name": "Xuan Song"
      }, 
      {
        "name": "Apichon Witayangkurn"
      }, 
      {
        "name": "Hiroshi Kanasugi"
      }, 
      {
        "name": "Ryosuke Shibasaki"
      }
    ], 
    "keywords": "Human Mobility Modeling, Trajectory Completion, Spatio-temporal data mining, Collaborative filtering", 
    "title": "A Collaborative Filtering Approach to Citywide Human Mobility Completion from Sparse Call Records", 
    "type": "paper"
  }, 
  "120": {
    "abstract": "Kernel alignment maximization criterion has been widely used for multiple kernel clustering. However, we observe that maximizing the kernel alignment globally: i) cannot effectively handle the variations among samples; and ii) does not well utilize the underlying structure of the data space, leading to unsatisfying clustering performance. We propose a novel multiple kernel clustering algorithm with local kernel alignment maximization to improve this situation. In specific, instead of aligning a group of pre-specified kernels globally, our algorithm only requires that the $k$-nearest neighbors of each sample is well aligned locally with the corresponding ideal kernel matrix. A two-step algorithm is developed to solve the resultant optimization problem. As experimentally demonstrated on six challenging MKL benchmark data sets, our algorithm significantly outperforms the state-of-the-art ones in the literature, verifying the effectiveness and advantages of maximizing the local kernel alignment.", 
    "authors": [
      {
        "name": "Miaomiao Li"
      }, 
      {
        "name": "Xinwang Liu"
      }, 
      {
        "name": "Lei Wang"
      }, 
      {
        "name": "Yong Dou"
      }, 
      {
        "name": "Jianping Yin"
      }, 
      {
        "name": "En Zhu"
      }
    ], 
    "keywords": "Multiple kernel learning, Kernel alignment maximization, Multi-view clustering", 
    "title": "Multiple Kernel Clustering with Local Kernel Alignment Maximization", 
    "type": "paper"
  }, 
  "1202": {
    "abstract": "We study a fair division problem with indivisible items, namely the computation of maximin share allocations.  Given a set of $n$ players, the maximin share of a single player is the best she can guarantee to herself, if she would partition the items in any way she prefers, into $n$ bundles, and then receive her least desirable bundle. The objective then is to find an allocation, so that each player is guaranteed her maximin share.  Previous works have studied this problem purely algorithmically, providing constant factor approximation algorithms. In this work, we embark on a mechanism design approach and investigate the existence of truthful mechanisms. We propose three models regarding the information that the mechanism attempts to elicit from the players, based on the cardinal and ordinal representation of preferences. We establish positive and negative (impossibility) results for each model and highlight the limitations imposed by truthfulness on the approximability of the problem. Finally, we pay particular attention to the case of two players, which already leads to challenging questions.", 
    "authors": [
      {
        "name": "Georgios Amanatidis"
      }, 
      {
        "name": "Georgios Birmpas"
      }, 
      {
        "name": "Evangelos Markakis"
      }
    ], 
    "keywords": "Fair Division, Maximin Share, Mechanism Design, Approximation Algorithms", 
    "title": "On Truthful Mechanisms for Maximin Share Allocations", 
    "type": "paper"
  }, 
  "1212": {
    "abstract": "This paper presents an effective local spatio-temporal descriptor for action recognition from depth video sequences. A depth sequence is divided into temporally overlapping depth segments with each segment contains a set of depth frames. Each segment of depth frames are used to generate three depth motion maps (DMMs) to capture the shape and motion cues. To cope with speed variations in actions, multiple frame lengths of depth segment are utilized, leading to a multi-temporal DMMs representation. Then, a set of local patch descriptors are extracted by partitioning the DMMs into dense patches and using the local binary patterns (LBP) descriptor to characterize local rotation invariant texture information in those patches. Further, the Fisher kernel is employed to encode the patch descriptors for a compact feature representation. Efficient classification is achieved by kernel extreme learning machine. Extensive experiments on the public MSR Action3D, MSR Gesture3D and DHA datasets show that our proposed method outperforms state-of-the-art approaches for depth-based action recognition.", 
    "authors": [
      {
        "name": "Chen Chen"
      }, 
      {
        "name": "Mengyuan Liu"
      }, 
      {
        "name": "Baochang Zhang"
      }, 
      {
        "name": "Jungong Han"
      }, 
      {
        "name": "Junjun Jiang"
      }, 
      {
        "name": "Hong Liu"
      }
    ], 
    "keywords": "Human action/gesture recognition, Vision-based action recognition, RGB-D camera, Depth sequences, Feature representation", 
    "title": "3D Action Recognition Using Multi-temporal Depth Motion Maps and Fisher Vector", 
    "type": "paper"
  }, 
  "1213": {
    "abstract": "A novel sparsity optimization method is proposed to select features for multi-class classification problems by directly optimizing a l_(2,p)-norm (0<p\u00e2\u0089\u00a41) based sparsity function subject to data-fitting inequality constraints to obtain large between-class margins. The direct sparse optimization method circumvents the empirical tuning of regularization parameters in existing sparse model based feature selection methods that adopt the sparsity model as a regularization term. To solve the direct sparsity optimization problem that is non-smooth and non-convex when 0<p<1, we propose an efficient iterative algorithm with proved convergence by converting it to a convex and smooth optimization problem at every iteration step. The proposed algorithm has been evaluated based on publicly available datasets, and extensive experiments have demonstrated that our algorithm could achieve feature selection performance competitive to state-of-the-art algorithms.", 
    "authors": [
      {
        "name": "Hanyang Peng"
      }, 
      {
        "name": "Yong Fan"
      }
    ], 
    "keywords": "Feature Selection, Sparse Reglarization, No Regularization Parameter", 
    "title": "Direct Sparsity Optimization Based Feature Selection for Multi-Class Classification", 
    "type": "paper"
  }, 
  "1219": {
    "abstract": "Providing sequence predictions that minimize Hamming loss is a challenging, but important, task. Directly minimizing this loss over a training sample is generally an NP-hard problem. Instead, existing sequence prediction methods minimize a convex upper bound that upper bounds the Hamming loss. Unfortunately, this often either leads to inconsistent predictors (e.g., max-margin methods) or predictions that are mismatched on the Hamming loss (e.g., conditional random fields). We present adversarial sequence classification, a consistent structured prediction framework for minimizing Hamming loss by pessimistically viewing uncertainty. Our approach pessimistically approximates the training data, yielding an adversarial game between the sequence predictor and the sequence labeler. We demonstrate the benefits of the approach on activity recognition and information extraction/segmentation tasks.", 
    "authors": [
      {
        "name": "Jia Li"
      }, 
      {
        "name": "Kaiser Asif"
      }, 
      {
        "name": "Hong Wang"
      }, 
      {
        "name": "Brian Ziebart"
      }, 
      {
        "name": "Tanya Berger-Wolf"
      }
    ], 
    "keywords": "sequence classification, adversarial game, time series", 
    "title": "Adversarial Sequence Classification", 
    "type": "paper"
  }, 
  "1224": {
    "abstract": "Probabilistic inference via model counting has emerged as a scalable technique with strong formal guarantees, thanks to recent advances in hashing-based approximate counting.  State-of-the-art hashing-based counting algorithms use an {\\NP} oracle, such that the number of oracle invocations grows linearly in the number of variables n in the input constraint. We present a new approach to hashing-based approximate model counting in which the number of oracle invocations grows logarithmically in $n$, while still providing strong theoretical guarantees.  Our experiments show that the new approach outperforms state-of-the-art techniques for approximate counting by 1-2 orders of magnitude in running time.", 
    "authors": [
      {
        "name": "Supratik Chakraborty"
      }, 
      {
        "name": "Kuldeep S. Meel"
      }, 
      {
        "name": "Moshe Y. Vardi"
      }
    ], 
    "keywords": "Approximate Probabilistic Inference, Approximate Model Counting, Hashing-based approaches", 
    "title": "Improving Approximate Counting for Probabilistic Inference: From Linear to Logarithmic SAT Solver Calls", 
    "type": "paper"
  }, 
  "1227": {
    "abstract": "In this paper we present a novel admissible pattern database heuristic (D) and tie-breaking rule (L) for Sokoban, allowing us to optimally solve 8 standard Sokoban instances more and to prove the optimality of 33 solutions in total compared to 26 of previous methods. The previously best heuristic for Sokoban (I) used the idea of an intermediate goal state to enable the effective use of pattern database heuristics in transportation domains where the mapping of movable objects to goal locations is not fixed beforehand. We extend this idea to allow the use of multiple intermediate goal states and show that I is no longer effective. We solve this problem and show that our heuristic D is effective in this situation. Sokoban is a well-known single-agent search problem characterized by a large branching factor, long solution lengths, and the presence of unsolvable states. Given the exponential growth in the complexity of instances in the standard set, the increase in the number of optimally solved instances represents a major advance in our understanding of how to search in extremely large search spaces.", 
    "authors": [
      {
        "name": "Andr\u00e9 Grahl Pereira"
      }, 
      {
        "name": "Robert Holte"
      }, 
      {
        "name": "Jonathan Schaeffer"
      }, 
      {
        "name": "Luciana Salete Buriol"
      }, 
      {
        "name": "Marcus Ritt"
      }
    ], 
    "keywords": "Single-Agent Search, Heuristic Search, Sokoban, Pattern Database", 
    "title": "Improved Heuristic and Tie-Breaking for Optimally Solving Sokoban", 
    "type": "paper"
  }, 
  "1228": {
    "abstract": "We consider the problem of query-driven repairing of inconsistent DL-Lite knowledge bases: query answers are computed under inconsistency-tolerant semantics, and the user provides feedback about which answers are erroneous or missing. The aim is to find a set of ABox modifications (deletions and additions), called a repair plan, that addresses as many of the defects as possible. After formalizing this problem and introducing different notions of optimality, we investigate the computational complexity of reasoning about optimal repair plans and propose interactive algorithms for computing such plans. For deletion-only repair plans, we also present a prototype implementation of the core components of the algorithm.", 
    "authors": [
      {
        "name": "Meghyn Bienvenu"
      }, 
      {
        "name": "Camille Bourgaux"
      }, 
      {
        "name": "Fran\u00e7ois Goasdou\u00e9"
      }
    ], 
    "keywords": "ontology-mediated query answering, interactive data repairing, inconsistency handling, description logics", 
    "title": "Query-driven Repairing of Inconsistent DL-Lite Knowledge Bases", 
    "type": "paper"
  }, 
  "123": {
    "abstract": "We study the extent to which online social networks can be connected to open knowledge bases. The problem is referred to as learning social knowledge graphs. We propose a multi-modal Bayesian embedding model, GenVector, to learn latent topics that generate word and network embeddings. GenVector leverages large-scale unlabeled data with embeddings and represents data of two modalities---i.e., social network users and knowledge concepts---in a shared latent topic space. Experiments on three datasets show that the proposed method clearly outperforms state-of-the-art methods. We then deploy the method on an online academic search system to connect a network of 38,049,189 researchers with a knowledge base with 35,415,011 concepts. In an online A/B test with live users, our method significantly decreases the error rate by 67%.", 
    "authors": [
      {
        "name": "Zhilin Yang"
      }, 
      {
        "name": "Jie Tang"
      }, 
      {
        "name": "William Cohen"
      }
    ], 
    "keywords": "social network, embedding, knowledge graph, generative model", 
    "title": "Multi-Modal Bayesian Embeddings for Learning Social Knowledge Graphs", 
    "type": "paper"
  }, 
  "1230": {
    "abstract": "Learning of user preferences has become a core issue in AI research. For example, recent studies investigate learning of Conditional Preference Networks (CP-nets) from partial information. To assess the optimality of learning algorithms as well as to better understand the combinatorial structure of CP-net classes, it is helpful to calculate certain learning-theoretic information complexity parameters. This paper provides theoretical justification for exact values (or in some cases bounds) of some of the most central information complexity parameters, namely the VC dimension, the (recursive) teaching dimension, the self-directed learning complexity, and the optimal mistake bound, for classes of acyclic CP-nets. We further provide an algorithm that learns tree-structured CP-nets from membership queries. Using our results on complexity parameters, we can assess the optimality of our algorithm as well as that of another query learning algorithm for acyclic CP-nets presented in the literature.", 
    "authors": [
      {
        "name": "Eisa Alanazi"
      }, 
      {
        "name": "Malek Mouhoub"
      }, 
      {
        "name": "Sandra Zilles"
      }
    ], 
    "keywords": "conditional preference networks, Query Learning, VC dimension, teaching dimension, membership queries", 
    "title": "The Complexity of Learning Acyclic CP-nets", 
    "type": "paper"
  }, 
  "1250": {
    "abstract": "We consider the task of predicting various traits of a person given an image of their face. We aim to estimate both objective traits, such as gender, ethnicity and age, as well as subjective traits, such as the emotion a person expresses or whether they are humorous or attractive. For sizeable experimentation, we contribute a new Face Attributes Dataset (FAD), comprising of roughly 200,000 attribute labels for the above traits, for over 10,000 facial images.   Due to the recent surge of research on Deep Convolutional Neural Networks (CNNs),  we begin by using a CNN architecture for estimating facial attributes and show that they indeed provide an impressive baseline performance. To further improve performance, we propose a novel approach that incorporates facial landmark information for input images as an additional channel, helping the CNN learn face-specific features so that the landmarks across various training images hold correspondence.  We empirically analyze the performance of our method, showing consistent improvement over the baseline across traits.", 
    "authors": [
      {
        "name": "Yoad Lewenberg"
      }, 
      {
        "name": "Yoram Bachrach"
      }, 
      {
        "name": "Sukrit Shankar"
      }, 
      {
        "name": "Antonio Criminisi"
      }
    ], 
    "keywords": "Personal Trait Prediction, Convolutional Neural Networks, Deep Learning, Facial Landmark Localization, Face Alignment", 
    "title": "Predicting Personal Traits from Facial Images using Convolutional Neural Networks Augmented with Facial Landmark Information", 
    "type": "paper"
  }, 
  "1254": {
    "abstract": "Sparse representation has been widely studied in visual tracking, which has shown promising tracking performance. Despite a lot of progress, the visual tracking problem is still a challenging task due to appearance variations over time. In this paper, we propose a novel sparse tracking algorithm that well addresses temporal appearance changes, by enforcing template representability and temporal consistency (TRAC). By modeling temporal consistency, our algorithm addresses the issue of drifting away from a tracking target. By exploring the templates' long-term-short-term representability, the proposed method adaptively updates the dictionary using the most descriptive templates, which significantly improves the robustness to target appearance changes. We compare our TRAC algorithm against 10 state-of-the-art approaches on 12 challenging benchmark image sequences. Both qualitative and quantitative results demonstrate that our algorithm significantly outperforms previous state-of-the-art trackers.", 
    "authors": [
      {
        "name": "Xue Yang"
      }, 
      {
        "name": "Fei Han"
      }, 
      {
        "name": "Hua Wang"
      }, 
      {
        "name": "Hao Zhang"
      }
    ], 
    "keywords": "Visual tracking, Sparse representation, Template representability, Temporal consistency", 
    "title": "Enforcing Template Representability and Temporal Consistency for Adaptive Sparse Tracking", 
    "type": "paper"
  }, 
  "1256": {
    "abstract": "We address the problem of recommending an appliance usage schedule to the homeowner which balances between maximising total savings and maintaining sufficient user convenience. An important challenge within this problem is how to elicit the the user preferences with low intrusiveness, in order to identify new schedules with high cost savings, that still lies within the user's comfort zone. To tackle this problem we propose iDR, an interactive  system for generating personalised appliance usage scheduling recommendations that maximise savings and convenience with minimal intrusiveness. In particular, our system learns when to stop interacting with the user during the preference elicitation process, in order to keep the bother cost (e.g., the amount of time the user spends, or the cognitive cost of interacting) minimal. We demonstrate through extensive empirical evaluation on real--world data that our approach improves savings by up to 35%, while maintaining a significantly lower bother cost, compared to state-of-the-art benchmarks.", 
    "authors": [
      {
        "name": "Ngoc Cuong Truong"
      }, 
      {
        "name": "Tim Baarslag"
      }, 
      {
        "name": "Sarvapali Ramchurn"
      }, 
      {
        "name": "Long Tran-Thanh"
      }
    ], 
    "keywords": "Demand Response, Demand Side Management System, Human-Agent Interaction, Home Energy Management", 
    "title": "Interactive Scheduling of Appliance Usage in the Home", 
    "type": "paper"
  }, 
  "1264": {
    "abstract": "In dictionary learning for analysis of images, spatial correlation from extracted patches can be leveraged to improve characterization power. We propose a Bayesian framework for dictionary learning, where spatial location dependencies are captured by imposing a multiplicative Gaussian process prior on the latent units representing binary activations. Data augmentation and Kronecker methods allow for efficient Markov chain Monte Carlo sampling. We further extend our model with a sigmoid belief network, linking Gaussian processes and high-level latent binary units to capture inter-dictionary dependencies, while yielding additional computational savings. Applications to image denoising, inpainting and depth-information restoration demonstrate that the proposed model outperforms traditional Bayesian dictionary learning approaches.", 
    "authors": [
      {
        "name": "Yizhe Zhang"
      }, 
      {
        "name": "Ricardo Henao"
      }, 
      {
        "name": "Chunyuan Li"
      }, 
      {
        "name": "Lawrence Carin"
      }
    ], 
    "keywords": "Bayesian methods, Gaussian process, Representation learning", 
    "title": "Bayesian Dictionary Learning with Gaussian Processes and Sigmoid Belief Networks", 
    "type": "paper"
  }, 
  "1265": {
    "abstract": "Nowadays, the rapid proliferation of data makes it possible to build complex models for many real applications. Such models, however, usually require large amounts of labeled data, and the labeling process can be both expensive and tedious for domain experts. To address this problem, researchers have resorted to crowdsourcing to collect labels from non-experts with much less cost. The key challenge with crowdsourcing is how to infer the true labels from the large number of noisy labels provided by non-experts.  Different from most existing techniques on crowdsourcing, which ignore the structure information in the labeling data provided by non-experts, in this paper, we propose a novel structured approach based on tensor augmentation and completion. It uses tensor representation for the labeled data, augments it with a ground truth layer, and explores two methods to estimate the ground truth layer via low rank tensor completion. Experimental results on 6 real data sets demonstrate the superior performance of the proposed approach over state-of-the-art techniques.", 
    "authors": [
      {
        "name": "Yao Zhou"
      }, 
      {
        "name": "Jingrui He"
      }
    ], 
    "keywords": "Crowdsourcing, Tensor Augmentation, Tensor Completion", 
    "title": "Crowdsourcing via Tensor Augmentation and Completion", 
    "type": "paper"
  }, 
  "1271": {
    "abstract": "Planning with hybrid domains modelled in PDDL+ has been gaining research interest in the Automated Planning community in recent years. Hybrid domain models capture a more accurate representation of real world problems that involve continuous processes than is possible using discrete systems. However, solving problems represented as PDDL+ domains is very challenging due to the construction of complex system dynamics, including non-linear processes and events. In this paper we introduce DiNo, a new planner capable of tackling complex problems with non-linear system dynamics governing the continuous evolution of states. DiNo is based on the discretise-and-validate approach and uses the novel Staged Relaxed Planning Graph+ (SRPG+) heuristic, which is introduced in this paper. Although several planners have been developed to work with subsets of PDDL+ features, or restricted forms of processes, DiNo is currently the only heuristic planner capable of handling non-linear system dynamics combined with the full PDDL+ feature set.", 
    "authors": [
      {
        "name": "Wiktor Piotrowski"
      }, 
      {
        "name": "Maria Fox"
      }, 
      {
        "name": "Derek Long"
      }, 
      {
        "name": "Daniele Magazzeni"
      }, 
      {
        "name": "Fabio Mercorio"
      }
    ], 
    "keywords": "Planning, Planning in Mixed Discrete/Continuous Domains, Planning as Model Checking, Hybrid systems", 
    "title": "Heuristic Planning for PDDL+ Domains", 
    "type": "paper"
  }, 
  "1274": {
    "abstract": "We propose a probabilistic-logical framework for group decision-making. Its main characteristic is that we derive group preferences  from agents' beliefs and utilities rather than from their individual preferences as done in social choice approaches. This can be more appropriate when the individual preferences hide too much of the individuals' opinions that determined their preferences. We introduce three preference relations and investigate the relationships between the group preferences and individual and  subgroup preferences.", 
    "authors": [
      {
        "name": "Nico Potyka"
      }, 
      {
        "name": "Erman Acar"
      }, 
      {
        "name": "Matthias Thimm"
      }, 
      {
        "name": "Heiner Stuckenschmidt"
      }
    ], 
    "keywords": "Probabilistic Logic, Probabilistic Belief Merging, Decision Theory", 
    "title": "Group Decision Making via Probabilistic Belief Merging", 
    "type": "paper"
  }, 
  "1275": {
    "abstract": "This paper focuses on argumentation graphs whose nodes are arguments and edges    represent supports (thus positive relations) between arguments. Each argument has a    weight reflecting its basic strength. The question of evaluating the overall acceptability    (or overall strength) of arguments raises thus naturally. To answer this question, semantics    should be defined. For that purpose, we start by presenting axioms that any semantics should    satisfy. Some of them are mandatory and others represent strategic choices.    Then, we define three semantics, each of which satisfies all the mandatory axioms as well    one strategic axiom.", 
    "authors": [
      {
        "name": "Leila Amgoud"
      }, 
      {
        "name": "Jonathan Ben-Naim"
      }
    ], 
    "keywords": "Argumentation, Support relations, Semantics", 
    "title": "Evaluation of arguments from support relations: Axioms and Semantics", 
    "type": "paper"
  }, 
  "1276": {
    "abstract": "A fundamental artificial intelligence challenge is how to design agents that intelligently trade off exploration and exploitation while quickly learning about an unknown environment. However, in order to learn quickly, we must somehow generalize experience across states. One promising approach is to use Bayesian methods to simultaneously cluster dynamics and control exploration; unfortunately, these methods tend to require computationally intensive MCMC approximation techniques which lack guarantees. We propose Thompson Clustering for Reinforcement Learning (TCRL), a family of Bayesian clustering algorithms for reinforcement learning that leverage structure in the state space to remain computationally efficient while controlling both exploration and generalization. TCRL-Theoretic achieves near-optimal Bayesian regret bounds while consistently improving over  a standard Bayesian exploration approach. TCRL-Relaxed is guaranteed to converge to acting optimally, and empirically outperforms state-of-the-art Bayesian clustering algorithms across a variety of simulated domains, even in cases where no states are similar.", 
    "authors": [
      {
        "name": "Travis Mandel"
      }, 
      {
        "name": "Yun-En Liu"
      }, 
      {
        "name": "Emma Brunskill"
      }, 
      {
        "name": "Zoran Popovic"
      }
    ], 
    "keywords": "Exploration/Exploitation, Bayesian Methods, Posterior Sampling, Thompson Sampling, State Aggregation, Markov Decision Process, Reinforcement Learning", 
    "title": "Efficient Bayesian Clustering for Reinforcement Learning", 
    "type": "paper"
  }, 
  "1278": {
    "abstract": "The runtime performance of modern SAT solvers on random k-CNF formulas is deeply connected with the `phase-transition' phenomenon seen empirically in the satisfiability of random k-CNF formulas. Recent universal hashing-based approaches to sampling and counting crucially depend on the runtime performance of SAT solvers on formulas expressed as the conjunction of both k-CNF and XOR constraints (known as k-CNF-XOR formulas), but the behavior of random k-CNF-XOR formulas is unexplored in prior work. In this paper, we present the first study of the satisfiability of random k-CNF-XOR formulas. We show empirical evidence of a surprising phase-transition that follows a linear trade-off between k-CNF and XOR constraints. Furthermore, we prove that a phase-transition for k-CNF-XOR formulas exists for k = 2 and (when the number of k-CNF constraints is small) for k > 2.", 
    "authors": [
      {
        "name": "Jeffrey M. Dudek"
      }, 
      {
        "name": "Kuldeep S. Meel"
      }, 
      {
        "name": "Moshe Y. Vardi"
      }
    ], 
    "keywords": "Phase-Transitions, Satisfiability Threshold, XOR, Constrained Counting, Constrained Sampling", 
    "title": "Combining the k-CNF and XOR Phase-Transitions", 
    "type": "paper"
  }, 
  "1282": {
    "abstract": "The existence of truthful social choice mechanisms strongly depends on whether monetary transfers are allowed. Without payments there are no truthful, non-dictatorial mechanisms under mild requirements, whereas the VCG mechanism guarantees truthfulness along with welfare maximization when there are payments and utility is quasi-linear in money.  In this paper we study mechanisms in which we can use payments but where agents have non quasi-linear utility functions. Our main result extends the Gibbard-Satterthwaite impossibility result by showing that, for two agents, the only truthful mechanism for at least three alternatives under general decreasing utilities remains dictatorial.  We then show how to extend the VCG mechanism to work under a more general utility space than quasi-linear (the \"parallel domain\") and show that the parallel domain is maximal---no mechanism with the VCG properties exists in any larger domain.", 
    "authors": [
      {
        "name": "Hongyao Ma"
      }, 
      {
        "name": "Reshef Meir"
      }, 
      {
        "name": "David C. Parkes"
      }
    ], 
    "keywords": "Social Choice, Strategy-proofness, Money", 
    "title": "Social Choice for Agents with General Utilities", 
    "type": "paper"
  }, 
  "1286": {
    "abstract": "We introduce a new topological feature representation for point-cloud objects. Specifically, we construct a Stochastic Multiresolution Persistent Homology (SMURPH) kernel which represents an object's persistent homology at different resolutions. Under the SMURPH kernel two objects are similar if they have similar number and sizes of \"holes\" at these resolutions. Our multiresolution kernel can capture both global topology and fine-grained topological texture in the data. Importantly, on large point-clouds the SMURPH kernel is more computationally tractable compared to existing topological data analysis methods. We demonstrate SMURPH's potential for clustering and classification on several applications, including eye disease classification and human activity recognition.", 
    "authors": [
      {
        "name": "Jerry Zhu"
      }, 
      {
        "name": "Ara Vartanian"
      }, 
      {
        "name": "Manish Bansal"
      }, 
      {
        "name": "Duy Nguyen"
      }, 
      {
        "name": "Luke Brandl"
      }
    ], 
    "keywords": "Topological data analysis, Persistent homology, Kernel methods", 
    "title": "Stochastic Multiresolution Persistent Homology Kernel", 
    "type": "paper"
  }, 
  "1288": {
    "abstract": "Semi-Autonomous Systems (SAS) encapsulate a stochastic decision process explicitly controlled by both an agent and a human, in order to leverage the distinct capabilities of each actor. Planning in SAS must address the challenge of transferring control quickly, safely, and smoothly back-and-forth between the agent and the human. We formally define SAS and the requirements to guarantee that the controlling entities are always able to act competently. We then consider applying the model to Semi-Autonomous VEhicles (SAVE), using a hierarchical approach in which micro-level transfer-of-control actions are governed by a high-fidelity POMDP model. Macro-level path planning in our hierarchical approach is performed by solving a Stochastic Shortest Path (SSP) problem. We analyze the integrated model and show that it provides the required guarantees. Finally, we test the SAVE model using real-world road data from Open Street Map (OSM) within 10 cities, showing the benefits of the collaboration between the agent and human.", 
    "authors": [
      {
        "name": "Kyle Wray"
      }, 
      {
        "name": "Luis Pineda"
      }, 
      {
        "name": "Shlomo Zilberstein"
      }
    ], 
    "keywords": "Semi-Autonomous System, POMDP, SSP, Autonomous Vehicle, Autonomous Car", 
    "title": "Hierarchical Approach to Transfer of Control in Semi-Autonomous Systems", 
    "type": "paper"
  }, 
  "1289": {
    "abstract": "Balanced knockout tournaments are very common in sports  competitions and are also used in decision-making and  elections. The traditional computational question of finding the optimal draw for a specific player in such a tournament has received a lot of attention in AI in recent years. While previous works consider the problem where the pairwise  winning probabilities are precisely given, we study the problem of robust draws when there are small errors in the pairwise winning  probabilities. First, we present several illuminating examples to establish  the following: (a)~there exist deterministic tournaments  (where the pairwise winning probabilities are~0 or~1) with  optimal draws such that one optimal draw is much more robust than the other; and  (b)~in general, there exist tournaments with $\\delta$-optimal draws,  for $\\delta>0$, that are more robust than all the optimal draws. Motivated by the above examples we study the computational  problem of existence of robust draws that guarantee a specified tournament  winning probability. Second, we present a polynomial time algorithm for approximating the robustness of a draw for sufficiently small errors in pairwise winning probabilities and conclude that the stated computational problem is NP-complete. We also show that two natural cases of deterministic tournaments  where the optimal draw could be computed in polynomial time also  admit polynomial-time algorithms to compute robust optimal draws.", 
    "authors": [
      {
        "name": "Krishnendu Chatterjee"
      }, 
      {
        "name": "Rasmus Ibsen-Jensen"
      }, 
      {
        "name": "Josef Tkadlec"
      }
    ], 
    "keywords": "algorithm, knockout tournament, robustness", 
    "title": "Robust Draws in Balanced Knockout Tournaments", 
    "type": "paper"
  }, 
  "1290": {
    "abstract": "We introduce WebGazer, an online eye tracker that uses common webcams already present in laptops and mobile devices to infer the eye-gaze locations of web visitors on a page in real-time. The eye tracking model self-calibrates by watching web visitors interact with the web page and trains a mapping between features of the eye and positions on the screen. This approach aims to provide a natural experience to everyday users that is not restricted to laboratories and highly controlled user studies. WebGazer has two key components: a pupil detector that can be combined with any eye detection library, and a gaze estimator using regression analysis informed by user interactions. We perform a large remote online study and a small in-person study to evaluate WebGazer. The findings show that WebGazer can learn from user interactions and that its accuracy is sufficient for approximating the user's gaze. As part of this paper, we release the first eye tracking library that can be easily integrated in any website for real-time gaze interactions, usability studies, or web research.", 
    "authors": [
      {
        "name": "Alexandra Papoutsaki"
      }, 
      {
        "name": "Patsorn Sangkloy"
      }, 
      {
        "name": "James Laskey"
      }, 
      {
        "name": "Nediyana Daskalova"
      }, 
      {
        "name": "Jeff Huang"
      }, 
      {
        "name": "James Hays"
      }
    ], 
    "keywords": "online eye tracking, cursor, user interactions, gaze prediction", 
    "title": "WebGazer: Scalable Eye Tracking Using User Interactions", 
    "type": "paper"
  }, 
  "1292": {
    "abstract": "\\emph{Inconsistent-tolerant semantics}, like the IAR and ICAR semantics, have been proposed as means to compute meaningful query answers over inconsistent Description Logic (DL) ontologies. In the current paper we present a framework for scalable query answering under both the IAR and ICAR semantics, which is based on highly efficient data saturation systems. Our approach is sound and complete for ontologies expressed in the lightweight DL DL-Lite, but for more expressive DLs the problem is known to be intractable, hence our algorithm only computes upper approximations. Nevertheless, its structure motivates a new type of ICAR-like semantics which can be computed in polynomial time for a very large family of DLs. We have implemented our techniques and conducted an experimental evaluation obtaining encouraging results as both our IAR- and ICAR-answering approaches are far more efficient that existing available IAR-based answering systems.", 
    "authors": [
      {
        "name": "Eleni Tsalapati"
      }, 
      {
        "name": "Giorgos Stoilos"
      }, 
      {
        "name": "Giorgos Stamou"
      }, 
      {
        "name": "George Koletsos"
      }
    ], 
    "keywords": "Description Logics, Consistent Query Answering, ICAR answering, IAR answering, Expressive Description Logics, Scalable Algorithms", 
    "title": "Efficient Query Answering Over Expressive Inconsistent Description Logics", 
    "type": "paper"
  }, 
  "1298": {
    "abstract": "Cooperating agents can make commitments to help each other, though these commitments might have to be probabilistic when actions have stochastic outcomes. In this paper, we consider the additional complication in cases where an agent might prefer to change its policy as it learns more about its reward function from experience.  How should such an agent be allowed to change its policy while still faithfully pursuing its commitment in a principled decision-theoretic manner? We address this question by defining a class of Dec-POMDPs with Bayesian reward uncertainty, and by developing a novel Commitment Constrained Iterative Mean Reward algorithm that implements the semantics of faithful commitment pursuit while still permitting the agent's response to the evolving understanding of its rewards. We bound the performance of our algorithm theoretically, and evaluate empirically how it effectively balances solution quality and computation cost.", 
    "authors": [
      {
        "name": "Qi Zhang"
      }, 
      {
        "name": "Edmund Durfee"
      }, 
      {
        "name": "Satinder Singh"
      }, 
      {
        "name": "Anna Chen"
      }, 
      {
        "name": "Stefan Witwicki"
      }
    ], 
    "keywords": "Commitment Semantics, Reward Uncertainty, Constrained Planning, Sequential Decision Making", 
    "title": "Commitment Semantics for Sequential Decision Making under Reward Uncertainty", 
    "type": "paper"
  }, 
  "130": {
    "abstract": "Feature selection has been studied in machine learning and data mining for many years, and is a valuable way to improve classification accuracy while reducing model complexity. Two main classes of feature selection methods - filter and wrapper - discard those features which are not selected, and do not consider them in the predictive model. We propose that these unselected features may instead be used as an additional source of information at train time. We outline a strategy called Learning using Unselected Features (LUFe) that allows different features to serve different functions in classification. In this framework, selected features are used directly to set the decision boundary, and unselected features are utilised in a secondary role, with no additional cost at test time. Our empirical results on 49 textual datasets show that LUFe can lead improve classification performance in comparison with standard wrapper and filter feature selection.", 
    "authors": [
      {
        "name": "Joseph Taylor"
      }, 
      {
        "name": "Viktoriia Sharmanska"
      }, 
      {
        "name": "Kristian Kersting"
      }, 
      {
        "name": "David Weir"
      }, 
      {
        "name": "Novi Quadrianto"
      }
    ], 
    "keywords": "Feature selection, Learning using privileged information, LUPI, Machine Learning, Learning using Unselected Features, Classification, SVM, Support Vector Machine", 
    "title": "Learning using Unselected Features (LUFe)", 
    "type": "paper"
  }, 
  "1302": {
    "abstract": "Many complex reasoning tasks in Artificial Intelligence (including relation extraction, knowledge base completion, and information integration) can be formulated as inference problems using a probabilistic first-order logic. However, due to the discrete nature of logical facts and predicates, it is challenging to generalize symbolic representations and represent first-order logic formulas in probabilistic relational models. In this work, we take a rather radical approach: we aim at learning continuous low-dimensional embeddings for first-order logic from scratch. In particular, we first consider a structural gradient based structure learning approach to generate plausible inference formulas from facts; Then, we build a grounded proof graph using background facts, training examples, and these inference formulas. To learn the embeddings for formulas, we rewrite the proof graph by switching the nodes and edges. Finally, a scalable matrix factorization approach is used to learn the latent vectors for the nodes (formulas) of the transformed graph. In experiments, we demonstrate the effectiveness of reasoning with first-order logic embeddings by comparing with several state-of-the-art baselines on two datasets in the task of knowledge base completion.", 
    "authors": [
      {
        "name": "William Yang Wang"
      }, 
      {
        "name": "William Cohen"
      }
    ], 
    "keywords": "knowledge base completion, personalized PageRank, knowledge graph embedding, first-order logic embedding", 
    "title": "Learning First-Order Logic Embeddings via Graph Rewriting", 
    "type": "paper"
  }, 
  "1309": {
    "abstract": "In ontology-mediated query (OMQ) answering, closed predicates have been advocated as a powerful tool to leverage the orthogonal open-world semantics of Description Logics (DLs), and the closed-world semantics of traditional relational databases. However, the relative expressiveness of OMQs with closed predicates in terms of more traditional query languages like DATALOG is not well understood. In this paper, we consider instance queries mediated by an ontology expressed in the very expressive DL ALCHIO with closed predicates. We observe that such queries are non-monotonic and cannot be expressed in monotonic variants of DATALOG, but a polynomial time translation into (disjunctive) DATALOG extended with negation as failure is feasible. If no closed predicates are present---in the case of classical instance checking in ALCHIO---our translation yields a positive disjunctive DATALOG program of polynomial size. To the best of our knowledge, this is the first polynomial time translation of an expressive (non-Horn) DL into disjunctive DATALOG.", 
    "authors": [
      {
        "name": "Shqiponja Ahmetaj"
      }, 
      {
        "name": "Magdalena Ortiz"
      }, 
      {
        "name": "Mantas Simkus"
      }
    ], 
    "keywords": "Description Logics, Closed Predicates, Datalog, Relative Expressiveness", 
    "title": "Polynomial Datalog Rewritings for Expressive Description Logics with Closed Predicates", 
    "type": "paper"
  }, 
  "1310": {
    "abstract": "Knowledge transfer between tasks can improve the performance of learned models, but requires an accurate estimate of the inter-task relationships to identify the relevant knowledge to transfer. These inter-task relationships are typically estimated based on training data for each task, which is inefficient in lifelong learning settings where the goal is to learn each consecutive task rapidly from as little data as possible. To reduce this burden, we develop a lifelong reinforcement learning method based on coupled dictionary learning that incorporates high-level task descriptors to model the inter-task relationships. We show that using task descriptors improves the performance of the learned task policies, providing both theoretical justification for the benefit and empirical demonstration of the improvement across a variety of dynamical control problems. Given only the descriptor for a new task, the lifelong learner is also able to accurately predict the task policy through zero-shot learning using the coupled dictionary, eliminating the need to pause to gather training data before addressing the task.", 
    "authors": [
      {
        "name": "David Isele"
      }, 
      {
        "name": "Eric Eaton"
      }, 
      {
        "name": "Mohammad Rostami"
      }
    ], 
    "keywords": "Lifelong Learning, Transfer Learning, Knowledge Transfer, Zero-shot Learning", 
    "title": "Using Task Features for Zero-Shot Knowledge Transfer in Lifelong Learning", 
    "type": "paper"
  }, 
  "1312": {
    "abstract": "Classical results in social choice theory on the susceptibility of voting rules to strategic manipulation make the assumption that the manipulator has complete information regarding the preferences of the other voters. In reality, however, voters only have incomplete information, which limits their ability to manipulate. We explore how these limitations affect both the manipulability of voting rules and the dynamics of systems in which voters may repeatedly update their own vote in reaction to the moves made by others. We focus on the Plurality, Veto, k-approval, Borda, Copeland, and Maximin voting rules, and consider several types of information that are natural in the context of these rules, namely in- formation on the current front-runner, on the scores obtained by each alternative, and on the majority graph induced by the individual preferences.", 
    "authors": [
      {
        "name": "Ulle Endriss"
      }, 
      {
        "name": "Svetlana Obraztsova"
      }, 
      {
        "name": "Maria Polukarov"
      }, 
      {
        "name": "Jeffrey S. Rosenschein"
      }
    ], 
    "keywords": "Social choice, Voting games, Incomplete information, Iterative voting", 
    "title": "Strategic Voting with Incomplete Information", 
    "type": "paper"
  }, 
  "1332": {
    "abstract": "An important approach for efficient inference in probabilistic graphical models exploits symmetries amongst objects in the domain. Symmetric variables (states) are collapsed into meta-variables (meta-states) and inference algorithms are run over the lifted graphical model instead of the flat one. Our paper extends existing definitions of symmetry by introducing the novel notion of contextual symmetry. Two states that are not globally symmetric, can be contextually symmetric under some specific assignment to a subset of variables, referred to as the context variables. Contextual symmetry subsumes previous symmetry definitions and can represent a  large class of symmetries not representable earlier. We show how to compute contextual symmetries by reducing it to the problem of graph isomorphism. We extend previous work on exploiting symmetries in the MCMC framework to the case of contextual symmetries. Our experiments on several domains of interest demonstrate that exploiting contextual symmetries can result in significant computational gains.", 
    "authors": [
      {
        "name": "Ankit Anand"
      }, 
      {
        "name": "Aditya Grover"
      }, 
      {
        "name": "Mausam ."
      }, 
      {
        "name": "Parag Singla"
      }
    ], 
    "keywords": "Contextual Symmetries, Lifted Inference, MCMC, Graphical Models", 
    "title": "Contextual Symmetries in Probabilistic Graphical Models", 
    "type": "paper"
  }, 
  "1338": {
    "abstract": "In biology, the construction of plasmids is a routine technique, yet under-optimal, expensive and time-consuming. In this paper, we model the Plasmid Cloning Problem (PCP) in constraint programing, in order to optimize the construction of plasmids. Our technique uses a new propagator for the AtMostNVector constraint. This constraint allows the design of strategies for constructing multiple plasmids at the same time. Our approach recommends the smallest number of different cloning steps, while selecting the most efficient steps. It provides optimal strategies for real instances in gene therapy for retinal blinding diseases.", 
    "authors": [
      {
        "name": "Thierry Petit"
      }, 
      {
        "name": "Lolita Petit"
      }
    ], 
    "keywords": "Constraint programming, Global constraint, Application, Gene therapy", 
    "title": "Optimizing  Molecular Cloning Of Multiple Plasmids", 
    "type": "paper"
  }, 
  "1339": {
    "abstract": "Reactive agents are suitable for representing physical resources in manufacturing control systems. An important challenge of agent-based manufacturing control systems is to develop formal and structured approaches to support their specification and verification. This paper  proposes a  logic-based approach, by generalising that of model checking  multiagent systems, for the reconfigurability of reactive multiagent systems. Two reconfigurability scenarios are studied, for the resulting system being a monolithic system or an individual module, and  their computational complexity results are given.", 
    "authors": [
      {
        "name": "Xiaowei Huang"
      }, 
      {
        "name": "Qingliang Chen"
      }, 
      {
        "name": "Jie Meng"
      }, 
      {
        "name": "Kaile Su"
      }
    ], 
    "keywords": "agent-based, manufacturing control systems, model checking, logic-based approach", 
    "title": "Reconfigurability in Reactive Multiagent Systems", 
    "type": "paper"
  }, 
  "1343": {
    "abstract": "Nowadays, a large number of new online businesses emerge rapidly. For these emerging businesses, existing recommendation models usually suffer from the data-sparsity. In this paper, we introduce a novel similarity measure, AmpSim (Augmented Meta Path-based Similarity) that takes both the linkage structures and the augmented link attributes into account. By traversing between heterogeneous networks through overlapping entities, AmpSim can easily gather side information from other networks and capture the rich similarity semantics between entities. We further incorporate the similarity information captured by AmpSim in a collective matrix factorization model such that the transferred knowledge can be iteratively propagated across networks to fit the emerging business. Extensive experiments conducted on real- world datasets demonstrate that our method significantly outperforms other state-of-the-art recommendation models in addressing item recommendation for emerging businesses.", 
    "authors": [
      {
        "name": "Chun-Ta Lu"
      }, 
      {
        "name": "Sihong Xie"
      }, 
      {
        "name": "Weixiang Shao"
      }, 
      {
        "name": "Lifang He"
      }, 
      {
        "name": "Philip Yu"
      }
    ], 
    "keywords": "Recommender Systems, Heterogeneous Networks, Augmented Meta Path, Emerging Businesses", 
    "title": "Item Recommendation for Emerging Online Businesses", 
    "type": "paper"
  }, 
  "1348": {
    "abstract": "We study the problem of finding a Pareto-efficient and envy-free allocation of a set of indivisible resources to a set of agents with either monotonic dichotomous preferences or with additive preferences. Motivated by results of Bouveret and Lang [JAIR 2008], we provide a refined computational complexity analysis by studying the influence of three natural parameters: the number n of agents, the number m of resources, and the number z of different numbers occurring in utility-based references of the agents. On the negative side, we show that small values for n and z alone do not significantly lower the computational complexity in most cases. On the positive side, devising fixed-parameter algorithms we show that all considered problems are tractable in case of small m. Furthermore, we develop a fixed-parameter algorithm, indicating that the problem with additive preferences becomes computationally tractable in case of small n and small z.", 
    "authors": [
      {
        "name": "Bernhard Bliem"
      }, 
      {
        "name": "Robert Bredereck"
      }, 
      {
        "name": "Rolf Niedermeier"
      }
    ], 
    "keywords": "fair division of indivisible goods, fixed-parameter algorithms, monotonic additive preferences", 
    "title": "Complexity of Efficient and Envy-Free Resource Allocation: Few Agents, Resources, or Utility Levels", 
    "type": "paper"
  }, 
  "1350": {
    "abstract": "We investigate the $\\ell_\\infty$-constrained representation which demonstrates robustness to quantization errors, utilizing the tool of deep learning. Based on the Alternating Direction Method of Multipliers (ADMM), we formulate the original convex minimization problem as a feed-forward neural network, named \\textit{Deep $\\ell_\\infty$ Encoder}, by introducing the novel Bounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as network biases. Such a structural prior acts as an effective network regularization, and facilitates the model initialization. We then investigate the effective use of the proposed model in the application of hashing, by coupling the proposed encoders under a supervised pairwise loss, to develop a \\textit{Deep Siamese $\\ell_\\infty$ Network}, which can be optimized from end to end. Extensive experiments demonstrate the impressive performances of the proposed model. We also provide an in-depth analysis of its behaviors against the competitors.", 
    "authors": [
      {
        "name": "Zhangyang Wang"
      }, 
      {
        "name": "Yingzhen Yang"
      }, 
      {
        "name": "Shiyu Chang"
      }, 
      {
        "name": "Qing Ling"
      }, 
      {
        "name": "Thomas Huang"
      }
    ], 
    "keywords": "deep learning, signal representation, hashing", 
    "title": "Learning A Deep $\\ell_\\infty$ Encoder for Hashing", 
    "type": "paper"
  }, 
  "1356": {
    "abstract": "This paper investigates online stochastic planning for problems with large factored state and action spaces. We introduce a novel algorithm that builds a symbolic representation capturing an approximation of the action-value Q-function in terms of action variables, and then performs gradient based search to pick an action for the current state. The algorithm can be seen as a symbolic extension of Monte-Carlo search, induced by independence assumptions on state and action variables, and augmented with gradients to speed up the search. This avoids the space explosion typically faced by symbolic methods, and the dearth of samples faced by Monte-Carlo methods when the action space is large. An experimental evaluation on  benchmark problems shows that the algorithm is competitive with state of the art across problem sizes and that it provides significant improvements for large factored action spaces.", 
    "authors": [
      {
        "name": "Hao Cui"
      }, 
      {
        "name": "Roni Khardon"
      }
    ], 
    "keywords": "MDP, Stochastic Planning, Scalability, Symbolic Planning, Monte Carlo Search", 
    "title": "Online Symbolic Gradient-Based Optimization for Factored Action MDPs", 
    "type": "paper"
  }, 
  "1357": {
    "abstract": "Information network mining often requires careful investigation of linkage relationships between nodes for analysis. Recently, network representation has emerged to represent each node in a vector format, embedding network structure, so off-the-shelf machine learning methods can be directly applied for analysis. To date, existing methods only focus on one aspect of node information and cannot leverage node labels. In this paper, we propose TriDNR, a tri-party deep network representation model, using information from three parties: node structure, node content, and node labels (if available) to jointly learn optimal node representation. TriDNR is based on our new two-level deep natural language module, whose learning is enforced at three levels: (1) at the network structure level, TriDNR exploits inter-node relationship by maximizing the probability of observing surrounding nodes given a node in random walks; (2) at the node content level, TriDNR captures node-word correlation by maximizing the co-occurrence of word sequence given a node; and (3) at the node label level, TriDNR models label-word correspondence by maximizing the probability of word sequence given a class label. The tri-party information is jointly fed into the neural network model to mutually enhance each other to learn optimal representation, and results in up to 79% classification accuracy gain, compared to state-of-the-art methods.", 
    "authors": [
      {
        "name": "Shirui Pan"
      }, 
      {
        "name": "Jia Wu"
      }, 
      {
        "name": "Xingquan Zhu"
      }, 
      {
        "name": "Chengqi Zhang"
      }
    ], 
    "keywords": "network representation, distributed representation, networked data, neural networks, deep learning", 
    "title": "Tri-Party Deep Network Representation", 
    "type": "paper"
  }, 
  "1358": {
    "abstract": "The subpath planning problem (SPP) is a branch of path planning problem, which has widespread applications in automated manufacturing process as well as vehicle and robot navigation. This problem aims to find the shortest path or tour subject to covering a set of given subpaths. Unlike existing meta-heuristic-based approaches, we propose a 2-approximation algorithm that runs in $O(n^3)$ time which is deterministic and finds near optimal solutions.", 
    "authors": [
      {
        "name": "Masoud Safilian"
      }, 
      {
        "name": "S. Mehdi Hashemi"
      }, 
      {
        "name": "Sepehr Eghbali"
      }, 
      {
        "name": "Ali Akbar Safilian"
      }
    ], 
    "keywords": "Path Planning Problem, Subpath Planning Problem, Travelling Salesman Problem, Approximation Algorithm, Christofides Algorithm", 
    "title": "An Approximation Approach for Solving Subpath Planning Problem", 
    "type": "paper"
  }, 
  "136": {
    "abstract": "Multi-view spectral clustering, which aims at yielding an agreement or consensus data objects grouping across multi-views with their graph laplacian matrices, is a fundamental clustering problem. Among existing methods, Low-Rank Representation (LRR) based method is quite superior in terms of its effectiveness, intuitiveness and robustness to noise corruptions. However, it aggressively tries to learn a common low-dimensional subspace for multi-view data, while inattentively ignoring the local manifold structure in each view, which is critically important to the spectral clustering. On the other hand, the low-rank minimization is enforced to achieve the data correlation consensus shared by views, failing to flexibly preserve the local nonlinear spectral graph structure for each individual view. In this paper, 1) we propose a multi-graph laplacian regularized LRR with each graph laplacian corresponding to one view to characterize its non-linear manifold structure. 2) Instead of directly enforcing the low-rank minimization among all views to yield the correlation consensus, we separately impose low-rank constraint to each view, coupled with a mutual structural consensus constraint, where it is able to not only well preserve the manifold structure from each view but also serve as a constraint for that from other views, which iteratively makes the views more agreeable for better multi-view clustering performance. Extensive experiments on real-world multi-view data sets demonstrate the superior performance of our approach.", 
    "authors": [
      {
        "name": "Yang Wang"
      }, 
      {
        "name": "Lin Wu"
      }
    ], 
    "keywords": "Multi-view clustering, iterative views agreement, low-rank structured optimization method", 
    "title": "Iterative Views Agreement: An Iterative Low-Rank based Structured Optimization Method to Multi-View Spectral Clustering", 
    "type": "paper"
  }, 
  "1362": {
    "abstract": "In cross-domain learning, there is a more challenging problem that the domain divergence involves more than one dominant factors, e.g., different view-points, various resolution, changing illuminations. Fortunately, an intermediate domain could always be found to build a bridge across them to facilitate the learning problem. In this paper, we propose a Coupled Marginalized Denoising Auto-encoders framework to address the cross-domain problem. Specifically, we design two marginalized denoising auto-encoders, one for the target and the other for source as well as the intermediate one. To better couple the two denoising auto-encoders learning, we incorporate a feature mapping matrix, which tends to transfer knowledge between the intermediate domain and the target one. Furthermore, the maximum margin criterion, e.g., intra-class compactness and inter-class penalty, on the output layer is imposed to seek more discriminative features across domains. Extensive experiments are conducted on both super-resolution person re-identification and kinship verification tasks, and the experimental results have demonstrated the superiority of our method over state-of-the-art methods.", 
    "authors": [
      {
        "name": "Shuyang Wang"
      }, 
      {
        "name": "Zhengming Ding"
      }, 
      {
        "name": "Yun Fu"
      }
    ], 
    "keywords": "Marginalized Auto-encoders, Cross-domain, Kinship verification, Person re-identification", 
    "title": "Coupled Marginalized Auto-encoders for Cross-domain Multi-view Learning", 
    "type": "paper"
  }, 
  "1368": {
    "abstract": "We determine the quality of randomized social choice mechanisms in a setting in which the agents have metric preferences: every agent has a cost for each alternative, and these costs form a metric. We assume that these costs are unknown to the mechanisms (and possibly even to the agents themselves), which means we cannot simply select the optimal alternative, i.e. the alternative that minimizes the total agent cost (or median agent cost). However, we do assume that the agents know their ordinal preferences that are induced by the metric space. We examine randomized social choice functions that require only this ordinal information and select an alternative that is good in expectation with respect to the costs from the metric. To quantify how good a randomized social choice function is, we bound the distortion, which is the worst-case ratio between expected cost of the alternative selected and the cost of the optimal alternative. We provide new distortion bounds for a variety of randomized mechanisms, for both general metrics and for important special cases. Our results show a sizable improvement in distortion over deterministic mechanisms.", 
    "authors": [
      {
        "name": "Elliot Anshelevich"
      }, 
      {
        "name": "John Postl"
      }
    ], 
    "keywords": "social choice, spacial preferences, metric preferences, ordinal approximation", 
    "title": "Randomized Social Choice Functions Under Metric Preferences", 
    "type": "paper"
  }, 
  "1372": {
    "abstract": "We address the problem of detecting changes in multivariate datastreams, and we investigate the intrinsic difficulty that change-detection methods have to face when the data-dimension scales. In particular, we consider the general approach that detects changes by comparing the distribution of the log-likelihood of the datastream over different time windows. Despite the fact that this approach constitutes the frame for several change-detection methods, its effectiveness when the dimension of data scales has never been investigated, which is indeed the goal of our paper.  We show that the magnitude of the change can be naturally measured by the symmetric Kullback-Leibler divergence between the pre- and post-change distributions, and that the detectability of a change of a given magnitude worsens when the data-dimension increases. This structural problem, which we refer to as \\emph{detectability loss}, is due to the linear relationship existing between the variance of the log-likelihood and the data dimension, and reveals to be harmful even at low data-dimensions (say, 10). We analytically derive the detectability loss on Gaussian-distributed datastreams, and empirically demonstrate that this problem holds also on real-world datasets.", 
    "authors": [
      {
        "name": "Cesare Alippi"
      }, 
      {
        "name": "Giacomo Boracchi"
      }, 
      {
        "name": "Diego Carrera"
      }, 
      {
        "name": "Manuel Roveri"
      }
    ], 
    "keywords": "Change detection, multivariate data, datastreams, curse of dimensionality", 
    "title": "Change Detection in Multivariate Datastreams: Likelihood and  Detectability Loss", 
    "type": "paper"
  }, 
  "1373": {
    "abstract": "This paper introduces a novel coupled unsupervised outlier detection (CUOT) framework  and its instantiation, i.e., a coupled biased random walk (CBRW) model, for identifying outliers in categorical data with diversified frequency distributions and many noisy features. Existing pattern-based outlier detection approaches are ineffective in handling such complex scenarios, as they misfit such data. CBRW estimates outlier scores of feature values by learning low-level feature value couplings, which carry intrinsic data characteristics, into a graph model to handle this complex data. The outlier scores of feature values can either measure the outlierness of an object or facilitate the existing approaches as a feature weighting and selection indicator. Substantial experiments show that CBRW can not only detect outliers in complex data significantly better than the state-of-the-art approaches, but also greatly improve the performance of existing approaches on data sets with many noisy features. Moreover, CBRW does not involve costly pattern searching and thus the CBRW-based models run much faster than the pattern-based approaches.", 
    "authors": [
      {
        "name": "Guansong Pang"
      }, 
      {
        "name": "Longbing Cao"
      }, 
      {
        "name": "Ling Chen"
      }
    ], 
    "keywords": "Outlier detection, Categorical data, Feature selection, Random walks", 
    "title": "Identifying Outliers in Complex Categorical Data by Modeling the Feature Value Couplings", 
    "type": "paper"
  }, 
  "1377": {
    "abstract": "Durak is a Russian card game in which players try to get rid of all their cards via a particular attack/defense mechanism. The last player standing with cards loses. We show that, even restricted to the perfect information two-player game, finding optimal moves is a hard problem. More precisely, we prove that, given a generalized durak position, it is PSPACE-complete to decide if a player has a winning strategy. We also show that deciding if an attack can be answered is NP-hard.", 
    "authors": [
      {
        "name": "\u00c9douard Bonnet"
      }
    ], 
    "keywords": "Durak, Card game, Computational complexity", 
    "title": "The Complexity of Playing Durak", 
    "type": "paper"
  }, 
  "1378": {
    "abstract": "We investigate the feasibility of predicting users\u00e2\u0080\u0099 confusion during the interaction with an Information Visualization, using eye tracking and mouse data. Such detection is important for designing adaptive visualizations that can dynamically support users in resolving the reason of their confusion. The data was collected during a user study with ValueChart, an interactive visualization designed to support users in preferential choices. We used machine learning to identify the best features and amount of interaction time to predict confusion. Our initial results are promising toward the real-time detection of confusion in InfoVis.", 
    "authors": [
      {
        "name": "S\u00e9bastien Lall\u00e9"
      }, 
      {
        "name": "Cristina Conati"
      }, 
      {
        "name": "Giuseppe Carenini"
      }
    ], 
    "keywords": "Information Visualization, User Modeling, Personalization, Classification, Eye tracking, Confusion", 
    "title": "Predicting Confusion in Information Visualization from Eye Tracking and Interaction Data", 
    "type": "paper"
  }, 
  "1381": {
    "abstract": "Ungrammatical sentences present challenges for statistical parsers because the well-formed trees they produce may not be appropriate for these sentences. We introduce a framework for reviewing the parses of ungrammatical sentences and extracting the coherent parts whose syntactic analyses make sense. We call this task parse tree fragmentation. In this paper, we propose a training methodology for fragmenting parse trees without using a task-specific annotated corpus. We also propose some fragmentation strategies and compare their performance on an extrinsic task -- fluency judgments in two domains: English-as-a-Second Language (ESL) and machine translation (MT). Experimental results show that the proposed fragmentation strategies are competitive with existing methods for making fluency judgments; they also suggest that the overall framework is a promising way to handle syntactically unusual sentences.", 
    "authors": [
      {
        "name": "Homa B. Hashemi"
      }, 
      {
        "name": "Rebecca Hwa"
      }
    ], 
    "keywords": "parsing ungrammatical sentences, re-interpreting parse trees, parse tree fragmentation", 
    "title": "Parse Tree Fragmentation of Ungrammatical Sentences", 
    "type": "paper"
  }, 
  "1383": {
    "abstract": "In this paper, we propose to learn cross-view binary identities (CBI) for fast person re-identification. To achieve this, two sets of discriminative hash functions for two different views are learned by simultaneously minimising their distance in the Hamming space, and maximising the cross-covariance and margin. Thus, similar binary codes can be found for images of a same person captured at different views by embedding the images into the Hamming space. Therefore, person re-identification can be solved by efficiently computing and ranking the Hamming distances between the images. Extensive experiments are conducted on two public datasets and CBI produces comparable results as state-of-the-art re-identification approaches but is at least 2200 times faster.", 
    "authors": [
      {
        "name": "Feng Zheng"
      }, 
      {
        "name": "Ling Shao"
      }
    ], 
    "keywords": "Person Re-identification, Hash Learning, Cross Camera", 
    "title": "Learning Cross-view Binary Identities for Fast Person Re-identification", 
    "type": "paper"
  }, 
  "139": {
    "abstract": "Accurate user profiling is crucial for a recommender system to provide proper personalized recommendations to its users. One of the most promising approaches to user profiling is collaborative filtering (CF), which aims to infer users' interests collaboratively from their historical records. In many real-world scenarios, a user's interests towards the items change over time. From our analysis, the changes usually cannot be observed by the recommender systems due to the sparseness in historical records. Therefore, on one hand, a static user profile generated by applying the CF techniques on the aggregation of all historical records may not be precise to model the user's interests in the future time intervals. On the other hand, historical records can be extremely sparse if we only consider the data from up-to-date time intervals, where the CF methods may also perform poor. In this paper, in order to capture the changes of a user's interests and to tackle the sparse data issue, we propose a matrix-factorization-based model named Collaborative Evolution (CE), which extends over the vector autoregressive model and the CF model for user profiling. To verify the effectiveness of the proposed model, we conduct experiments on a real-world dataset, which is obtained from the online shopping website of Tencent Inc.---{\\em www.51buy.com} and contains more than $1$ million users' shopping records in a time span of more than $180$ days. Experimental analyses demonstrate that our proposed CE model can be used to make better {\\em future} recommendations compared to several state-of-the-art methods.", 
    "authors": [
      {
        "name": "Zhongqi Lu"
      }, 
      {
        "name": "Sinno Jialin Pan"
      }, 
      {
        "name": "Yong Li"
      }, 
      {
        "name": "Jie Jiang"
      }, 
      {
        "name": "Qiang Yang"
      }
    ], 
    "keywords": "Collaborative filtering, Matrix factorization, Autoregressive model, Evolutionary user profiling", 
    "title": "Collaborative Evolution for User Profiling in Recommender Systems", 
    "type": "paper"
  }, 
  "1392": {
    "abstract": "Inductive definitions and justifications are well-studied concepts.   They have been integrated in SAT-solvers, but several of their computationally nice properties have never been exploited to improve these solvers. In this paper, we present a new notion called \\emph{relevance}. We determine a class of literals that are relevant for a given definition and partial interpretation, and show that choices on irrelevant atoms can never benefit the search for a model.  We propose an  \\emph{early stopping criterion} and a \\emph{modification of existing heuristics} that exploit relevance. We present a first implementation in MinisatID and experimentally evaluate our approach, and study how often existing solvers make choices on irrelevant atoms.", 
    "authors": [
      {
        "name": "Joachim Jansen"
      }, 
      {
        "name": "Bart Bogaerts"
      }, 
      {
        "name": "Jo Devriendt"
      }, 
      {
        "name": "Gerda Janssens"
      }, 
      {
        "name": "Marc Denecker"
      }
    ], 
    "keywords": "well-founded semantics, satisfiability, SAT(ID), justifications", 
    "title": "Relevance for SAT(ID)", 
    "type": "paper"
  }, 
  "1398": {
    "abstract": "We present a novel approach Minimal Reconstruction Bias Hashing (MRH) to learning similarity preserving binary codes which jointly optimizes both projection and quantization stages. Our work tackles an important problem of how to elegantly connect optimizing projection with optimizing quantization, and to maximize the complementary effects of two stages. Distinct from previous works, MRH has extended the binary coding optimization to the projection dimensionality for effectively balancing the information loss from both projection and quantization. Extensive experiments have shown the proposed MRH significantly outperforms a variety of  state-of-the-art methods.", 
    "authors": [
      {
        "name": "Zhe Wang"
      }, 
      {
        "name": "Ling-Yu Duan"
      }, 
      {
        "name": "Junsong Yuan"
      }, 
      {
        "name": "Tiejun Huang"
      }, 
      {
        "name": "Wen Gao"
      }
    ], 
    "keywords": "Approximate Nearest Neighbor search, Binary Coding, Hashing, Data Compression, Quantization", 
    "title": "To Project More or to Quantize More: Minimize Reconstruction Bias for Learning Compact Binary Codes", 
    "type": "paper"
  }, 
  "1402": {
    "abstract": "Balancing between computational efficiency and sample efficiency is an important goal in reinforcement learning. Temporal difference (TD) learning algorithms stochastically update the value function, with a linear time complexity in the number of features, whereas least-squares temporal difference (LSTD) algorithms are sample efficient but can be quadratic in the number of features. In this work, we develop an efficient incremental low-rank LSTD(\u00ce\u00bb) algorithm that progresses towards the goal of better balancing computation and sample efficiency. The algorithm reduces the computation and storage complexity to the number of features times the chosen rank parameter while summarizing past samples efficiently to nearly obtain the sample complexity of LSTD. We derive a simulation bound on the solution given by truncated low-rank approximation, illustrating a bias- variance trade-off dependent on the choice of rank. We demonstrate that the algorithm effectively balances computational complexity and sample efficiency for policy evaluation in a benchmark task and a high-dimensional energy allocation domain.", 
    "authors": [
      {
        "name": "Clement Gehring"
      }, 
      {
        "name": "Yangchen Pan"
      }, 
      {
        "name": "Martha White"
      }
    ], 
    "keywords": "reinforcement learning, temporal difference, machine learning", 
    "title": "Incremental Truncated LSTD", 
    "type": "paper"
  }, 
  "1416": {
    "abstract": "Distance metric learning (DML) is critical for a wide variety of machine learning algorithms and pattern recognition applications. Transfer metric learning (TML) leverages the side information (e.g., similar/dissimilar constraints over pairs of samples) from related domains to help the target metric learning (with limited information). Multi-task metric learning (MTML) specializes TML in that the transfer is performed across all related domains. Current TML tools usually assume that different domains exploit the same feature representation, and thus are not applicable to tasks where data are drawn from heterogeneous domains. Heterogeneous transfer learning approaches handle heterogeneous domains by usually learning feature transformations across different domains. The learned transformation can be used to derive a metric, but these approaches are mostly limited by their capability of only handling two domains. This motivates the proposed heterogeneous multi-task metric learning (HMTML) framework for handling multiple domains by combining side information and unlabeled data. Specifically, HMTML learns the metrics for all different domains simultaneously by maximizing their high-order correlation (parameterized by feature covariance of unlabeled data) in a common subspace, which is induced by the transformations derived from the metrics. There exist a few heterogeneous transfer learning approaches that deal with multiple domains, but they heavily rely on the side information and ignore the high-order statistics (correlation information) which can only be discovered by simultaneously exploring all domains. Compared with these approaches, the proposed HMTML can utilize large amounts of unlabeled data to effectively explore such high-order information, and thus obtain more reliable feature transformations and also metrics. Extensive experiments on both multi-language text categorization and multi-view social image annotation demonstrate the effectiveness of the proposed method.", 
    "authors": [
      {
        "name": "Yong Luo"
      }, 
      {
        "name": "Yonggang Wen"
      }, 
      {
        "name": "Dacheng Tao"
      }
    ], 
    "keywords": "Heterogeneous domain, distance metric learning, multi-task, tensor, high-order statistics, unlabeled data", 
    "title": "On Combining Side Information and Unlabeled Data for Heterogeneous Multi-task Metric Learning", 
    "type": "paper"
  }, 
  "1433": {
    "abstract": "Recently, \\emph{Local Matrix Factorization} (LMF) has been shown to be more effective than traditional matrix factorization for rating prediction. The core idea for LMF is to first partition the original matrix  into several smaller submatrices, further exploit local structures of submatrices for better low-rank approximation. Various clustering-based methods with heuristic extensions have been proposed for LMF in the literature. To develop a more principled solution for LMF, this paper presents a Bayesian Probabilistic Multi-Topic Matrix Factorization model.  We treat the set of the rated items by a user as a document, and employ latent topic models to cluster items as topics. Subsequently, a user has a distribution over the set of topics. We further set topic-specific latent vectors for both users and items. The final prediction is obtained by an ensemble of the results from the corresponding topic-specific latent vectors in each topic. Using a multi-topic latent representation, our model is more powerful to reflect the complex characteristics for users and items in rating prediction, and enhance the model interpretability. Extensive experiments on large real-world datasets demonstrate the effectiveness of the proposed model.", 
    "authors": [
      {
        "name": "Keqiang Wang"
      }, 
      {
        "name": "Wayne Xin Zhao"
      }, 
      {
        "name": "Hongwei Peng"
      }, 
      {
        "name": "Xiaoling Wang"
      }
    ], 
    "keywords": "recommendation, matrix factorization, collaborative filtering, topic model", 
    "title": "Bayesian Probabilistic Multi-Topic Matrix Factorization For Rating Prediction", 
    "type": "paper"
  }, 
  "1435": {
    "abstract": "We consider a disaster response scenario where emergency responders have to complete rescue tasks in dynamic and uncertain environment with the assistance of multiple UAVs to collect information about the disaster space. To capture the uncertainty and partial observability of the domain, we model this problem as a POMDP. However, the resulting model is computationally intractable to be solved by most existing POMDP solvers due to the large state and action space. Therefore, we exploit the problem structure and propose a novel online planning algorithm to solve this model. Specifically, we generate plans for the responders based on Monte-Carlo simulations and compute actions for the UAVs according to the value of information. Experimental results in simulation confirm that our algorithm significantly outperforms the state-of-the-art both in time and solution quality.", 
    "authors": [
      {
        "name": "Feng Wu"
      }, 
      {
        "name": "Sarvapali Ramchurn"
      }
    ], 
    "keywords": "POMDPs, Disaster Response, Human-Agent Collective", 
    "title": "Coordinating Human-UAV Teams in Disaster Response", 
    "type": "paper"
  }, 
  "1437": {
    "abstract": "Mobile Landmark Search (MLS) recently receives increasing attention. However, it still remains unsolved due to two important issues. One is high bandwidth consumption of query transmission, and the other is the huge visual variations of query images. This paper proposes a Canonical View based Compact Visual Representation (2CVR) to handle these problems via novel three-stage learning. First, a submodular function is designed to measure visual representativeness and redundance of a view set. With it, canonical views, which capture key visual appearances of landmark with limited redundance, are efficiently discovered with an iterative mining strategy. Second, multimodal sparse coding is applied to transform multiple visual features into an intermediate representation which can robustly characterize visual contents of varied landmark images with only fixed canonical views. Finally, compact binary codes are learned on intermediate representation within a tailored binary embedding model which preserves visual relations of images measured on canonical views and removes noises. With 2CVR, robust visual query processing, low-cost of query transmission, and fast search process are simultaneously supported. Experiments demonstrate the superior performance of 2CVR over several state-of-the-art methods.", 
    "authors": [
      {
        "name": "Lei Zhu"
      }, 
      {
        "name": "Jialie Shen"
      }, 
      {
        "name": "Xiaobai Liu"
      }, 
      {
        "name": "Liang Xie"
      }
    ], 
    "keywords": "Mobile Landmark Search, Canonical View based Compact Visual Representation, Submodular function, Intermediate representation, Binary embedding model", 
    "title": "Learning Compact Visual Representation with Canonical Views for Robust Mobile Landmark Search", 
    "type": "paper"
  }, 
  "144": {
    "abstract": "The growth of Wikipedia, limited by the availability of knowledgeable authors, cannot keep pace with the ever increasing requirements and demands of the readers. In this work, we propose WikiWrite, a system capable of generating content for new Wikipedia articles automatically.   First, our technique obtains feature representations of entities on Wikipedia. We adapt an existing work on document embeddings to obtain vector representations of words and paragraphs. Using the representations, we identify articles that are very similar to the new entity on Wikipedia. We train machine learning classifiers using content from the similar articles to assign web retrieved content on the new entity into relevant sections in the Wikipedia article.  Second, we propose a novel abstractive summarization technique that uses a two-step integer-linear programming (ILP) model to synthesize the assigned content in each section and rewrite the content to produce a well-formed informative summary.  Our experiments show that our technique is able to reconstruct existing articles in Wikipedia with high accuracies. We also create several articles using our approach in the English Wikipedia, most of which have been retained in the online encyclopedia.", 
    "authors": [
      {
        "name": "Siddhartha Banerjee"
      }, 
      {
        "name": "Prasenjit Mitra"
      }
    ], 
    "keywords": "Wikipedia article generation, Automatic Summarization, Sentence rewriting, Document embeddings, Template generation", 
    "title": "WikiWrite: Generating Wikipedia Articles Automatically", 
    "type": "paper"
  }, 
  "1441": {
    "abstract": "We study the problem of locating a public facility on a real line or an interval, when agents' costs are their (expected) distances from the location of the facility. Our goal is to minimize the maximum envy over all agents, which we will refer to as the minimax envy objective, while at the same time ensuring that agents will report their most preferred locations truthfully. First, for the problem of locating the facility on a real line, we propose a class of truthful-in-expectation mechanisms that generalize the well-known LRM mechanism the best of which has performance arbitrarily close to the social optimum. Then, we restrict the possible locations of the facility to a real interval and consider two cases; when the interval is determined relatively to the agents' reports and when the interval is fixed in advance. For the former case, we prove that for any choice of such an interval, there is a mechanism in the aforementioned class with additive approximation arbitrarily close to the best approximation achieved by any truthful-in-expectation mechanism. For the latter case, we prove that the approximation of the best truthful-in-expectation mechanism is between 1/3 and 1/2.", 
    "authors": [
      {
        "name": "Qingpeng Cai"
      }, 
      {
        "name": "Aris Filos-Ratsikas"
      }, 
      {
        "name": "Pingzhong Tang"
      }
    ], 
    "keywords": "facility location, minimax envy, truthfulness, mechanism design without money", 
    "title": "Facility Location with Minimax Envy", 
    "type": "paper"
  }, 
  "1442": {
    "abstract": "Along with the increasing requirements, the hashtag recommendation task for microblogs has been receiving considerable attention in recent years. Various researchers have studied the problem from different aspects. However, most of these methods usually need  handcrafted features. Motivated by the successful use of convolutional neural networks (CNNs) for many natural language processing tasks, in this paper, we adopt CNNs to perform the hashtag recommendation problem. To incorporate the trigger words whose effectiveness have been experimentally evaluated in several previous works, we propose a novel architecture with an attentional mechanism. The results of experiments on the data collected from a real world microblogging service demonstrated that the proposed model outperforms state-of-the-art methods. By incorporating trigger words into the consideration, the relative improvement of the proposed method over the state-of-the-art method is around 11.1\\% in the F1-score.", 
    "authors": [
      {
        "name": "Yeyun Gong"
      }, 
      {
        "name": "Qi Zhang"
      }
    ], 
    "keywords": "Hashtag Recommendation, Convolutional Neural Network, Attention mechanism", 
    "title": "Hashtag Recommendation using Attention-based Convolutional Neural Network", 
    "type": "paper"
  }, 
  "1444": {
    "abstract": "Automatically extracting adverse drug events (drug-induced-disease relations) receives much research attention in the biomedical community. Previous work adopts pipeline models, firstly recognizing drug/disease entity mentions and then identifying adverse drug events from drug/disease pairs. In this paper, we investigate joint models for simultaneously extracting drugs, diseases and adverse drug events. Compared with pipeline models, joint models have two main advantages. First, they make use of information integration to facilitate performance improvement; second, they reduce error propagation in pipeline methods. We compare a discrete model and a deep neural model for joint extraction of drugs, diseases and adverse drug events. Experimental results on a standard ADE corpus show that the discrete joint model outperforms a state-of-the-art baseline pipeline significantly. In addition, when discrete features are replaced by neural features, the recall is further improved.", 
    "authors": [
      {
        "name": "Fei Li"
      }, 
      {
        "name": "Yue Zhang"
      }, 
      {
        "name": "Meishan Zhang"
      }, 
      {
        "name": "Donghong Ji"
      }
    ], 
    "keywords": "adverse drug events, joint models, discrete, neural", 
    "title": "Joint Models for Extracting Adverse Drug Events from Biomedical Text", 
    "type": "paper"
  }, 
  "1446": {
    "abstract": "Reasoning about the nested beliefs of others is important in many multi-agent scenarios. While epistemic and doxastic logics lay a solid groundwork to approach such reasoning, the computational complexity of these logics is often too high for many tasks. Proper Epistemic Knowledge Bases (PEKBs) enforce certain syntactic restrictions on formulae to obtain efficient querying: both disjunction and infinitely long nestings of modal operators are not permitted. Earlier work showed that PEKBs can be compiled into a prime implicate formula that can be queried in polynomial time, while more recently, it was shown that consistent PEKBs had certain logical properties that meant this compilation was unnecessary, while still retaining polynomial-time querying. In this paper, we present a sound belief update mechanism for PEKBs that ensures the knowledge base remains consistent knowledge when new information is added. This is achieved by first removing any formulae that contradict this new information. We show that this update mechanism can be computed in polynomial time, and assess it against the well-known KM postulates for belief update.", 
    "authors": [
      {
        "name": "Tim Miller"
      }, 
      {
        "name": "Christian Muise"
      }
    ], 
    "keywords": "belief update, proper epistemic knowledge bases, epistemic logic, multi-agent systems", 
    "title": "Belief Update for Proper Epistemic Knowledge Bases", 
    "type": "paper"
  }, 
  "1449": {
    "abstract": "Learning to hash has become a crucial technique for big data analytics. Among existing methods, supervised learning approaches play an important role as they can produce compact code and enable semantic search. However, the size of an instance-pairwise similarity matrix used in most supervised hashing methods is quadratic to the size of labeled training data, which is very expensive in terms of space, especially for a large-scale learning problem. This limitation hinders the full utilization of labeled instances for learning a more precise hashing model. To overcome this limitation, we propose a class-wise supervised hashing method that trains a model based on a class-pairwise similarity matrix, whose size is much smaller than the instance-pairwise similarity matrix. In addition, besides a set of hash functions, our proposed method learns a set of class-wise code-prototypes with active bits for different classes. These class-wise code-prototypes can make the constructed binary codes for training data be sparse, and thus speed up searching time for information retrieval. Experimental results verify the superior effectiveness of our proposed method over other baseline hashing methods.", 
    "authors": [
      {
        "name": "Long-Kai Huang"
      }, 
      {
        "name": "Sinno Jialin Pan"
      }
    ], 
    "keywords": "Learning to Hash, Approximate Nearest Neighbor Search, Active bits, Class-wise code-prototype", 
    "title": "Class-wise Supervised Hashing with Taxonomy and Active Bits", 
    "type": "paper"
  }, 
  "1450": {
    "abstract": "Variance Reducing (VR) stochastic methods are fast-converging alternatives to the classical \\emph{Stochastic Gradient Descent} (SGD) for solving large-scale regularized finite sum problems, e.g. Ridge Regression and $l_2$-Logistic Regression, especially when a highly accurate solution is required. \tOne critical step in VR is the function sampling.  \tState-of-the-art VR algorithms such as SVRG and SAGA, employ either Uniform Probability (UP) or Importance Probability (IP), which is inferior in reducing the variance and hence leads to suboptimal convergence rate.  \tTo tackle such problem, in this paper, we propose a novel sampling scheme that explicitly computes some Adaptive Probability (AP) at each iteration.  \tWe show that, equipped with AP, both SVRG and SAGA yield provably better convergence rate than the ones with UP or IP, which is confirmed by our experiments. Additionally, to cut down the per iteration computation load, an efficient variant is proposed by utilizing AP periodically, whose performance is empirically validated.", 
    "authors": [
      {
        "name": "Zebang Shen"
      }, 
      {
        "name": "Hui Qian"
      }
    ], 
    "keywords": "Stochastic Optimization, Variance Reducing, Adaptive Sampling", 
    "title": "Adaptive Variance Reducing for Stochastic Gradient Descent", 
    "type": "paper"
  }, 
  "1458": {
    "abstract": "Trace norm based rank regularization techniques have been successfully applied to learn a low-rank recovery for high-dimensional noise data. In many applications, it is desirable to add new samples to previously recovered data which is known as out-of-sample data recovery problem. However, traditional trace norm based regularization methods can not naturally cope with new samples and thus fail to deal with out-of-sample data recovery. In this paper, we propose a new robust out-of-sample data recovery (ROSR) model for trace normbased regularization methods. An effective iterative algorithm, with the proof of convergence, is presented to find the global optimal solution of ROSR problem. As an application, we apply our ROSR to image classification task. Experimental results on six image datasets demonstrate the effectiveness and benefits of the proposed ROSR method.", 
    "authors": [
      {
        "name": "Bo Jiang"
      }, 
      {
        "name": "Chris Ding"
      }, 
      {
        "name": "Bin Luo"
      }
    ], 
    "keywords": "Data representation, Low rank representation, Semi-supervised Learning", 
    "title": "Robust Out-of-Sample Data Representation", 
    "type": "paper"
  }, 
  "146": {
    "abstract": "Generally speaking, different persons tend to describe images from various aspects due to their subjective perception. As a result, generating the appropriate descriptions of images with both diversity and high quality is of great importance. In this paper, we propose a framework called GroupTalk to learn multiple image caption distributions simultaneously and effectively mimic the diversity of the image captions written by human beings. In particular, a novel iterative update strategy is proposed to separate training sentence samples into groups and learn their distributions. Furthermore, we introduced an efficient classifier to solve the problem brought about by the non-linear and discontinuous nature of language distributions which will impair performance. Experiments on several benchmark datasets show that GroupTalk naturally diversifies the generated captions of each image without sacrificing the accuracy.", 
    "authors": [
      {
        "name": "Zhuhao Wang"
      }, 
      {
        "name": "Fei Wu"
      }, 
      {
        "name": "Weiming Lu"
      }, 
      {
        "name": "Jun Xiao"
      }, 
      {
        "name": "Xi Li"
      }, 
      {
        "name": "Zitong Zhang"
      }, 
      {
        "name": "Yueting Zhuang"
      }
    ], 
    "keywords": "image captioning, diversity, neural network", 
    "title": "Diversely Image Captioning via GroupTalk", 
    "type": "paper"
  }, 
  "1466": {
    "abstract": "Video-based person re-identification (re-id) is an important application in practice. However, only a few methods have been presented for this problem. Since large variations exist between different pedestrian videos, as well as within each video, it\u00e2\u0080\u0099s challenging to conduct re-identification between pedestrian videos. In this paper, we propose a simultaneous intra-video and inter-video distance learning (SI2DL) approach for video-based person re-id. Specifically, SI2DL simultaneously learns an intra-video distance metric and an inter-video distance metric from the training videos. The intra-video distance metric is to make each video more compact, and the inter-video one is to make that the distance between two truly matching videos is smaller than that between two wrong matching videos. To enhance the discriminability of learned metrics, we design a video relationship model, i.e., video triplet, for SI2DL. Experiments on the public iLIDS-VID and PRID 2011 image sequence datasets show that our approach achieves the state-of-the-art performance.", 
    "authors": [
      {
        "name": "Xiaoke Zhu"
      }, 
      {
        "name": "Xiao-Yuan Jing"
      }, 
      {
        "name": "Fei Wu"
      }, 
      {
        "name": "Wangmeng Zuo"
      }, 
      {
        "name": "Hui Feng"
      }
    ], 
    "keywords": "Video-based person re-identification, Intra-video and inter-video distance metrics, Discriminability of metrics, Video triplet, Simultaneous intra-video and inter-video distance learning (SI2DL)", 
    "title": "Video-based Person Re-identification by Simultaneously Learning Intra-video and Inter-video Distance Metrics", 
    "type": "paper"
  }, 
  "1474": {
    "abstract": "Advances in video technology and data storage have made large scale video data collections of complex activities readily accessible. An increasingly popular approach for automatically inferring the details of a video is to associate the spatio-temporal segments in a video with its natural language descriptions. Most algorithms for connecting natural language with video rely on pre-aligned supervised training data. Recently, several models have been shown to be effective for unsupervised alignment of objects in video with language. However, it remains difficult to generate good spatio-temporal video segments for actions that align well with language. This paper presents a framework that extracts higher level representations of low-level action features through hyperfeature coding from video and aligns them with language. We propose a two-step process that creates a high-level action feature codebook with temporally consistent motions, and then applies an unsupervised alignment algorithm over the action codewords and verbs in the language to identify individual activities. We show an improvement over previous alignment models of objects and nouns on videos of biological experiments, and also evaluate our system on a larger scale collection of videos involving kitchen activities.", 
    "authors": [
      {
        "name": "Young Chol Song"
      }, 
      {
        "name": "Iftekhar Naim"
      }, 
      {
        "name": "Abdullah Al Mamun"
      }, 
      {
        "name": "Kaustubh Kulkarni"
      }, 
      {
        "name": "Parag Singla"
      }, 
      {
        "name": "Jiebo Luo"
      }, 
      {
        "name": "Daniel Gildea"
      }, 
      {
        "name": "Henry Kautz"
      }
    ], 
    "keywords": "Language and Vision, Grounded Language Learning, Video Alignment", 
    "title": "Unsupervised Alignment of Actions in Video with Text Descriptions", 
    "type": "paper"
  }, 
  "1479": {
    "abstract": "Subset selection that selects a few variables from a large set is a fundamental problem in many areas. The recently emerged Pareto Optimization for Subset Selection (POSS) method is a powerful approximation solver for this problem. However, POSS is not readily parallelizable, restricting its large-scale applications on modern computing architectures. In this paper, we propose PPOSS, a parallel version of POSS. Our theoretical analysis shows that PPOSS has good properties for parallelization while preserving the approximation quality: when the number of processors is limited (less than the total number of variables), the running time of PPOSS can be reduced linearly with respect to the number of processors; with increasing number of processors, the running time can be further reduced, eventually to a constant. Empirical studies verify the effectiveness of PPOSS, and moreover suggest that the asynchronous implementation is more efficient with little quality loss.", 
    "authors": [
      {
        "name": "Chao Qian"
      }, 
      {
        "name": "Jing-Cheng Shi"
      }, 
      {
        "name": "Yang Yu"
      }, 
      {
        "name": "Ke Tang"
      }, 
      {
        "name": "Zhi-Hua Zhou"
      }
    ], 
    "keywords": "subset selection, Pareto optimization, multi-objective evolutionary optimization, parallelization, theoretical analysis, empirical study", 
    "title": "Parallel Pareto Optimization for Subset Selection", 
    "type": "paper"
  }, 
  "1480": {
    "abstract": "Weighted voting games model decision-making bodies where decisions are made by a majority vote. In such games, each agent has a weight, and a coalition of agents wins the game if the sum of the weights of its members exceeds a certain quota. The Shapley value has been used as an index for the true power held by the agents in such games.  Earlier work has studied the implications of setting the value of the quota on the agents' power under the assumption that the game is given with a fixed set of agent weights. We focus on a model where the agent weights originate from a stochastic process, resulting in weight uncertainty. We analyze the expected effect of the quota on voting power given the weight generating process.  We examine two extreme cases of the balls and bins model: uniform and exponentially decaying probabilities. We show that the choice of a quota may have a large influence on the power disparity of the agents, even when the governing distribution is likely to result in highly similar weights for the agents. We characterize various interesting repetitive fluctuations patterns in agents' power as a function of the quota.", 
    "authors": [
      {
        "name": "Yoram Bachrach"
      }, 
      {
        "name": "Yuval Filmus"
      }, 
      {
        "name": "Joel Oren"
      }, 
      {
        "name": "Yair Zick"
      }
    ], 
    "keywords": "game theory, Average case analysis, Weighted voting games, Balls and bins", 
    "title": "A Characterization of Voting Power for Discrete Weight Distributions", 
    "type": "paper"
  }, 
  "1485": {
    "abstract": "Various hedonic content (e.g. video, music, news, jokes, pictures, social networks etc.), organized in a sequence of content items, increasingly dominates people's daily spare life. This paper studies common regularities of browsing behavior in these systems. Large empirical studies reveal that the distribution for the number of pages that a user browses within a visit obeys the inverse Gaussian distribution in spite of different visit time and user types, indicating that the choice threshold model of decision making on continuing or leave exists. Furthermore, how the stimulus intensity, in terms of the amount of recent enjoyed items,  affects the probability of continuing is measured, demonstrating a rising-falling mountain curve. A possible model to explain this curve is proposed based on the Joy-Satiety Contest model, which can successfully recover the original inverse Gaussian distribution for the number of visit pages. These browsing regularities can be used to develop technologies for increasing the user dwell time in hedonic content systems.", 
    "authors": [
      {
        "name": "Ping Luo"
      }, 
      {
        "name": "Ganbin Zhou"
      }, 
      {
        "name": "Qing He"
      }
    ], 
    "keywords": "user behavior, inverse Gaussian distribution, choice threshold, hedonic content systems", 
    "title": "Browsing Regularities in Hedonic Content Systems: the More the Merrier?", 
    "type": "paper"
  }, 
  "1489": {
    "abstract": "Overlapping community detection has drawn much attention recently since it allows nodes in a network to have multiple community memberships. A standard framework to deal with overlapping community detection is Matrix Factorization (MF). Although all existing MF-based approaches use links as input to identify communities, the relationship between links and communities is still under-investigated. Most of the approaches only view links as consequences of communities (community-to-link) but fail to explore how nodes' community memberships can be represented by their linked neighbors (link-to-community). In this paper, we propose a Homophily-based Non-negative Matrix Factorization (HNMF) to model both-sided relationships between links and communities. From the community-to-link perspective, we apply a preference-based pairwise function by assuming that nodes with common communities have a higher probability to build links than those without common communities. From the link-to-community perspective, we propose a community representation learning with network embedding by assuming that linked nodes have similar community representations. We conduct experiments on several real-world networks and the results show that our HNMF model is able to find communities with better quality compared with state-of-the-art baselines.", 
    "authors": [
      {
        "name": "Hongyi Zhang"
      }, 
      {
        "name": "Tong Zhao"
      }, 
      {
        "name": "Irwin King"
      }, 
      {
        "name": "Michael R. Lyu"
      }
    ], 
    "keywords": "Community Detection, Social Networks, Matrix Factorization", 
    "title": "Modeling the Homophily Effect between Links and Communities for Overlapping Community Detection", 
    "type": "paper"
  }, 
  "1497": {
    "abstract": "Predicting anchor links across social networks has important implications to an array of applications, including cross-network information diffusion and cross-domain recommendation. One challenging problem is: whether and to what extent we can address the anchor link prediction problem, if only structural information of networks is available. Most existing methods, unsupervised or supervised, directly work on networks themselves rather than on their intrinsic structural regularities, and thus their effectiveness is sensitive to the high dimension and sparsity of networks. To offer a robust method, we propose a novel supervised model, called PALE, which employs network embedding with awareness of observed anchor links as supervised information to capture the major and specific structural regularities and further learns a stable cross-network mapping for predicting anchor links. Through extensive experiments on two realistic datasets, we demonstrate that PALE significantly outperforms the state-of-the-art methods.", 
    "authors": [
      {
        "name": "Tong Man"
      }, 
      {
        "name": "Huawei Shen"
      }, 
      {
        "name": "Shenghua Liu"
      }, 
      {
        "name": "Xiaolong Jin"
      }, 
      {
        "name": "Xueqi Cheng"
      }
    ], 
    "keywords": "Link Prediction, Anchor Link, Network Embedding", 
    "title": "Predict Anchor Links across Social Networks via an Embedding Approach", 
    "type": "paper"
  }, 
  "1504": {
    "abstract": "In the past decades, forgetting has been investigated for many logics and has found many applications in knowledge representation and reasoning. However, forgetting in multi-agent modal logics has largely been unexplored. In this paper, we study forgetting in multi-agent modal logics. We adopt the semantic definition of existential bisimulation quantifiers as that of forgetting. We propose a syntactical way of performing forgetting based on the canonical formulas of modal logics introduced by Moss. We show that the result of forgetting a propositional atom from a satisfiable canonical formula can be computed by simply eliminating the literals of the atom. Thus we show that Kn, Dn, Tn, K45n, KD45n and S5n are closed under forgetting, and hence have uniform interpolation.", 
    "authors": [
      {
        "name": "Liangda Fang"
      }, 
      {
        "name": "Yongmei Liu"
      }, 
      {
        "name": "Hans van Ditmarsch"
      }
    ], 
    "keywords": "Forgetting, Uniform interpolation, Multi-agent modal logics, Bisimluation quantifiers", 
    "title": "Forgetting in Multi-Agent Modal Logics", 
    "type": "paper"
  }, 
  "1509": {
    "abstract": "The paper investigates the relationship between knowledge representation (KR) languages P-log and LP^MLN designed for representing and reasoning with logic and probability. We give a translation from LP^MLN to P-log which preserves probabilistic functions defined by LP^MLN programs and complements recent research by the authors of LP^MLN where they give a similar translation from a subset of P-log to their language. This work sheds light on the different ways to treat inconsistency provided by these languages.", 
    "authors": [
      {
        "name": "Evgenii Balai"
      }, 
      {
        "name": "Michael Gelfond"
      }
    ], 
    "keywords": "Knowledge Representation, Probabilistic Reasoning, Logic programming, Answer Set Programming", 
    "title": "On the relationship between P-log and LP^MLN", 
    "type": "paper"
  }, 
  "1513": {
    "abstract": "Person re-identification, as an important task in video surveillance and forensics applications, has been widely studied. But most of previous approaches are based on the key assumption that images for comparison have the same resolution and a uniform scale. Some recent works investigate how to match low resolution query images against high resolution gallery images, but still assume that the low-resolution query images have the same scale. In real scenarios, person images may not only be with low-resolution but also have different scales. Through investigating the distance variation behavior by changing image scales, we observe that scale-distance functions, generated by image pairs under different scales from the same person or different persons, are distinguishable and can be classified as feasible (for a pair of images from the same person) or infeasible (for a pair of images from different persons). The scale-distance functions are further represented by parameter vectors in the scale-distance function space. On this basis, we propose to learn a discriminating surface separating these feasible and infeasible functions in the scale-distance function space, and use it for re-identifying persons. Experimental results on two simulated datasets and one public dataset demonstrate the effectiveness of the proposed framework.", 
    "authors": [
      {
        "name": "Zheng Wang"
      }, 
      {
        "name": "Ruimin Hu"
      }, 
      {
        "name": "Yi Yu"
      }, 
      {
        "name": "Junjun Jiang"
      }, 
      {
        "name": "Chao Liang"
      }, 
      {
        "name": "Jinqiao Wang"
      }
    ], 
    "keywords": "person re-identification, scale-adaptive, low-resolution", 
    "title": "Scale-adaptive Low-resolution Person Re-identification via Learning A Discriminating Surface", 
    "type": "paper"
  }, 
  "1517": {
    "abstract": "This paper proposes a logical framework for representing and reasoning about imperfect information games. We first extend the game description language (GDL) with the standard epistemic operators and provide it with a semantics based on the epistemic state transition model. We then demonstrate how to use the language to represent game rules and formalize epistemic properties of a game with imperfect information,  and how to use the framework to reason about agents' knowledge of game states as well as of the other agents' knowledge on the basis of the game rules. We finally show that the model-checking problem of the framework is \\Theta_2^p-hard yet in \\Delta_2^p. These results indicate that the framework makes a good balance between expressive power and computational efficiency.", 
    "authors": [
      {
        "name": "Guifei Jiang"
      }, 
      {
        "name": "Dongmo Zhang"
      }, 
      {
        "name": "Laurent Perrussel"
      }, 
      {
        "name": "Heng Zhang"
      }
    ], 
    "keywords": "Epistemic and strategic reasoning, Strategic logic, Game description language, Epistemic logic, Multi-agent system", 
    "title": "Epistemic GDL: A logic for representing and reasoning about imperfect information games", 
    "type": "paper"
  }, 
  "1526": {
    "abstract": "With the rapid increase in the available data, it becomes computationally harder to extract useful information. Thus, several techniques like PCA were proposed to embed high-dimensional data into low-dimensional latent space. However, these techniques don't take the data relations into account. This motivated the development of other techniques like MDS and LLE which preserve the relations between the data instances. Nonetheless, all these techniques still use latent features, which are difficult for data analysts to understand and grasp the information encoded in them. In this work, a new embedding technique is proposed to mitigate the previous problems by projecting the data to a space described by few points (i.e, exemplars) which preserves the relations between the data points. The proposed method Exemplar-based Kernel Preserving (EBEK) embedding is shown theoretically to achieve the lowest reconstruction error of the kernel matrix. Using EBEK in approximate nearest neighbor task shows its ability to outperform related work by up to 60% in the recall while maintaining a good running time. In addition, our interpretability experiments show that EBEK\u00e2\u0080\u0099s selected basis are more understandable than the latent basis in images datasets.", 
    "authors": [
      {
        "name": "Rania Ibrahim"
      }, 
      {
        "name": "Ahmed Elbagoury"
      }
    ], 
    "keywords": "Dimensionality Reduction, Similarity Preserving Embedding, Exemplar-based Method", 
    "title": "EBEK: Exemplar-based Kernel Preserving Embedding", 
    "type": "paper"
  }, 
  "1529": {
    "abstract": "Troubleshooting is the process of diagnosing and repairing a system that is behaving abnormally. Diagnostic and repair actions may incur costs, and traditional troubleshooting algorithms aim to minimize the costs incurred until the system is fixed. We propose an anticipatory troubleshooting algorithm, which is able to reason about both current and future troubleshooting processes, thus able to minimize troubleshooting costs over time. To reason about failures over time, we incorporated statistical tools from survival analysis that allows predicting when a failures are likely to occur. This additional prognostic information allows the resulting troubleshooting algorithm is better diagnose current faults as well as minimize costs over time.", 
    "authors": [
      {
        "name": "Netantel Hasidi"
      }, 
      {
        "name": "Roni Stern"
      }, 
      {
        "name": "Meir Kalech"
      }, 
      {
        "name": "Shulamit Reches"
      }
    ], 
    "keywords": "Model-based diagnosis, Automated troubleshooting, Planning", 
    "title": "Anticipatory Troubleshooting", 
    "type": "paper"
  }, 
  "1531": {
    "abstract": "Deep neural networks have been shown to achieve state-of-the-art performance in several machine learning tasks. Stochastic Gradient Descent (SGD) is the preferred optimization algorithm for training these networks and asynchronous SGD (ASGD) has been widely adopted for accelerating the training of large-scale deep networks in a distributed computing environment. However, in practice it is quite challenging to tune the training hyperparameters (such as learning rate) when using ASGD so as achieve convergence and linear speedup, since the stability of the optimization algorithm is strongly influenced by the asynchronous nature of parameter updates. In this paper, we propose a variant of the ASGD algorithm in which the learning rate is modulated according to the gradient staleness and provide theoretical guarantees for convergence of this algorithm. Experimental verification is performed on commonly-used image classification benchmarks: CIFAR10 and Imagenet to demonstrate the superior effectiveness of the proposed approach, compared to SSGD (Synchronous SGD) and the conventional ASGD algorithm.", 
    "authors": [
      {
        "name": "Wei Zhang"
      }, 
      {
        "name": "Suyog Gupta"
      }, 
      {
        "name": "Xiangru Lian"
      }, 
      {
        "name": "Ji Liu"
      }
    ], 
    "keywords": "ASGD, distributed, learning rate tuning, deep learning", 
    "title": "Staleness-aware Async-SGD for Distributed Deep Learning", 
    "type": "paper"
  }, 
  "1533": {
    "abstract": "Deep neural networks frequently require the careful tuning of model hyperparameters. Recent research has shown that automated early termination of underperformance runs can speed up hyperparameter searches. However, these studies have used only learning curve for predicting the eventual model performance. In this study, we propose using weight features extracted from network weights at an early stage of the learning process as explanation variables for predicting the eventual model performance. We conduct experiments on hyperparameter searches with various types of network architecture on three image datasets and apply the random forest method for predicting the eventual model performance. The results show that use of the weight features improves the predictive performance compared with use of the learning curve. In all three datasets, the most important feature for the prediction was related to weight changes in the last convolutional layers. Our findings demonstrate that using weight features can help construct prediction models with a smaller number of training samples and terminate underperformance runs at an earlier stage of the learning process of DNNs than the conventional use of learning curve, thus facilitating the speed-up of hyperparameter searches.", 
    "authors": [
      {
        "name": "Yasunori Yamada"
      }, 
      {
        "name": "Tetsuro Morimura"
      }
    ], 
    "keywords": "Deep neural networks, Hyperparameter search, Prediction, Early termination", 
    "title": "Weight features for predicting future model performance of deep neural networks", 
    "type": "paper"
  }, 
  "1535": {
    "abstract": "Learning from human behaviors in the real world is important for building human-aware intelligent systems such as personalized digital assistants and autonomous humanoid robots. Everyday activities of human life can now be measured through wearable sensors. However, innovations are required to learn this sensory data in an online incremental manner over an extended period of time.  Here we propose a dual memory architecture that processes slow-changing global patterns as well as keeps track of the fast-changing local behaviors over a lifetime.  The lifelong learnability is achieved by developing new techniques, such as weight transfer and an online learning algorithm with incremental features. The proposed model outperformed other comparable methods on two real-life data-sets: the image-stream dataset and the real-world lifelogs collected through the Google Glass for 46 days.", 
    "authors": [
      {
        "name": "Sang-Woo Lee"
      }, 
      {
        "name": "Chung-Yeon Lee"
      }, 
      {
        "name": "Dong Hyun Kwak"
      }, 
      {
        "name": "Jiwon Kim"
      }, 
      {
        "name": "Jeonghee Kim"
      }, 
      {
        "name": "Byoung-Tak Zhang"
      }
    ], 
    "keywords": "Lifelong Learning, Wearable Device, Online Learning, Deep Learning, Dual Memory Architecture", 
    "title": "Dual-Memory Deep Learning Architectures for Lifelong Learning of Everyday Human Behaviors", 
    "type": "paper"
  }, 
  "1541": {
    "abstract": "In this paper, we adopt the representation learning approach to align users across multiple social networks where the social structures of the users are exploited. In particular, we propose to learn a network embedding with the follower-ship/followee-ship of each user explicitly modeled as input/output context vector representations so as to preserve the proximity of users with ``similar'' followers/followees in the embedded space. For the alignment, we add both known and potential anchors users across the networks to facilitate the transfer of context information across networks. We solve both the network embedding learning problem and the user alignment problem simultaneously under a unified optimization framework. The  stochastic gradient descent and negative sampling algorithm are used to address the issue of the scalability. Extensive experiments on large-scale social network datasets demonstrate the effectiveness and efficiency of the proposed approach compared with several state-of-the-art methods.", 
    "authors": [
      {
        "name": "Li Liu"
      }, 
      {
        "name": "William Kwok-Wai Cheung"
      }, 
      {
        "name": "Xin Li"
      }, 
      {
        "name": "Lejian Liao"
      }
    ], 
    "keywords": "User Alignment, Network Embedding, Social Network", 
    "title": "Aligning Users Across Social Networks Using Network Embedding", 
    "type": "paper"
  }, 
  "1547": {
    "abstract": "Information diffusion in online social networks has attracted substantial research effort. Although recent models begin to incorporate interactions among contagions, they still don't consider the comprehensive interactions involving users and contagions as a whole. Moreover, the interactions obtained in previous work are modeled as latent factors and thus are difficult to understand and interpret. In this paper, we investigate the contagion adoption behavior by incorporating various types of interactions into a coherent model, and propose a novel interaction-aware diffusion framework called IAD. IAD exploits the social network structures to distinguish user roles,  and uses both structures and texts to categorize contagions. Experiments with large-scale Weibo dataset demonstrate that IAD outperforms the state-of-art baseline in terms of F1-score and accuracy, as well as the runtime for learning. In addition, the interactions obtained through learning reveal interesting findings, e.g., food-related contagions have the strongest capability to suppress other contagions' propagation, while advertisement-related contagions have the weakest capability.", 
    "authors": [
      {
        "name": "Yuan Su"
      }, 
      {
        "name": "Xi Zhang"
      }, 
      {
        "name": "Philip S. Yu"
      }, 
      {
        "name": "Wen Hua"
      }, 
      {
        "name": "Xiaofang Zhou"
      }, 
      {
        "name": "Binxing Fang"
      }
    ], 
    "keywords": "Social Networks, Information Diffusion, Information Interaction, User Interaction", 
    "title": "Understanding Information Diffusion under Interactions", 
    "type": "paper"
  }, 
  "1555": {
    "abstract": "Heterogeneity of features and lack of correspondence between data points of different domains are the two primary challenges while performing feature transfer. In this paper, we present a novel supervised domain adaptation algorithm (SHDA-RF) that learns the mapping between heterogeneous features of different dimensions. Our algorithm uses the shared label distributions present across the domains as pivots for learning a sparse feature transformation. The shared label distributions and the relationship between the feature spaces and the label distributions are estimated in a supervised manner using random forests. We conduct extensive experiments on three diverse datasets of varying dimensions and sparsity to verify the superiority of the proposed approach over other baseline and state of the art transfer approaches.", 
    "authors": [
      {
        "name": "Sanatan Sukhija"
      }, 
      {
        "name": "Narayanan C Krishnan"
      }, 
      {
        "name": "Gurkanwal Singh"
      }
    ], 
    "keywords": "Domain Adaptation, Activity Recognition, Feature Remapping", 
    "title": "Supervised Heterogeneous Domain Adaptation via Random Forests", 
    "type": "paper"
  }, 
  "156": {
    "abstract": "Selecting a set of alternatives based on the preferences of agents is  an important problem in committee selection and beyond. Among the various criteria put forth for desirability of the committee, Pareto optimality is a minimal and important requirement. As asking agents to specify their preferences over exponentially many subsets of alternatives is practically infeasible, we assume that each agent specifies a weak order on single alternatives, from which a preference relation over subsets is derived using some preference extension. We consider four prominent set extensions (responsive,  leximax, best, and worst). For each of them, we consider the corresponding Pareto optimality notion, and we study the complexity of computing and verifying Pareto optimal outcomes. We also consider strategic issues: for three of the set extensions, we present linear-time, Pareto optimal and strategyproof algorithms that work even for weak preferences.", 
    "authors": [
      {
        "name": "Haris Aziz"
      }, 
      {
        "name": "J\u00e9r\u00f4me Lang"
      }, 
      {
        "name": "Jerome Monnot"
      }
    ], 
    "keywords": "computational social choice, multi-winner voting, committee voting, proportional representation", 
    "title": "Computing Pareto Optimal Committees", 
    "type": "paper"
  }, 
  "1562": {
    "abstract": "This paper presents an end-to-end neural network model, named Neural Generative Question Answering (GenQA), that can generate answers to simple factoid questions, both in natural language. More specifically, the model is built on the encoder-decoder framework for sequence-to-sequence learning, while equipped with the ability to access an embedded knowledge-base through an attention-like mechanism. The model is trained on a corpus of question-answer pairs, with their associated triples in the given knowledge-base. Empirical study shows the proposed model can effectively deal with the language variation of the question and generate a right answer by referring to the facts in the knowledge-base. The experiment on question answering demonstrates that the proposed model can outperform the embedding-based QA model as well as the neural dialogue models trained on the same data.", 
    "authors": [
      {
        "name": "Jun Yin"
      }, 
      {
        "name": "Xin Jiang"
      }, 
      {
        "name": "Zhengdong Lu"
      }, 
      {
        "name": "Lifeng Shang"
      }, 
      {
        "name": "Hang Li"
      }, 
      {
        "name": "Xiaoming Li"
      }
    ], 
    "keywords": "generative question answering, knowledge base embedding, neural dialogue model", 
    "title": "Neural Generative Question Answering", 
    "type": "paper"
  }, 
  "1565": {
    "abstract": "In this paper we study the problem of learning discriminative features (segments) from unlabelled time series data. The discriminative segments are often referred to as shapelets [Ye and Keogh, 2009] of time series. Discovering shapelets for time series classification has been widely studied, where many search-based algorithms are proposed to efficiently scan and select segments from a pool of candidates. However, search-based algorithms may incur high time cost when the segment candidate pool is large. Alternatively, a recent work [Grabocka et al., 2014] uses regression learning to directly learn, instead of searching, shapelets from time series. We propose a new Unsupervised Shapelets Learning Model (USLM for short) which can efficiently learn shapelets from unlabeled time series data. The learning function combines the strength of pseudo-label, spectral analysis, shaplets regularization and regularized least-square to auto-learn shapelets, class labels and classification boundaries simultaneously. A coordinate descent algorithm is used to iteratively solve the learning function. Experiments on real-world time series data show that USLM outperforms search-based algorithms on unlabeled time series data, e.g., USLM obtains 14% improvement compared to k-Shape on real-world data.", 
    "authors": [
      {
        "name": "Qin Zhang"
      }, 
      {
        "name": "Peng Zhang"
      }, 
      {
        "name": "Jia Wu"
      }, 
      {
        "name": "Yingjie Tian"
      }, 
      {
        "name": "Chengqi Zhang"
      }
    ], 
    "keywords": "Unsupervised, feature learning, time series", 
    "title": "Unsupervised Feature Learning from Time Series", 
    "type": "paper"
  }, 
  "1567": {
    "abstract": "Standard subspace algorithms learn Linear Dynamical Systems (LDSs) from time series with the least-square method, where the stability of the system is not naturally guaranteed. In this paper, we propose a novel approach for learning stable systems by enforcing stability directly on the least-square solutions. To this end, we first explore the spectral-radius property of the least-square transition matrix and then determine the key component that incurs the instability of the transition matrix. By multiplying the unstable component with a weight matrix on the right side, we obtain a weighted-least-square transition matrix that is further optimized to minimize the reconstruction error of the state sequence while still maintaining the stable constraint. Comparative experimental evaluations demonstrate that our proposed methods outperform the state-of-the-art methods regarding the reconstruction accuracy and the learning efficiency.", 
    "authors": [
      {
        "name": "Wenbing Huang"
      }, 
      {
        "name": "Lele Cao"
      }, 
      {
        "name": "Fuchun Sun"
      }, 
      {
        "name": "Deli Zhao"
      }, 
      {
        "name": "Huaping Liu"
      }, 
      {
        "name": "Shanshan Yu"
      }
    ], 
    "keywords": "linear dynamical systems, subspace methods, time series, stability, least square method", 
    "title": "Learning Stable Linear Dynamical Systems with the Weighted Least Square Method", 
    "type": "paper"
  }, 
  "1574": {
    "abstract": "The partition-based clustering algorithms, like K-means and fuzzy K-means, are most widely and successfully used in data mining in the past decades. In this paper, we present a sparse and robust fuzzy K-means clustering algorithm for numerical data, wich is an extension to the standard fuzzy K-means algorithm by incorporating robust functions, rather than the square data fitting term, to handle outliers. More importantly, combined with the concept of sparse coding, the proposed algorithm further introduces a penalty term to make the object-clusters membership of each sample with a suitable sparsity. Experimental results on benchmark datasets demonstrate that the proposed algorithm not only can ensure the effectiveness of such robust soft clustering algorithm in real world applications, but also can avoid the performance degradation by considering the membership sparsity.", 
    "authors": [
      {
        "name": "Jinglin Xu"
      }, 
      {
        "name": "Junwei Han"
      }, 
      {
        "name": "Kai Xiong"
      }, 
      {
        "name": "Feiping Nie"
      }
    ], 
    "keywords": "Clustering, Fuzzy K-means, Robustness and sparsity", 
    "title": "Robust and Sparse Fuzzy K-Means Clustering", 
    "type": "paper"
  }, 
  "1576": {
    "abstract": "The estimation of causal effects from data is a fundamental problem in science and engineering, but relies on our ability to identify effects from the underlying causal model.  In this paper, we extend graph-based identification methods for linear models by allowing background knowledge in the form of externally evaluated parameters. Such information could be obtained, for example, from a previously conducted randomized experiment, from substantive understanding of the domain, or even from another identification technique. To incorporate such information systematically, we propose the addition of auxiliary variables to the model, which are constructed so that certain paths will be conveniently cancelled.  This cancellation allows the auxiliary variables to help conventional methods of identification (e.g., single-door criterion, instrumental variables, half-trek criterion), as well as model testing (e.g., d-separation, over-identification).  Moreover, by iteratively alternating steps of identification and adding auxiliary variables, we can improve the power of existing identification and model testing methods, even without additional knowledge. We operationalize this general approach for instrumental sets (a generalization of instrumental variables) and show that the resulting procedure subsumes the most general identification method for linear systems known to date. We further discuss the application of this new operation in the tasks of model testing and z-identification.", 
    "authors": [
      {
        "name": "Bryant Chen"
      }, 
      {
        "name": "Judea Pearl"
      }, 
      {
        "name": "Elias Bareinboim"
      }
    ], 
    "keywords": "causal inference, structural equation models, identification, graphical models, instrumental variables, instrumental sets", 
    "title": "Identification of Causal Effects by Auxiliary Instruments in Linear Systems", 
    "type": "paper"
  }, 
  "158": {
    "abstract": "Expert finding for question answering is a challenging problem in Community-based Question Answering (CQA) site, arising in many applications such as question routing and identification of best answers. In order to provide high-quality experts, many existing approaches learn the user model from their past question-answering activities in CQA sites, which suffer from the sparsity problem of CQA data. In this paper, we consider the problem of expert finding from the viewpoint of learning ranking metric embedding. We propose a novel ranking metric network learning framework for expert finding by exploiting users\u00e2\u0080\u0099 relative quality rank to the given questions and their social relations. We then develop a random-walk based learning method with recurrent neural networks for ranking metric network embedding. The extensive experiments on a large-scale dataset from a real world CQA site show that our method achieve better performance than other state-of-the-art solutions to the problem.", 
    "authors": [
      {
        "name": "Zhou Zhao"
      }, 
      {
        "name": "Qifan Yang"
      }, 
      {
        "name": "Deng Cai"
      }, 
      {
        "name": "Xiaofei He"
      }, 
      {
        "name": "Yueting Zhuang"
      }
    ], 
    "keywords": "expert finding, community-based question answering, ranking metric network learning", 
    "title": "Expert Finding for Question Answering via Ranking Metric Network Learning", 
    "type": "paper"
  }, 
  "1581": {
    "abstract": "The paper presents a new relaxation for hybrid planning with continuous numeric and propositional state variables based on subgoaling, generalising \"$ h^1 $-like\" heuristics ($ h_{add} $, $ h_{max} $) to such problems. Our relaxation improves on existing interval-based relaxations by taking into account some negative interactions between effects in achieving a subgoal, resulting in better estimates. We show conditions on the planning model that ensure this new relaxation is tractable, and, for the $ h_{max} $ version, admissible. The new heuristic can be combined with the interval-based relaxation, making it applicable to general numeric planning, while still providing more informed estimates for the subgoals that meet these conditions. Experiments show the effectiveness of its inadmissible and admissible version on satisficing and optimal numeric planning, respectively. As far as we know, this is the first admissible heuristic enabling optimal hybrid planning of this kind", 
    "authors": [
      {
        "name": "Enrico Scala"
      }, 
      {
        "name": "Patrik Haslum"
      }, 
      {
        "name": "Sylvie Thiebaux"
      }
    ], 
    "keywords": "Numeric Planning, Heuristics, Mixed continuous and discrete variables", 
    "title": "Heuristics for Numeric Planning via Subgoaling", 
    "type": "paper"
  }, 
  "1583": {
    "abstract": "Bilingual embedding has been shown helpful for Statistical Machine Translation (SMT). However, most existing methods suffer from two obvious drawbacks. First, they only focus on simple context such as word count or co-occurrence in document or sliding window to build word embedding, ignoring latent useful information from selected context.  Second, word sense but not word is supposed to be the minimal semantic unit while most existing works are still for word representation. This paper presents Bilingual Graph-based Semantic Model (BGSM) to alleviate such shortcomings. By means of maximum complete sub-graph (clique) for context selection, BGSM is capable of effectively modeling word sense representation instead of word itself. The proposed model is applied to phrase pair translation probability estimation and generation for SMT. The results show that BGSM can enhance SMT both in performance (up to +1.3 BLEU) and efficiency in comparison with existing methods.", 
    "authors": [
      {
        "name": "Rui Wang"
      }, 
      {
        "name": "Hai Zhao"
      }, 
      {
        "name": "Sabine Ploux"
      }, 
      {
        "name": "Bao-Liang Lu"
      }, 
      {
        "name": "Masao Utiyama"
      }
    ], 
    "keywords": "Bilingual Graph-based Semantic Model, Statistical Machine Translation, Bilingual Embedding", 
    "title": "A Bilingual Graph-based Semantic Model for Statistical Machine Translation", 
    "type": "paper"
  }, 
  "1585": {
    "abstract": "Weakly-supervised object detection (WOD) is one of the most interesting yet challenging problems in computer vision field in recent years. In this issue, the key problem is to simultaneously infer the exact object locations in the training images and train the object detectors, given only the training images with weak image-level labels. Intuitively, by simulating the selective attention mechanism of human visual system, saliency detection method can automatically select attractable objects in scenes and thus appears to be a promising way to provide useful priors for WOD. However, the way to adopt saliency detection in WOD is not trivial as directly using saliency detection in WOD can hardly obtain satisfactory results. To this end, this paper first comprehensively analyzes the challenges in applying saliency detection for WOD. Then, we make one of the earliest efforts to bridge saliency detection to WOD via the self-paced curriculum learning, which can guide the learning procedure to gradually achieve faithful knowledge of multi-class objects from easy to hard. The experimental results on benchmark dataset demonstrate that the proposed approach can successfully bridge the performance gap between saliency detection and WOD and finally achieve better object detection results than previous state-of-the-arts.", 
    "authors": [
      {
        "name": "Dingwen Zhang"
      }, 
      {
        "name": "Deyu Meng"
      }, 
      {
        "name": "Long Zhao"
      }, 
      {
        "name": "Junwei Han"
      }
    ], 
    "keywords": "Weakly-supervised learning, Self-paced Curriculum Learning, Object detection", 
    "title": "Bridging Saliency Detection to Weakly Supervised Object Detection Based on Self-paced Curriculum Learning", 
    "type": "paper"
  }, 
  "1595": {
    "abstract": "We study the problem of bag-level classification from generalized multiple-instance (GMI) data. GMI learning is an extension of the popular multiple-instance setting. In GMI data, bags are labeled positive if they contain instances of certain types, and avoid instances of other types. For example, an image of a \"sunny beach\" should contain sand and sea, but not clouds. We formulate a novel generative process for the GMI setting in which bags are distributions over instances. In this model, we show that a broad class of distribution-distance kernels is sufficient to represent arbitrary GMI concepts. Further, we show that a variety of previously proposed kernel approaches to the standard MI and GMI settings can be unified under the distribution kernel framework. We perform an extensive empirical study which indicates that the family of distribution distance kernels is accurate for a wide variety of real-world MI and GMI tasks as well as efficient when compared to a large set of baselines. Our theoretical and empirical results indicate that distribution-distance kernels can serve as a unifying framework for learning bag labels from GMI (and therefore MI) problems.", 
    "authors": [
      {
        "name": "Gary Doran"
      }, 
      {
        "name": "Andrew Latham"
      }, 
      {
        "name": "Soumya Ray"
      }
    ], 
    "keywords": "multiple-instance learning, kernel methods, machine learning", 
    "title": "A Unifying Framework for Learning Bag Labels from Generalized Multiple-Instance Data", 
    "type": "paper"
  }, 
  "1597": {
    "abstract": "We propose a new subspace clustering model to segment data which is drawn from multiple linear or affine subspaces. Unlike the well-known sparse subspace clustering (SSC) and low-rank representation (LRR) which transfer the subspace clustering problem into two steps\u00e2\u0080\u0099 algorithm including building the affinity matrix and spectral clustering, our proposed model directly learns the different subspaces\u00e2\u0080\u0099 indicator so that low-rank based different groups are obtained clearly. To better approximate the low-rank constraint, we propose to use Schatten p-norm to relax the rank constraint instead of using trace norm. We tactically avoid the integer programming problem imposed by group indicator constraint to make our algorithm more efficient and scalable. Furthermore, we extend our discussion to the general case in which subspaces don\u00e2\u0080\u0099t pass the original point. We also provide the convergence proof for our new algorithm. We evaluate the proposed method on synthetic data, motion segmentation Hopkins 155 data, and image clustering benchmark data. All experimental results demonstrate the effectiveness of our proposed model.", 
    "authors": [
      {
        "name": "Feiping Nie"
      }, 
      {
        "name": "Heng Huang"
      }
    ], 
    "keywords": "Subspace Clustering, Low-Rank Model, Discrete Constraint", 
    "title": "Subspace Clustering via New Discrete Group Structure Constrained Low-Rank Model", 
    "type": "paper"
  }, 
  "1599": {
    "abstract": "A probabilistic program defines a probability measure over its semantic structures.  One common goal of probabilistic programming languages (PPLs) is to compute posterior probabilities for arbitrary models and queries, given observed evidence, using a generic inference engine.  Most PPL inference engines---even the compiled ones---incur significant run-time interpretation overhead, especially for contingent and open-universe models. This paper describes Swift, a compiler for the BLOG PPL. Swift-generated code incorporates optimizations that eliminate interpretation overhead, maintain dynamic dependencies efficiently, and handle memory management for possible worlds of varying sizes. Experiments comparing \\name with other PPL engines on a variety of inference problems demonstrate speedups ranging from 12x to 326x.", 
    "authors": [
      {
        "name": "Yi Wu"
      }, 
      {
        "name": "Lei Li"
      }, 
      {
        "name": "Stuart Russell"
      }, 
      {
        "name": "Rastislav Bodik"
      }
    ], 
    "keywords": "probabilistic programming language, probabilistic inference, compilation, open-universe model", 
    "title": "Swift: Compiled Inference for Probabilistic Programming Languages", 
    "type": "paper"
  }, 
  "16": {
    "abstract": "We consider the application of Bayesian spike-and-slab models in high-dimensional feature selection problems. To do so,  we propose a simple yet effective fast approximate Bayesian inference algorithm based on Laplace's method. We exploit two efficient optimization methods, GIST and L-BFGS, to obtain the mode of the posterior distribution. Then we propose an ensemble Nystrom approach to calculate the diagonal of the inverse Hessian over the mode to obtain the approximate posterior marginals in O(knp) time, $k << p$. The theoretical analysis of the ensemble method is also provided. With the posterior marginals of model weights, we use quadrature integration to estimate the marginal posteriors of selection probabilities and indicator variables for all features, which quantify the selection uncertainty. Our method not only maintains the benefits of the Bayesian treatment (eg uncertainty quantification) but also possesses the  computational efficiency, and oracle properties of the frequentist methods. Simulation shows that our method estimates better or comparable selection probabilities and indicator variables than alternative approximate inference methods such as VB and EP, but with less running time. Extensive experiments on large real datasets demonstrate that our method often improves prediction accuracy over Bayesian automatic relevance determination, EP, and frequentist l_1 type methods.", 
    "authors": [
      {
        "name": "Syed Abbas Zilqurnain Naqvi"
      }, 
      {
        "name": "Shandian Zhe"
      }, 
      {
        "name": "Yuan Qi"
      }, 
      {
        "name": "Jieping Ye"
      }
    ], 
    "keywords": "spike-and-slab models, feature selection, Scalable Laplace approximation, approximate Bayesian inference", 
    "title": "Fast Laplace Approximation for Sparse Bayesian Spike and Slab Models", 
    "type": "paper"
  }, 
  "1606": {
    "abstract": "Hostnames such as en.wikipedia.org and www.amazon.com are strong indicators of the content they host. The relevant hostnames for a query can be a signature that captures the query intent. In this study, we learn the hostname preference of queries, which are further utilized to enhance search relevance. Implicit and explicit query intent are modeled simultaneously by a feature aware matrix completion framework. A block-wise parallel algorithm was developed on top of the Spark MLlib for fast optimization of feature aware matrix completion. The optimization completes within minutes at the scale of a million x million matrix, which enables efficient experimental studies at the web scale. Evaluation of the learned hostname preference is performed both intrinsically on test errors, and extrinsically on the impact on search ranking relevance. Experimental results demonstrate that capturing hostname preference can significantly boost the retrieval performance.", 
    "authors": [
      {
        "name": "Jingjing Wang"
      }, 
      {
        "name": "Changsung Kang"
      }, 
      {
        "name": "Yi Chang"
      }, 
      {
        "name": "Jiawei Han"
      }
    ], 
    "keywords": "hostname preference, feature aware matrix completion, bock-wise parallel algorithm", 
    "title": "Learning Hostname Preference to Enhance Search Relevance", 
    "type": "paper"
  }, 
  "1608": {
    "abstract": "Top-N recommendation systems have great impact on many real world applications such as E- commerce platforms and social networks. Most existing methods produce personalized top-N recommendations by minimizing a specific uniform loss such as pair- wise ranking loss or pointwise recovery loss. In this paper, we propose a novel personalized top-N recommendation approach that minimizes a combined heterogeneous loss based on linear self-recovery models. The heterogeneous loss integrates the strengths of both pairwise ranking loss and pointwise recovery loss to provide more informative recommendation predictions. We formulate the learning problem with heterogeneous loss as a constrained convex minimization problem and develop a projected stochastic gradient descent optimization algorithm to solve it. We evaluate the proposed approach on a set of personalized top-N recommendation tasks. The experimental results show the proposed approach outperforms a number of state-of-the-art methods.", 
    "authors": [
      {
        "name": "Feipeng Zhao"
      }, 
      {
        "name": "Yuhong Guo"
      }
    ], 
    "keywords": "heterogeneous losses, top-N recommendation, convex minimization", 
    "title": "Improving Top-N Recommendation with Heterogeneous Losses", 
    "type": "paper"
  }, 
  "1618": {
    "abstract": "Consider a binary classification problem in which the learner is given a labeled training set, an unlabeled test set, and is restricted to choosing exactly k test points to output as positive predictions. Problems of this kind---{\\it transductive precision@k}---arise in information retrieval, digital advertising, and reserve design for endangered species. Previous methods separate the training of the model from its use in scoring the test points. This paper introduces a new approach, Transductive Top K (TTK), that seeks to minimize the hinge loss over all training instances under the constraint that exactly k test instances are predicted as positive. The paper presents two optimization methods for this challenging problem. Experiments and analysis confirm the importance of incorporating the knowledge of k into the learning process. Experimental evaluations of the TTK approach show that the performance of TTK matches or exceeds existing state-of-the-art methods on 7 UCI datasets and 3 reserve design problem instances.", 
    "authors": [
      {
        "name": "Liping Liu"
      }, 
      {
        "name": "Thomas Dietterich"
      }, 
      {
        "name": "Nan Li"
      }, 
      {
        "name": "Zhi-Hua Zhou"
      }
    ], 
    "keywords": "ranking, transductive learning, top precision", 
    "title": "Transductive Optimization of Top k Precision", 
    "type": "paper"
  }, 
  "1621": {
    "abstract": "Intersubjectivity is a very important concept in psychology and sociology. Some researchers believe that this concept is the driving force in the cognitive revolution over seventy thousand years ago, which makes Homo sapiens overcome other homo races. In this paper, we approach the notion of intersubjectivity as the sharing of stances or subjective viewpoints by two or more individules, and incorporate the concept of intersubjectivity into  sentiment analysis. We construct an intersubjectivity network linking review writers, terms they use and polarities, and propose a method to learn user embeddings which are subsequently incorporated into a convolutional neural network for sentiment classification. Our results on the IMDB and Yelp 2013 and 2014 datasets show that our propsoed approach has achived the state-of-the-art performance.", 
    "authors": [
      {
        "name": "Lin Gui"
      }, 
      {
        "name": "Ruifeng Xu"
      }, 
      {
        "name": "Yulan He"
      }, 
      {
        "name": "Qin Lu"
      }, 
      {
        "name": "Zhongyu Wei"
      }
    ], 
    "keywords": "Sentiment Analysis, Intersubjectivity, Representation Learning", 
    "title": "Intersubjectivity and Sentiment: from Language to Knowledge", 
    "type": "paper"
  }, 
  "1622": {
    "abstract": "We consider a Plurality-voting scenario, where the candidates are split between parties, and each party nominates exactly one candidate for the final election. We study the computational complexity of deciding if there is a set of nominees such that a candidate from a given party wins in the final election. In our second problem, the goal is to decide if a candidate from a given party always wins, irrespective who is nominated. We show that these problems are computationally hard, but are polynomial-time solvable for restricted settings.", 
    "authors": [
      {
        "name": "Piotr Faliszewski"
      }, 
      {
        "name": "Laurent Gourv\u00e8s"
      }, 
      {
        "name": "J\u00e9r\u00f4me Lang"
      }, 
      {
        "name": "Julien Lesca"
      }, 
      {
        "name": "J\u00e9r\u00f4me Monnot"
      }
    ], 
    "keywords": "elections, complexity, plurality voting, possible winner, single-peaked preferences", 
    "title": "How Hard Is It for a Party to Nominate an Election Winner?", 
    "type": "paper"
  }, 
  "1626": {
    "abstract": "We consider several natural subclasses of committee scoring rules, namely, weakly separable, representation-focused, top-k-counting, OWA-based, and decomposable rules. We study some of their axiomatic properties, especially properties of monotinicity, and concentrate on containment relations between these subclasses. All these subclasses are  already studied in the literature, expect for the subclass of decomposable rules, which we introduce in this paper; we describe some applications of rules in this subclass and show that it strictly contains the subclass of OWA-based rules. We characterize SNTV, Bloc, and k-Approval Chamberlin--Courant, as the only rules in certain intersections of these subclasses.", 
    "authors": [
      {
        "name": "Piotr Faliszewski"
      }, 
      {
        "name": "Piotr Skowron"
      }, 
      {
        "name": "Arkadii Slinko"
      }, 
      {
        "name": "Nimrod Talmon"
      }
    ], 
    "keywords": "multiwinner voting, committee scoring rules, axioms, monotonicity", 
    "title": "Committee Scoring Rules: Axiomatic Classification and Hierarchy", 
    "type": "paper"
  }, 
  "1634": {
    "abstract": "Two major tasks in spoken language understanding (SLU) are intent determination (ID) and slot filling (SF). Recurrent neural networks (RNNs) have been proved effective in SF, while there is no prior work using RNNs in ID. Based on the idea that the intent and semantic slots of a sentence are correlative, we propose a joint model for both tasks. Gated recurrent unit (GRU) is used to learn the representation of each time step, by which the label of each slot is predicted. Meanwhile, a max-pooling layer is employed to capture global features of a sentence for intent classification. The representations are shared by two tasks and the model is trained by a united loss function. We conduct experiments on two datasets, and the experimental results demonstrate that our model outperforms the state-of-the-art approaches on both tasks.", 
    "authors": [
      {
        "name": "Xiaodong Zhang"
      }, 
      {
        "name": "Houfeng Wang"
      }
    ], 
    "keywords": "Joint model, RNNs, Spoken language understanding", 
    "title": "A Joint Model of Intent Determination and Slot Filling for Spoken Language Understanding", 
    "type": "paper"
  }, 
  "1640": {
    "abstract": "Crowdsourcing, as a popular paradigm to outsource work to individuals, is widely adopted in many domains. In the machine learning community, crowdsourcing is commonly used as a cost-saving way to collect labels of training data. While a lot of effort has been spent on developing methods for inferring labels from a crowd, few work concentrates on the theoretical basis of crowdsourcing. In this paper, some basic issues about crowdsourcing is investigated theoretically, including label quality analysis and cost analysis for inferring the true label. We also make PAC-style analysis for learning from a crowd. Furthermore, we present an upper bound for the minimal sufficient number of crowd labels to train a model. Theoretical results are verified empirically on real-world datasets as well.", 
    "authors": [
      {
        "name": "Lu Wang"
      }, 
      {
        "name": "Zhi-Hua Zhou"
      }
    ], 
    "keywords": "crowdsourcing, classification, cost-sensitive learning", 
    "title": "Learning by Crowdsourcing", 
    "type": "paper"
  }, 
  "1643": {
    "abstract": "In energy conservation research, energy disaggregation becomes an increasingly critical task, which takes a whole home electricity signal and decomposes it into its component appliances. While householder's daily energy usage behavior acts as one powerful cue for breaking down the entire household's energy consumption, existing works rarely modeled it straightforwardly. Instead, they either ignore the influence between users' energy usage behaviors, or model the influence between the energy usages of appliances instead. With ambiguous appliance usage membership of householders, we find it difficult to appropriately model the influence between of appliances, since such influence is determined by human behaviors in energy usage.   To address this problem, we propose to model the influence between householders' energy usage behaviors directly through a novel probabilistic model, which combines topic models with the Hawkes processes. The proposed model simultaneously disaggregates the whole home electricity signal into each component appliance and infers the appliance usage membership of household members, and enable those two tasks mutually benefit each other.  Experimental results on both synthetic data and four real world data sets demonstrate the effectiveness of our model, which outperforms state-of-the-art approaches in decomposing the entire consumed energy to each appliance in houses. We further analyze the inferred appliance-householder assignment and the corresponding influence within the appliance usage of each householder and across different householders, which provides insight into appealing human behavior patterns in appliance usage.", 
    "authors": [
      {
        "name": "Liangda Li"
      }, 
      {
        "name": "Hongyuan Zha"
      }
    ], 
    "keywords": "Energy Disaggregation, Household Structure Analysis, Hawkes Process", 
    "title": "Enhancing Energy Disaggregation through Household Structure Analysis via Hawkes Processes", 
    "type": "paper"
  }, 
  "1658": {
    "abstract": "We present and evaluate a system for processing of dynamic and static spatial relations in embodied robotic interaction setups. Within this context, robots describe an environment of moving blocks using English phrases that include dynamic spatial relations such as ``across'', as well as static spatial relations, such as ``in front of'' etc. Particular attention is paid to modeling a complete system, that allows agents to refer to image schematic referents such as source, path and goal aspects of movement events. We evaluate the system in robot-robot interactions, with a focus on testing the robustness of the system. Results show that the system is robust against visual perception errors, as well as language transmission omissions and ungrammatical utterances.", 
    "authors": [
      {
        "name": "Michael Spranger"
      }, 
      {
        "name": "Jakob Suchan"
      }, 
      {
        "name": "Mehul Bhatt"
      }
    ], 
    "keywords": "integrated systems for natural language interaction, reasoning, computational construction grammar, computational cognitive semantics, spatial-language, human-robot interaction", 
    "title": "Robust Natural Language Processing - Combining Reasoning, Cognitive Semantics and Construction Grammar for Spatial Language", 
    "type": "paper"
  }, 
  "166": {
    "abstract": "Knowledge graph embedding aims at offering a numerical knowledge representation paradigm by transforming the entities and relations into continuous vector space. However, existing methods could not characterize the knowledge graph in a fine degree to make a precise link prediction. There are two reasons: being an ill-posed algebraic system and adopting an overstrict geometric form. As precise link prediction is critical, we propose a manifold-based embedding principle (\\textbf{ManifoldE}) which could be treated as a well-posed algebraic system that expands the position of golden triples from one point in current models to a manifold in ours. Extensive experiments show that the proposed models achieve substantial improvements against the state-of-the-art baselines especially for the precise prediction task, and yet maintain high efficiency.", 
    "authors": [
      {
        "name": "Han Xiao"
      }, 
      {
        "name": "Minlie Huang"
      }, 
      {
        "name": "Xiaoyan Zhu"
      }
    ], 
    "keywords": "Precise Link Prediction, Knowledge Graph, Embedding, Manifold", 
    "title": "From One Point to A Manifold: Knowledge Graph Embedding For Precise Link Prediction", 
    "type": "paper"
  }, 
  "1666": {
    "abstract": "Online class imbalance learning deals with data streams having very skewed class distributions in a timely fashion. Although a few methods have been proposed to handle such problems, most of them focus on two-class cases. Multi-class imbalance can occur in real-world applications. It can impose additional challenges in learning, given that it increases the data complexity and aggravates the imbalanced distribution. Very little work has considered the learning issues in imbalanced data streams with more than one minority/majority class. This paper studies the combined challenges posed by multi-class imbalance and online learning, and aims at a more effective and adaptive solution. First, we introduce two resampling-based ensemble methods, called MOOB and MUOB, which can process multi-class data directly and strictly online with an adaptive sampling rate. Then, we look into the impact of multi-minority and multi-majority cases on MOOB and MUOB in comparison to other methods (i.e. VWOS-ELM and OB) under static and dynamic scenarios. Both multi-minority and multi-majority make a negative impact on the online performance of the discussed methods, especially the minority-class recall of MOOB and majority-class recall of VWOS-ELM. MOOB shows the best and most stable G-mean in most static and dynamic cases.", 
    "authors": [
      {
        "name": "Shuo Wang"
      }, 
      {
        "name": "Leandro L. Minku"
      }, 
      {
        "name": "Xin Yao"
      }
    ], 
    "keywords": "class imbalance learning, online learning, multi-class, ensemble learning", 
    "title": "Dealing with Multiple Classes in Online Class Imbalance Learning", 
    "type": "paper"
  }, 
  "1668": {
    "abstract": "As a classical subspace learning method, Probabilistic PCA (PPCA) has been extended to several bilinear variants for dealing with matrix observations. However, they are all based on the Tucker model, leading to a restricted subspace representation and the problem of rotational ambiguity. To address these problems, this paper proposes a bilinear PPCA method named as Probabilistic Rank-One Matrix Analysis (PROMA). PROMA is based on the CP model, which leads to a more flexible subspace representation and does not suffer from rotational ambiguity. For better generalization without losing the scale invariance of the CP model, concurrent regularization is introduced to regularize the whole matrix subspace, rather than column and row factors separately. Experiments on both synthetic and real-world data demonstrate the superiority of PROMA in subspace estimation and classification as well as the effectiveness of concurrent regularization in regularizing bilinear PPCAs.", 
    "authors": [
      {
        "name": "Yang Zhou"
      }, 
      {
        "name": "Haiping Lu"
      }
    ], 
    "keywords": "Probabilistic Principal Component Analysis, Dimensionality Reduction, Regularization, Bilinear Model", 
    "title": "Probabilistic Rank-One Matrix Analysis with Concurrent Regularization", 
    "type": "paper"
  }, 
  "167": {
    "abstract": "Iterative Quantization (ITQ) is one of the most successful hashing based nearest-neighbor search methods for large-scale information retrieval in the past a few years due to its simplicity and superior performance. However, the performance of this algorithm degrades significantly when dealing with noisy data. Additionally, it can barely facilitate a wide range of applications as the distortion measurement only limits to $\\ell_{2}$ \\textit{norm}.  In this paper, we propose an ITQ+ algorithm, aiming to enhance both robustness and generalization of the original ITQ algorithm. Specifically, a $\\ell_{p,q}$-norm loss function is proposed to conduct the $\\ell_{p}$-norm similarity search, rather than a $\\ell_{2}$ \\textit{norm} search. Despite the fact that changing the loss function to $\\ell_{p,q}$-norm makes our algorithm more robust and generic, it brings us a challenge that minimizes the obtained \\textit{orthogonality constrained} $\\ell_{p,q}$-\\textit{norm function}, which is non-smooth and non-convex. To solve it, we propose a novel and efficient optimization scheme, which is an important theoretical contribution of this paper. Extensive experiments on benchmark datasets demonstrate that ITQ+ is overwhelmingly  better than the original ITQ algorithm, especially when searching similarity in noisy data.", 
    "authors": [
      {
        "name": "Yuchen Guo"
      }, 
      {
        "name": "Guiguang Ding"
      }, 
      {
        "name": "Jungong Han"
      }
    ], 
    "keywords": "Binary Code, Optimization, Orthogonality, Experiment", 
    "title": "Robust Iterative Quantization for Efficient $\\ell_{p}$-norm Similarity Search", 
    "type": "paper"
  }, 
  "1673": {
    "abstract": "In this article, reinforcement learning is used to learn an optimal turn-taking strategy for vocal human-machine dialogue. The Majordomo dialogue system, allowing the users to have conversations within a smart home, has been upgraded to an incremental version. First, a user simulator is built in order to generate a dialogue database which thereafter trains a Fitted-Q algorithm to optimise the turn-taking strategy from delayed rewards. Then, real users test and evaluate the new-learnt strategy, versus a non-incremental and a handcrafted incremental strategies. The data-driven strategy is shown to significantly improve the task completion ratio and to be preferred by the users according to subjective metrics.", 
    "authors": [
      {
        "name": "Hatim Khouzaimi"
      }, 
      {
        "name": "Romain Laroche"
      }, 
      {
        "name": "Fabrice Lef\u00e8vre"
      }
    ], 
    "keywords": "Incremental Dialogue Systems, Turn-taking, Reinforcement Learning", 
    "title": "Reinforcement Learning applied to Incremental Spoken Dialogue Systems", 
    "type": "paper"
  }, 
  "1681": {
    "abstract": "In this paper we define and study an extension of autoepistemic logic (AEL) called distributed autoepistemic logic (dAEL) with multiple agents that have full introspection in their own knowledge as well as in that of others. This mutual full introspection between agents is motivated by an application of dAEL in access control. We define 2- and 3-valued semantic operators for dAEL. Using these operators, approximation fixpoint theory, an abstract algebraic framework that unifies different knowledge representation formalisms, immediately yields us a family of semantics for dAEL, each based on different intuitions that are well-studied in the context of AEL. The application in access control also motivates an extension of dAEL with inductive definitions (dAEL(ID)). We explain a use-case from access control to demonstrate how dAEL(ID) can be fruitfully applied to this domain and discuss how well-suited the different semantics are for the application in access control.", 
    "authors": [
      {
        "name": "Pieter Van Hertum"
      }, 
      {
        "name": "Marcos Cramer"
      }, 
      {
        "name": "Bart Bogaerts"
      }, 
      {
        "name": "Marc Denecker"
      }
    ], 
    "keywords": "Autoepistemic Logic, Approximation Fixpoint Theory, Semantics, Access Control, Epistemic Logic", 
    "title": "Distributed Autoepistemic Logic and its Application to Access Control", 
    "type": "paper"
  }, 
  "1689": {
    "abstract": "We address the problem of making general video game playing agents play in a human-like manner. To this end, we introduce a simple modification of the UCT formula used in Monte Carlo Tree Search that biases action selection towards repeating the current action. Playtraces of human players are used to model their propensity for repeated actions; this model is used within the biasing of the UCT formula. Experiments show that the modified MCTS player plays quantitatively similar to human players, as measured by the distribution of repeated actions. A survey of human observers reveal that the modified MCTS player exhibits human-like playing style in some games but not others.", 
    "authors": [
      {
        "name": "Ahmed Khalifa"
      }, 
      {
        "name": "Aaron Isaksen"
      }, 
      {
        "name": "Julian Togelius"
      }, 
      {
        "name": "Andy Nealen"
      }
    ], 
    "keywords": "general video game playing, monte carlo tree search, games", 
    "title": "Modifying MCTS for Human-like General Video Game Playing", 
    "type": "paper"
  }, 
  "169": {
    "abstract": "To save the labeling efforts for training a classification model, we can simultaneously adopt Active Learning (AL) to select the most informative samples for human labeling, and Semi-supervised Learning (SSL) to construct effective classifiers using a few labeled samples and a large number of unlabeled samples. Recently, transferring knowledge from auxiliary data sources, i.e., Transfer Learning (TL), to enhance AL and SSL, i.e., T-SS-AL, has gained considerable attention. However, existing T-SS-AL methods mostly focus on the situation where the source domain and the target domain share the same classes. In this paper, we consider a more practical and challenging setting where the source domain and the target domain have different but related classes. We propose a novel cross-class sample transfer based T-SS-AL method, called CC-SS-AL, to exploit the information from the source domain. Our key idea is to select samples from the source domain which are very similar to the target domain classes and assign pseudo labels to them for classifier training. Extensive experiments on three datasets verify the efficacy of the proposed method.", 
    "authors": [
      {
        "name": "Yuchen Guo"
      }
    ], 
    "keywords": "Semi-superivised Learning, Active Learning, Transfer Learning, Cross-class learning, Optimization, Experiment", 
    "title": "Semi-supervised Active Learning with Cross-class Sample Transfer", 
    "type": "paper"
  }, 
  "1692": {
    "abstract": "The best telescopes are scarce and expensive resources: it is therefore essential to maximize the scientific potential of the observations that are to be scheduled on a telescope. The observations must be chosen from a list of candidate targets with various observation constraints and, in this paper, we study the case when the duration of observation is at least half that of the observation window. We prove that the problem is NP-complete, and we propose a constraint-programming-based branch and price algorithm to solve it. Through experiments on real and realistic datasets, we show that the method provides optimal solutions very efficiently.", 
    "authors": [
      {
        "name": "Nicolas Catusse"
      }, 
      {
        "name": "Hadrien Cambazar"
      }, 
      {
        "name": "Nadia Brauner"
      }, 
      {
        "name": "Pierre Lemaire"
      }, 
      {
        "name": "Bernard Penz"
      }, 
      {
        "name": "Anne-Marie Lagrange"
      }, 
      {
        "name": "Pascal Rubini"
      }
    ], 
    "keywords": "branch and price, constraint programming, complexity, scheduling", 
    "title": "A branch-and-price algorithm for scheduling observations on a  telescope", 
    "type": "paper"
  }, 
  "1693": {
    "abstract": "A number of existing works have focused on the problem of malicious following activity detection in microblog services. However, most of them make the assumption that the spamming following relationships are either from fraudulent accounts or compromised legitimate users. They therefore developed detection methodologies based on the features derived from this assumption. Recently, a new type of malicious crowdturfing following relationship is provided by the follower market, called voluntary following. Followers who provide voluntary following services (or named volwers) are normal users who are willing to trade their following activities for profit. Since most of their behaviors follow normal patterns, it is difficult for existing methods to detect volwers and their corresponding customers. In this work, we try to solve the voluntary following problem through a newly proposed detection method named DetectVC. This method incorporates both structure information in user following behavior graphs and prior knowledge collected from follower markets. Experimental results on large scale practical microblog data set show that DetectVC is able to detect volwers and their customers simultaneously and it also significantly outperforms existing solutions.", 
    "authors": [
      {
        "name": "Yuli Liu"
      }, 
      {
        "name": "Yiqun Liu"
      }, 
      {
        "name": "Min Zhang"
      }, 
      {
        "name": "Shaoping Ma"
      }
    ], 
    "keywords": "Crowdturfing, Voluntary Following, Social Network, Detection", 
    "title": "Pay Me and I\u2019ll Follow You: Detection of Crowdturfing Following Activities in Microblog Environment", 
    "type": "paper"
  }, 
  "1697": {
    "abstract": "Modeling policies in reproducing kernel Hilbert space (RKHS) renders policy gradient reinforcement learning algorithms non-parametric methods. Under this powerful approach, the policies become very flexible and have rich representation potential without a pre-defined set of basis. However, their performances might be either non-covariant under re-parameterization of the chosen kernel, or very sensitive to step-size selection. In this paper, we propose to use the general framework to derive a new RKHS policy search technique. The new derivation lead to formation of both a Natural RKHS Actor-Critic algorithm and RKHS expectation maximization (EM) inspried policy search algorithm. On the other hand, we show that non-parametric modelling enables learning in partially observable (POMDP) tasks which is considered daunting for parametric approaches. Via sparsification, a small set of features representing history are shown to be effectively discovered. For evaluations, we use two simulated (PO)MDP reinforcement learning tasks, and a robotic manipulation task on a physical Willow Garage PR2 platform. Empirical results show the appealing effectiveness of the new RKHS policy search framework, in comparison to the plain RKHS actor-critic, episodic natural actor-critic, plain actor-critic, and PoWER approaches.", 
    "authors": [
      {
        "name": "Vien Ngo"
      }, 
      {
        "name": "Peter Englert"
      }, 
      {
        "name": "Marc Toussaint"
      }
    ], 
    "keywords": "Policy search in RKHS, reinforcement learning, actor-critic, natural actor-critic, EM-inspired policy search", 
    "title": "Policy Search in Reproducing Kernel Hilbert Space", 
    "type": "paper"
  }, 
  "1698": {
    "abstract": "Network coarsening refers to a new class of graph `zoom-out' operations by grouping similar nodes and edges together so that a smaller equivalent representation of the graph can be obtained for big network analysis. Existing network coarsening methods consider that network structures are static and thus cannot handle dynamic networks. On the other hand, data-driven approaches can infer dynamic network structures by using network information spreading data. However, existing data-driven approaches neglect static network structures that are potentially useful for inferring big networks. In this paper, we present a new \\emph{semi-data-driven} network coarsening model to learn coarsened networks by embedding both \\emph{static} network structure data and \\emph{dynamic} network information spreading data. We prove that the learning model is convex and the Accelerated Proximal Gradient algorithm is adapted to achieve the global optima. Experiments on both synthetic and real-world data sets demonstrate the performance of the proposed method.", 
    "authors": [
      {
        "name": "Li Gao"
      }, 
      {
        "name": "Jia Wu"
      }, 
      {
        "name": "Hong Yang"
      }, 
      {
        "name": "Zhi Qiao"
      }, 
      {
        "name": "Chuan Zhou"
      }, 
      {
        "name": "Yue Hu"
      }
    ], 
    "keywords": "Network coarsening, semi-data-driven approach, graph mining", 
    "title": "Semi-Data-Driven Network Coarsening", 
    "type": "paper"
  }, 
  "1703": {
    "abstract": "We present complexity results and algorithms for optimal status enforcement in abstract argumentation. Status enforcement is the task of adjusting a given argumentation framework (AF) to support given positive and negative argument  statuses, i.e., to accept and reject specific arguments. Merging the well-known problems of credulous and skeptical acceptance with recent research on AF dynamics, we study optimal status enforcement as the problem of finding a structurally closest AF supporting given argument statuses. We establish complexity results for optimal status enforcement under several central AF semantics, develop constraint-based algorithms for NP and second-level complete variants of the problem, and present empirical results on an implementation of the procedures.", 
    "authors": [
      {
        "name": "Andreas Niskanen"
      }, 
      {
        "name": "Johannes P. Wallner"
      }, 
      {
        "name": "Matti J\u00e4rvisalo"
      }
    ], 
    "keywords": "abstract argumentation, computational complexity, dynamics of argumentation, status enforcement, encodings, algorithms, Boolean optimization, maximum satisfiability", 
    "title": "Optimal Status Enforcement in Abstract Argumentation", 
    "type": "paper"
  }, 
  "1713": {
    "abstract": "Semantic similarity of text plays an important role in many NLP tasks. It requires using both local information like lexical semantics and structural information like syntactic structures. Recent progress in word representation makes it a good resource for lexical semantics, and advances in natural language analysis tools make it possible to efficiently generate syntactic and semantic annotations. However, how to combine them to capture the semantic of text is still an open question. In this work, we propose a new alignment-based approach to learn semantic similarity. It uses a hybrid representation, attributed relational graphs, to encode lexical, syntactic and semantic information. Alignment of two such graphs combines local and structural information to support similarity estimation. To improve alignment, we introduced structural constraints inspired by a cognitive theory of similarity and analogy. Usually only similarity labels are known in training data and the alignments are latent, we address the learning problem using two approaches: alignment as feature extraction and alignment as latent variable. Our approach is evaluated on the paraphrase identification task and achieved results competitive with the state-of-the-art.", 
    "authors": [
      {
        "name": "Chen Liang"
      }, 
      {
        "name": "Praveen Paritosh"
      }, 
      {
        "name": "Vinodh Rajendran"
      }, 
      {
        "name": "Kenneth Forbus"
      }
    ], 
    "keywords": "Semantic Similarity, Paraphrase Identification, Structured Prediction, Latent Variable Model, Analogy", 
    "title": "Learning Text Semantic Similarity with Structural Alignment", 
    "type": "paper"
  }, 
  "1715": {
    "abstract": "Part of the long lasting cultural heritage of humanity is the art of classical poems, which are created by fitting words into certain formats and representations. Automatic poetry composition by computers is considered as a challenging problem which requires high Artificial Intelligence assistance. This study attracts more and more attention in the research community. In this paper, we formulate the poetry composition task as a natural language generation problem using recurrent neural networks. Given user specified writing intents, the system generates a poem via sequential language modeling. Unlike the traditional one-pass generation for previous neural network models, poetry composition needs polishing to satisfy certain requirements. Hence, we propose a new generative model with a polishing schema, and output a refined poem composition. In this way, the poem is generated incrementally and iteratively by refining each line. We run experiments based on large datasets of 61,960 classic poems in Chinese. A comprehensive evaluation, using perplexity and BLEU measurements as well as human judgments, has demonstrated the effectiveness of our proposed approach.", 
    "authors": [
      {
        "name": "Rui Yan"
      }
    ], 
    "keywords": "Poetry generation, Iterative polishing schema, Neural networks, Natural language generation", 
    "title": "i, Poet: Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema", 
    "type": "paper"
  }, 
  "1723": {
    "abstract": "LexLeader, a state of the art static symmetry breaking method, adds a symmetry breaking constraint for each variable symmetry of the problem to select the lexicographically least solution.  In practice, the same method can also be used for partial symmetry breaking by only breaking a subset of symmetries. We propose a new total ordering, reflex, as basis of a new symmetry breaking constraint that collaborates better with each other as well as other symmetry breaking methods, thereby breaking more composition symmetries in partial symmetry breaking. An efficient GAC filtering algorithm is presented for the reflex ordering constraint. We propose the ReflexLeader method, which is a variant of LexLeader using the reflex ordering instead, and give conditions when ReflexLeader is safe to combine with the Precedence and multiset ordering constraints. Extensive experimentations demonstrate the advantages of the reflex ordering over the lexicographic ordering in partial symmetry breaking.", 
    "authors": [
      {
        "name": "Jimmy Lee"
      }, 
      {
        "name": "Zichen Zhu"
      }
    ], 
    "keywords": "Constraint Satisfaction Problems, Symmetry Breaking, Global Constraints", 
    "title": "Static Symmetry Breaking with the Reflex Ordering", 
    "type": "paper"
  }, 
  "1727": {
    "abstract": "The traditional recommendation systems usually aim to improve the recommendation accuracy while overlook the diversity within the recommended lists. In this paper, we propose a general framework to recommend relevant and diverse items. Experimental results on MovieLens dataset demonstrate that our approach outperforms state-of-the-art techniques in terms of both accuracy and diversity.", 
    "authors": [
      {
        "name": "Chaofeng Sha"
      }, 
      {
        "name": "Xiaowei Wu"
      }, 
      {
        "name": "Junyu Niu"
      }
    ], 
    "keywords": "item recommendation, diversity, greedy search", 
    "title": "A Framework for Recommending Relevant and Diverse Items", 
    "type": "paper"
  }, 
  "1738": {
    "abstract": "Conceptual clustering combines two long-standing machine learning tasks: the unsupervised grouping of similar instances and their description by symbolic concepts. In this paper, we decouple the problems of finding descriptions and forming clusters by first mining formal concepts, also known as closed itemsets,  and searching for the best $k$ clusters that can be described with those itemsets. Existing approaches that perform the two steps separately are of a heuristic nature and produce results of varying quality. Instead, we address the problem of finding an optimal constrained conceptual clustering by using linear programming techniques. Most other generic approaches that tackle this problem lack of scalability.  Our approach takes advantageous of both techniques, the general framework of linear programming, and high-speed specialized approaches of data mining. Experiments performed on a variety of well-known UCI datasets show that  our approach efficiently finds clusterings of consistently high quality.", 
    "authors": [
      {
        "name": "Abdelkader Ouali"
      }, 
      {
        "name": "Samir Loudni"
      }, 
      {
        "name": "Yahia Lebbah"
      }, 
      {
        "name": "Patrice Boizumault"
      }, 
      {
        "name": "Albrecht Zimmermann"
      }, 
      {
        "name": "Lakhdar Loukil"
      }
    ], 
    "keywords": "Integer Linear Programming, Conceptual Clustering, Data mining, Unsupervised Learning", 
    "title": "Efficiently Finding Conceptual Clustering Models with Integer Linear Programming", 
    "type": "paper"
  }, 
  "1752": {
    "abstract": "The Mixed-Membership Stochastic Blockmodel (MMSB) is a popular framework for modelling social  relationships by fully exploiting each individual node's participation (or membership) in a social network. Despite its powerful representations, MMSB assumes that the membership indicators of each pair of nodes (i.e., people) are distributed independently. However, such an assumption often does not hold in real-life social networks, in which certain known groups of people may correlate with each other in terms of factors such as their membership categories. To expand MMSB's ability to model such dependent relationships, a new framework - a Copula Mixed-Membership Stochastic Blockmodel - is introduced in this paper for modeling intra-group correlations, namely an individual Copula function jointly models the membership pairs of those nodes within the group of interest. This framework enables various Copula functions to be used on demand, while maintaining the membership indicator's marginal distribution needed for modelling membership indicators with other nodes outside of the group of interest. Sampling algorithms for both the finite and infinite number of groups are also detailed. Our experimental results show its superior performance in capturing group interactions when compared with the baseline models on both synthetic and real world datasets.", 
    "authors": [
      {
        "name": "Xuhui Fan"
      }, 
      {
        "name": "Richard Yi Da Xu"
      }
    ], 
    "keywords": "Copula, MCMC, Mixed-Membership Stochastic Blockmodel", 
    "title": "Copula Mixed-Membership Stochastic Blockmodel", 
    "type": "paper"
  }, 
  "1754": {
    "abstract": "Genetic algorithms are stochastic search heuristics which are popular for their broad applicability, especially in combinatorial search problems.   The search mechanism relies on an abstraction of genetic evolution and selection as seen in nature.   This paper introduces a topological structure for the search space which is consistent with existing theory and practice for genetic algorithms, namely forma analysis.    A notion of convexity is defined within this context and connections between this definition and forma analysis are established.   This framework provides an alternative perspective on the exploitation/exploration dilemma as well as population convergence, which relates directly to the genetic operators employed to drive the evolution process.    It also provides a different interpretation of design constraints associated with genetic algorithm implementations.   The intention is to provide a different analytical perspective for genetic algorithms, and to establish a connection with exact search methods through the concept of convexity with the hope that some theoretical and methodological ideas can be ported between the two, thus ultimately improving the methodological aspects of genetic algorithms.", 
    "authors": [
      {
        "name": "David Hofmeyr"
      }
    ], 
    "keywords": "Genetic Algorithm, Topology, Convexity, Uniformity, Equivalence Relation", 
    "title": "On the Topology of Genetic Algorithms", 
    "type": "paper"
  }, 
  "1757": {
    "abstract": "Monte Carlo Tree Search (MCTS) methods have proven powerful in planning for large scale sequential decision-making problems such as Go and video games, but they suffer when the planning depth and sampling trajectories are limited or when the rewards are sparse. We present an adaptation of PGRD (policy-gradient for reward-design) for learning a reward-bonus function to improve UCT (a MCTS algorithm). Unlike previous applications of PGRD in which the space of reward-bonus functions was limited to linear functions of hand-coded state-action-features, we use PGRD with a multi-layer convolutional neural network to automatically learn features from raw perception as well as to adapt the non-linear reward-bonus function parameters. We also adopt a variance-reducing gradient method to improve PGRD's performance. The new method improves UCT's performance on multiple ATARI games compared to UCT without the reward bonus. Combining PGRD and Deep Learning in this way should make adapting rewards for MCTS algorithms far more widely and practically applicable than before.", 
    "authors": [
      {
        "name": "Xiaoxiao Guo"
      }, 
      {
        "name": "Satinder Singh"
      }, 
      {
        "name": "Richard Lewis"
      }, 
      {
        "name": "Honglak Lee"
      }
    ], 
    "keywords": "Deep Learning, Reinforcement Learning, Reward Design, Monte Carlo Tree Search", 
    "title": "Deep Learning for Reward Design to Improve Monte Carlo Tree Search in ATARI Games", 
    "type": "paper"
  }, 
  "1758": {
    "abstract": "Human activity recognition (HAR) in ubiquitous computing is beginning to adopt deep learning to substitute for well-established analysis techniques that rely on hand-crafted feature extraction and classification techniques. From these isolated applications of custom deep architectures it is, however, difficult to gain an overview of their suitability for problems ranging from the recognition of manipulative gestures to the segmentation and identification of physical activities like running or ascending stairs. In this paper we rigorously explore deep, convolutional, and recurrent approaches across three representative datasets that contain movement data captured with wearable sensors. We describe how to train recurrent approaches in this setting, introduce a novel regularisation approach, and illustrate how they outperform the state-of-the-art on a large benchmark dataset. Across thousands of recognition experiments with randomly sampled model configurations we investigate the suitability of each model for different tasks in HAR, explore the impact of hyperparameters using the fANOVA framework, and provide guidelines for the practitioner who wants to apply deep learning in their problem setting.", 
    "authors": [
      {
        "name": "Nils Y. Hammerla"
      }, 
      {
        "name": "Shane Halloran"
      }, 
      {
        "name": "Thomas Ploetz"
      }
    ], 
    "keywords": "Deep Learning, Convolutional, Recurrent, Evaluation, Intertial Sensors, Activity Recognition, f-anova", 
    "title": "Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables", 
    "type": "paper"
  }, 
  "1759": {
    "abstract": "The hyperlink structure of Wikipedia constitutes a key resource for many Natural Language Processing tasks and applications, as it provides several million semantic annotations of entities in context. Yet, only a small fraction of mentions across the entire Wikipedia corpus is linked. In this paper we present the automatic construction and evaluation of a Semantically Enriched Wikipedia, where the overall amount of linked mentions has more than tripled by solely exploiting the structure of Wikipedia itself and the wide-coverage sense inventory of BabelNet. As a result, we obtain a sense-annotated corpus with more than 200 million annotations of over 4 million different concepts and named entities, and we show that it leads to competitive results in multiple tasks, such as Entity Linking and Word Similarity.", 
    "authors": [
      {
        "name": "Alessandro Raganato"
      }, 
      {
        "name": "Claudio Delli Bovi"
      }, 
      {
        "name": "Roberto Navigli"
      }
    ], 
    "keywords": "Semantic Annotation, Corpus Annotation, Entity Linking, Sense Annotation, Natural Language Processing, Disambiguation", 
    "title": "Automatic Construction and Evaluation of a Large Semantically Enriched Wikipedia", 
    "type": "paper"
  }, 
  "1764": {
    "abstract": "Existing diffusion models for social networks often assume that the activation of a node depends independently on its parents' activations. Some recent work showed that incorporating the structural and behavioral dependency among parent nodes allows more accurate diffusion models to be inferred. In this paper, we postulate that the latent temporal activation patterns (or motifs) of nodes of different social roles embedded in the information cascades observed over a social network form the underlying information diffusion mechanisms. We infer the latent temporal activation motifs and a corresponding motif-based diffusion model using a unified probabilistic framework. Social roles are considered in our model and a two-level EM algorithm is derived so as to infer the motifs and the diffusion probabilities simultaneously. We applied the proposed model to several real-world datasets with significant improvement on modelling accuracy. We also illustrate how the inferred motifs can be interpreted as the underlying mechanisms causing the diffusion process to happen in different social networks.", 
    "authors": [
      {
        "name": "Qing Bao"
      }, 
      {
        "name": "William Kwok Wai Cheung"
      }, 
      {
        "name": "Jiming Liu"
      }
    ], 
    "keywords": "social networks, diffusion networks, stochastic temporal sequence motifs", 
    "title": "Inferring Motif-Based Diffusion Models for Social Networks", 
    "type": "paper"
  }, 
  "1774": {
    "abstract": "Continuous dimensional emotion recognition from audio is a sequential regression problem, where the goal is to maximize correlation between sequences of regression outputs and continuous-valued emotion contours, while minimizing the average deviation. As in other domains, deep neural networks trained on simple acoustic features achieve good performance on this task. Yet, the usual squared error objective functions for neural network training do not fully take into account the above-named goal. Hence, in this paper we introduce a novel technique for the discriminative training of deep neural networks using the concordance correlation coefficient as cost function, which unites both correlation and mean squared error in a single differentiable function. Results on the MediaEval \n1777\tMain Track\tOn Consensus Extraction\tEric Gregoire, S\u00c3\u00a9bastien Konieczny and Jean Marie Lagniez\tKnowledge Representation, Satisfiabiity, Consensus, Belief change, Negotiation\"", 
    "authors": [
      {
        "name": "Felix Weninger"
      }, 
      {
        "name": "Fabien Ringeval"
      }, 
      {
        "name": "Erik Marchi"
      }, 
      {
        "name": "Bj\u00f6rn Schuller"
      }
    ], 
    "keywords": "Automatic Emotion Recognition, Recurrent Neural Networks, Discriminative Training, Concordance Correlation Coefficient", 
    "title": "Discriminatively trained recurrent neural networks for continuous dimensional emotion recognition from audio", 
    "type": "paper"
  }, 
  "1781": {
    "abstract": "Maintaining Generalized Arc Consistency (GAC) during search is among the most efficient way to solve non-binary constraint satisfactory problems. Bit-based representations have been shown to be efficient for AC algorithms. In this paper, we propose the STRbit GAC algorithm based on simple tabular reduction (STR) which employs an efficient bit vector support data structure. STRbit is then extended to deal with compression of the underlying table with c-tuples. We show on an extensive set of benchmarks that both algorithms are the fastest on most benchmark categories (in some cases, an order of magnitude faster) compared with the STR2, STR2-C, STR3, STR3-C and MDDc algorithms. The exception are problems with small tables where more complex data structures do not payoff.", 
    "authors": [
      {
        "name": "Ruiwei Wang"
      }, 
      {
        "name": "Wei Xia"
      }, 
      {
        "name": "Roland Yap"
      }, 
      {
        "name": "Zhanshan Li"
      }
    ], 
    "keywords": "bit representation, generalized arc consistency, table constraint, simple tabular reduction", 
    "title": "Optimizing Simple Table Reduction with Bitwise Representation", 
    "type": "paper"
  }, 
  "1791": {
    "abstract": "Storyline extraction from news streams aims to extract events under a certain news topic and reveal how those events evolve over time. It requires algorithms capable of accurately extracting events from news articles published in different time periods and linking these extracted events into coherent stories. The two tasks are often solved separately, which might suffer from the problem of error propagation. Existing unified approaches often consider events as topics, ignoring their structured representations. In this paper, we propose a non-parametric generative model to extract structured representations and evolution patterns of storylines simultaneously. In the model, each storyline is modelled as a joint distribution over some locations, organizations, persons, keywords  and a set of topics. We further combine this model with the Chinese restaurant process so that the number of storylines can be determined automatically without human intervention. Moreover, per-token Metropolis-Hastings sampler based on light latent Dirichlet allocation is employed to reduce sampling complexity. The proposed model has been evaluated on three news corpora and the experimental results show that it outperforms several baseline approaches.", 
    "authors": [
      {
        "name": "Deyu Zhou"
      }, 
      {
        "name": "Haiyang Xu"
      }, 
      {
        "name": "Yulan He"
      }, 
      {
        "name": "Xinyu Dai"
      }
    ], 
    "keywords": "Storyline extraction, Unsupervised learning, Latent Dirichlet allocation, Chinese restaurant process, Metropolis-Hastings", 
    "title": "Unsupervised Storyline Extraction on News Articles", 
    "type": "paper"
  }, 
  "1794": {
    "abstract": "We investigate efficient representations of formulas in the modal logic of knowledge, S5, and more generally of sets of sets of propositional assignments. One motivation for this study is contingent planning, for which many approaches rely on operations on such knowledge formulas, and can clearly take advantage of efficient representations. We study the known Epistemic Negation Normal Form (ENNF) language for S5 and a natural variant of it that uses Binary Decision Diagrams (BDDs) at the propositional level. We also introduce an alternative language, called Epistemic Splitting Diagrams, which provides more compact representations. We compare all three languages from the complexity-theoretic viewpoint of Knowledge Compilation and also through experiments. Our work sheds light on the pros and cons of each representation in both theory and practice.", 
    "authors": [
      {
        "name": "Alexandre Niveau"
      }, 
      {
        "name": "Bruno Zanuttini"
      }
    ], 
    "keywords": "decision diagrams, knowledge compilation, modal logic S5", 
    "title": "Efficient Representations for the Modal Logic S5", 
    "type": "paper"
  }, 
  "1802": {
    "abstract": "Recent work has introduced fork-decoupled search, addressing classical planning problems where a single center component provides preconditions for several leaf components. Given a fixed center path pi^C, the leaf moves compliant with pi^C can then be scheduled independently for each leaf. Fork-decoupled search thus searches over center paths only, maintaining the compliant paths for each leaf separately. This can yield dramatic benefits. It is empirically complementary to partial order reduction via strong stubborn sets, in that each method yields its strongest reductions in different benchmarks. Here we show that the two methods can be combined, in the form of strong stubborn sets for fork-decoupled search. This can yield exponential advantages relative to both methods. Empirically, the combination reliably inherits the best of its components, and often outperforms both.", 
    "authors": [
      {
        "name": "Daniel Gnad"
      }, 
      {
        "name": "Martin Wehrle"
      }, 
      {
        "name": "J\u00f6rg Hoffmann"
      }
    ], 
    "keywords": "classical planning, fork-decoupled search, state pruning, strong stubborn sets", 
    "title": "Decoupled Strong Stubborn Sets", 
    "type": "paper"
  }, 
  "1808": {
    "abstract": "We introduce the idea of control actions in fair division problems where a benevolent or malicious central organizer changes the structure of the fair division problem to benefit one, some or all agents. One motivation for this framework is to achieve fairness by minimally changing the parameters and structure of fair division problems. As a case study, we consider the problem of adding or deleting the minimum number of items so as to achieve ordinal envy-freeness. We present polynomial-time algorithms to achieve ordinal envy-freeness by adding and by deleting items for the case of two agents. For three agents, we show that both problems as well as the more basic problem of checking whether an envy-free allocation exists is NP-complete thereby answering an open problem. Our framework leads to a number of interesting directions in the area of fair division.", 
    "authors": [
      {
        "name": "Haris Aziz"
      }, 
      {
        "name": "Ildi Schlotter"
      }, 
      {
        "name": "Toby Walsh"
      }
    ], 
    "keywords": "social choice, fair division, control, envy-freeness", 
    "title": "Control of Fair Division", 
    "type": "paper"
  }, 
  "1820": {
    "abstract": "Learning and generating Chinese poems is a charming yet challenging task. Traditional approaches involve various language modeling and machine translation techniques, however, they perform not as well when generating poems with complex pattern constraints, for example Song Iambic, a famous type of poems that involve variable-length sentences and strict rhythmic patterns.  This paper applies the attention-based sequence-to-sequence model to generate Chinese song iambic. Specifically, we encode the cue sentences by a bi-directional Long-Short Term Memory (LSTM) model and then predict the entire iambic with the information provided by the encoder, in the form of an attention-based LSTM that can regularize the generation process by the fine structure of the input cues. Several techniques are investigated to improve the model, including global context integration, hybrid style training, character vector initialization and adaptation. Both the automatic and subjective evaluation results show that our model indeed can learn the complex structural and rhythmic patterns of Song iambics, and the generation is rather successful.", 
    "authors": [
      {
        "name": "Qixin Wang"
      }, 
      {
        "name": "Tianyi Luo"
      }, 
      {
        "name": "Dong Wang"
      }, 
      {
        "name": "Chao Xing"
      }
    ], 
    "keywords": "Neural network, Song Iambic verses, Natural language generation", 
    "title": "Chinese Song Iambics Generation with Neural Attention-based Model", 
    "type": "paper"
  }, 
  "1827": {
    "abstract": "To date, algorithms for many real-world problems are most commonly designed following a manual, ad-hoc, trial & error approach, making algorithm design a tedious, time-consuming and costly process.   Recently, Programming by Optimization (PbO) has been proposed as an alternative design paradigm in which algorithmic choices are left open by design and algorithm configuration methods (e.g. ParamILS, iRace) are used to automatically generate the best algorithm for a specific use-case. We argue that, while powerful, contemporary configurators limit themselves by abstracting information that can otherwise be exploited to speedup the optimization process as well as improve the quality of the resulting design. In this work, we propose an alternative white box approach, reformulating the algorithm design problem as a Markov Decision Process, capturing the intrinsic relationships between design decisions and their respective contribution to overall algorithm performance. Subsequently, we discuss and illustrate the benefits of this formulation experimentally.", 
    "authors": [
      {
        "name": "Steven Adriaensen"
      }, 
      {
        "name": "Ann Now\u00e9"
      }
    ], 
    "keywords": "Automatic Algorithm Configuration, Markov Decision Process, Reinforcement Learning", 
    "title": "Towards a White Box Approach to Automated Algorithm Design", 
    "type": "paper"
  }, 
  "1830": {
    "abstract": "Graph as an important structure in machine learning, has been widely applied in many learning tasks, among which a representative one is graph-based semi-supervised learning (GSSL). However, the quality of graph seriously affects the performance of GSSL methods; moreover, an inappropriate graph may even deteriorate the performance. To this end, judging the quality of graph so as to derive a performance-safe GSSL method is desired. To alleviate this problem, in this paper we propose a large margin separation method. Our basic idea is that, if a certain graph owns a high quality, its predictive results on unlabeled data may have a large margin separation. We should exploit the large margin graphs while keeping the small margin graphs (which might be risky) to be rarely exploited. Based on this recognition, we formulate safe GSSL as the classical Semi-Supervised SVM (S3VM) optimization and present an efficient algorithm. Extensive experimental results demonstrate that our proposed method can effectively improve the safeness of GSSL, in addition achieve highly competitive accuracy with many state-of-the-art GSSL methods.", 
    "authors": [
      {
        "name": "Yu-Feng Li"
      }, 
      {
        "name": "Shao-Bo Wang"
      }, 
      {
        "name": "Zhi-Hua Zhou"
      }
    ], 
    "keywords": "Graph-based learning method, Semi-supervised learning, Safe", 
    "title": "Graph Quality Judgement: A Large Margin Expedition", 
    "type": "paper"
  }, 
  "1836": {
    "abstract": "Categories play a fundamental role in human cognition. Defining features (short for DFs) are the key elements to define a category, which enables machines to categorize objects. Categories enriched with their DFs significantly improve the machine's ability of categorization and benefit many applications built upon categorization.  However, defining features can rarely be found for categories in current knowledge bases. Traditional efforts such as manual construction by domain experts are not practical to find defining features for millions of categories. In this paper, we made the first attempt to automatically find defining features for millions of categories in the real world. We formalize the defining feature learning problem and propose a bootstrapping solution to learn defining features from the features of entities belonging to a category. Experimental results show the effectiveness and efficiency of our method. Finally, we find defining features for overall 60,247 categories with accuracy as 94%.", 
    "authors": [
      {
        "name": "Bo Xu"
      }, 
      {
        "name": "Chenhao Xie"
      }, 
      {
        "name": "Yi Zhang"
      }, 
      {
        "name": "Yanghua Xiao"
      }, 
      {
        "name": "Haixun Wang"
      }, 
      {
        "name": "Wei Wang"
      }
    ], 
    "keywords": "defining feature mining, bootstrapping framework, frequent itemset mining", 
    "title": "Learning Defining Features for Categories", 
    "type": "paper"
  }, 
  "1839": {
    "abstract": "In this paper, we construct and compare algorithmic approaches to solve the Preference Consistency Problem for preference statements based on hierarchical models. Instances of this problem contain a set of preference statements that are direct comparisons (strict and non-strict) between some alternatives, and a set of evaluation functions by which all alternatives can be rated. An instance is consistent, if the preference statements do not contradict each other. More specifically, an instance is consistent based on hierarchical preference models, if there exists an hierarchical model on the evaluation functions that induces an order relation on the alternatives by which all relations given by the preference statements are satisfied. Deciding if an instance is consistent is known to be NP-complete for hierarchical models.   We develop three approaches to solve this decision problem. The first involves a Mixed Integer Linear Programming (MILP) formulation, the other two are recursive algorithms that are based on properties of the problem by which the search space can be pruned. Our experiments on synthetic data show that the recursive algorithms are not only faster than solving the MILP formulation, but that the ratio between the running times increases exponentially.", 
    "authors": [
      {
        "name": "Anne-Marie George"
      }, 
      {
        "name": "Nic Wilson"
      }, 
      {
        "name": "Barry O'Sullivan"
      }
    ], 
    "keywords": "Preference Consistency, Preference Inference, Hierarchical Models, Combinatorial Optimization, Mixed Integer Linear Programming", 
    "title": "Towards Fast Algorithms for the Preference Consistency Problem Based on Hierarchical Models", 
    "type": "paper"
  }, 
  "1840": {
    "abstract": "Multi-view tagging has become increasingly popular in the applications where data representations by multiple views exist. A robust multi-view tagging method must have the capability to meet the two challenging requirements: limited labeled training samples and noisy labeled training samples. In this paper, we investigate this challenging problem of learning with limited and noisy tagging and propose a discriminative model, called MSMC, that exploits both labeled and unlabeled data through a semi-parametric regularization and takes advantage of the multi-label space consistency into the optimization. While MSMC is a general method for learning with multi-view, limited, and noisy tagging, in the evaluations we focus on the specific application of noisy image tagging with limited labeled training samples on a benchmark dataset. Extensive evaluations in comparison with state-of-the-art literature demonstrate that MSMC outstands with a superior performance.", 
    "authors": [
      {
        "name": "Yingming Li"
      }, 
      {
        "name": "Ming Yang"
      }, 
      {
        "name": "Zenglin Xu"
      }, 
      {
        "name": "Zhongfei Zhang"
      }
    ], 
    "keywords": "Multi-view learning, Limited learning, Noisy tagging", 
    "title": "Multi-view Learning with Limited and Noisy Tagging", 
    "type": "paper"
  }, 
  "1845": {
    "abstract": "Large scale smuggling of illegal goods is a long-standing problem, with $1.4b and thousands of agents assigned to protect the borders from such activity in the US-Mexico border alone. Illegal smuggling activities are usually blocked via inspecting stations or ad-hoc checkpoints/roadblocks. Security resources are insufficient to man all stations at all times; furthermore, smugglers regularly conduct surveillance activities. This paper makes several contributions toward the challenging task of optimally interdicting an illegal network flow: i) A new Stackelberg game model for network flow interdiction; ii) A novel Column and Constraint Generation approach for computing the optimal defender strategy; iii) Complexity analysis of the column generation subproblem; iv) Compact convex nonlinear programs for solving the subproblems; v) Novel greedy and heuristic approaches for subproblems with good approximation guarantee. Experimental evaluation shows that our approach can obtain a robust enough solution outperforming the existing methods and heuristic baselines significantly and scale up to realistic-sized problems.", 
    "authors": [
      {
        "name": "Qingyu Guo"
      }, 
      {
        "name": "Bo An"
      }, 
      {
        "name": "Yair Zick"
      }, 
      {
        "name": "Chunyan Miao"
      }
    ], 
    "keywords": "Stackelberg Security Games, Game Theory, Optimization", 
    "title": "Optimal Interdiction of Illegal Network Flow", 
    "type": "paper"
  }, 
  "1848": {
    "abstract": "The emerging research field of stream reasoning faces the challenging trade-off between expressiveness of query programs and data throughput. Towards optimizing programs for efficient evaluation, methods are needed to tell whether two programs are semantically equivalent. With the ultimate aim of providing practical reasoning techniques on streams, we study LARS programs, which can be seen as an extension of Answer Set Programming (ASP) for stream reasoning that supports the use of windows on streams for discarding information. We define different notions of equivalence between such programs and give semantic characterizations in terms of suitable notions of models. We show how a practically relevant instance of the fragment can be alternatively captured using Here-and-There models, yielding an extension of equilibrium semantics of ASP to this class of programs. Finally, we characterize the computational complexity of deciding the considered equivalence relations.", 
    "authors": [
      {
        "name": "Harald Beck"
      }, 
      {
        "name": "Minh Dao-Tran"
      }, 
      {
        "name": "Thomas Eiter"
      }
    ], 
    "keywords": "Stream Reasoning, Answer Set Programming, Knowledge Representation, Nonmonotonic Reasoning, Logic Programming", 
    "title": "Equivalent Stream Reasoning Programs", 
    "type": "paper"
  }, 
  "1852": {
    "abstract": "Spoken Language Understanding in Interactive Robotics provides computational models of human-machine communication based on the voice. However, robots operate in specific environments and the correct interpretation depends on the physical, cognitive and linguistic aspects triggered by the operational environment. Grounded language processing should exploit both the physical constraints of the context as well as knowledge assumptions of the robot. These include the subjective and partial perception of the environment that explicitly affects linguistic reasoning. In this work, a standard linguistic pipeline for semantic parsing is extended towards a form of perceptually informed natural language learning that combines discriminative learning and distributional semantics. Empirical results achieve up to a 40\\% of relative error reduction.", 
    "authors": [
      {
        "name": "Emanuele Bastianelli"
      }, 
      {
        "name": "Danilo Croce"
      }, 
      {
        "name": "Andrea Vanzo"
      }, 
      {
        "name": "Roberto Basili"
      }, 
      {
        "name": "Daniele Nardi"
      }
    ], 
    "keywords": "Spoken Language Understanding, Grounded Language Learning, Interactive Robotics", 
    "title": "A Discriminative Approach to Grounded Natural Language Learning in Interactive Robotics", 
    "type": "paper"
  }, 
  "186": {
    "abstract": "Subspace clustering aims to cluster unlabeled samples into multiple groups by implicitly seeking a subspace to fit each group. Most of existing subspace clustering methods are based on a shallow linear model, which may fail in handling complex data with nonlinear  structure. In this paper, we propose a novel subspace clustering method~\\textendash~deeP subspAce clusteRing with sparsiTY prior (PARTY)~\\textendash~based on a new deep learning architecture.  PARTY explicitly learns to progressively transform input data into nonlinear latent space and to be adaptive to the local and global subspace structure simultaneously.  In particular, considering local structure, PARTY learns representation for the input data with minimal reconstruction error. Moreover, PARTY incorporates a \\emph{prior} sparsity information into the hidden representation learning to preserve the sparse reconstruction relation over the whole data set. To the best of our knowledge, PARTY is the first deep learning based subspace clustering method. Extensive experiments on several real-world data sets with $13$ subspace clustering methods demonstrate the superiority of PARTY in terms of $5$ evaluation metrics.", 
    "authors": [
      {
        "name": "Xi Peng"
      }, 
      {
        "name": "Shijie Xiao"
      }, 
      {
        "name": "Jiashi Feng"
      }, 
      {
        "name": "Wei-Yun Yau"
      }, 
      {
        "name": "Zhang Yi"
      }
    ], 
    "keywords": "subspace clustering, subspace segmentation, spectral clustering, structure prior, unsupervised representation learning, deep neural network, globality, locality", 
    "title": "Deep Subspace Clustering with Sparsity Prior", 
    "type": "paper"
  }, 
  "1862": {
    "abstract": "Individual differences across subjects and non-stationary characteristic of electroencephalography (EEG) limit the generalization of affective brain-computer interfaces in real-world applications. On the other hand, it is very time consuming and expensive to acquire a large number of subject-specific labeled data for learning subject-specific models. In this paper, we propose to build personalized EEG-based affective models without labeled target data using transfer learning techniques. We mainly explore two types of subject-to-subject transfer approaches. One is to exploit shared structure underlying source domain (source subject) and target domain (target subject). The other is to train multiple individual classifiers on source subjects and transfer knowledge about classifier parameters to target subjects, and its aim is to learn a regression function that maps the relationship between feature distribution and classifier parameters. We compare the performance of five different approaches on an EEG dataset for constructing an affective model with three affective states: positive, neutral and negative. The experimental results demonstrate that our proposed subject transfer framework achieves the mean accuracy of 76.31% in comparison with a conventional generic classifier with 56.73% in average.", 
    "authors": [
      {
        "name": "Wei-Long Zheng"
      }, 
      {
        "name": "Bao-Liang Lu"
      }
    ], 
    "keywords": "Affective brain-computer interfaces, Transfer learning, EEG, Affective models", 
    "title": "Personalizing EEG-based Affective Models with Transfer Learning", 
    "type": "paper"
  }, 
  "1863": {
    "abstract": "Learning video representation is not a trivial task, as video is an information-intensive media where each frame does not exist independently. Locally, a video frame is visually and semantically similar with its adjacent frames. Holistically, a video has its inherent structure---the correlations among video frames. For example, even the frames far from each other may also hold similar semantics. Such context information is therefore important to characterize the intrinsic representation of a video frame. In this paper, we present a novel approach to learn the deep video representation by exploring both local and holistic contexts. Specifically, we propose a triplet sampling mechanism to encode the local temporal relationship of adjacent frames based on their deep representations. In addition, we incorporate the graph structure of the video, as a priori, to holistically preserve the inherent correlations among video frames. Our approach is fully unsupervised and trained in an end-to-end deep convolutional neural network architecture. By extensive experiments, we show that our learned representation can significantly boost several video recognition tasks (retrieval, classification, and highlight detection) over traditional video representations.", 
    "authors": [
      {
        "name": "Yingwei Pan"
      }, 
      {
        "name": "Yehao Li"
      }, 
      {
        "name": "Ting Yao"
      }, 
      {
        "name": "Tao Mei"
      }, 
      {
        "name": "Houqiang Li"
      }, 
      {
        "name": "Yong Rui"
      }
    ], 
    "keywords": "Video representation, Deep learning, Computer vision", 
    "title": "Learning Deep Intrinsic Video Representation by Exploring Temporal Coherence and Graph Structure", 
    "type": "paper"
  }, 
  "1866": {
    "abstract": "Human answer patterns in psychological reasoning experiments systematically deviate from predictions of classical logic. When interactions between any artificial reasoning system and humans are necessary this difference can be useful in some cases and lead to problems in other cases. Hence, other approaches than classical logic might be better suited to capture human inference processes. Evaluations are rare of how good such other approaches, e.g., non-monotonic logics, can explain psychological findings. In this article we consider the so-called Suppression Task, a core example in cognitive science about human reasoning that demonstrates that some additional information can lead to the suppression of simple inferences like the modus ponens. The psychological findings for this task have often been replicated and demonstrate a key-effect of human inferences. We analyze inferences of selected formal approaches and compare them by their capacity to cover human inference observed in the Suppression Task. A discussion on formal properties of successful theories conclude the paper.", 
    "authors": [
      {
        "name": "Marco Ragni"
      }, 
      {
        "name": "Christian Eichhorn"
      }, 
      {
        "name": "Gabriele Kern-Isberner"
      }
    ], 
    "keywords": "Cognitive Modeling, Experimental Results, Human Reasoning", 
    "title": "Simulating Human Inferences in the Light of New Information: A Formal Analysis", 
    "type": "paper"
  }, 
  "1868": {
    "abstract": "Strategy representation and reasoning for incomplete information concurrent games has recently received much attention in multi-agent system and AI communities. However, most of the logical frameworks are based on concrete game models, lack the abilities to reason about strategies explicitly or specify strategies procedurally, and ignore the issue of coordination within a coalition. In this paper, by a simple extension of a variant of multi-agent epistemic situation calculus with a strategy sort, we develop a general framework for strategy representation and reasoning for incomplete information concurrent games. Based on Golog, we propose a strategy programming language which can be conveniently used to specify collective strategies of coalitions at different granularities. We present a formalization of joint abilities of coalitions under commitments to strategy programs. Different kinds of individual strategic abilities can be distinguished in our framework. Both strategic abilities in ATL and joint abilities of Ghaderi et al. can be considered as joint abilities under special programs in our framework. We illustrate our work with a variant of Levesque\u00e2\u0080\u0099s Squirrels World.", 
    "authors": [
      {
        "name": "Liping Xiong"
      }, 
      {
        "name": "Yongmei Liu"
      }
    ], 
    "keywords": "Strategy representation and reasoning, incomplete information concurrent games, multi-agent epistemic situation calculus, strategy programming language, joint ability", 
    "title": "Strategy Representation and Reasoning for Incomplete Information Concurrent Games in the Situation Calculus", 
    "type": "paper"
  }, 
  "187": {
    "abstract": "Traffic congestion causes huge economic loss worldwide in each year due to wasted fuel, excessive air pollution, lost time and reduced productivity. Understanding how human move and select their transportation mode throughout large-scale transportation network is vital for urban congestion prediction and transportation scheduling. In this study, we collect big and heterogeneous  data (e.g., GPS records of 1.6 million users over three years and transportation network data), and we build an intelligent system, namely, DeepTransport, for simulating and predicting human mobility and their transportation mode at a citywide level. The key component of DeepTransport is based on the deep learning architecture that that aims to understand human mobility and their transportation patterns from big and heterogeneous data. Based on the learning model, given any time period, specific location of the city or human observed movements, our system can accurately simulate or predict the persons' future movements and their transportation mode in the large-scale transportation network. Experimental results and validations demonstrate the efficiency and superior performance of our system, and suggest that human transportation mode may be predicted and simulated  more easily than previously thought.", 
    "authors": [
      {
        "name": "Xuan Song"
      }, 
      {
        "name": "Hiroshi Kanasugi"
      }, 
      {
        "name": "Ryosuke Shibasaki"
      }
    ], 
    "keywords": "Intelligent Transportation Systems, Human Mobility, Big Data Application", 
    "title": "DeepTransport: Prediction and Simulation of Human Mobility and Transportation Mode at a Citywide Level", 
    "type": "paper"
  }, 
  "1884": {
    "abstract": "This paper establishes new relationships between existing formalisms for qualitative spatial and temporal representation and reasoning (QSTR). QSTR is concerned with infinite spatial and temporal domains which are abstracted to represent configurations of objects using a finite vocabulary of relations, often referred to as a qualitative calculus. Classically, reasoning in QSTR is based on constraints. An important task is to identify decision procedures able to handle constraints using relations from a single calculus or constraints using relations from several calculi. In particular the latter aspect is a longstanding challenge due to the multitude of calculi proposed. In this paper we consider Boolean combinations of qualitative constraints (their propositional closure) which enables progress with respect to both facets. Propositional closure allows several translations between distinct calculi to be established. This enables joint reasoning and provides new insights into computational  complexity of individual calculi. We conclude that the study of propositional languages instead of previously considered purely relational languages is a viable research direction for QSTR leading to expressive formalisms and practical algorithms.", 
    "authors": [
      {
        "name": "Diedrich Wolter"
      }, 
      {
        "name": "Jae Hee Lee"
      }
    ], 
    "keywords": "spatial and temporal reasoning, qualitative reasoning, constraints", 
    "title": "Connecting Qualitative Spatial and Temporal Representations by Propositional Closure", 
    "type": "paper"
  }, 
  "1887": {
    "abstract": "Ontology-based data access is concerned with the problem of querying incomplete data sources in the presence of an ontology. A key notion in this setting is that of ontology-mediated query, which is a database query coupled with an ontology. An interesting issue that arises is whether the answer to an ontology-mediated query can be computed by parallelizing the query over the connected components of the database, i.e., whether the query distributes over components. This implies that query evaluation can be done without any communication over a network using a distribution where every computing node is assigned some connected components of the database. In this work, we investigate distribution over components for classes of ontology-mediated queries where the database query is a conjunctive query and the ontology is formulated using existential rules. For each class in question, we syntactically characterize its fragment that distributes over components via the notion of connectedness for existential rules and conjunctive queries. In addition, we study the problem of deciding whether a query distributes over components, and we obtain several complexity results.", 
    "authors": [
      {
        "name": "Gerald Berger"
      }, 
      {
        "name": "Andreas Pieris"
      }
    ], 
    "keywords": "Ontology-based data access, Query answering, Conjunctive queries, Existential rules, Coordination-free evaluation", 
    "title": "Ontology-Mediated Queries Distributing Over Components", 
    "type": "paper"
  }, 
  "1891": {
    "abstract": "We investigate the problem of personalized review-based rating prediction, which aims at predicting users' ratings for items that they have not evaluated by using their historical reviews and ratings. Most of existing methods solve this problem by integrating topic model and latent factor model to learn interpretable user and items factors. However, these methods can not utilize word local context information of reviews. Moreover, it simply restricts user and item representations equivalent to their review representations, which may bring some irrelevant information in review text and harm the prediction accuracy.  In this paper, we propose a novel Collaborative Multi-Level Embedding (CMLE) model to address these limitations. The main technical contribution of CMLE is to integrate word embedding model with standard matrix factorization model through a projection level. This allows CMLE to inherit the ability of capturing word local context information from word embedding model and relax the strict equivalence requirement by projecting review embedding to user and item embeddings. A joint optimization problem is formulated and solved through an efficient stochastic gradient ascent algorithm. Empirical evaluations on real datasets show CMLE outperforms several competitive methods and can solve the two limitations well.", 
    "authors": [
      {
        "name": "Wei Zhang"
      }, 
      {
        "name": "Quan Yuan"
      }, 
      {
        "name": "Jiawei Han"
      }, 
      {
        "name": "Jianyong Wang"
      }
    ], 
    "keywords": "Rating Prediction, Review Analysis, Word Embedding, Matrix Factorization", 
    "title": "Collaborative Multi-Level Embedding Learning from Reviews for Rating Prediction", 
    "type": "paper"
  }, 
  "1893": {
    "abstract": "Derivative-free optimization methods are suitable for sophisticated optimization problems, while are hard to scale to high dimensionality (e.g., larger than 1,000). Previously, the random embedding technique has been shown successful for solving the high-dimensional problems with a low effective dimension. However, it is unrealistic to assume a low effective dimension in many applications. This paper turns to study the high-dimensional problems with a low optimal epsilon-effective dimension, which allow all dimensions to be effective but many of them only have a small bounded effect. We characterize the properties of random embedding for this kind of problems, and propose the sequential random embeddings (SRE) to reduce the embedding gap while running the optimization algorithms in a low-dimensional space. We apply SRE to several state-of-the-art derivative-free optimization methods, and conduct experiments on synthetic functions as well as non-convex classification tasks with up to 100,000 variables. Experiment results verify the effectiveness of SRE.", 
    "authors": [
      {
        "name": "Hong Qian"
      }, 
      {
        "name": "Yi-Qi Hu"
      }, 
      {
        "name": "Yang Yu"
      }
    ], 
    "keywords": "derivative-free optimization method, high-dimensional non-convex optimization, optimal epsilon-effective dimension, sequential random embedding", 
    "title": "Derivative-Free Optimization of High-Dimensional Non-Convex Functions by Sequential Random Embeddings", 
    "type": "paper"
  }, 
  "1898": {
    "abstract": "Since the appearance of the faces at close ages are similar, the face images at the neighboring ages can be utilized while learning a particular age. Label Distribution Learning (LDL) is proposed to use this observation by assigning an age distribution instead of a single label of the age to each face image. However, some existed LDL methods are limited because they can hardly extract enough useful information from complex image features. In this paper, Sparsity Conditional Energy Label Distribution Learning (SCE-LDL) is proposed to solve this problem. In the proposed SCE-LDL, age distributions are used as the training targets and energy function is utilized to define the age distribution. By assigning a suitable energy function, SCE-LDL can learn distributed representations, which provides the model with strong expressiveness for capturing enough of the  complexity of interest from image features. The sparsity constraints are also incorporated in the hidden layer to ameliorate the model. Experiment results in two age datasets show remarkable advantages of the proposed SCE-LDL model over  the previous proposed age estimation methods.", 
    "authors": [
      {
        "name": "Xu Yang"
      }, 
      {
        "name": "Xin Geng"
      }, 
      {
        "name": "De-Yu Zhou"
      }
    ], 
    "keywords": "Age Estimation, Label Distribution Learning, Energy based model, Sparsity Constraints", 
    "title": "Sparsity Conditional Energy Label Distribution Learning for Age Estimation", 
    "type": "paper"
  }, 
  "1904": {
    "abstract": "There are a variety of grand challenges for text extraction in scene videos, e.g., heterogeneous background, varied text, nonuniform illumination, arbitrary motion and poor contrast. Most previous video text detection methods are investigated with local information, i.e., within individual frames, obviously, with a limited performance. In this paper, we propose a unified tracking based scene text detection system by learning locally and globally, which uniformly integrates detection, tracking, recognition and their interactions. In this system, scene text is first detected locally in individual frames. Second, an optimal tracking trajectory is learned and linked globally with all detection, recognition and prediction information by dynamic programming. With the tracking trajectory, final detection and tracking results are simultaneously and immediately obtained. Moreover, our proposed techniques are extensively evaluated on several public scene video text databases, and are much better than the state-of-the-art methods.", 
    "authors": [
      {
        "name": "Wei-Yi Pei"
      }, 
      {
        "name": "Shu Tian"
      }, 
      {
        "name": "Xu-Cheng Yin"
      }
    ], 
    "keywords": "text detection, text tracking, text in scene videos, dynamic programming", 
    "title": "Scene Text Detection in Video by Learning Locally and Globally", 
    "type": "paper"
  }, 
  "1908": {
    "abstract": "Symmetry reduction has significantly contributed to the success of classical planning as heuristic search. However, it is an open question if symmetry reduction techniques can be lifted to fully observable nondeterministic (FOND) planning. We generalize the concepts of structural symmetries and symmetry reduction to FOND planning and specifically to the LAO* algorithm. Our base implementation of LAO* in the Fast Downward planner is competitive with the state-of-the-art FOND planner myND. Our experiments further show that symmetry reduction can yield strong performance gains compared to our base implementation of LAO*.", 
    "authors": [
      {
        "name": "Dominik Winterer"
      }, 
      {
        "name": "Martin Wehrle"
      }, 
      {
        "name": "Michael Katz"
      }
    ], 
    "keywords": "Fully Observable Nondeterministic Planning, Symmetries, LAO* Algorithm", 
    "title": "Structural Symmetries for Fully Observable Nondeterministic Planning", 
    "type": "paper"
  }, 
  "1915": {
    "abstract": "We define a clause tableaux calculus for MaxSAT, prove its soundness and completeness, and describe an exact  algorithm for MaxSAT. Given a multiset of clauses S, the algorithm computes the minimum number of clauses that can be falsified in S, as well as an optimal assignment,by applying finitely many times tableaux-like inference rules. We also describe how the algorithm can be extended to deal with weighted MaxSAT and weighted partial MaxSAT instances.", 
    "authors": [
      {
        "name": "Chu-Min Li"
      }, 
      {
        "name": "Felip Manya"
      }, 
      {
        "name": "Joan Ramon Soler"
      }
    ], 
    "keywords": "MaxSAT, Clause Tableaux, Completeness", 
    "title": "A Clause Tableaux Calculus for MaxSAT", 
    "type": "paper"
  }, 
  "1919": {
    "abstract": "This paper deals with the generalization ability of classifiers trained from non-iid evolutionary-related data in which all training and testing examples correspond to leaves of a phylogenetic tree. For the realizable case, we prove PAC-type upper and lower bounds based on symmetries and matchings in such trees.", 
    "authors": [
      {
        "name": "Ond\u0159ej Ku\u017eelka"
      }, 
      {
        "name": "Yuyi Wang"
      }, 
      {
        "name": "Jan Ramon"
      }
    ], 
    "keywords": "learning theory, non-iid data, evolutionary data, upper bounds, lower bounds", 
    "title": "Bounds for Learning from Evolutionary-Related Data in the Realizable Case", 
    "type": "paper"
  }, 
  "1925": {
    "abstract": "In some auction domains, there is uncertainty regarding the final availability of the goods being auctioned off. For example, a government may auction off spectrum from its public safety network, but it may need this spectrum back in times of emergency. In such a domain, standard combinatorial auctions perform poorly because they lead to violations of individual rationality (IR), even in expectation, and to very low efficiency. In this paper, we study the design of core-selecting payment rules. In contrast to prior work, however, our new rules are \"execution-contingent,\" i.e., the payments are conditioned on the realization of the availability of the goods. We design two core-selecting rules that always satisfy IR in expectation. To study the performance of our rules we perform a computational Bayes-Nash equilibrium analysis. We show that, in equilibrium, our new rules have better incentives, higher efficiency, and a lower rate of ex-post IR violations than standard core-selecting rules.", 
    "authors": [
      {
        "name": "Dmitry Moor"
      }, 
      {
        "name": "Tobias Grubenmann"
      }, 
      {
        "name": "Sven Seuken"
      }, 
      {
        "name": "Abraham Bernstein"
      }
    ], 
    "keywords": "combinatorial auctions, core-selecting payment rules, execution-contingency", 
    "title": "Core-Selecting Payment Rules for Combinatorial Auctions with Uncertain Availability of Goods", 
    "type": "paper"
  }, 
  "1941": {
    "abstract": "Fork-decoupled search is a recent approach to classical planning that exploits fork structures, where a single center component provides preconditions for several leaf components. The decoupled states in this search consist of a center state, along with a price for every leaf state. Given this, when does one decoupled state dominate another?  Such state-dominance criteria can be used to prune dominated search states. Prior work has devised only a trivial criterion. We devise several more powerful criteria, show that they preserve optimality, and establish their interrelations. We show that they can yield exponential reductions. Experiments on IPC benchmarks attest to the possible practical benefits.", 
    "authors": [
      {
        "name": "\u00c1lvaro Torralba"
      }, 
      {
        "name": "Daniel Gnad"
      }, 
      {
        "name": "Patrick Dubbert"
      }, 
      {
        "name": "Joerg Hoffmann"
      }
    ], 
    "keywords": "classical planning, cost-optimal planning, decoupled search, dominance pruning", 
    "title": "On State-Dominance Criteria in Fork-Decoupled Search", 
    "type": "paper"
  }, 
  "1947": {
    "abstract": "Symbolic bidirectional uniform-cost search is a prominent technique for cost-optimal planning. Thus, the question whether it can be further improved by making use of heuristic functions raises naturally. However, the use of heuristics in bidirectional search does not always improve its performance. We propose a novel way to use abstraction heuristics in symbolic bidirectional search in which the search only resorts to heuristics when it becomes unfeasible.  We adapt the definition of partial and perimeter abstractions to bidirectional search, where A* is used to traverse the abstract state spaces and/or generate the perimeter. The results show that abstraction heuristics can further improve symbolic bidirectional search in some domains. In fact, the resulting planner, SymBA*, was the winner of the optimal-track of the last IPC.", 
    "authors": [
      {
        "name": "\u00c1lvaro Torralba"
      }, 
      {
        "name": "Carlos Linares Lopez"
      }, 
      {
        "name": "Daniel Borrajo"
      }
    ], 
    "keywords": "Cost-optimal planning, Bidirectional heuristic search, Symbolic search, Abstraction heuristics", 
    "title": "Abstraction Heuristics for Symbolic Bidirectional Search", 
    "type": "paper"
  }, 
  "196": {
    "abstract": "Inspired by the progress of deep neural network (DNN) in single-media retrieval, the researchers have applied the DNN to cross-media retrieval. These methods are mainly two-stage learning: the first stage is to generate the separate representation for each media type, and the existing methods only model the intra-media information but ignore the inter-media correlation with the rich complementary context to the intra-media information. The second stage is to get the shared representation by learning the cross-media correlation, and the existing methods learn the shared representation through a shallow network structure, which cannot fully capture the intrinsic cross-media correlation. For addressing the above problems, we propose the cross-media multiple deep network (CMDN) to exploit the intrinsic and rich cross-media correlation by hierarchical learning. In the first stage, the CMDN jointly models the intra-media and inter-media information for getting the separate representation of each media type. In the second stage, the CMDN combines the inter-media and intra-media representations to further learn the intrinsic cross-media correlation by a two-level network strategy, and finally get the shared representation by a stacked network style. Experiment results show that the CMDN achieves better performance comparing with several state-of-the-art methods on 3 extensively used cross-media datasets.", 
    "authors": [
      {
        "name": "Yuxin Peng"
      }, 
      {
        "name": "Xin Huang"
      }, 
      {
        "name": "Jinwei Qi"
      }
    ], 
    "keywords": "Cross-media retrieval, Cross-media shared representation, Hierarchical learning, Multiple deep networks", 
    "title": "Cross-media Shared Representation by Hierarchical Learning with Multiple Deep Networks", 
    "type": "paper"
  }, 
  "1966": {
    "abstract": "We present two algorithms for computing the number of linear extensions of a given n-element poset. Our first approach builds upon an O(2^n n)-time dynamic programming algorithm by splitting subproblems into connected components and recursing on them independently. The recursion may run over two alternative subproblem spaces, and we provide heuristics for choosing the more efficient one. Our second algorithm is based on bucket elimination via inclusion--exclusion and runs in time O(n^(tw+4)), where tw is the treewidth of the cover graph. We demonstrate experimentally that these new algorithms outperform previously suggested ones for a wide range of posets, in particular when the posets are sparse.", 
    "authors": [
      {
        "name": "Juho-Kustaa Kangas"
      }, 
      {
        "name": "Teemu Hankala"
      }, 
      {
        "name": "Teppo Niinim\u00e4ki"
      }, 
      {
        "name": "Mikko Koivisto"
      }
    ], 
    "keywords": "partially ordered set, linear extension, counting, treewidth, bucket elimination", 
    "title": "Counting Linear Extensions of Sparse Posets", 
    "type": "paper"
  }, 
  "1974": {
    "abstract": "Epistemic planning is a variant of automated planning in the framework of dynamic epistemic logic. In recent works, the epistemic planning problem has been proved to be undecidable when preconditions of events can be epistemic formulas of arbitrary modal depth. It is known however that when preconditions are   propositional (and there are no postconditions), the problem is between PSPACE and EXPSPACE. In this  work we bring two new pieces to the picture. First, we prove that the epistemic planning problem  with   propositional preconditions and without postconditions is in PSPACE, and is thus PSPACE-complete. Second, we prove that very simple epistemic preconditions are enough to make the epistemic planning problem undecidable: preeconditions of modal depth at most two suffice. The case of preconditions of modal depth one remains open.", 
    "authors": [
      {
        "name": "Tristan Charrier"
      }, 
      {
        "name": "Bastien Maubert"
      }, 
      {
        "name": "Francois Schwarzentruber"
      }
    ], 
    "keywords": "Epistemic planning, Dynamic epistemic logic, Complexity, Decidability", 
    "title": "On the impact of modal depth in epistemic planning", 
    "type": "paper"
  }, 
  "1985": {
    "abstract": "Most existing learning to hash methods assume that there are sufficient data, either labeled or unlabeled, on the domain of interest (i.e., the target domain) for training. However, this assumption cannot be satisfied in some real-world applications. To address this data sparsity issue in hashing, inspired by transfer learning, we propose a new framework named Transfer Hashing with Privileged Information (THPI). Specifically, we extend the standard learning to hash method, Iterative Quantization (ITQ), in a transfer learning manner, namely ITQ+. In ITQ+, a new slack function is learned from auxiliary data to approximate the quantization error in ITQ. We developed an alternating optimization approach to solve the resultant optimization problem for ITQ+. We further extend ITQ+ to LapITQ+ by utilizing the geometry structure among the auxiliary data for learning more precise binary codes in the target domain. Extensive experiments on several benchmark datasets verify the effectiveness of our proposed approaches through comparisons with several state-of-art baselines.", 
    "authors": [
      {
        "name": "Joey Tianyi Zhou"
      }, 
      {
        "name": "Xinxing Xu"
      }, 
      {
        "name": "Sinno Jialin Pan"
      }, 
      {
        "name": "Ivor Tsang"
      }, 
      {
        "name": "Qin Zheng"
      }, 
      {
        "name": "Rick Siow Mong Goh"
      }
    ], 
    "keywords": "Learning to hash, Transfer Learning, Information retrieval", 
    "title": "Transfer Hashing with Privileged Information", 
    "type": "paper"
  }, 
  "1986": {
    "abstract": "Energy efficiency is a concern for any software running on mobile devices.  As such software employ machine-learned models to make predictions, this motivates research on efficiently executable models.  In this paper, we propose a variant of the widely used Naive Bayes (NB) learner that yields a more efficient predictive model.  In contrast to standard NB, where the learned model inspects all features to come to a decision, or NB with feature selection, where the model uses a fixed subset of the features, our model dynamically determines, on a case-by-case basis, when to stop inspecting features.  We show that our approach is often much more efficient than the current state of the art, without loss of accuracy.", 
    "authors": [
      {
        "name": "A\u00e4ron Verachtert"
      }, 
      {
        "name": "Jesse Davis"
      }, 
      {
        "name": "Hendrik Blockeel"
      }
    ], 
    "keywords": "Machine learning, Naive Bayes, Energy efficient prediction", 
    "title": "Dynamic Early Stopping for Naive Bayes", 
    "type": "paper"
  }, 
  "1987": {
    "abstract": "Submodular function maximization finds application in a variety of real-world decision-making problems. However, most existing methods, based on greedy maximization, assume it is computationally feasible to evaluate $F$, the function being maximized.  Unfortunately, in many realistic settings $F$ is too expensive to evaluate exactly even once. We present probably approximately correct greedy maximization, which requires access only to cheap anytime confidence bounds on $F$ and uses them to prune elements.  We show that, with high probability, our method returns an approximately optimal set.  Furthermore, we propose novel, cheap confidence bounds for conditional entropy, which appears in many common choices of $F$ and for which it is difficult to find unbiased or bounded estimates. Finally, results from a real-world dataset from a multi-camera tracking system in a shopping mall demonstrate that our approach performs comparably to existing methods, but at a fraction of the computational cost.", 
    "authors": [
      {
        "name": "Yash Satsangi"
      }, 
      {
        "name": "Shimon Whiteson"
      }, 
      {
        "name": "Frans Oliehoek"
      }
    ], 
    "keywords": "Submodularity, Information Gain Estimates, Sensor Selection, Greedy Maximisation, Probably Approximately Correct Learning, Unbiased estimator, Entropy estimation, Information gain bounds", 
    "title": "PAC Greedy Maximisation with Efficient Bounds on Information Gain for Sensor Selection.", 
    "type": "paper"
  }, 
  "1990": {
    "abstract": "The objective of partial satisfaction planning is to achieve an as valuable as possible state, accounting for the cost of its achievement. Net-benefit and oversubscription planning are two variants of partial satisfaction planning. In this work we investigate the computational complexity of restricted fragments of these two variants. In particular, we examine two specific restrictions, the causal graph structure and variable domain size. We show that even for the most restricted fragment with respect to these two restrictions optimal oversubscription planning is hard, while optimal net-benefit planning is tractable. We then partially relax these restrictions in order to find the boundary of tractability for both variants of partial satisfaction planning. In addition, for the family of 0-binary value functions we show a strong connection between the complexity of cost-optimal classical and optimal oversubscription planning.", 
    "authors": [
      {
        "name": "Michael Katz"
      }, 
      {
        "name": "Vitaly Mirkis"
      }
    ], 
    "keywords": "Partial Satisfaction Planning, Oversubscription Planning, Net-Benefit Planning, Complexity Analysis", 
    "title": "In Search of Tractability for Partial Satisfaction Planning", 
    "type": "paper"
  }, 
  "1995": {
    "abstract": "We introduce a setting for learning possibilistic logic theories from defaults of the form \u00e2\u0080\u009cif alpha then typically beta\u00e2\u0080\u009d. We first analyse this problem from the point of view of machine learning theory, determining the VC dimension of possibilistic stratifications as well as the complexity of the associated learning problems, after which we present a heuristic learning algorithm that can easily scale to thousands of defaults. An important property of our approach is that it is inherently able to handle noisy and conflicting sets of defaults. Among others, this allows us to learn possibilistic logic theories from crowdsourced data and to approximate propositional Markov logic networks using approximate MAP solvers. We present experimental results that demonstrate the effectiveness of this approach.", 
    "authors": [
      {
        "name": "Ond\u0159ej Ku\u017eelka"
      }, 
      {
        "name": "Jesse Davis"
      }, 
      {
        "name": "Steven Schockaert"
      }
    ], 
    "keywords": "Possibilistic logic, Default rules, Crowdsourcing, MAP inference", 
    "title": "Learning Possibilistic Logic Theories from Default Rules", 
    "type": "paper"
  }, 
  "1996": {
    "abstract": "In domain-independent planning, dependencies of operators and variables often prevent the effective application of planning techniques that rely on \"loosely coupled\" problems (like factored planning or partial order reduction). In this paper, we propose a generic approach for factorizing a classical planning problem into an equivalent problem with fewer operator and variable dependencies. Our approach is based on variable factorization, which can be reduced to the well-studied problem of graph factorization. While the state spaces of the original and the factorized problems are isomorphic, the factorization offers the potential to exponentially reduce the complexity of planning techniques like factored planning and partial order reduction.", 
    "authors": [
      {
        "name": "Martin Wehrle"
      }, 
      {
        "name": "Silvan Sievers"
      }, 
      {
        "name": "Malte Helmert"
      }
    ], 
    "keywords": "classical planning, factorization of planning problems, graph factorization", 
    "title": "Graph-Based Factorization of Classical Planning Problems", 
    "type": "paper"
  }, 
  "20": {
    "abstract": "Merger and Acquisition (M&A) has been a critical practice about corporate restructuring. Previous studies are mostly devoted to evaluating the suitability of M&A between a pair of investor and target company, or a target company for its propensity of being acquired. This paper focuses on the dual problem of predicting an investor's prospective M&A based on its past activities and firmographics.  We propose to use a mutually-exciting point process with a regression prior to quantify the investor's M&A behavior. Our model is motivated by the so-called contagious `wave-like' M&A phenomenon, which has been historically recognized among the economist communities. Efficient model learning algorithm is devised which incorporates both static profile covariates and past M&A activities. Experimental results on CrunchBase demonstrate the superiority of our model. The collected dataset and code will be released together with the paper.", 
    "authors": [
      {
        "name": "Junchi Yan"
      }, 
      {
        "name": "Shuai Xiao"
      }, 
      {
        "name": "Changsheng Li"
      }, 
      {
        "name": "Bo Jin"
      }, 
      {
        "name": "Xiangfeng Wang"
      }, 
      {
        "name": "Bin Ke"
      }, 
      {
        "name": "Xiaokang Yang"
      }, 
      {
        "name": "Hongyuan Zha"
      }
    ], 
    "keywords": "Merger and Acquisition, CrunchBase, Machine learning application", 
    "title": "Modeling contagious M&A via point processes with a profile regression prior", 
    "type": "paper"
  }, 
  "2009": {
    "abstract": "Stochastic gradient descent (SGD) has been widely used in machine learning, especially for training deep neural networks. In order to accelerate the convergence rate of SGD, a few advanced techniques have been developed in recent years, including variance reduction, stochastic coordinate sampling, and Nesterov\u00e2\u0080\u0099s acceleration method. Furthermore, in order to improve the training speed and/or leverage larger-scale training data, asynchronous parallelization of SGD has also been widely used. Then, a natural question is whether these techniques can be seamlessly integrated with each other, and whether the integration has desirable theoretical guarantee on its convergence.  In this paper, we will provide our formal answer to this question. In particular, we focus on the asynchronous parallelization of SGD, accelerated by leveraging variance reduction, coordinate sampling, and Nesterov\u00e2\u0080\u0099s method. We call the new algorithm asynchronous accelerated SGD, or AASGD for short. Theoretically, we proved a convergence rate of AASGD, which indicates that (i) asynchronous parallel does not hurt the convergence, and (ii) the three acceleration methods are complementary to each other and can make their own contributions to the improvement of the convergence rate. Empirically, we tested AASGD on a few benchmark datasets. The experimental results verified our theoretical findings and indicate that AASGD is a highly effective and efficient algorithm to use in practice.", 
    "authors": [
      {
        "name": "Qi Meng"
      }, 
      {
        "name": "Wei Chen"
      }, 
      {
        "name": "Jingcheng Yu"
      }, 
      {
        "name": "Taifeng Wang"
      }, 
      {
        "name": "Tieyan Liu"
      }
    ], 
    "keywords": "asynchronous, optimization, stochastic gradient descent, variance reduction, random block coordinate sampling", 
    "title": "Asynchronous Accelerated Stochastic Gradient Descent", 
    "type": "paper"
  }, 
  "2011": {
    "abstract": "We consider the problem of whether a coalition of agents has a knowledge-based strategy to ensure some outcome under a resource bound. We extend previous work on verification of multi-agent systems where actions of agents  produce and consume resources, by adding epistemic pre- and postconditions to actions. This allows us to model scenarios where agents perform both actions which change the world, and  actions which change their knowledge about the world, such as observation and communication. To avoid logical omniscience and obtain a compact model of the system, our model of agents' knowledge is syntactic. We define a class of coalition uniform strategies with respect to any (decidable) notion of coalition knowledge.  We show that the model-checking problem for the resulting logic is decidable for any notion of coalition uniform strategies in this class.", 
    "authors": [
      {
        "name": "Natasha Alechina"
      }, 
      {
        "name": "Mehdi Dastani"
      }, 
      {
        "name": "Brian Logan"
      }
    ], 
    "keywords": "Model-checking, Coalition knowledge, Resources", 
    "title": "Verifying existence of resource-bounded coalition uniform strategies", 
    "type": "paper"
  }, 
  "2014": {
    "abstract": "We propose a simple relaxation of Reiter's basic action theories, based on fluents without successor state axioms, that accommodates incompleteness beyond the initial database.   We prove that fundamental results about basic action theories can be fully recovered and that the generalized framework allows for natural specifications of various forms of incomplete causal laws. We illustrate this by showing how the evolution of incomplete databases, guarded action theories, and non-deterministic actions can be conveniently specified.", 
    "authors": [
      {
        "name": "Marcelo Arenas"
      }, 
      {
        "name": "Jorge Baier"
      }, 
      {
        "name": "Juan Navarro"
      }, 
      {
        "name": "Sebastian Sardina"
      }
    ], 
    "keywords": "Basic Action Theories, Incomplete databases, Free Fluents, Non-deterministic Actions, Null values, Situation Calculus", 
    "title": "Incomplete Causal Laws in the Situation Calculus using Free Fluents", 
    "type": "paper"
  }, 
  "2015": {
    "abstract": "QUACQ is a constraint acquisition system that assists a non-expert user to model her problem as a constraint network by classifying (partial) examples as positive or negative. For each negative example, QUACQ focuses onto a constraint of the target network.   The drawback is that the user may need to answer a great number of such examples to learn all the constraints.  In this paper, we provide a new approach  that is able to learn a maximum number of constraints violated by a given  negative example. Finally we give an experimental evaluation that shows that  our approach improves the state of the art.", 
    "authors": [
      {
        "name": "Robin Arcangioli"
      }, 
      {
        "name": "Christian Bessiere"
      }, 
      {
        "name": "Nadjib Lazaar"
      }
    ], 
    "keywords": "Constraint Acquisition, Constraint Modelling, Constraint Programming, Minimal Unsatisfiable Subsets, Machine Learning", 
    "title": "Multiple Constraint Acquisition", 
    "type": "paper"
  }, 
  "2020": {
    "abstract": "Crowdsourcing has been a helpful mechanism to leverage human intelligence to acquire useful knowledge for well defined tasks. However, when aggregating the crowd knowledge based on the currently developed voting algorithms, it often results in common knowledge that may not be expected. In this paper, we consider the problem of collecting as specific as possible knowledge via crowdsourcing. With the help of using external knowledge base such as WordNet, we incorporate the semantic relations between the alternative answers into a probabilistic model to determine which answer is more specific. We formulate the probabilistic model considering both worker's ability and task's difficulty, and solve it by expectation-maximization (EM) algorithm. Experimental results show that our approach achieved 35.88% improvement over majority voting when more specific answers are expected.", 
    "authors": [
      {
        "name": "Tao Han"
      }, 
      {
        "name": "Hailong Sun"
      }, 
      {
        "name": "Yangqiu Song"
      }, 
      {
        "name": "Yili Fang"
      }, 
      {
        "name": "Xudong Liu"
      }
    ], 
    "keywords": "crowdsourcing, knowledge acquisition, EM algorithm, crowd intelligence", 
    "title": "Incorporating External Knowledge into Crowd Intelligence for More Specific Knowledge Acquisition", 
    "type": "paper"
  }, 
  "2021": {
    "abstract": "We study the problem of incentivizing reliable demand-response in modern electricity grids. In our model, each agent is uncertain about her future availability, hence potentially unreliable. First, a set of agents are selected and asked to try to reduce demand. With some probability each selected agent is able to reduce demand, and can then choose whether to do so in return for a payment or face a penalty.  The goal is to reliably achieve a demand-reduction target with a minimal set of agents. A first  mechanism elicits response probabilities and costs directly from agents. We also provide an indirect mechanism that infers probabilities from agents' willingness to accept a penalty in case of non-response. We benchmark against the use of a simple spot auction. Both mechanisms achieve the reliability target in a dominant strategy equilibrium, at low total cost, and with much lower variance in payments than the spot auction.", 
    "authors": [
      {
        "name": "Hongyao Ma"
      }, 
      {
        "name": "Valentin Robu"
      }, 
      {
        "name": "Na Li"
      }, 
      {
        "name": "David C. Parkes"
      }
    ], 
    "keywords": "mechanism design, demand response, reliability bounds", 
    "title": "Incentivizing Reliability in Demand-Side Response", 
    "type": "paper"
  }, 
  "2032": {
    "abstract": "This paper studies a search problem involving a robot that is searching for a certain item in an uncertain environment (e.g., searching minerals on Mars) that allows only limited interaction with humans. The uncertainty of the environment comes from the rewards of undiscovered items and the availability of costly human help. The goal of the robot is to maximize the reward of the items found while minimising the search costs. We show that this search problem is polynomially solvable with a novel integration of the human help, which has not been studied in the literature before. Furthermore, we empirically evaluate our solution with simulations and show that it significantly outperforms several benchmark approaches.", 
    "authors": [
      {
        "name": "Shaofei Chen"
      }, 
      {
        "name": "Tim Baarslag"
      }, 
      {
        "name": "Dengji Zhao"
      }, 
      {
        "name": "Jing Chen"
      }, 
      {
        "name": "Lincheng Shen"
      }
    ], 
    "keywords": "Robot-Human search, Uncertain Knowledge, Human Availability, Optimal Search Strategy", 
    "title": "A Polynomial Time Optimal Algorithm for Robot-Human Search under Uncertainty", 
    "type": "paper"
  }, 
  "2033": {
    "abstract": "Hashing techniques has been intensively investigated for large scale vision applications. Recent research has shown that leveraging supervised information can lead to high quality hashing. However, most existing supervised hashing methods only construct similarity-preserving hash codes. Observing that semantic structures carry complementary information, we propose the idea of co-training for hashing, by jointly learning projections from image representations to hash codes and classification. Specifically, a novel deep semantics-preserving and ranking-based hashing (DSRH) architecture is presented, which consists of three components: a deep CNN for learning image representations, a hash stream of a binary mapping layer by evenly dividing the learnt representations into multiple bags and encoding each bag into one hash bit, and a classification stream. Meanwhile, our model is learnt under two constraints at the top loss layer of hash stream: a triplet ranking loss and orthogonality constraint. The former aims to preserve the relative similarity ordering in the triplets, while the latter makes different hash bits as independent as possible. We have conducted experiments on CIFAR-10 and NUS-WIDE image benchmarks, demonstrating that our approach can provide superior image search accuracy than other state-of-the-art hashing techniques.", 
    "authors": [
      {
        "name": "Ting Yao"
      }, 
      {
        "name": "Fuchen Long"
      }, 
      {
        "name": "Tao Mei"
      }, 
      {
        "name": "Yong Rui"
      }
    ], 
    "keywords": "Hashing, Similarity Learning, Image Retrieval, Deep Convolutional Neural Networks", 
    "title": "Deep Semantics-preserving and Ranking-based Hashing for Image Retrieval", 
    "type": "paper"
  }, 
  "2052": {
    "abstract": "Many tasks in AI require the design of complex programs and representations, whether for programming robots, designing game-playing programs, or conducting textual or visual transformations. This paper explores a novel inductive logic programming approach to learn such programs from examples. To reduce the complexity of the learned programs, and thus the search for such a program, we introduce higher-level operations, involving an alternation of Abstraction and Invention. Abstractions are described in the form logic program definitions containing higher-order predicate variables. Inventions involve the construction of definitions which instantiate the variabilised predicates used in the Abstractions. The use of Abstraction steps extends the Meta-Interpretive Learning framework, and is supported by the use of higher-order logic programming operators including a user-extendable set of programming operators such as if-then-else, while, until, map, reduce, and filter. Using these operators reduces the textual complexity required to express target classes of programs. Our experiments comparing the learning of such targets both with and without the abstraction operators indicate resulting reductions in the numbers of examples required to reach high predictive accuracy, as well as significant reductions in overall learning time.", 
    "authors": [
      {
        "name": "Andrew Cropper"
      }, 
      {
        "name": "Stephen Muggleton"
      }
    ], 
    "keywords": "Relational learning, Structured learning, Machine learning", 
    "title": "Learning Higher-Order Logic Programs through Abstraction and Invention", 
    "type": "paper"
  }, 
  "2062": {
    "abstract": "Vehicle-to-grid (V2G) is a promising approach whereby electric vehicles (EVs) are used to store excess electricity supply (e.g. from renewable sources) which is sold back to the grid in times of scarcity. In this paper we consider the setting of a smart car park, where EVs come and go, and can be used for V2G while parked. We develop novel allocation and payment mechanisms which truthfully elicit EV owner preferences and constraints including arrival, departure, required charge, as well as the costs of discharging due to loss of efficiency of the battery. The car park will schedule the charging and discharging of each EV, ensuring the constraints of the EVs are met, and taking into consideration predictions about future electricity prices. Optimally solving the global problem is intractable, and we present three novel heuristic online scheduling algorithms. We show that, under certain conditions, two of these satisfy monotonicity and are therefore truthful. We furthermore evaluate the algorithms using simulations.", 
    "authors": [
      {
        "name": "Enrico Gerding"
      }, 
      {
        "name": "Sebastian Stein"
      }, 
      {
        "name": "Sofia Ceppi"
      }, 
      {
        "name": "Valentin Robu"
      }
    ], 
    "keywords": "online mechanism design, smart grid, V2G, electric vehicles", 
    "title": "Online Mechanism Design for Vehicle-to-Grid Car Parks", 
    "type": "paper"
  }, 
  "2073": {
    "abstract": "Recommender systems have been widely studied in the literature as they have real-world impacts in many E- commerce platforms and social networks. Most pre- vious systems are based on user-item recommenda- tion matrix, which contains users\u00e2\u0080\u0099 history recommen- dation activities on items. In this paper, we propose a novel predictive collaborative filtering approach that exploits both the partially observed user-item recommendation matrix and the item-based side information to produce top-N recommender systems. The proposed approach automatically identifies the most interesting items for each user from his non-recommended item pool by aggregating over his recommended items via a low-rank coefficient matrix. Moreover, it also simultaneously builds linear regression models to predict the item recommendation scores from the item-based features for each user. The proposed approach is formulated as a rank constrained joint minimization problem over integrated least squares losses, for which an efficient analytical solution can be derived. To evaluate the proposed learning technique, we conduct empirical evaluations on a number of recommendation tasks. The experimental results demonstrate the efficacy and ef- ficiency of the proposed approach comparing to other comparison methods.", 
    "authors": [
      {
        "name": "Feipeng Zhao"
      }, 
      {
        "name": "Yuhong Guo"
      }
    ], 
    "keywords": "recommender systems, low rank completion, predictive model", 
    "title": "Efficient Collaborative Filtering with  Side Information", 
    "type": "paper"
  }, 
  "2077": {
    "abstract": "This paper addresses the task of change detection from noisy multivariate time-series data. One major feature of our approach is to leverage directional statistics as the noise-robust signature of time-series data. To capture major patterns, we introduce a regularized maximum likelihood equation for the von Mises-Fisher distribution, which simultaneously learns directional statistics and sample weights to filter out unwanted samples contaminated by the noise. We show that the optimization problem is reduced to the trust region subproblem in a certain limit, where global optimality is guaranteed. To evaluate the amount of changes, we introduce a novel distance measure on the Stiefel manifold. We demonstrate the utility of our approach using real-world data taken from an ore mining system.", 
    "authors": [
      {
        "name": "Tsuyoshi Ide"
      }, 
      {
        "name": "Dzung Phan"
      }, 
      {
        "name": "Jayant Kalagnanam"
      }
    ], 
    "keywords": "change detection, von Mises-Fisher distribution, trust region subproblem", 
    "title": "Change Detection using Directional Statistics", 
    "type": "paper"
  }, 
  "2080": {
    "abstract": "Bug reports provide an effective way for end-users to disclose potential bugs hidden in a software system, while automatically locating the potential buggy source codes according to a bug report remains a great challenge in software maintenance. Many previous studies treat the source codes as natural language by representing both the bug report and source codes based on bag-of-words feature representations, and correlate the bug report and source codes by measuring similarity in the same lexical feature space. However, these approaches fail to consider the structure information of source codes which carries additional semantics beyond the lexical terms. Such information is important in modeling program functionality. In this paper, we propose a novel convolutional neural network NP-CNN, which leverage both lexical and program structure information to learn unified features from natural language and source codes in programming language for automatically locating the potential buggy source codes according to bug report. Experimental results on widely-used software projects indicate that NP-CNN significantly outperforms the state-of-the-art methods in locating the buggy source files.", 
    "authors": [
      {
        "name": "Xuan Huo"
      }, 
      {
        "name": "Ming Li"
      }, 
      {
        "name": "Zhi-Hua Zhou"
      }
    ], 
    "keywords": "Software Mining, Machine Learning, Bug Localization, Convolutional Neural Networks", 
    "title": "Learning Unified Features from Natural and Progrmming Languages for Locating Buggy Source Codes", 
    "type": "paper"
  }, 
  "2081": {
    "abstract": "Many Web applications require efficient querying of large Knowledge Graphs (KGs). We propose KOGNAC, a dictionary-encoding algorithm designed to improve SPARQL querying with a judicious combination of statistical and semantic techniques. In KOGNAC, frequent terms are detected with a frequency approximation algorithm, and encoded to maximise compression. Infrequent terms are semantically grouped into ontological classes and encoded to increase data locality. We evaluated KOGNAC in combination with state-of-the-art RDF engines, and observed that it significantly improves SPARQL querying on KGs with up to 1B edges.", 
    "authors": [
      {
        "name": "Jacopo Urbani"
      }, 
      {
        "name": "Sourav Dutta"
      }, 
      {
        "name": "Sairam Gurajada"
      }, 
      {
        "name": "Gerhard Weikum"
      }
    ], 
    "keywords": "Knowledge Graphs, RDF, SPARQL, Encoding, Performance, Semantic Web", 
    "title": "KOGNAC: Efficient Encoding of Large Knowledge Graphs", 
    "type": "paper"
  }, 
  "2088": {
    "abstract": "Policy Iteration (PI) is a widely-used family of algorithms for computing an optimal policy for a Markov Decision Problem (MDP). Starting with an arbitrary initial policy, PI repeatedly updates to a dominating policy until an optimal policy is found. The update step involves switching the actions corresponding to a set of ``improvable'' states, which are easily identified. Whereas progress is guaranteed even if just one improvable state is switched at every step, the canonical variant of PI, attributed to Howard (1960), switches every improvable state for obtaining the next iterate. For MDPs with $n$ states and 2 actions per state, the tightest known bound on the complexity of Howard's PI is $O(2^{n}/n)$ iterations. The tightest known bound over all variants of PI is $O(1.7172^{n})$ iterations (in expectation) for a randomised variant introduced by Mansour and Singh (1999).  We introduce Batch-Switching Policy Iteration (BSPI), a family of deterministic PI algorithms that switches states in ``batches'', taking the batch size $b$ as a parameter. By varying $b$, BSPI interpolates between Howard's PI and another previously-studied variant called Simple PI (Melekopoglou and Condon, 1994). Our main contribution is a bound of $O(1.6479^{n})$ on the number of iterations taken by an instance of BSPI. We believe this is the tightest bound shown yet for any variant of PI. We also present experimental results to suggest that Howard's PI might itself enjoy an even tighter bound.", 
    "authors": [
      {
        "name": "Shivaram Kalyanakrishnan"
      }, 
      {
        "name": "Utkarsh Mall"
      }, 
      {
        "name": "Ritish Goyal"
      }
    ], 
    "keywords": "Markov Decision Problem, Policy Iteration, Planning, Algorithms, Complexity Bound", 
    "title": "Batch-Switching Policy Iteration", 
    "type": "paper"
  }, 
  "2090": {
    "abstract": "The paper presents Leviathan, an LTL satisfiability checking tool based on a novel one-pass, tree-like tableau system, which is way simpler than existing solutions. Despite the simplicity of the algorithm, the tool has performance comparable in speed and memory consumption with other tools on a number of standard benchmark sets, and, in various cases, it outperforms the other tableau-based tools.", 
    "authors": [
      {
        "name": "Matteo Bertello"
      }, 
      {
        "name": "Nicola Gigante"
      }, 
      {
        "name": "Angelo Montanari"
      }, 
      {
        "name": "Mark Reynolds"
      }
    ], 
    "keywords": "Linear Temporal Logic, Satisfiability, Tableau", 
    "title": "Leviathan: a new LTL satisfiability checking tool based on a one-pass tree-shaped tableau", 
    "type": "paper"
  }, 
  "2092": {
    "abstract": "We use statistical machine learning to develop methods for automatically designing mechanisms without money. Our goal is to find a mechanism that best approximates a given target function subject to a design constraint (e.g. strategy-proofness or stability). The proposed approach involves identifying a rich parametrized class of mechanisms that resemble discriminant-based multiclass classifiers, and relaxing the resulting search problem into an SVM-style surrogate optimization problem. We use this methodology to design strategy-proof mechanisms for social choice problems with single-peaked preferences, and stable mechanisms for two-sided matching problems. To the best of our knowledge, ours is the first automated approach for designing stable matching rules. Experiments on synthetic and real-world data confirm the usefulness of our methods.", 
    "authors": [
      {
        "name": "Harikrishna Narasimhan"
      }, 
      {
        "name": "Shivani Agarwal"
      }, 
      {
        "name": "David Parkes"
      }
    ], 
    "keywords": "Mechanism Design, Machine Learning, Social Choice, Strategy-proofness, Stable Matching, Single-peaked Preferences", 
    "title": "Automated Mechanism Design without Money via Machine Learning", 
    "type": "paper"
  }, 
  "21": {
    "abstract": "Evaluating a scientist's past and future potential impact is key in decision making concerning with recruitment and funding, and is increasingly linked to publication citation count. Meanwhile, timely identifying those valuable work with great potential before they receive wide recognition is both useful for readers and authors in many regards. We propose a method for predicting the citation count of individual publications, over an arbitrary time period. Our approach modulates paper-specific covariates, and a point process model to account for the aging and triggering effects of recent citations, through which papers lose and gain their popularity, respectively. Empirical results on the Microsoft Academic Graph data suggests that our model has utility for both prediction and interpretability.  As addressing a relatively open research problem, this work gives an independent study, corresponding to the mixed results from the recent studies on \\emph{Science communications} about the reliability of a cit", 
    "authors": [
      {
        "name": "Shuai Xiao"
      }, 
      {
        "name": "Junchi Yan"
      }, 
      {
        "name": "Changsheng Li"
      }, 
      {
        "name": "Bo Jin"
      }, 
      {
        "name": "Xiangfeng Wang"
      }, 
      {
        "name": "Xiaokang Yang"
      }, 
      {
        "name": "Stephen Chu"
      }, 
      {
        "name": "Hongyuan Zha"
      }
    ], 
    "keywords": "citation prediction, behavioral model, Microsoft Academic Graph", 
    "title": "On modeling and predicting individual paper citation count over time", 
    "type": "paper"
  }, 
  "210": {
    "abstract": "Markov Random Field (MRF) is an important tool in many machine learn and computer vision problems. As a special case, the MRFs with two-dimensional (2D) labels have proved useful to many applications, such as image matching and optical flow. Due to the huge label set, traditional optimization algorithms tend to be slow for the inference of two-dimensional (2D) label MRF, and its high computation costs are a barrier to practical use. This paper borrows lessons from the coordinate descent in convex optimization and presents an efficient algorithm, named FastLCD. Unlike previous popular move-making algorithms (e.g., \\alpha-expansion) that should consider labels exhaustively, the FastLCD optimizes the 2D label MRFs by performing label coordinate descents alternately in horizontal, vertical and diagonal directions, and by this way, it does not need to visit all the labels exhaustively. The FastLCD greatly reduces the search space of the label set and benefits from a lower time complexity. Experimental results show that FastLCD is much faster, but still yields high quality results.", 
    "authors": [
      {
        "name": "Kangwei Liu"
      }, 
      {
        "name": "Junge Zhang"
      }, 
      {
        "name": "Peipei Yang"
      }, 
      {
        "name": "Kaiqi Huang"
      }
    ], 
    "keywords": "Energy minimization, Markov random field, Graph cut, Discrete optimization, Coordinate gradient descent", 
    "title": "FastLCD: Fast Label Coordinate Descent for the Optimization of MRFs with Two-Dimensional Labels", 
    "type": "paper"
  }, 
  "2102": {
    "abstract": "Social robots will need both learning and emotional capabilities to successfully enter domestic environments. We propose to couple both challenges (learning and emotion) for mutual benefit. Most computational implementations in this direction use model-free reinforcement learning (RL). However, important emotions like hope and fear need anticipation (i.e. forward simulation), which can only be obtained in a model-based context. The current work introduces the first anticipatory models of hope and fear in a learning agent. Taking inspiration from psychological emotion theory, we show how hope and fear can be estimated from the best and worst forward traces through efficient planning (UCT) in a model-based RL framework. Test results in two known RL domains show how emotional signals develop during learning, and illustrate their plausibility in individual Pacman game settings. Our method, which is easily integrated in any model-based RL architecture, functionally grounds emotions in the learning process. This is a critical step towards robots that naturally express hope and fear and, moreover, can explain what anticipated events caused these.", 
    "authors": [
      {
        "name": "Thomas Moerland"
      }, 
      {
        "name": "Joost Broekens"
      }, 
      {
        "name": "Catholijn Jonker"
      }
    ], 
    "keywords": "Emotion, Model-Based Reinforcement Learning, Anticipation, Hope, Fear", 
    "title": "Fear and Hope Emerge from Anticipation in Model-Based Reinforcement Learning", 
    "type": "paper"
  }, 
  "211": {
    "abstract": "The rapid growth of open data on the Web promotes the development of data portals that facilitate finding useful datasets. To help users quickly inspect a dataset found in a portal, we propose to summarize its contents and generate a hierarchical grouping of entities connected by relations. Our generic approach, called HIEDS, considers coverage of dataset, height of hierarchy, cohesion within groups, overlap between groups, and homogeneity of groups, and integrates these configurable factors into a combinatorial optimization problem to solve. We present an efficient solution, to serve users with dynamically configured summaries with acceptable latency. We systematically experiment with our approach on real-world RDF datasets", 
    "authors": [
      {
        "name": "Gong Cheng"
      }, 
      {
        "name": "Cheng Jin"
      }, 
      {
        "name": "Yuzhong Qu"
      }
    ], 
    "keywords": "dataset summary, hierarchical summarization, open data portal, combinatorial optimization", 
    "title": "HIEDS: A Generic and Efficient Approach to Hierarchical Dataset Summarization", 
    "type": "paper"
  }, 
  "2115": {
    "abstract": "In crowdsourcing when there is a lack of verification for contributed answers, output agreement mechanisms are often used to incentivize participants to provide truthful answers when the correct answer is hold by the majority. In this paper, we focus on using output agreement mechanisms to elicit effort, in addition to eliciting truthful answers, from a population of workers. We consider a setting where workers have heterogeneous cost of effort exertion and examine the data requester's problem of deciding the reward level in output agreement for optimal elicitation. In particular, when the requester knows the cost distribution, we derive the optimal reward level for output agreement mechanisms. This is achieved by first characterizing Bayesian Nash equilibria of output agreement mechanisms for a given reward level. When the requester does not know the cost distribution, we develop sequential mechanisms that combine learning the cost distribution with incentivizing effort exertion to approximately determine the optimal reward level.", 
    "authors": [
      {
        "name": "Yang Liu"
      }, 
      {
        "name": "Yiling Chen"
      }
    ], 
    "keywords": "crowdsourcing, incentives, learning, mechanism design", 
    "title": "Learning to Incentivize: Eliciting Effort via Output Agreement", 
    "type": "paper"
  }, 
  "212": {
    "abstract": "Implicit feedback based recommendation has recently been an important task with the accumulated user-item interaction data. However, it is very challenging to produce recommendations from implicit feedback due to the sparseness of data and the lack of negative feedback/rating. Although various factor models have been proposed to tackle this problem, they either focus on rating prediction that may lead to inaccurate top-k recommendations or are dependent on the sampling of negative feedback that often results in bias. To this end, we propose a Relaxed Ranking-based Factor Model, RRFM, to relax pairwise ranking into a SVM-like task, where positive and negative preferences are separated by the soft boundaries, and their non-separate property is employed to capture the characteristic of unobserved data. A smooth and scalable algorithm is developed to solve group- and instance- level's optimization and parameter estimation. Extensive experiments based on real-world datasets demonstrate the effectiveness and advantage of our approach.", 
    "authors": [
      {
        "name": "Huayu Li"
      }, 
      {
        "name": "Yong Ge"
      }
    ], 
    "keywords": "Recommender System, Implicit Feedback, Matrix Factorization", 
    "title": "A Relaxed Ranking-based Factor Model for Recommender System  from Implicit Feedback", 
    "type": "paper"
  }, 
  "2126": {
    "abstract": "Existentially quantified variables in goals and action preconditions are part of the standard PDDL planning language, yet few planners support them, while those that do compile them away at an exponential cost. In this work, we argue that existential variables are an essential feature for representing and reasoning with constraints in planning, and that it is harmful to compile them away or avoid them altogether, since this hides part of the problem structure that can be computationally exploited. We show how to do this by proposing an extension of the standard delete-relaxation heuristics that handles existential variables. While this extension is simple, the consequences for both modeling and computation are important. Furthermore, by allowing existential variables in STRIPS and treating them properly, CSPs can be represented and solved in a direct manner as action-less, fluent-less STRIPS planning problems, something important for problems involving restrictions. In addition, functional fluents in Functional STRIPS can be compiled away with no effect on the structure and informativeness of the resulting heuristic. Experiments are reported comparing our native \u00e2\u0088\u0083-STRIPS planner with a Functional STRIPS planner and with state-of-the-art STRIPS planners over compiled and propositional encodings.", 
    "authors": [
      {
        "name": "Guillem Franc\u00e8s"
      }, 
      {
        "name": "Hector Geffner"
      }
    ], 
    "keywords": "Classical planning, Planning languages and heuristics, Constraint satisfaction, First-order logic", 
    "title": "$\\exists$-STRIPS: Existential Quantification in Planning and Constraint Satisfaction", 
    "type": "paper"
  }, 
  "2127": {
    "abstract": "Constraint acquisition systems assist the non-expert user in modeling her problem as a constraint network.  Most existing constraint acquisition systems interact with the user by asking her to classify an example as positive or negative. Such queries do not use the structure of the problem and can thus lead the user to answer a large number of queries. In this paper, we propose Predict&Ask, an algorithm  based on the prediction of missing constraints in the partial network learned so far. Such missing constraints are directly asked to the user through recommendation queries, a new, more informative kind of queries. Predict&Ask can be plugged in any constraint acquisition system. We experimentally  compare the QuAcq system to an extended version boosted by the use of our recommendation queries. The results show that the extended version improves the basic QuAcq.", 
    "authors": [
      {
        "name": "Abderrazak Daoudi"
      }, 
      {
        "name": "Younes Mechqrane"
      }, 
      {
        "name": "Christian Bessiere"
      }, 
      {
        "name": "Nadjib Lazaar"
      }, 
      {
        "name": "El Houssine Bouyakhf"
      }
    ], 
    "keywords": "Constraint Programming, Constraint Satisfaction Problems, Constraint Acquisition, Modeling, Link prediction in dynamic graphs, Recommendation Systems", 
    "title": "Constraint Acquisition using Recommendation Queries", 
    "type": "paper"
  }, 
  "2139": {
    "abstract": "We introduce a class of resource games where resources and preferences are described with the language of a resource-sensitive logic. We present two decision problems, the first of which is deciding whether an action profile is a Nash equilibrium. When dealing with resources, interesting questions arise as to whether some undesirable equilibria can be eliminated by a central authority by redistributing the available resources among the agents. We will thus study the decision problems of rational elimination. We will consider them in the contexts of dichotomous or pseudo-dichotomous preferences, and of logics that admit or not the weakening rule. This will offer a variety of complexity results that are applicable to a large number of settings.", 
    "authors": [
      {
        "name": "Nicolas Troquard"
      }
    ], 
    "keywords": "resources, game theory, Nash, logic, linear logic", 
    "title": "Nash equilibria and their elimination in resource games", 
    "type": "paper"
  }, 
  "2142": {
    "abstract": "State-of-the-art applications of Stackelberg security games -- including wildlife protection -- offer a wealth of data, which can be used to learn the behavior of the adversary. But existing approaches either make strong assumptions about the structure of the data, or gather new data through online algorithms that are likely to play severely suboptimal strategies. We develop a new approach to learning the parameters of the behavioral model of the attacker (thereby pinpointing a near optimal strategy), by observing how the attacker responds to only three defender strategies. We also validate our approach using experiments on real and synthetic data.", 
    "authors": [
      {
        "name": "Nika Haghtalab"
      }, 
      {
        "name": "Fei Fang"
      }, 
      {
        "name": "Thanh Nguyen"
      }, 
      {
        "name": "Arunesh Sinha"
      }, 
      {
        "name": "Ariel Procaccia"
      }, 
      {
        "name": "Milind Tambe"
      }
    ], 
    "keywords": "Stackelberg security games, Wildlife protection, Regression learning", 
    "title": "Three strategies to success: Learning adversary models in security games", 
    "type": "paper"
  }, 
  "2148": {
    "abstract": "Probabilistic logics, especially those based on logic programming (LP), are gaining popularity as modelling and reasoning tools, since they combine the power of logic to represent knowledge with the ability of probability theory to deal with uncertainty. In this paper, we propose a hybrid extension for probabilistic logic programming, which allows for a much wider class of continuous distributions than existing extensions allowing for exact inference. At the same time, our extension allows one to compute approximations with bounded and arbitrary small error. We propose a novel anytime algorithm exploring the logical and continuous structure of distributions and experimentally show that our algorithm is competitive with state-of-the-art sampling algorithms, in spite of the fact that it provides much stronger guarantees, for typical relational problems, which are characterised by a moderate number of dimensions and a significant amount of structure. Our method outperforms sampling methods by far if rare events with deterministic structure are provided as evidence.", 
    "authors": [
      {
        "name": "Steffen Michels"
      }, 
      {
        "name": "Arjen Hommersom"
      }, 
      {
        "name": "Peter Lucas"
      }
    ], 
    "keywords": "probabilistic inference, hybrid distributions, probabilistic logic programming", 
    "title": "Approximate Probabilistic Inference with Bounded Error for Hybrid Probabilistic Logic Programming", 
    "type": "paper"
  }, 
  "2151": {
    "abstract": "Answer Set Programming (ASP) is a well-known problem solving approach based on non-monotonic logic programs and efficient solvers. HEX-programs extend ASP with external atoms for access to arbitrary external information. In this work, we extend the evaluation principles of external atoms to partial assignments, lift nogood learning to this setting, and  introduce a variant of nogood minimization that enables external sources to guide the search for answer sets akin to theory propagation. In contrast to related techniques in SMT or constraint ASP solving, ours is more general and does not depend on hand-crafted learning techniques that are specific for each external source. This allows a broad range of users, without prior expert knowledge on solver construction, to harness performance gained by novel learning techniques. Our benchmark experiments demonstrate a clear improvement in efficiency over the state-of-the-art HEX-solver.", 
    "authors": [
      {
        "name": "Thomas Eiter"
      }, 
      {
        "name": "Tobias Kaminski"
      }, 
      {
        "name": "Christoph Redl"
      }, 
      {
        "name": "Antonius Weinzierl"
      }
    ], 
    "keywords": "answer set programming, nonmonotonic reasoning, external sources", 
    "title": "Exploiting Partial Assignments for Efficient Evaluation of Answer Set Programs with External Source Access", 
    "type": "paper"
  }, 
  "2160": {
    "abstract": "We study strategic games on weighted directed graphs, where the payoff of a player is defined as the sum of the weights on the edges from players who chose the same strategy augmented by a fixed non-negative bonus for picking a given strategy.  These games capture the idea of coordination in the absence of globally common strategies. Prior work shows that the problem of determining the existence of a pure Nash equilibrium for these games is NP-complete already for graphs with all weights equal to one and no bonuses.  However, for several classes of graphs (e.g. DAGs and cliques) pure Nash equilibria or even strong equilibria always exist and can be found by simply following a particular improvement or coalition-improvement path, respectively.   In this paper we identify several natural classes of graphs for which a finite improvement or coalition-improvement path of polynomial length always exists, and, as a consequence, a Nash equilibrium or strong equilibrium in them can be found in polynomial time.  We also argue that these results are optimal in the sense that in natural generalisations of these classes of graphs, a pure Nash equilibrium may not even exist.", 
    "authors": [
      {
        "name": "Sunil Easaw Simon"
      }, 
      {
        "name": "Dominik Wojtczak"
      }
    ], 
    "keywords": "finite improvement path, weak acyclicity, c-weak acyclicity, equilibrium computation, pure Nash equilibria, strong equilibria", 
    "title": "Efficient Local Search in Coordination Games on Graphs", 
    "type": "paper"
  }, 
  "2164": {
    "abstract": "Recent years have seen the development of efficient and provably correct spectral algorithms for learning models of partially observable environments arising in many applications. But despite the high hopes raised by this new class of algorithms, their practical impact is still below expectations. One reason for this is the difficulty in adapting spectral methods to exploit structural constraints about different target environments which can be known beforehand. A natural structure intrinsic to many dynamical systems is a multi-resolution behaviour where interesting phenomena occur at different time scales during the evolution of the system. In this paper we introduce the multi-step predictive state representation (M-PSR) and an associated learning algorithm that finds and leverages frequent patterns of observations at multiple scales in dynamical systems with discrete observations. We perform experiments on robot exploration tasks in a wide variety of environments and conclude that the use of M-PSR improves over the classical PSR for varying amounts of data, environment sizes, and number of observations symbols.", 
    "authors": [
      {
        "name": "Lucas Langer"
      }, 
      {
        "name": "Borja Balle"
      }, 
      {
        "name": "Doina Precup"
      }
    ], 
    "keywords": "Time series prediction, Predictive state representations, Spectral learning", 
    "title": "Learning Multi-Step Predictive State Representations", 
    "type": "paper"
  }, 
  "2171": {
    "abstract": "We consider a variant of the well-studied multi-armed bandit problem in which the reward from each action evolves monotonically in the number of times the decision maker chooses to take that action. We are motivated by settings in which we must give a series of homogeneous tasks to a finite set of workers (arms) whose performance may improve (due to learning) or decay (due to loss of interest) with repeated trials. We assume that the arm-dependent rates at which the rewards change are unknown to the decision maker, and propose algorithms with provably optimal policy regret bounds, a much stronger notion than the often-studied external regret. For the case where the rewards are increasing and concave, we give an algorithm whose policy regret is sublinear and has a (provably necessary) dependence on the time required to distinguish the optimal arm from the rest. We illustrate the behavior and performance of this algorithm via simulations. For the decreasing case, we present a simple greedy approach and show that the policy regret of this algorithm is constant and upper bounded by the number of arms.", 
    "authors": [
      {
        "name": "Hoda Heidari"
      }, 
      {
        "name": "Michael Kearns"
      }, 
      {
        "name": "Aaron Roth"
      }
    ], 
    "keywords": "Policy regret, Monotone Bandits, Sublinear policy regret", 
    "title": "Tight Policy Regret Bounds for Improving and Decaying Bandits", 
    "type": "paper"
  }, 
  "2173": {
    "abstract": "Recent work on plan recognition as planning has shown great promise in the use of a domain theory and general planning algorithms for the plan recognition problem. In this paper, we propose to extend previous work to (1) address observations over fluents, (2) better address unreliable observations (i.e., noisy or missing observations), and (3) recognize plans in addition to goals.  To this end, we introduce a relaxation of the plan-recognition-as-planning formulation that allows unreliable observations. That is, in addition to the original costs of the plan, we define two additional objectives that account for missing and noisy observations, and optimize for a linear combination of all objectives. We compute the posterior probabilities of generated plans by taking into account the combined costs that include penalties for missing or noisy observations, and normalizing over a sample set of plans generated by finding either diverse or high-quality plans. Our experiments show that this approach improves goal recognition in most domains when observations are unreliable. In addition, we evaluate plan recognition performance and show that the high-quality plan generation approach should be preferred in most domains.", 
    "authors": [
      {
        "name": "Shirin Sohrabi"
      }, 
      {
        "name": "Anton Riabov"
      }, 
      {
        "name": "Octavian Udrea"
      }
    ], 
    "keywords": "AI Planning, Plan Recognition, Diverse Planning, High-quality plans, Unreliable Observations", 
    "title": "Plan Recognition as Planning Revisited", 
    "type": "paper"
  }, 
  "2174": {
    "abstract": "Investigations into probabilistic graphical models for decision making have predominantly centered on influence diagrams (IDs) and decision circuits (DCs) for representation and computation of decision rules that maximize the expected utility. IDs have the benefit of making the conditional independence structure explicit and DCs ensure that decision making can be done exactly in linear time with respect to the size of the circuit. Since IDs are typically handcrafted and DCs are compiled from IDs, in this paper we propose an approach to learn the structure and the parameters of decision making problems directly from data. We also propose a new representation called sum-product-max network (SPMN) that generalizes sum-product networks (SPNs) to decision making by introducing two new types of nodes (max nodes and utility nodes). Our learning algorithm produces an SPMN, which is equivalent to a DC and therefore enables optimal decision making in linear time with respect to the size of the learned representation. This approach is significant because it facilitates a novel paradigm of learning tractable decision-making models from data.", 
    "authors": [
      {
        "name": "Mazen Melibari"
      }, 
      {
        "name": "Pascal Poupart"
      }, 
      {
        "name": "Prashant Doshi"
      }
    ], 
    "keywords": "Decision Making, Influence Diagrams, Decision Networks, Graphical models", 
    "title": "Sum-Product-Max Networks for Tractable Decision Making", 
    "type": "paper"
  }, 
  "2178": {
    "abstract": "We propose a new distributed algorithm for decoupling the Multiagent Simple Temporal Network (MaSTN) problem. The agents cooperatively decouple the MaSTN simultaneously optimizing a sum of concave objectives local to each agent. Several schedule flexibility measures are applicable in this framework. We pose the MaSTN decoupling problem as a distributed convex optimization problem subject to constraints having a block angular structure; we adapt existing variants of Alternating Direction Method of Multiplier (ADMM) type methods to perform decoupling optimally. The algorithm presented in this paper is an iterative procedure that is guaranteed to converge.Communication is only between agents that are temporally constrained with each other and the information exchanged between them is carried out in a privacy preserving manner. We present experimental results for the proposed method on varying problem sizes and demonstrate its effectiveness in terms of solving quality and computational cost.", 
    "authors": [
      {
        "name": "Jayanth Krishna Mogali"
      }, 
      {
        "name": "Stephen Smith"
      }, 
      {
        "name": "Zachary Rubinstein"
      }
    ], 
    "keywords": "Simple Temporal Network, Decoupling, Distributed Optimization, Privacy, Multi Agent Scheduling, ADMM", 
    "title": "Distributed Decoupling Of Multiagent Simple Temporal Problems", 
    "type": "paper"
  }, 
  "2185": {
    "abstract": "Recently logics for strategic ability have gained pre-eminence in the modelisation and analysis of game-theoretic scenarios. In this paper we provide a first contribution to the comparison of two popular frameworks: concurrent game structure (CGS) and the coalition logic of propositional control (CL-PC). Specifically, we ground the abstract abilities of agents in CGS on the theory of propositional control, thus obtaining a class of CGS that has the same expressive power as CL-PC. We study the computational properties of this setting. Further we relax some of the assumptions of CL-PC so as to introduce a wider class of computationally-grounded CGS.", 
    "authors": [
      {
        "name": "Francesco Belardinelli"
      }, 
      {
        "name": "Andreas Herzig"
      }
    ], 
    "keywords": "Reasoning about Strategic Ability, Alternating-time Temporal Logic, Propositional Control, Concurrent Game Structures, Logics for Game Theory", 
    "title": "On Logics of Strategic Ability based on Propositional Control", 
    "type": "paper"
  }, 
  "2191": {
    "abstract": "Social norms are powerful formalism in coordinating autonomous agents' behaviour to achieve certain objectives. In this paper, we propose a stateful normative system to enable the reasoning of the changes of norms under different circumstances, which cannot be done in the existing stateless normative systems. We study two important problems (norm synthesis and norm recognition) related to the autonomy of the entire system and the agents, and characterise the computational complexities of solving these problems.", 
    "authors": [
      {
        "name": "Xiaowei Huang"
      }, 
      {
        "name": "Ji Ruan"
      }, 
      {
        "name": "Qingliang Chen"
      }, 
      {
        "name": "Kaile Su"
      }
    ], 
    "keywords": "Normative system, autonomy, multiagent system, norm synthesis, norm recognition", 
    "title": "The Complexity of Norm Synthesis and Recognition in Normative Multiagent Systems", 
    "type": "paper"
  }, 
  "2193": {
    "abstract": "Owing to the tremendous increase in the volume and variety of user generated content, train\u00e2\u0080\u0093once\u00e2\u0080\u0093apply\u00e2\u0080\u0093forever models are insufficient for supervised learning tasks. Thus, developing algorithms that adapt across domains by leveraging data from multiple domains is critical. However, existing adaptation algorithms often fail to identify the right sources to use for adaptation. In this work, we present a novel multi-source iterative domain adaptation algorithm (MSIDA) that leverages knowledge from selective sources to improve the performance in a target domain. The algorithm first chooses the best K sources from possibly numerous existing domains taking into account both similarity and complementarity properties of the domains. Then it learns target specific features in an iterative manner building on the common shared representations from the source domains. We give theoretical justifications for our source selection procedure and also give mistake bounds for the MSIDA algorithm. Experimental results justify the theory as MSIDA significantly  outperforms existing cross-domain classification approaches on a real world  online social media dataset and the benchmark Amazon product review dataset.", 
    "authors": [
      {
        "name": "Himanshu Bhatt"
      }, 
      {
        "name": "Arun Rajkumar"
      }, 
      {
        "name": "Shourya Roy"
      }
    ], 
    "keywords": "Domain adaptation, Multi-source adaptation, Sentiment classification, Cross-domain classification, Web content analysis, Transfer learning, Machine Learning", 
    "title": "Multi-source Iterative Adaptation for Cross-domain Classification", 
    "type": "paper"
  }, 
  "2201": {
    "abstract": "We consider environments, like private houses, in which smart devices equipped with limited communication and computation capabilities have to cooperate to self-configure their state in an energy-efficient manner, as to meet user-defined requirements. Such requirements are expressed as \\emph{rules}, programmed by the user using an intuitive interface that connects conditions on sensors' and actuators' states and actions on actuators.  We translate this smart environment configuration problem into a constraint optimization problem. As to install distributiveness, robustness, and openness, we solve it using distributed message-passing algorithms. We illustrate our approach through a running example, and evaluate the performances of the implemented protocols on a simulated realistic environment.", 
    "authors": [
      {
        "name": "Pierre Rust"
      }, 
      {
        "name": "Gauthier Picard"
      }, 
      {
        "name": "Fano Ramparany"
      }
    ], 
    "keywords": "distributed constraint optimization, message-passing algorithms, ambient intelligence, smart environment", 
    "title": "Using Message-passing DCOP Algorithms to Solve Energy-efficient Smart Environment Configuration Problems", 
    "type": "paper"
  }, 
  "2220": {
    "abstract": "In multiagent systems, often agents need to be assigned to different roles.  Multiple aspects should be taken into account for this, such as agents' skills and constraints posed by existing assignments.  In this paper, we focus on another aspect: when the agents are self-interested, careful role assignment is necessary to make cooperative behavior an equilibrium of the repeated game.  We formalize this problem and provide an easy-to-check necessary and sufficient condition for a given role assignment to induce cooperation.  However, we show that finding whether such a role assignment exists is in general NP-hard.  Nevertheless, we give two algorithms for solving the problem.  The first is based on a mixed-integer linear program formulation.  The second is based on a dynamic program, and runs in pseudopolynomial time if the number of agents is constant.  Minor modifications of these algorithms also allow for determination of the minimal subsidy necessary to induce cooperation.  In our experiments, the IP performs much, much faster.", 
    "authors": [
      {
        "name": "Catherine Moon"
      }, 
      {
        "name": "Vincent Conitzer"
      }
    ], 
    "keywords": "game theory, repeated games, role assignment", 
    "title": "Role Assignment for Game-Theoretic Cooperation", 
    "type": "paper"
  }, 
  "2228": {
    "abstract": "In this paper, we present a method using AI techniques to solve a case of pure mathematics applications for finding narrow admissible tuples. The original problem is formulated into a combinatorial optimization problem. In particular, we show how to exploit the local search structure to formulate the problem landscape for dramatic reductions in search space and for non-trivial elimination in search barriers, and then to realize intelligent search strategies for effectively escaping from local minima. Experimental results demonstrate that the proposed method is able to efficiently find best known solutions. This research sheds light on exploiting the local problem structure for an efficient search in combinatorial landscapes as an application of AI to a new problem domain.", 
    "authors": [
      {
        "name": "Xiao-Feng Xie"
      }, 
      {
        "name": "Zun-Jing Wang"
      }
    ], 
    "keywords": "Combinatorial optimization, Heuristic search, optimization, applications", 
    "title": "Exploiting Problem Structure in Combinatorial Landscapes: A Case Study on Pure Mathematics Application", 
    "type": "paper"
  }, 
  "223": {
    "abstract": "Deep artificial neural networks have made remarkable progress in different tasks in the field of computer vision. However, the empirical analysis of these models and investigation of their failure cases has received attention recently. In this work, we show that deep learning models cannot generalize to atypical images that are substantially different from training images. This is in contrast to the superior generalization ability of the visual system in the human brain. We focus on Convolutional Neural Networks (CNN) as the state-of-the-art models in object recognition and classification; investigate this problem in more detail, and hypothesize that training CNN models suffer from unstructured loss minimization. We propose computational models to improve the generalization capacity of CNNs by considering how typical a training image looks like. By conducting an extensive set of experiments we show that involving a typicality measure can improve the classification results on a new set of images by a large margin. More importantly, this significant improvement is achieved without fine-tuning the CNN model on the target image set.", 
    "authors": [
      {
        "name": "Babak Saleh"
      }, 
      {
        "name": "Ahmed Elgammal"
      }, 
      {
        "name": "Jacob Feldman"
      }
    ], 
    "keywords": "Computer Vision, Machine Learning, Human Perception and Cognition, Convolutional Neural Networks, Deep Learning, Object Categorization, Abnormality detection, Typicality estimation", 
    "title": "The Role of Typicality in Object Classification: Improving The Generalization Capacity of Convolutional Neural Networks", 
    "type": "paper"
  }, 
  "2232": {
    "abstract": "Clark's completion plays an important role in ASP computation: it discards unsupported models via unit resolution; hence, it improves the performance of ASP solvers, and at the same time it simplifies their implementation. In the disjunctive case, however, Clark's completion is usually preceded by another transformation known as shift, whose size is quadratic in general. A different approach is proposed in this paper: Clark's completion is extended to disjunctive programs without the need of intermediate program rewritings such as the shift. As in the non-disjunctive case, the new completion is linear in size, and discards unsupported models via unit resolution. Moreover, an ad-hoc propagator for supported model search is presented.", 
    "authors": [
      {
        "name": "Mario Alviano"
      }, 
      {
        "name": "Carmine Dodaro"
      }
    ], 
    "keywords": "answer set programming, Clark's completion, supported models", 
    "title": "Completion of Disjunctive Logic Programs", 
    "type": "paper"
  }, 
  "2233": {
    "abstract": "Practical reasoning is reasoning about and toward actions with respect to the agent\u00e2\u0080\u0099s goals. In addition, in a normative environment an agent\u00e2\u0080\u0099s actions are not only directed by the agent\u00e2\u0080\u0099s goals but also by the norms imposed on the agent. However, the potential conflicts within and between the agent\u00e2\u0080\u0099s goals and norms makes decision-making in these frameworks a challenging task. The questions we are addressing in this paper are: (i) how should an agent act in a normative environment while it has conflicting goals and norms?, (ii) how can the agent point out why it acted in a certain way? We propose a solution in which a normative planning problem serves as the basis for a practical reasoning approach based on argumentation. Best plans are identified based on preference information over goals and norms or quantity of goals and norms in absence of preference information. The properties of the best plan with respect to goal achievement and norm compliance are mapped to arguments that can be used to explain why a plan is justified, using an existing proof dialogue game.", 
    "authors": [
      {
        "name": "Zohreh Shams"
      }, 
      {
        "name": "Marina De Vos"
      }, 
      {
        "name": "Nir Oren"
      }, 
      {
        "name": "Julian Padget"
      }
    ], 
    "keywords": "Argumentation, Practical reasoning, Norms", 
    "title": "Normative Practical Reasoning via Argumentation and Dialogue", 
    "type": "paper"
  }, 
  "2243": {
    "abstract": "In this paper we propose an approach to preference elicitation that is suitable to large configuration spaces beyond the reach of existing state-of-the-art approaches. Our setwise max-margin method can be viewed as a generalization of max-margin learning to sets, and can produce a set of \u00e2\u0080\u009cdiverse\u00e2\u0080\u009d items that can be used to ask informative queries to the user. Moreover, the approach can encourage sparsity in the parameter space, in order to favor the assessment of utility towards combinations of weights that concentrate on just few features. We present a mixed integer linear programming formulation and show how our approach compares favourably with Bayesian preference elicitation alternatives and easily scales to realistic datasets.", 
    "authors": [
      {
        "name": "Stefano Teso"
      }, 
      {
        "name": "Andrea Passerini"
      }, 
      {
        "name": "Paolo Viappiani"
      }
    ], 
    "keywords": "preference elicitation, maximum margin, learning with constraints", 
    "title": "Constructive Preference Elicitation by Setwise Max-margin Learning", 
    "type": "paper"
  }, 
  "2260": {
    "abstract": "We compare sorting and hashing for implicit graph search using disk storage.  We first describe efficient pipelined implementations of both algorithms, which reduce disk I/O.  We then compare the two algorithms and find that hashing is faster, but that sorting requires less disk storage.  We also compare disk-based with in-memory search, and find that there is little or no time overhead associated with disk-based search.  We present experimental results on the sliding-tile puzzles, Rubik's Cube, and the 4-peg Towers of Hanoi.", 
    "authors": [
      {
        "name": "Richard Korf"
      }
    ], 
    "keywords": "search algorithms, external memory, combinatorial search", 
    "title": "Comparing Search Algorithms using Sorting and Hashing on Disk and in Memory", 
    "type": "paper"
  }, 
  "2263": {
    "abstract": "A classifier able to minimize the generalization error of a particular problem for any set of unseen samples is named Bayes-optimal classifier. The hypothesis induced by such classifier is equivalent to the optimal Bayes point, which is approximately equivalent to the center of mass of the version space. However, there are only a few methods for estimating the center of mass and most of them are computationally expensive or impractical, especially for large datasets. In this paper we present the Version Space Reduction Machine (VSRM), a new method that obtains an approximation of the center of mass. The method works by means of successive reductions of the version space which are consistent with an oracle's decision. This oracle is represented by the majority voting of an ensemble, whose components must contain a reasonable diversity level to ensure an effective approximation. We conduct an experimental study on microarray datasets and assess the performance of the proposed method compared to Support Vector Machine and Bayes Point Machine. Our method consistently outperforms the others. Such result indicates that the proposed method provides a better approximation of the center of mass.", 
    "authors": [
      {
        "name": "Karen Braga Enes"
      }, 
      {
        "name": "Saulo Moraes Villela"
      }, 
      {
        "name": "Raul Fonseca Neto"
      }
    ], 
    "keywords": "Version space, Optimal Bayes point, Binary classification, Perceptron", 
    "title": "Version space reduction based on ensembles of dissimilar balanced Perceptrons", 
    "type": "paper"
  }, 
  "2264": {
    "abstract": "In order to be proactive, robots should be capable of generating and selecting their own goals, and pursuing activities towards their achievement. Goal reasoning has focused on the former set of cognitive abilities, and automated planning on the latter. Despite the existence of robots that possess both capabilities, we lack a general understanding of how these problems are related. This paper introduces the notion of equilibrium maintenance as a contribution to this understanding. We provide formal evidence that equilibrium maintenance is conducive to proactive robots, and demonstrate our approach in a closed loop with a real robot in a smart home.", 
    "authors": [
      {
        "name": "Jasmin Grosinger"
      }, 
      {
        "name": "Federico Pecora"
      }, 
      {
        "name": "Alessandro Saffiotti"
      }
    ], 
    "keywords": "Goal autonomy, Planning, Deliberation, Proactivity, Robotics", 
    "title": "Making Robots Proactive through Equilibrium Maintenance", 
    "type": "paper"
  }, 
  "2269": {
    "abstract": "Goal Recognition Design(GRD) problems involve identifying the best ways to modify the underlying environment that the agents operate in, typically by making a subset of feasible actions infeasible, in such a way that agents are forced to reveal their goals as early as possible. Thus far, existing work assumes that the outcomes of the actions of the agents are deterministic, which might be unrealistic in real-world problems. For example, wheel slippage in robots cause the outcomes of their movements to be stochastic. In this paper, we generalize the GRD problem to Stochastic GRD(S-GRD) problems, which handle stochastic action outcomes. We also generalize the worst-case distinctiveness(wcd) measure, which measures the goodness of a solution, to take stochasticity into account. Finally, we introduce Markov decision process(MDP) based algorithms to compute the wcd and minimize it by making up to k actions infeasible.", 
    "authors": [
      {
        "name": "Christabel Wayllace"
      }, 
      {
        "name": "Ping Hou"
      }, 
      {
        "name": "William Yeoh"
      }, 
      {
        "name": "Tran Cao Son"
      }
    ], 
    "keywords": "Goal Recognition Design, Plan Recognition, Markov Decision Process", 
    "title": "Goal Recognition Design with Stochastic Agent Action Outcomes", 
    "type": "paper"
  }, 
  "2272": {
    "abstract": "We study convergence properties of opinion dynamics with local interactions and limited information exchange. We adopt a general model where the agents update their opinions in rounds by taking a weighted average of the opinions in their (possibly opinion-dependent) neighborhoods. For fixed neighborhoods, we present a simple randomized protocol that converges in expectation to the stable state of the Friedkin-Johnsen model and discuss its convergence time. For opinion-dependent neighborhoods, we show that the Hegselmann-Krause model converges to a stable state if each agent's neighborhood is restricted either to a subset of her acquaintances or to a small random subset of agents. Our experimental findings indicate that for a wide range of parameters, the convergence time and the number of opinion clusters of the neighborhood-restricted variant are comparable to those of the standard Hegselmann-Krause model.", 
    "authors": [
      {
        "name": "Dimitris Fotakis"
      }, 
      {
        "name": "Dimitris Palyvos-Giannas"
      }, 
      {
        "name": "Stratis Skoulakis"
      }
    ], 
    "keywords": "Opinion dynamics, Local agent interaction, Dynamics in social networks, Hegselmann-Krause model, Friedkin-Johnsen model", 
    "title": "Opinion Dynamics with Local Interactions", 
    "type": "paper"
  }, 
  "2274": {
    "abstract": "Understanding the relation between different semantics in abstract argumentation is a central topic in AI, not least since such semantics capture the basic ingredients of different approaches to nonmonotonic reasoning. The question we are interested in relates two semantics as follows: What are the necessary and sufficient conditions, such that we can decide, for any two sets of extensions, whether there exists an argumentation framework which has exactly the first extension set under one semantics, and the other extension set under the other semantics. We investigate in total nine argumentation semantics and give a nearly complete landscape of exact characterizations. As we shall argue, such results not only give an account on the independency between semantics, but might also prove useful in argumentation systems by providing guidelines for how to prune the search space.", 
    "authors": [
      {
        "name": "Paul Dunne"
      }, 
      {
        "name": "Thomas Linsbichler"
      }, 
      {
        "name": "Christof Spanring"
      }, 
      {
        "name": "Stefan Woltran"
      }
    ], 
    "keywords": "abstract argumentation, realizability, argumentation framework, argumentation semantics", 
    "title": "Investigating the Relationship between Argumentation Semantics via Signatures", 
    "type": "paper"
  }, 
  "2277": {
    "abstract": "This paper contributes a novel framework that enables a robot agent to efficiently learn and synthesize believable handwriting motion. We situate the framework as a foundation with the goal of allowing children to observe, correct and engage with the robot agent to learn themselves the handwriting skill. The framework adapts the principle behind ensemble methods - where improved performance is obtained by combining the output of multiple simple algorithms - in an inverse optimal control problem. This integration addresses the challenges of rapid extraction and representation of multiple-mode motion trajectories, with the cost forms that are transferable and interpretable in the development of the robot compliance control. It also introduces the incorporation of a human movement inspired feature, which provides intuitive motion modulation to generalize the synthesis with poor robotic written samples for children to identify and correct. We present the results on the success of synthesizing a variety of natural-looking motion samples based upon the learned cost functions. The framework is validated by a user study, where the synthesized dynamical motion is shown to be hard to distinguish from the real human handwriting.", 
    "authors": [
      {
        "name": "Hang Yin"
      }, 
      {
        "name": "Patricia Alves-Olivera"
      }, 
      {
        "name": "Francesco S. Melo"
      }, 
      {
        "name": "Aude Billard"
      }, 
      {
        "name": "Ana Paiva"
      }
    ], 
    "keywords": "Learning from demonstrations, Human-robot interaction, Ensemble methods", 
    "title": "Synthesizing Robotic Handwriting Motion by Learning from Human Demonstrations", 
    "type": "paper"
  }, 
  "2280": {
    "abstract": "Discrimination discovery is to unveil discrimination against a specific individual by analyzing the historical dataset. In this paper, we develop a general technique to capture discrimination  based on the legally grounded situation testing methodology. For any individual, we find pairs of tuples from the dataset with similar characteristics apart from belonging or not to the protected-by-law group and assign them in two groups. The individual is considered as discriminated if significant difference is observed between the decisions from the two groups. To find similar tuples, we make use of the Causal Bayesian Networks and the associated causal inference methods as a guideline. The causal structure of the dataset and the causal effect of each attribute on the decision are used to facilitate the similarity measurement. Through empirical assessments on a real dataset, our approach shows good efficacy both in accuracy and efficiency.", 
    "authors": [
      {
        "name": "Lu Zhang"
      }, 
      {
        "name": "Yongkai Wu"
      }, 
      {
        "name": "Xintao Wu"
      }
    ], 
    "keywords": "discrimination discovery, Causal Bayesian Network, causal inference, situation testing", 
    "title": "Situation Testing-Based Discrimination Discovery: An Causal Inference Approach", 
    "type": "paper"
  }, 
  "2282": {
    "abstract": "Existing open-domain human-computer conversation systems are typically passive: they either synthesize or retrieve a reply provided a human-issued utterance. It is generally presumed that humans should take the role to lead the conversation and introduce new content when a stalemate occurs, and that the computer only needs to ``respond.'' In this paper, we propose StalemateBreaker, a conversation system that can proactively introduce new content when appropriate. We design a pipeline to determine when, what, and how to introduce new content during human-computer conversation. We further propose a novel reranking algorithm \\name\\ to enable rich interaction between conversation context and candidate replies. Experimental results show that both the content introducing approach and the reranking algorithm are effective. Our full StalemateBreaker model outperforms a state-of-the-practice conversation system by +14.4% p@1 when a stalemate occurs.", 
    "authors": [
      {
        "name": "Xiang Li"
      }, 
      {
        "name": "Rui Yan"
      }, 
      {
        "name": "Lili Mou"
      }, 
      {
        "name": "Ming Zhang"
      }
    ], 
    "keywords": "Proactive content introducing, Open-domain human-computer conversation, Ranking models", 
    "title": "StalemateBreaker: A Proactive Content Introducing Approach for Automatic Human-Computer Conversation", 
    "type": "paper"
  }, 
  "2283": {
    "abstract": "Satisfiability-checking of formulas in the theory of linear rational arithmetic (LRA) has broad applications in program verification and synthesis.  Satisfiability Modulo Theories (SMT) solvers are effective at checking satisfiability of the ground fragment of LRA, but applying them to quantified formulas requires a costly quantifier elimination step.  This article presents a novel decision procedure for LRA which leverages SMT solvers for the ground fragment of LRA, but which avoids explicit quantifier elimination.  The intuition behind the algorithm stems from an interpretation of a quantified formula as a game between two players, whose goals are to prove that the formula is either satisfiable or not.  The algorithm synthesizes a winning strategy for one of the players by iteratively improving candidate strategies for both.  Experimental results demonstrate that our procedure out-performs existing solvers.", 
    "authors": [
      {
        "name": "Azadeh Farzan"
      }, 
      {
        "name": "Zachary Kincaid"
      }
    ], 
    "keywords": "Satisfiability Modulo Theories, Theorem Proving, Quantifiers", 
    "title": "Linear Arithmetic Satisfiability via Strategy Improvement", 
    "type": "paper"
  }, 
  "2290": {
    "abstract": "Current electricity tariffs for retail rarely provide incentives for intelligent demand-response of flexible customers. Such customers could otherwise contribute to balancing supply and demand in future smart grids. This paper proposes an innovative risk-sharing tariff to incentivize intelligent customer behavior. A two-step parameterized payment scheme is proposed, consisting of a pre-payment based on the expected consumption, and a supplementary payment for any observed deviation from the anticipated consumption. Within a game-theoretical analysis, we capture the strategic conflict of interest between a retailer and a customer in a two-player game, and we present optimal, i.e., best response, strategies for both players in this risk-sharing tariff game. We show analytically that the proposed tariff provides customers of varying flexibility with variable incentives to assume and alleviate a fraction of the balancing risk, contributing in this way to the uncertainty reduction in the envisioned smart-grid.", 
    "authors": [
      {
        "name": "Georgios Methenitis"
      }, 
      {
        "name": "Michael Kaisers"
      }, 
      {
        "name": "Han La Poutr\u00e9"
      }
    ], 
    "keywords": "risk-sharing, tariff, game, demand-response, balancing, future electricity systems, smart-grids", 
    "title": "Incentivizing Intelligent Customer Behavior in Smart-Grids: A Risk-Sharing Tariff & Optimal Strategies", 
    "type": "paper"
  }, 
  "2291": {
    "abstract": "The aggregates have greatly extended the representation power, in both theory and practice, of Answer Set Programming. Significant understanding of programs with aggregates has been gained in the last decade.  However, there is still a substantial difficulty in understanding the semantics due to the nonmonotonic behaviors of aggregates, which is demonstrated by several distinct semantics for aggregates in the existing work. In this work, we aim to understand these distinct semantics in a more uniform way. Particularly, we employed a self contained and intuitive method, based on the basic concept of satisfiability of rules, to define the semantics of logic programs with aggregates. Under this method, the relation between distinct semantics becomes simple, clear and explicit.", 
    "authors": [
      {
        "name": "Yuanlin Zhang"
      }, 
      {
        "name": "Maede Rayatidamavandi"
      }
    ], 
    "keywords": "Answer Set Programming, Aggregates, Semantics, Logic Programming", 
    "title": "A Characterization of the Semantics of Logic Programs with Aggregates", 
    "type": "paper"
  }, 
  "2295": {
    "abstract": "Single-agent planning in a multi-agent environment is challenging because the actions of other agents can affect our ability to achieve a goal. From a given agent's perspective, actions of others can be viewed as non-deterministic outcomes of that agent's actions. While simple conceptually, this interpretation of planning in a multi-agent environment as non-deterministic planning remains challenging, not only due to the non-determinism resulting from others' actions, but because it is not clear how to compactly model the possible actions of others in the environment. In this paper, we cast the problem of planning in a multi-agent environment as one of Fully-Observable Non-Deterministic (FOND) planning. We extend a non-deterministic planner to plan in a multi-agent setting, allowing non-deterministic planning technology to solve a new class of planning problems. To improve the efficiency in domains too large for solving optimally, we propose a technique to use the goals and possible actions of other agents to focus the search on a set of plausible actions. We evaluate our approach on existing and new multi-agent benchmarks, demonstrating that modelling the other agents' goals improves the quality of the resulting solutions.", 
    "authors": [
      {
        "name": "Christian Muise"
      }, 
      {
        "name": "Paolo Felli"
      }, 
      {
        "name": "Tim Miller"
      }, 
      {
        "name": "Adrian Pearce"
      }, 
      {
        "name": "Liz Sonenberg"
      }
    ], 
    "keywords": "multi-agent planning, fond, non-determinism", 
    "title": "Planning for a Single Agent in a Multi-Agent Environment Using FOND", 
    "type": "paper"
  }, 
  "2296": {
    "abstract": "We consider how SQL-like query language over object-relational schema can be preserved in the setting of ontology based data access (OBDA), thus leveraging wide familiarity with relational technology. This is enabled by the adoption of the logic CFDI_nc, a member of the CFD family of description logics (DLs). Of particular note is that this logic properly contains DL-Lite_core^F, a member of the DL-Lite family commonly used in the OBDA setting. Our main results present efficient algorithms that allow computation of certain answers with respect to CFDI_nc knowledge bases. Notably, the algorithms facilitate direct access to a pre-existing row-based relational encoding of the data without any need for mappings to triple-based intermediate representations such as RDF or the typical ABox perspective of data in DLs. Finally, we report on an experimental evaluation that provides strong evidence of the practicality of CFDI_nc-based OBDA.", 
    "authors": [
      {
        "name": "Jason St. Jacques"
      }, 
      {
        "name": "David Toman"
      }, 
      {
        "name": "Grant Weddell"
      }
    ], 
    "keywords": "tractable description logics, ontology based data access, relational data sources", 
    "title": "Object-Relational Queries over CFDI_nc Knowledge Bases: OBDA for the SQL-Literate", 
    "type": "paper"
  }, 
  "2303": {
    "abstract": "We design an extension DHS of datalog with hyperrectangle generalisations of Halpen-Shoham's modal operators on intervals and a corresponding query language. We prove that, over n-dimensional  spaces comprised of integers (Z) and reals (R), finding certain answers to DHS queries can be reduced to standard datalog query answering. We present experimental results showing expressivity and efficiency of DHS on historical data.", 
    "authors": [
      {
        "name": "Roman Kontchakov"
      }, 
      {
        "name": "Laura Pandolfo"
      }, 
      {
        "name": "Luca Pulina"
      }, 
      {
        "name": "Vladislav Ryzhikov"
      }, 
      {
        "name": "Michael Zakharyaschev"
      }
    ], 
    "keywords": "temporal reasoning, spatial reasoning, ontology-based data access", 
    "title": "Temporal and Spatial OBDA with Many-Dimensional Halpern-Shoham Logic", 
    "type": "paper"
  }, 
  "2306": {
    "abstract": "Many combinatorial problems are solved with a Depth-first search guided by a heuristic and it is well-known that heuristic mistakes at high levels of the search tree may be fatal. A standard method to minimize this effect is to search by increasing number of discrepancies. This approach has been found useful in a number of domains where the search structure is a depth-bounded OR tree.  In this paper we investigate this approach in the AND/OR search trees context. We introduce an AND/OR generalization of the Limited Discrepancy Search algorithm LDS. We show that, for a fixed number of discrepancies, the leafs visited by the  AND/OR algorithm contain all the leafs visited by the standard OR LDS, while many more leafs can visited due to the multiplicative effect of the AND/OR decomposition.  We demonstrate the relevance of our algorithm in the Graphical Models Context, where we experiment with LDS as an any-time upper-bound provider of minimization tasks. We show that the AND/OR version of the algorithm often outperforms its OR counterpart.", 
    "authors": [
      {
        "name": "Javier Larrosa"
      }, 
      {
        "name": "Emma Rollon"
      }, 
      {
        "name": "Rina Dechter"
      }
    ], 
    "keywords": "Heuristic Search, Limited Discrepancy Search, AND/OR Search, MPE/MAP in Graphical Models, Weighted CSPs", 
    "title": "Limited Discrepancy AND/OR Search", 
    "type": "paper"
  }, 
  "2308": {
    "abstract": "For many applications, the observed data may be incomplete and there often exist variables that are unobserved  but play an important role in capturing the underlying relationships. In this work, we propose a method to identify local latent variables and to determine their structural relations with the observed variables.  We formulate the local latent variable discovery as discovering the Markov Blanket (MB) of a target variable. To efficiently search the latent variable space, we exploit MB topology to divide the latent space into different subspaces. Within each subspace, we employ a constrained structure expectation-maximization algorithm to greedily learn the MB with latent variables. We theoretically prove the advantages of the MB with latent variables learned using our method over the MB without latent variables.  We evaluate the performance of our method on synthetic data to demonstrate its effectiveness in identifying the correct latent variables.  We further apply our algorithm to feature discovery and selection problem, and show that the latent variables learned through the proposed method can improve the classification accuracy in benchmark feature selection and discovery datasets.", 
    "authors": [
      {
        "name": "Tian Gao"
      }, 
      {
        "name": "Qiang Ji"
      }
    ], 
    "keywords": "Structure Learning, Bayesian Network, Latent Variable, Markov Blanket, EM algorithm", 
    "title": "Constrained Local Latent Variable Discovery", 
    "type": "paper"
  }, 
  "2312": {
    "abstract": "Handling temporal uncertainty is an important requirement for planning in real-world applications. Advances have been made for checking the dynamic controllability of plans. However these approaches assume the contingent timepoints in a plan to be observable.  In this paper, we relax this assumption: we consider Partially Observable Simple Temporal Networks with Uncertainty (POSTNUs), which contain hidden as well as visible contingent points. We propose a procedure for testing the dynamic controllability of POSTNUs.  Further, we define an algorithm for choosing, among the observable contingent points, what to observe by adding sensing actions in a plan to make the plan dynamically controllable. We show how these procedures can be incrementally integrated into a constraint-based hierarchical temporal planner and report on empirical results of our implementation.", 
    "authors": [
      {
        "name": "Arthur Bit-Monnot"
      }, 
      {
        "name": "Malik Ghallab"
      }, 
      {
        "name": "Felix Ingrand"
      }
    ], 
    "keywords": "STNU, Partial Observability, Dynamic Controllability, Sensing actions, temporal planning", 
    "title": "Which Contingent Events to Observe for the Dynamic Controllability of a Plan", 
    "type": "paper"
  }, 
  "2315": {
    "abstract": "M-Modes for graphical models is the problem of finding top M label configurations of highest probability in their local neighborhoods. The state-of-the-art method for solving M-Modes is a dynamic programming algorithm which computes global modes by first computing local modes of each subgraph and then search through all their consistent combinations. A drawback of the algorithm is that most of its time is wasted on computing local modes that are never used in the global modes. In this work, we introduce heuristic search algorithms that directly search the space of consistent local modes in finding the global modes. Such search is enabled by a novel search operator designed to search a subgraph of variables at each time. As a result, the search algorithms only need to generate and verify a small number of local modes and can hence lead to significant improvement in efficiency and scalability.", 
    "authors": [
      {
        "name": "Cong Chen"
      }, 
      {
        "name": "Changhe Yuan"
      }, 
      {
        "name": "Chao Chen"
      }
    ], 
    "keywords": "Graphical models, M-Modes, Heuristic search", 
    "title": "Solving M-Modes Using Heuristic Search", 
    "type": "paper"
  }, 
  "2318": {
    "abstract": "Voting systems in which voters are partitioned to districts encourage accountability by providing voters an easily identifiable district representative, but can result in a disproportionate allocation of representatives. In some extreme cases, a party may have a majority of the popular vote, but lose the elections due to districting effects.   We define a ratio called the Misrepresentation Ratio which quantifies the deviation from proportional representation in a district-based election, and provide bounds for this ratio under various voting rules.  We also examine probabilistic models for election outcomes, and provide an algorithm for approximating the expected Misrepresentation Ratio under a given probabilistic election model.   Finally, we provide simulation results for several such probabilistic election models, showing the effects of the number of voters and candidates on the misrepresentation ratio.", 
    "authors": [
      {
        "name": "Yoad Lewenberg"
      }, 
      {
        "name": "Yoram Bachrach"
      }, 
      {
        "name": "Yair Zick"
      }, 
      {
        "name": "Omer Lev"
      }
    ], 
    "keywords": "Social Choice, District Voting Systems, Scoring Rules, Plurality Voting, Copeland, Borda, Gerrymandering", 
    "title": "Misrepresentation in District Voting", 
    "type": "paper"
  }, 
  "2320": {
    "abstract": "We investigate a suite of recommendation algorithms for audio news listening applications. This domain presents several challenges that distinguish it from more commonly studied applications such as movie recommendations: (1) we do not receive explicit rating feedback, instead only observing when a user skips a story; (2) new stories arrive continuously, increasing the importance of making recommendations for items with few observations (the cold start problem); (3) story attributes have high dimensionality, making it challenging to identify similar stories. To address the first challenge, we formulate the problem as predicting the percentage of a story a user will listen to; to address the remaining challenges, we propose several matrix factorization algorithms that cluster users, n-grams, and stories simultaneously, while optimizing prediction accuracy. We empirically evaluate our approach on a dataset of 50K users, 26K stories, and 975K interactions collected over a five month period. We find that while simple models work well for stories with many observations, our proposed approach performs best for stories with few ratings, which is critical for the real-world deployment of such an application.", 
    "authors": [
      {
        "name": "Ehsan Mohammady Ardehaly"
      }, 
      {
        "name": "Aron Culotta"
      }, 
      {
        "name": "Vivek Sundararaman"
      }, 
      {
        "name": "Alwar Narayanan"
      }
    ], 
    "keywords": "Recommendation systems, cold-start, matrix factorization, Personalized news recommendation", 
    "title": "Cold-start recommendations for audio news stories using matrix factorization", 
    "type": "paper"
  }, 
  "2331": {
    "abstract": "Learning from Label Proportions (LLP) is a machine learning problem in which the training data consist of bags of instances, and only the class label distribution for each bag is known. In some domains label proportions are readily available; for example, by grouping social media users by location, one can use census statistics to build a classifier for user demographics. However, label proportions are unavailable in many domains, such as product review sites. The goal of this paper is to determine whether an LLP classifier fit in one domain can be modified to classify instances from another domain. To do so, we propose a domain adaptation algorithm that uses an LLP model fit on the source domain to generate label proportions for the target domain. A new LLP model is then fit on the target domain, and this self-training process is repeated to adapt the model from source to target. Our experiments on five diverse tasks indicate an 11% average absolute improvement in accuracy as compared to using LLP without domain adaptation. In contrast to existing domain adaptation algorithms, our approach requires only label proportions in the source domain, and the results suggest that the approach is effective even when the target domain is substantially different from the source domain.", 
    "authors": [
      {
        "name": "Ehsan Mohammady Ardehaly"
      }, 
      {
        "name": "Aron Culotta"
      }
    ], 
    "keywords": "Domain adaptation, Learning from Label Proportions, Transfer learning, Social media analysis, Text classification, Latent Dirichlet Allocation, Lightly supervised learning, Label regularization", 
    "title": "Domain adaptation for learning from label proportions using self-training", 
    "type": "paper"
  }, 
  "2338": {
    "abstract": "Trembling hand (TH) equilibria were indtroduced by Selten [19]. Intuitively, these are Nash equilibria that remain stable when players assume that other players will choose off-equilibrium strategies with small probability. This concept is useful for equilibrium refinement, i.e., selecting the most plausible Nash equilibria when the set of all Nash equilibria can be very large, as is the case, for instance, for Plurality voting with strategic voters. In this paper, we analyze TH equilibria of Plurality voting from an algorithmic perspective. We provide an efficient algorithm for computing a TH best response and establish many useful properties of TH equilibria in this setting. On the negative side, we provide an example of a Plurality voting game with no TH equlibria, and show that checking whether a given preference profile admits a TH equlibrium with a specific winner under Plurality voting is NP-hard.", 
    "authors": [
      {
        "name": "Svetlana Obraztsova"
      }, 
      {
        "name": "Zinovi Rabinovich"
      }, 
      {
        "name": "Edith Elkind"
      }, 
      {
        "name": "Maria Polukarov"
      }, 
      {
        "name": "Nick Jennings"
      }
    ], 
    "keywords": "Social choice, Plurality, Voting games, Trembling hand equilibrium", 
    "title": "Trembling Hand Equilibria of Plurality Voting", 
    "type": "paper"
  }, 
  "2341": {
    "abstract": "Several methods exist for making collective decisions on a set of attributes when voters possibly have preferential dependencies. None is based on approval voting.  We define  a family of rules for approval-based elections on multi-attribute (or combinatorial), where standard approval ballots are generalized into 'conditional approval ballots', allowing a voter to approve values for an attribute conditionally on the values of other attributes. We propose three rules. The first two generalize the so-called 'minisum' and 'minimax' rules for committee elections. The third one is an approval-based version of sequential voting on combinatorial domains. We study some properties of these rules,  and discuss their conditions of applicability.", 
    "authors": [
      {
        "name": "Nathana\u00ebl Barrot"
      }, 
      {
        "name": "J\u00e9r\u00f4me Lang"
      }
    ], 
    "keywords": "computational social choice, approval voting, combinatorial domains", 
    "title": "Conditional and Sequential Approval Voting on Combinatorial Domains", 
    "type": "paper"
  }, 
  "2347": {
    "abstract": "A compact representation for non-transferable utility games founding on answer set programming is proposed. The representation is fully expressive, in that it can capture all games defined over a finite set of alternatives. Moreover, due to the knowledge representation capabilities of answer sets programs, it can easily accommodate the definition of games within a wide range of application domains, ranging from scheduling, to routing and planning, just to name a few. The computational complexity of the proposed framework is studied, in particular, by focusing on the core as the prototypical solution concept. A system supporting the basic reasoning tasks arising therein is also made available, and results of experimental activity are discussed.", 
    "authors": [
      {
        "name": "Giovanni Amendola"
      }, 
      {
        "name": "Gianluigi Greco"
      }, 
      {
        "name": "Nicola Leone"
      }, 
      {
        "name": "Pierfrancesco Veltri"
      }
    ], 
    "keywords": "Coalitional Games, Compact Representations, Answer Set Programming, Computational Complexity", 
    "title": "Modeling and Reasoning about NTU Games via Answer Set Programming", 
    "type": "paper"
  }, 
  "235": {
    "abstract": "The budget allocation problem is an optimization problem arising from advertising planning. In the problem, an advertiser has limited budgets to allocate across media, and seeks to optimize the allocation such that the largest fraction of customers can be influenced. It is known that this problem admits a (1-1/e)-approximation algorithm. However, no previous studies on this problem considered adjusting the allocation adaptively based upon the effect of the past campaigns, which is a usual strategy in the real setting. Our main contribution in this paper is to analyze adaptive strategies for the budget allocation problem. We define a greedy strategy, referred to as the insensitive policy, and then give a provable performance guarantee. This result is obtained by extending the adaptive submodularity, which is a concept studied in the context of active learning and stochastic optimization, to the functions over an integer lattice.", 
    "authors": [
      {
        "name": "Daisuke Hatano"
      }, 
      {
        "name": "Takuro Fukunaga"
      }, 
      {
        "name": "Ken-Ichi Kawarabayashi"
      }
    ], 
    "keywords": "adaptive submodular, computational advertising, adaptive optimization", 
    "title": "Adaptive budget allocation for maximizing influence of advertisements", 
    "type": "paper"
  }, 
  "2350": {
    "abstract": "Anomaly detection plays an important role in modern data-driven security applications, such as detecting suspicious access to a socket from a process. In many cases, such events can be described as a collection of categorical values that are considered as entities of different types, which we call heterogeneous categorical events. Due to the lack of intrinsic distance measures among entities, and the exponentially large event space, most existing work relies heavily on heuristics to calculate abnormal scores for events. Different from previous work, we propose a principled and unified probabilistic model APE (\\underline{A}nomaly detection via \\underline{P}robabilistic pairwise interaction and \\underline{E}ntity embedding) that directly models the likelihood of events. In this model, we embed entities into a common latent space using their observed co-occurrence in different events. More specifically, we first model the compatibility of each pair of entities according to their embeddings. Then we utilize the weighted pairwise interactions of different entity types to define the event probability. Using Noise-Contrastive Estimation with ``context-dependent\" noise distribution, our model can be learned efficiently regardless of the large event space. Experimental results on real enterprise surveillance data show that our methods can accurately detect abnormal events compared to other state-of-the-art abnormal detection techniques.", 
    "authors": [
      {
        "name": "Ting Chen"
      }, 
      {
        "name": "Lu-An Tang"
      }, 
      {
        "name": "Yizhou Sun"
      }, 
      {
        "name": "Zhengzhang Chen"
      }, 
      {
        "name": "Kai Zhang"
      }
    ], 
    "keywords": "Entity Embedding, Anomaly Detection, Probabilistic Model, Unsupervised Learning", 
    "title": "Entity Embedding-based Anomaly Detection for Heterogeneous Categorical Events", 
    "type": "paper"
  }, 
  "236": {
    "abstract": "This paper explores techniques for fast solving the maximum weight clique problem (MWCP) in very large scale real-world graphs. Because of the size of such graphs and the intractability of MWCP, previously developed algorithms may not be applicable. Although recent heuristic algorithms make progress in solving MWCP in massive graphs, they still need considerable time to get a good solution. However, in many applications we are required to provide a solution as good as possible in much shorter time. In this work, we propose a new method for MWCP which interleaves between clique construction and graph reduction. We also propose three novel ideas to make it efficient, and develop an algorithm called FastWClq. Experiments on massive graphs from various applications show that, FastWClq finds better solutions than state of the art algorithms while the run time is much less. Further, FastWClq proves the optimal solution for about half of the graphs in an averaged time less than one second.", 
    "authors": [
      {
        "name": "Shaowei Cai"
      }, 
      {
        "name": "Jinkun Lin"
      }
    ], 
    "keywords": "Maximum Weight Clique, Massive Graphs, Fast Solving", 
    "title": "Fast Solving Maximum Weight Clique Problem in Massive Graphs", 
    "type": "paper"
  }, 
  "2362": {
    "abstract": "Goal Driven Autonomy is an agent model for reasoning about goals while acting in a dynamic environment. Since anomalous events may cause an agent's current goal to become invalid, GDA agents monitor the environment for such anomalies. When domains are both partially observable and dynamic, agents must reason about sensing and planning actions. Previous GDA work evaluated agents in domains that were partially observable, but does not address sensing actions with associated costs. Furthermore, partial observability still enabled generation of a grounded plan to reach the goal. We evaluate agents where observability is more limited: the agent cannot generate a grounded plan because it does not know which future actions will be available until it explores more of the environment. We present a formalism of the problem that includes sensing costs, a GDA algorithm using this formalism, an examination of four methods of expectations under this formalism,  and an implementation of the algorithm and empirical study.", 
    "authors": [
      {
        "name": "Dustin Dannenhauer"
      }, 
      {
        "name": "Hector Munoz-Avila"
      }, 
      {
        "name": "Michael Cox"
      }
    ], 
    "keywords": "goal-driven autonomy, execution monitoring, information gathering", 
    "title": "Informed Expectations to Guide GDA Agents in Partially Observable Environments", 
    "type": "paper"
  }, 
  "2364": {
    "abstract": "Effective interaction relies upon agents being able to deal with conflicting commitments.  We describe Coco, an approach for reasoning about commitments applying to individual parties in light of dominance relations between specific commitments and general types of commitments.  Coco adapts answer-set programming to determine maximal nondominated sets of commitments.  It provides a modeling language and tool geared to support practical applications.", 
    "authors": [
      {
        "name": "Nirav Ajmeri"
      }, 
      {
        "name": "Jiaming Jiang"
      }, 
      {
        "name": "Rada Chirkova"
      }, 
      {
        "name": "Jon Doyle"
      }, 
      {
        "name": "Munindar P. Singh"
      }
    ], 
    "keywords": "Commitments, Normative System, Dominance Reasoning", 
    "title": "Coco: Runtime Reasoning about Conflicting Commitments", 
    "type": "paper"
  }, 
  "2368": {
    "abstract": "Hi(OWL 2 QL) is a new ontology language with the OWL 2 QL syntax, and a higher-order semantics designed to support metamodeling and metaquerying. In this paper we investigate the problem of answering metaqueries in Hi(OWL 2 QL), which are unions of conjunctive queries with both ABox and TBox atoms. We first focus on a specific class of ontologies, called TBox-complete, where there is no uncertainty about TBox axioms, and show that query answering over ontologies of such class has the same complexity (both data and combined) as the problem of answering unions of conjunctive queries in OWL 2 QL. We then move to the case of general ontologies, and show that answering metaqueries is coNP-complete with respect to ontology complexity, \u00ce\u00a02p-complete with respect to combined complexity, and remains AC0 with respect to ABox complexity. We prove that our lower bound result holds for conjunctive metaqueries already, thus sharpening previous lower-bound results showing the intractability in the presence of union. Finally, we present an optimized query answering algorithm that can be used for TBox-complete ontologies, and show that it outperforms previous algorithms based on mere metagrounding.", 
    "authors": [
      {
        "name": "Maurizio Lenzerini"
      }, 
      {
        "name": "Lorenzo Lepore"
      }, 
      {
        "name": "Antonella Poggi"
      }
    ], 
    "keywords": "Metaquerying, OWL 2 QL, Ontologies, Complexity, Higher-order semantics", 
    "title": "Answering metaqueries over Hi(OWL 2 QL) ontologies", 
    "type": "paper"
  }, 
  "2374": {
    "abstract": "The investigation of computational and model-theoretic properties of logical languages constitutes a central field of research in logic-based knowledge representation. Datalog is a very common logical formalism, a de-facto standard and baseline for expressing and querying knowledge. Diverse results exist regarding the expressivity of Datalog and its extension by input negation (semipositive Datalog) and/or a linear order (order-invariant Datalog). When classifying the expressivity of logical formalisms through their model-theoretic properties, a very natural and prominent such property is preservation under homomorphisms. This paper solves the remaining open questions needed to arrive at a complete picture regarding the interrelationships between the class of homomorphism-closed queries and the query classes related to the four versions of Datalog. Most notably, we exhibit a query that is both homomorphism-closed and computable in polynomial time but cannot be expressed in order-invariant Datalog.", 
    "authors": [
      {
        "name": "Sebastian Rudolph"
      }, 
      {
        "name": "Micha\u00ebl Thomazo"
      }
    ], 
    "keywords": "knowledge representation, expressivity, datalog variants, homomorphism closedness", 
    "title": "Expressivity of Datalog Variants -- Completing the Picture", 
    "type": "paper"
  }, 
  "2376": {
    "abstract": "Repair based techniques are a standard way of dealing with inconsistency in the context of ontology based data access. We propose a novel non-objection inference relation (along with its variants) where a query is considered as valid if it follows from at least one repair and it is consistent with all the repairs. The inferences are strictly more productive than universal inference while preserving the consistency of their set of conclusions. We study the productivity, rational properties and complexity of these new inferences.", 
    "authors": [
      {
        "name": "Zied Bouraoui"
      }, 
      {
        "name": "Salem Benferhat"
      }, 
      {
        "name": "Madalina Croitoru"
      }, 
      {
        "name": "Odile Papini"
      }, 
      {
        "name": "Karim Tabia"
      }
    ], 
    "keywords": "Inconsistency Handling, Inconsistency-tolerant query answering, Lightweight ontologies", 
    "title": "On The Use Of Non-Objection Inference In Inconsistent Lightweight Ontologies", 
    "type": "paper"
  }, 
  "2385": {
    "abstract": "We study the optimization problem of designing the program of a conference with parallel sessions, so that the intended participants are as happy as possible from the talks they can attend. Interestingly, this can be thought of as a two-dimensional extension of a scheme proposed by Chamberlin and Courant [1983] for achieving proportional representation in multi-winner elections. We show that different variations of the problem are computationally hard by exploiting relations of the problem with well-known hard graph problems. On the positive side, we present polynomial-time algorithms that compute conference programs that have a social utility that is provably close to the optimal one (within constant factors). Our algorithms are either combinatorial or based on linear programming and randomized rounding.", 
    "authors": [
      {
        "name": "Ioannis Caragiannis"
      }, 
      {
        "name": "Laurent Gourves"
      }, 
      {
        "name": "Jerome Monnot"
      }
    ], 
    "keywords": "proportional representation, multi-winner elections, approximation algorithms, linear programming, randomized rounding", 
    "title": "Achieving proportional representation in conference programs", 
    "type": "paper"
  }, 
  "2392": {
    "abstract": "To enable large-scale multi-agent coordination, this paper abstracts a multi-agent placement problem and uses its solution to decompose a large multi-agent task assignment and scheduling problem into a group of smaller problems. The key challenge of this multi-agent placement problem is that its solution computation depends upon solutions to task assignment and scheduling subproblems, which are not trivial to compute. To address this challenge, we formulate them together as a multi-level optimization problem and develop a multi-abstraction search approach for solving this problem. This approach begins with a highly abstract agent placement problem and the rapid computation of an initial solution, which is then improved upon using a hill climbing algorithm for a less abstract problem; finally, the solution is fine-tuned within the original problem space. Empirical results verify that this multi-abstraction approach significantly outperforms a regular hill climbing algorithm and an approximate mixed-integer linear programming approach.", 
    "authors": [
      {
        "name": "Chongjie Zhang"
      }, 
      {
        "name": "Julie Shah"
      }
    ], 
    "keywords": "Task Assignment and Scheduling, Multi-level optimization, Multi-agent placement", 
    "title": "Co-Optimization Multi-Agent Placement with Task Assignment and Scheduling", 
    "type": "paper"
  }, 
  "2394": {
    "abstract": "Non-linear regression is a fundamental and yet under-developing methodology in solving many problems in Artificial Intelligence. The canonical control and predictions mostly utilize linear models or multi-linear models. However, due to the high non-linearity of the systems, those linear prediction models cannot fully cover the complexity of the problems. In this paper, we propose a robust two-stage hierarchical regression approach, to solve a popular Human-Computer Interaction, the unconstrained face-in-the-wild keypoint detection problem for computers. The environment is the still images, videos and live camera streams from machine vision. We firstly propose a holistic regression model to initialize the facial landmarks under different head pose assumptions. Second, to reduce local shape variance, a hierarchical part-based regression method is further proposed to refine the global regression output. Experiments on several challenging faces-in-the-wild datasets demonstrate the consistently better accuracy of our method, when compared to the state-of-the-art.", 
    "authors": [
      {
        "name": "Xiang Yu"
      }, 
      {
        "name": "Shaoting Zhang"
      }, 
      {
        "name": "Zhe Lin"
      }, 
      {
        "name": "Dimitris Metaxas"
      }
    ], 
    "keywords": "Human-Computer Interaction, Face Alignment, Landmark Localization, Shape Fitting", 
    "title": "Nonlinear Hierarchical Part-based Regression for Unconstrained Face Alignment", 
    "type": "paper"
  }, 
  "2399": {
    "abstract": "Prediction markets are often used as mechanisms to aggregate information about a future event, for example, whether a candidate will win an election. The event is typically assumed to be exogenous. In reality, participants may influence the outcome, and therefore (1) running the prediction market could change the incentives of participants in the process that creates the outcome (for example, agents may want to change their vote in an election), and (2) simple results such as the myopic incentive compatibility of proper scoring rules no longer hold in the prediction market itself. We introduce a model of games of this kind, where agents first trade in a prediction market mediated by a market scoring rule and then take an action that affects the market outcome. Our two-stage model captures two aspects of real-world prediction markets: (1) agents may directly influence the outcome, (2) some of the agents instrumental in deciding the outcome may not take part in the prediction market. We show that this game has two different types of perfect Bayesian equilibria, which we term LPP and HPP, depending on the values of the belief parameters: In the LPP domain, equilibrium prices reveal expected market outcomes conditional on the participants\u00e2\u0080\u0099 private information, whereas HPP equilibria are collusive \u00e2\u0080\u0093 participants effectively coordinate in an uninformative and untruthful way.", 
    "authors": [
      {
        "name": "Mithun Chakraborty"
      }, 
      {
        "name": "Sanmay Das"
      }
    ], 
    "keywords": "Prediction Markets, Information Elicitation, Market Scoring Rule, Outcome Manipulation, Perfect Bayesian Equilibrium", 
    "title": "Trading On A Rigged Game: Outcome Manipulation In Prediction Markets", 
    "type": "paper"
  }, 
  "2407": {
    "abstract": "Discovering association rules from transaction database is one of the most studied data mining task. Many effective techniques have been proposed over the years.  All these algorithms share the same two steps methodology: frequent itemsets enumeration followed by effective association rules generation step. In this paper, we propose a new propositional satisfiability based approach to mine association rules in a single step. The task is modeled as a Boolean formula whose models correspond to the rules to be mined.  To highlight the  flexibility of our proposed framework, we also address two other variants, namely the closed and indirect association rules mining tasks.  Experiments on many datasets show that on both closed and indirect association rules mining tasks, our declarative approach achieves better performance with respect to the state-of-the-art specialized techniques.", 
    "authors": [
      {
        "name": "Abdelhamid Boudane"
      }, 
      {
        "name": "Said Jabbour"
      }, 
      {
        "name": "Lakhdar Sais"
      }, 
      {
        "name": "Yakoub Salhi"
      }
    ], 
    "keywords": "Data mining, Association rules, Propositional satisfiability", 
    "title": "A SAT-based Approach for Mining Association Rules", 
    "type": "paper"
  }, 
  "2421": {
    "abstract": "In this paper, we employ simulation-based methods to study the role of a market maker in improving price discovery in a prediction market. In our model, traders receive a lagged signal of a ground truth, which is based on real price data from prediction markets on NBA games in the 2014--2015 season. We employ empirical game-theoretic analysis to identify equilibria under different settings of market maker liquidity and spread. We study two settings: one in which traders only enter the market once, and one in which traders have the option to reenter to trade later. We evaluate welfare and the profits accrued by traders, and we characterize the conditions under which the market maker promotes price discovery in both settings.", 
    "authors": [
      {
        "name": "Elaine Wah"
      }, 
      {
        "name": "Sebastien Lahaie"
      }, 
      {
        "name": "David Pennock"
      }
    ], 
    "keywords": "Prediction market, Market maker, Price discovery, Agent-based simulation", 
    "title": "An Empirical Game-Theoretic Analysis of Price Discovery in Prediction Markets", 
    "type": "paper"
  }, 
  "2422": {
    "abstract": "We consider a nonatomic selfish routing model with independent stochastic travel times for each edge, represented by mean and variance latency functions that depend on arc flows. This model can apply to traffic in the Internet or in a road network. Variability negatively impacts packets or drivers, by introducing jitter in transmission delays which lowers quality of streaming audio or video, or by making it more difficult to predict the arrival time at destination. The price of risk aversion (PRA) has been defined as the worst-case ratio of the cost of an equilibrium with risk-averse players who seek risk-minimizing paths, and that of an equilibrium with risk-neutral users who minimize the mean travel time of a path [Nikolova and Stier-Moses, 2015]. This inefficiency metric captures the degradation of system performance caused by variability and risk aversion. In this paper, we provide the first lower bounds on the PRA. First, we show a family of structural lower bounds, which grow linearly with the size of the graph and players\u00e2\u0080\u0099 risk-aversion. They are tight for graph sizes that are powers of two. We also provide asymptotically tight functional bounds that depend on the allowed latency functions but not on the topology. The functional bounds match the price-of-anarchy bounds for congestion games multiplied by an extra factor that accounts for risk aversion. Finally, we provide a closed-form formula of the PRA for a family of graphs that generalize series-parallel graphs and the Braess graph. This formula also applies to the mean-standard deviation user objective\u00e2\u0080\u0094a much more complex model of risk-aversion due to the cost of a path being non-additive over edge costs.", 
    "authors": [
      {
        "name": "Thanasis Lianeas"
      }, 
      {
        "name": "Evdokia Nikolova"
      }, 
      {
        "name": "Nicolas Stier-Moses"
      }
    ], 
    "keywords": "Selfish Routing, Price of Risk Aversion, Stochastic Delays, Mean-Variance Model", 
    "title": "Asymptotically tight bounds for inefficiency in risk-averse selfish routing", 
    "type": "paper"
  }, 
  "2426": {
    "abstract": "Learning detectors that can recognize concepts, such as people actions, objects, etc., in the video content is an interesting but challenging problem. In this paper, we study the problem of automatically learning detectors from the big video data on the web without any additional manual annotations. The contextual information available on the web provides noisy labels to the video content. To leverage the noisy web labels, we propose a novel method called WEbly-Labeled Learning (WELL). It is established on the theories called curriculum learning and self-paced learning and of useful properties that can be theoretically verified. For example, we provide compelling insights on the latent non-convex robust loss that is being minimized on the noisy data. In addition, we propose two novel techniques that not only enable WELL to be applied to big data but also lead to more accurate results. The efficacy and the scalability of WELL have been extensively demonstrated on two public benchmarks, including the largest multimedia dataset and the largest manually-labeled video set. Experimental results show that WELL significantly outperforms the state-of-the-art methods. Notably, to the best of our knowledge, WELL achieves by far the best reported performance on the two webly-labeled big video datasets.", 
    "authors": [
      {
        "name": "Junwei Liang"
      }, 
      {
        "name": "Lu Jiang"
      }, 
      {
        "name": "Deyu Meng"
      }, 
      {
        "name": "Alexander Hauptmann"
      }
    ], 
    "keywords": "Video Understanding, Curriculum learning, Prior Knowledge, Web Label, Big Data", 
    "title": "Learning to Detect Concepts from Webly-Labeled Video Data", 
    "type": "paper"
  }, 
  "2430": {
    "abstract": "Sparsity-constrained optimization is an important and challenging problem that has wide applicability in data mining, machine learning, and statistics.  In this paper, we focus on sparsity-constrained optimization in cases where the cost function is a general nonlinear function and, in particular, the sparsity constraint is defined by a graph-structured sparsity model. Existing methods explore this problem in the context of sparse estimation in linear models. To the best of our knowledge, this is the first work to present an efficient approximation algorithm, namely, Graph-structured Matching Pursuit (Graph-Mp), to optimize a nonlinear objective function of arbitrary form. We prove that our algorithm enjoys the strong guarantees analogous to those designed for linear models in terms of convergence rate and approximation accuracy. As a case study, we specialize Graph-Mp to optimize a number of well-known graph scan statistic models for the connected subgraph detection task, and empirical evidence demonstrates that our general algorithm performs superior over state-of-the-art methods that are designed specifically for the task of connected subgraph detection on both running time and accuracy.", 
    "authors": [
      {
        "name": "Feng Chen"
      }, 
      {
        "name": "Baojian Zhou"
      }
    ], 
    "keywords": "Matching Pursuit, Graph-Structured Sparsity, Graph Scan Statistics, Connected Subgraph Detection", 
    "title": "A Generalized Matching Pursuit Approach for Graph-Structured Sparsity", 
    "type": "paper"
  }, 
  "2435": {
    "abstract": "We need to reason about rankings of objects in  a wide variety of domains including  information retrieval, sports tournaments, bibliometrics, and statistics. We propose a global constraint therefore for modeling rankings.  One important application for rankings is in reasoning about the correlation or uncorrelation between two sequences. For example, we might wish to have consecutive delivery schedules correlated to make it easier for clients and employees, or uncorrelated to avoid predictability and complacence. We therefore also consider global correlation constraints between rankings.  For both ranking and correlation constraints, we  propose efficient filtering algorithms and decompositions, and report experimental results demonstrating the promise of our proposed approach.", 
    "authors": [
      {
        "name": "Christian Bessiere"
      }, 
      {
        "name": "Emmanuel Hebrard"
      }, 
      {
        "name": "George Katsirelos"
      }, 
      {
        "name": "Zeynep Kiziltan"
      }, 
      {
        "name": "Toby Walsh"
      }
    ], 
    "keywords": "constraint satisfaction, scheduling, ranking, global constraint, correlation", 
    "title": "Ranking Constraints", 
    "type": "paper"
  }, 
  "2444": {
    "abstract": "In this paper, we explore how ontological knowledge expressed via existential rules can be combined with possibilistic networks (i) to represent qualitative preferences along with domain knowledge, and (ii) to realize some preference-based answering of conjunctive queries (CQs). We call these combinations ontological possibilistic networks (OP-nets). We define skyline and k-rank answers to CQs and we provide an algorithm for computing them based on the user\u00e2\u0080\u0099s preferences encoded in an OP-net. We also provide precise complexity (including data tractability) results.", 
    "authors": [
      {
        "name": "Stefan Borgwardt"
      }, 
      {
        "name": "Bettina Fazzinga"
      }, 
      {
        "name": "Thomas Lukasiewicz"
      }, 
      {
        "name": "Akanksha Shrivastava"
      }, 
      {
        "name": "Oana Tifrea-Marciuska"
      }
    ], 
    "keywords": "possibilistic networks, existential rules, computational complexity", 
    "title": "Preferential Query Answering over the Semantic Web with Possibilistic Networks", 
    "type": "paper"
  }, 
  "2448": {
    "abstract": "To date, the automated detection of cyberbullying has focused on natural language processing for the analyses of text in which bullying is suspected to be present. In this work, we investigate the notion that there may be some identifiable features inherent to posted content itself that are more likely to trigger abusive reactions among responders. In particular, given the overwhelming increase in media accompanying text in online social networks, we investigate the contribution offered by posted images and captions for the improved detection of bullying in response to shared content. We validate our approaches on a dataset of over 3000 images along with peer-generated comments posted on the Instagram photo-sharing network, running comprehensive experiments over a variety of classifiers and feature sets. In addition to standard image and text features, we leverage several novel features including topics determined from image captions and a pre-trained convolutional neural network on image pixels. We identify the importance of these advanced features in detecting occurrences of cyberbullying in posted comments. We also provide results on the classification of images and captions themselves as potential targets for cyberbullies. The latter proves to be a challenging  and yet important task,  as it can help design preventative methods for cyberbullying instances.", 
    "authors": [
      {
        "name": "Haoti Zhong"
      }, 
      {
        "name": "Hao Li"
      }, 
      {
        "name": "Anna Squicciarini"
      }, 
      {
        "name": "Sarah Rajtmajer"
      }, 
      {
        "name": "Christopher Griffin"
      }, 
      {
        "name": "David Miller"
      }, 
      {
        "name": "Cornelia Caragea"
      }
    ], 
    "keywords": "Cyberbullyism, Text and Image mining, supervised learning, deep learning", 
    "title": "Content-driven Detection of Cyberbullying on the Instagram Social Network", 
    "type": "paper"
  }, 
  "2450": {
    "abstract": "Hierarchical phrase-based translation systems (HPBs) perform translation using a synchronous context free grammar which has only one unified non-terminal for every translation rule. While the usage of the unified non-terminal brings freedom to generate translations with almost arbitrary structures, it also takes the risks to generate low-quality translations which has a wrong syntactic structure. In this paper, we propose tree state models to discriminate the good or bad usage of translation rules based on the syntactic structures of the source sentence. We propose to use statistical models and context dependent features to estimate the probability of each tree state for each translation rule and punish the usage of rules in the translation system which violates their tree states. Experimental results demonstrate that this simple model could bring significant improvement to the translation quality.", 
    "authors": [
      {
        "name": "Shujian Huang"
      }, 
      {
        "name": "Huifeng Sun"
      }, 
      {
        "name": "Chengqi Zhao"
      }, 
      {
        "name": "Jinsong Su"
      }, 
      {
        "name": "Xinyu Dai"
      }, 
      {
        "name": "Jiajun Chen"
      }
    ], 
    "keywords": "Machina Translation, Syntactic Constraint, Rule Selection, Natural Language Processing", 
    "title": "Tree-state based Rule Selection Models for Hierarchical Phrase-based Machine Translation", 
    "type": "paper"
  }, 
  "2451": {
    "abstract": "We initiate a research agenda of mechanism design for categorized domain allocation problems (CDAPs), where indivisible items from multiple categories are allocated to agents without monetary transfer and each agent gets at least one item per category.  We focus on basic CDAPs, where each agent gets exactly one item from each category. We first characterize serial dictatorships by a minimal set of three axiomatic properties: strategy-proofness, non-bossiness, and category-wise neutrality. Then, we propose a natural extension of serial dictatorships called categorial sequential allocation mechanisms (CSAMs), which allocate the items in multiple rounds: in each round, the designated agent chooses an item from a designated category. We fully characterize the worst-case rank efficiency of CSAMs for optimistic and pessimistic agents.", 
    "authors": [
      {
        "name": "Erika Mackin"
      }, 
      {
        "name": "Lirong Xia"
      }
    ], 
    "keywords": "Resource allocation, categorized domains, strategy-proofness, rank efficiency", 
    "title": "Allocating Indivisible Items in Categorized Domains", 
    "type": "paper"
  }, 
  "2460": {
    "abstract": "We propose a variable elimination algorithm for influence diagrams that includes reductions that allow an influence diagram to be solved by a generalization of the classic dynamic programming approach for solving partially observable Markov decision problems. To motivate this approach, we also propose a related bidirectional algorithm that solves the last part of an influence diagram in a backwards direction using the new reductions, and the initial part in a forwards direction using traditional reductions.", 
    "authors": [
      {
        "name": "Eric Hansen"
      }, 
      {
        "name": "Jinchuan Shi"
      }, 
      {
        "name": "Arindam Khaled"
      }
    ], 
    "keywords": "influence diagram, variable elimination, POMDP", 
    "title": "A POMDP Approach to Influence Diagram Evaluation", 
    "type": "paper"
  }, 
  "2466": {
    "abstract": "We analyse epistemic boolean games in a computationally grounded dynamic epistemic logic. The agents' knowledge is determined by what is visible for them, including higher-order visibility: agents may observe whether another agent observes an atom or not. The agents' actions consist in modifying the truth values of atoms. We provide an axiomatisation of the logic, establish that the model checking problem is in PSpace, and show how one can reason about equilibria in epistemic boolean games.", 
    "authors": [
      {
        "name": "Andreas Herzig"
      }, 
      {
        "name": "Emiliano Lorini"
      }, 
      {
        "name": "Faustine Maffre"
      }, 
      {
        "name": "Francois Schwarzentruber"
      }
    ], 
    "keywords": "epistemic logic, game theory, modal logic", 
    "title": "Epistemic boolean games based on a logic of visibility and control", 
    "type": "paper"
  }, 
  "2472": {
    "abstract": "Jump Point Search, an algorithm developed for fast search on uniform cost grids, has successfully improved the performance of grid-based search. But, the approach itself is actually a set of diverse ideas applied together. In this paper we identify one key contribution of the approach, a canonical ordering on paths in the grid. After studying the correctness of using a canonical ordering, we then show that this idea can be more generally applied. We show how canonical orderings can be applied to a variety of search problems, including single-source shortest path search, suboptimal search, agent-centered search, and three-dimensional search. Each of these problems reveals a different aspect of how canonical orderings influence search.", 
    "authors": [
      {
        "name": "Nathan Sturtevant"
      }, 
      {
        "name": "Steve Rabin"
      }
    ], 
    "keywords": "grid, search, pathfinding", 
    "title": "Canonical Orderings on Grids", 
    "type": "paper"
  }, 
  "2477": {
    "abstract": "Recent years have seen a growing interest in player modeling for digital games. While goal recognition is a key problem in player modeling, accurately recognizing players\u00e2\u0080\u0099 goals from observations of low-level player actions poses significant challenges because of the inherent complexity and uncertainty that pervades gameplay. In this paper, we formulate player goal recognition as a sequence labeling task and introduce a goal recognition framework based on long short-term memory (LSTM) networks. Results from an evaluation of the LSTM goal recognition framework show that LSTM-based goal recognition is significantly more accurate than previous state-of-the-art approaches. Because of the increased recognition accuracy and the fact that LSTM-based goal recognition eliminates the requirement for extensive feature engineering, it provides a promising approach to a central problem of player modeling in open-world digital games.", 
    "authors": [
      {
        "name": "Wookhee Min"
      }, 
      {
        "name": "Bradford Mott"
      }, 
      {
        "name": "Jonathan Rowe"
      }, 
      {
        "name": "Barry Liu"
      }, 
      {
        "name": "James Lester"
      }
    ], 
    "keywords": "goal recognition, intent recognition, player modeling, digital games", 
    "title": "Player Goal Recognition in Open-World Digital Games with Long Short-Term Memory Networks", 
    "type": "paper"
  }, 
  "2480": {
    "abstract": "We present a method for detecting driver frustration from both video and audio streams captured during the driver's interaction with an in-vehicle voice-based navigation system. The video is of the driver's face when the machine is speaking, and the audio is of the driver's voice when he or she is speaking. We analyze a dataset of 20 drivers that contains 596 audio epochs (audio clips, with duration from 1 sec to 15 sec) and 615 video epochs (video clips, with duration from 1 sec to 45 sec). The dataset is balanced across 2 age groups, 2 vehicle systems, and both genders. The model was subject-independently trained and tested using 4-fold cross-validation. We achieve an accuracy of 77.4% for detecting frustration from a single audio epoch and 81.2% for detecting frustration from a single video epoch. We then treat the video and audio epochs as a sequence of interactions and use decision fusion to characterize the trade-off between decision time and classification accuracy, which improved the prediction to 88.5% accuracy after 9 epochs.", 
    "authors": [
      {
        "name": "Irman Abdi\u0107"
      }, 
      {
        "name": "Lex Fridman"
      }, 
      {
        "name": "Daniel McDuff"
      }, 
      {
        "name": "Erik Marchi"
      }, 
      {
        "name": "Bryan Reimer"
      }, 
      {
        "name": "Bj\u00f6rn Schuller"
      }
    ], 
    "keywords": "Frustration Detection, Affective Computing, Voice Control, Voice Navigation System, HCI", 
    "title": "Driver Frustration Detection From Audio and Video in the Wild", 
    "type": "paper"
  }, 
  "2484": {
    "abstract": "We present SGDPLL(T), an algorithm that solves (among many other problems) probabilistic inference modulo theories, that is, inference problems over probabilistic models defined via a logic theory provided as a parameter (currently, equalities and inequalities on discrete sorts). While many solutions to probabilistic inference over logic representations have been proposed, SGDPLL(T) is simultaneously (1) lifted, (2) exact and (3) modulo theories, that is, parameterized by a background logic theory, which offers a foundation for extending it to rich logic languages such as data structures and relational data. By lifted, we mean that our proposed algorithm can leverage first-order representations to solve some inference problems in constant or polynomial time in the domain size (the number of values that variables can take), as opposed to exponential time offered by propositional algorithms. We have so far implemented propositional, equality over categorical types, and inequalities over bounded integer theory solvers, and present experiments showing how the inequalities solver performs over UAI benchmark problems.", 
    "authors": [
      {
        "name": "Rodrigo de Salvo Braz"
      }, 
      {
        "name": "Ciaran O'Reilly"
      }, 
      {
        "name": "Vibhav Gogate"
      }, 
      {
        "name": "Rina Dechter"
      }
    ], 
    "keywords": "probabilistic logic reasoning, lifted probabilistic reasoning, probabilistic reasoning, modulo theories, weighted model counting", 
    "title": "Probabilistic Inference Modulo Theories", 
    "type": "paper"
  }, 
  "2485": {
    "abstract": "Real world applications of artificial intelligence often require agents to sequentially choose actions from continuous action spaces with execution uncertainty. When good actions are sparse, domain knowledge is often used to identify a discrete set of promising actions.  These actions and their uncertain effects are typically evaluated using a recursive search procedure to select an action to execute. The reduction of the problem to a discrete search problem causes severe limitations, notably, not exploiting all of the sampled outcomes when evaluating actions, and not using outcomes to help find new actions outside the original set. We propose a new Monte Carlo tree search (MCTS) algorithm specifically designed for exploiting an execution model in this setting. Using kernel regression, it generalizes the information about action quality between actions and to unexplored parts of the action space. It uses this generalization power to both better guide exploration, and to identify actions not originally suggested by the domain knowledge. In a high fidelity simulator of the olympic sport of curling, we show that this approach significantly outperforms existing MCTS methods.", 
    "authors": [
      {
        "name": "Timothy Yee"
      }, 
      {
        "name": "Viliam Lisy"
      }, 
      {
        "name": "Michael Bowling"
      }
    ], 
    "keywords": "Monte Carlo Tree Search, Continuous Action Spaces, Curling, Execution Error, Kernel Regression", 
    "title": "Monte Carlo Tree Search in Continuous Action Spaces with Execution Uncertainty", 
    "type": "paper"
  }, 
  "2494": {
    "abstract": "Interactive dynamic influence diagrams (I-DIDs) provide an explicit way of modeling how a subject agent solves decision making problems in the present of other agents in a common setting.  To optimize its decisions, the subject agent needs to predict the other agents' behavior, that is generally obtained by solving their candidate models. This becomes extremely difficult since the model space may be  rather large, and grows when the other agents act and observe over the time.  A recent proposal for solving I-DIDs lies in a concept of value equivalence (VE)  that shows potential advances on significantly reducing the model space.  In this paper, we establish a principled framework  to implement the VE techniques and propose an approximate method to compute VE of candidate models.  The development offers ample opportunity of exploiting VE  to further improve the scalability of I-DID solutions.  We theoretically analyze properties of the approximate techniques and show empirical results in multiple problem domains.", 
    "authors": [
      {
        "name": "Ross Conroy"
      }, 
      {
        "name": "Yifeng Zeng"
      }, 
      {
        "name": "Jing Tang"
      }
    ], 
    "keywords": "influence diagrams, decision making, multiple agents", 
    "title": "Approximating Value Equivalence in Interactive Dynamic Influence Diagrams Using Behavioral Coverage", 
    "type": "paper"
  }, 
  "250": {
    "abstract": "Modeling in constraint programming is a hard task that requires considerable expertise. Automated model reformulation aims at assisting a naive user in modeling constraint problems. In this context, formal specification languages have been devised to express constraint problems in a manner similar to natural yet rigorous specifications that use a mixture of natural language and discrete mathematics. Yet, a gap remains between such languages and the natural language in which humans informally describe problems. This work aims to alleviate this issue by proposing a method for detecting constraints in natural language problem descriptions using a structured-output classifier. To evaluate the method, we develop an original annotated corpus which gathers 110 problem descriptions from several resources. Our results show considerable accuracy with respect to metrics  used in cognate tasks.", 
    "authors": [
      {
        "name": "Zeynep Kiziltan"
      }, 
      {
        "name": "Marco Lippi"
      }, 
      {
        "name": "Paolo Torroni"
      }
    ], 
    "keywords": "Constraint programming, Automated model reformulation, Abstract constraint specification, Automated processing of natural language problem descriptions, Structured-output classifier application", 
    "title": "Constraint Detection in Natural Language Problem Descriptions", 
    "type": "paper"
  }, 
  "252": {
    "abstract": "Recent researches have shown consensus clustering can enhance the accuracy of human action categorization models by combining multiple clusterings, which can be obtained from various types of local descriptors, such as HOG, HOF and MBH. However, consensus clustering yields final clustering without access to the underlying feature representations of the human action data, which always makes the final partition limited to the quality of existing basic clusterings. To solve this problem, we present a novel and effective Consensus Information Bottleneck (CIB) method for unsupervised human action categorization. CIB is capable of learning action categories from feature variable and auxiliary clusterings simultaneously. Specifically, by performing Maximization of Mutual Information (MMI), CIB maximally preserves the information between feature variable and existing auxiliary clusterings. Thus, CIB can solve the overreliance of consensus clustering on existing clusterings. Moreover, to solve MMI optimization, a sequential solution is proposed to update data partition. Extensive experiments on five realistic human action data sets show that CIB can consistently and significantly beat other state-of-the-art consensus and multi-view clustering methods.", 
    "authors": [
      {
        "name": "Xiaoqiang Yan"
      }, 
      {
        "name": "Yangdong Ye"
      }, 
      {
        "name": "Xueying Qiu"
      }
    ], 
    "keywords": "Human action, Consensus clustering, Information bottleneck, Mutual information", 
    "title": "Unsupervised Human Action Categorization with Consensus Information Bottleneck Method", 
    "type": "paper"
  }, 
  "2526": {
    "abstract": "To accomplish tasks in human-centric indoor environments, robots need to represent and understand the world in terms of objects and their attributes. We refer to this attribute-based representation as a world model, and consider how to acquire it via noisy perception and maintain it over time, as objects are added, changed, and removed in the world. Previous work has framed this as multiple-target tracking problem, where objects are potentially in motion at all times. Although this approach is general, it is computationally expensive. We argue that such generality is not needed in typical world modeling tasks, where objects only change state occasionally. More efficient approaches are enabled by restricting ourselves to such semi-static environments.  We consider a previously-proposed clustering-based world modeling approach that assumed static environments, and extend it to semi-static domains by applying a dependent Dirichlet process (DDP) mixture model. We derive a novel approximate MAP inference algorithm under this model, subject to data association constraints. We demonstrate our approach improves computational performance in semi-static environments.", 
    "authors": [
      {
        "name": "Lawson L.S. Wong"
      }, 
      {
        "name": "Thanard Kurutach"
      }, 
      {
        "name": "Tomas Lozano-Perez"
      }, 
      {
        "name": "Leslie Pack Kaelbling"
      }
    ], 
    "keywords": "World modeling, Data asssociation, Dependent Dirichlet process", 
    "title": "Object-based World Modeling in Semi-Static Environments with Dependent Dirichlet Process Mixtures", 
    "type": "paper"
  }, 
  "2527": {
    "abstract": "We study plan synthesis for a variant of Knowledge and Action Bases (KABs). KABs have been recently introduced as a rich, dynamic framework where states are full-fledged description logic (DL) knowledge bases (KBs), and actions manipulate the extensional part of such KBs, possibly introducing new objects taken from an infinite domain. We show that, in general, plan existence over KABs is undecidable even under severe restrictions. We then focus on the class of state-bounded KABs, for which plan existence is decidable, and we provide sound and complete plan synthesis algorithms, through a novel combination of techniques based on standard planning, DL query answering, and finite-state abstractions. All results hold for any DL with decidable query answering. We finally show that for lightweight DLs, plan synthesis can be compiled into standard ADL planning.", 
    "authors": [
      {
        "name": "Diego Calvanese"
      }, 
      {
        "name": "Marco Montali"
      }, 
      {
        "name": "Fabio Patrizi"
      }, 
      {
        "name": "Michele Stawowy"
      }
    ], 
    "keywords": "Knowledge and Action Bases, Plan Synthesis, Planning over infinite domains", 
    "title": "Plan Synthesis for Knowledge and Action Bases", 
    "type": "paper"
  }, 
  "2530": {
    "abstract": "Graphical models, as applied to multi-target prediction problems, commonly utilize interaction terms to impose the structure among the output variables. Often, such structure is based on the assumption that related outputs need to be similar and interaction terms that force them to have closer values are adopted. Here we relax that assumption and propose a feature that is based on distance and can adapt to ensure that variables have smaller or larger difference in values. We utilized a Gaussian Conditional Random Field model, where we have extended its originally proposed interaction potential to include a distance term. The extended model is compared to the baseline in various synthetic and real world structured regression setups. Increase in predictive accuracy as well as in stability of the learned model parameters were observed on both synthetic examples and real-world applications, including datasets from climate and healthcare.", 
    "authors": [
      {
        "name": "Ivan Stojkovic"
      }, 
      {
        "name": "Vladisav Jelisavcic"
      }, 
      {
        "name": "Veljko Milutinovic"
      }, 
      {
        "name": "Zoran Obradovic"
      }
    ], 
    "keywords": "structured learning, multi-target regression, gaussian conditional random fields", 
    "title": "Distance Based Modeling of Interactions in Structured Regression", 
    "type": "paper"
  }, 
  "2531": {
    "abstract": "Congestion games are a well-studied class of games that has been used to model real-world systems such as Internet routing. In many congestion games, each player's number of strategies can be exponential in the natural description of the game. Most existing algorithms for game theoretic computation, from computing expected utilities and best responses to finding Nash equilibrium and other solution concepts, all involve enumeration of pure strategies. As a result, such algorithms would take exponential time on these congestion games. In this work, we study congestion games in which each player's strategy space can be described compactly using a set of  linear constraints. For instance, network congestion games naturally fall into this subclass as each player's strategy can be described by a set of flow constraints. We show that we can represent any mixed strategy compactly using marginals which specify the probability of using each resource. As a consequence, the expected utilities and the best responses can be computed in polynomial time given the marginals. We reduce the problem of computing  a best/worst symmetric approximate mixed-strategy Nash equilibrium in symmetric congestion games to a constraint optimization problem on a graph formed by the resources and the strategy constraints. As a result, we present a fully polynomial time approximation scheme (FPTAS) for  this problem when the graph has bounded treewidth.", 
    "authors": [
      {
        "name": "Hau Chan"
      }, 
      {
        "name": "Albert Jiang"
      }
    ], 
    "keywords": "Nash equilibrium, congestion games, fully polynomial time approximation scheme (FPTAS), discretization, marginals, best responses, expected utilities, symmetric games, linear constraints, compact representation, bounded treewidth, message passing, dynamic programming", 
    "title": "Congestion Games with Polytopal Strategy Spaces", 
    "type": "paper"
  }, 
  "2537": {
    "abstract": "Predictive State Representations (PSRs) are a powerful method of modelling partially observable dynamical systems. By using functions of a set of observable quantities, called tests, to represent the state of the model, PSRs have shown advantages over the latent state-based approaches. As a consequence, discovering the set of tests for representing the state is one of the central problem in PSRs. However, existing techniques usually either discover these tests using iterative methods, which can only be applied in some toy problems, or just avoid this problem by maintaining a very large set of tests, which may be prohibitively expensive. In this paper, with the benefits of Monte-Carlo tree search (MCTS) for finding solutions in complex problems, and by proposing the concept of model entropy for measuring the model accuracy as the evaluation function of MCTS, the discovery problem is formalized as a sequential decision making problem. Then such a decision making problem can be solved using MCTS, the set of tests for representing state can be obtained and the PSR model of the underlying system can be built straightforwardly. We conduct experiments on several domains including one extremely large domain, experimental results show the effectiveness of our approach.", 
    "authors": [
      {
        "name": "Yunlong Liu"
      }, 
      {
        "name": "Hexing Zhu"
      }, 
      {
        "name": "Yifeng Zeng"
      }, 
      {
        "name": "Zongxiong Dai"
      }
    ], 
    "keywords": "Predictive State Representations, Monte-Carlo tree search, Decision Making", 
    "title": "Learning Predictive State Representations via Monte-Carlo Tree Search", 
    "type": "paper"
  }, 
  "254": {
    "abstract": "The outputs of a trained neural network contain much richer information than just an one-hot classifier. For example, a neural network might give an image of a dog the probability of one in a million of being a cat but it is still much larger than the probability of being a car. To reveal the hidden structure in them, we apply two unsupervised learning algorithms, PCA and ICA, to the outputs of a deep Convolutional Neural Network trained on the ImageNet of 1000 classes. The PCA/ICA embedding of the object classes reveals their visual similarity and the PCA/ICA components can be interpreted as common visual features shared by similar object classes. For an application, we proposed a new zero-shot learning method, in which the visual features learned by PCA/ICA are employed. Our zero-shot learning method achieves the state-of-the-art results on the ImageNet of over 20000 classes.", 
    "authors": [
      {
        "name": "Yao Lu"
      }
    ], 
    "keywords": "unsupervised learning, deep learning, zero-shot learning, convolutional neural network", 
    "title": "Unsupervised Learning on Neural Network Outputs", 
    "type": "paper"
  }, 
  "2543": {
    "abstract": "We study access to temporal data with TEL, a temporal extension of the tractable description logic EL. Our aim is to establish a clear computational complexity landscape for the atomic query answering problem, in terms of both data and combined complexity. Atomic queries in full TEL turn out to be undecidable even in data complexity. Motivated by this negative result, we identify well-behaved yet expressive fragments of TEL. Our main contributions are a semantic and sufficient syntactic conditions for decidability and three orthogonal tractable fragments, which are based on restricted use of rigid roles, temporal operators, and novel acyclicity conditions on the ontologies.", 
    "authors": [
      {
        "name": "V\u00edctor Guti\u00e9rrez Basulto"
      }, 
      {
        "name": "Jean Christoph Jung"
      }, 
      {
        "name": "Roman Kontchakov"
      }
    ], 
    "keywords": "temporal description logic, query answering, computational complexity, tractable description logics", 
    "title": "Temporalized EL Ontologies for Accessing Temporal  Data: Complexity of Atomic Queries", 
    "type": "paper"
  }, 
  "2557": {
    "abstract": "We study the classical social choice problem where a group of $n$ voters report their  preferences over $m$ alternatives and a voting rule is used to select an alternative. We show  that when the preferences of voters are positively correlated according to the Kendall-Tau distance, the probability that any scoring rule is not ordinally Bayesian incentive compatible (OBIC) goes to zero exponentially fast with the number of voters, improving over the previously known rate of $1/\\sqrt{n}$ for independent preferences. Motivated by rank-order models from machine learning, we introduce two examples of positively-correlated belief systems, namely Conditional Mallows and Conditional Plackett-Luce. Conditional Mallows satisfies the Kendall-Tau correlation model and fits our positive result. We also prove that Conditional Plackett-Luce becomes OBIC exponentially quickly.", 
    "authors": [
      {
        "name": "Debmalya Mandal"
      }, 
      {
        "name": "David Parkes"
      }
    ], 
    "keywords": "Social Choice Theory, Voting with Incomplete Information, Incentive compatibility, Rank-order models", 
    "title": "Correlated Voting", 
    "type": "paper"
  }, 
  "2561": {
    "abstract": "We describe a framework for building abstraction hierarchies whereby an agent alternates skill- and representation-construction phases to build a sequence of increasingly abstract Markov decision processes. Our formulation builds on recent results showing that the appropriate abstract representation of a problem is specified by the agent\u00e2\u0080\u0099s skills. We describe how such a hierarchy can be used for fast planning, and illustrate the construction of an appropriate hierarchy for the Taxi domain.", 
    "authors": [
      {
        "name": "George Konidaris"
      }
    ], 
    "keywords": "Reinforcement Learning, Hierarchy, Planning", 
    "title": "Constructing Abstraction Hierarchies Using a Skill-Symbol Loop", 
    "type": "paper"
  }, 
  "2566": {
    "abstract": "As publishers gather more information about their users, they can use that information to enable advertisers to create increasingly targeted campaigns. This enables better usage of advertising inventory. However, it also dramatically increases the complexity that the publisher faces when optimizing campaign admission decisions and inventory allocation to campaigns. We develop an optimal anytime algorithm for abstracting fine-grained audience segments into coarser abstract segments that are not too numerous for use in such optimization. Compared to the segment abstraction algorithm by Walsh et al. [AAAI-10] for the same problem, it yields two orders of magnitude improvement in run time and significant improvement in abstraction quality. These benefits hold both for guaranteed and non-guaranteed campaigns. The performance stems from three improvements: 1) a quadratic-time (as opposed to doubly exponential or heuristic) algorithm for finding an optimal split of an abstract segment, 2) a better scoring function for evaluating splits, and 3) splitting time lossily like any other targeting attribute (instead of losslessly segmenting time first).", 
    "authors": [
      {
        "name": "Fei Peng"
      }, 
      {
        "name": "Tuomas Sandholm"
      }
    ], 
    "keywords": "channel abstraction, segment abstraction, computational advertising, advertising markets, advertising campaign admission optimization, advertising campaign inventory allocation optimization, combinatorial markets, targeted advertising, column generation, integer programming", 
    "title": "Scalable Segment Abstraction Method for Advertising Campaign Admission and Inventory Allocation Optimization", 
    "type": "paper"
  }, 
  "2568": {
    "abstract": "Ensuring the stability is the most important requirement for the navigation control of multi-robot systems with no reference trajectory. The popular heuristic-search methods cannot provide theoretical guarantees on stability. In this paper, we propose a Hierarchical Model Predictive Control scheme that employs reachable sets to decouple the navigation problem of linear dynamical multi-robot systems. The proposed control scheme guarantees the stability and feasibility and be more efficient and viable than other Model Predictive Control schemes, as evidenced by our simulation results.", 
    "authors": [
      {
        "name": "Chao Huang"
      }, 
      {
        "name": "Xin Chen"
      }, 
      {
        "name": "Yifan Zhang"
      }, 
      {
        "name": "Xuandong Li"
      }, 
      {
        "name": "Shengchao Qin"
      }, 
      {
        "name": "Yifeng Zeng"
      }
    ], 
    "keywords": "Multi-robot System collaboration, hierarchical model predictive control, feasibility, stability", 
    "title": "Hierarchical Model Predictive Control for Navigation of Multi-Robot Systems", 
    "type": "paper"
  }, 
  "2588": {
    "abstract": "Considerable work has focused on modifying the semantics of Hierarchical Task Networks (HTNs) in order to advance the state-of-the-art in hierarchical planning. For instance, the Hierarchical Goal Network ( HGN ) formalism operates over a hierarchy of goals to facilitate tighter integration of decompositional planning with classical planning. Another example is the Action Notation Markup Language (ANML) which adds aspects of generative planning and task-sharing to the standard HTN semantics. The aim of this work is to theoretically analyze the effects that these modifications to HTN semantics have on the computational complexity and ex- pressivity of HTN planning. To facilitate analysis, we unify goal and task planning into Goal-Task Network (GTN) planning. GTN models use HTN and HGN constructs, but have a solution-preserving mapping back to HTN planning. We then show theoretical results that provide new insights into both the expressivity as well as computational complexity of GTN planning under a number of different semantics. Our work lays a firm footing to clarify exact semantics for recent planners based on ANML, HGNs, and similar hierarchical languages.", 
    "authors": [
      {
        "name": "Ron Alford"
      }, 
      {
        "name": "Vikas Shivashankar"
      }, 
      {
        "name": "Mark Roberts"
      }, 
      {
        "name": "Jeremy Frank"
      }, 
      {
        "name": "David Aha"
      }
    ], 
    "keywords": "Automated Planning, Hierarchical Task Networks, Hierarchical Goal Networks, Hierarchical Planning, Theory", 
    "title": "Hierarchical Planning: Relating Task and Goal Decomposition with Task Sharing", 
    "type": "paper"
  }, 
  "2590": {
    "abstract": "Traditional metric learning methods usually make decisions based on a fixed threshold, which may result in a sub-optimal metric when the inter-class and inner-class variations are complex. To address this issue, in this paper we propose an effective metric learning method by exploiting privileged information to relax the fixed threshold under the empirical risk minimization framework. Privileged information describes useful high-level semantic information that is only available during training. Our goal is to improve performance by incorporating privileged information to design a locally adaptive decision function. We jointly learn two distance metrics by minimizing the empirical loss penalizing the difference between the distance in the original space and that in the privileged space. The distance in the privileged space functions as a locally adaptive decision threshold, which can guide the decision making like a teacher. We optimize the objective function using the Accelerated Proximal Gradient approach to obtain a global optimum solution. Experiment results show that by leveraging privileged information, our proposed method can outperform the state-of-the-art methods.", 
    "authors": [
      {
        "name": "Xun Yang"
      }, 
      {
        "name": "Meng Wang"
      }, 
      {
        "name": "Luming Zhang"
      }, 
      {
        "name": "Dacheng Tao"
      }
    ], 
    "keywords": "Metric Learning, Learning Using Privileged Information, machine learning", 
    "title": "Empirical Risk Minimization for Metric Learning Using Privileged Information", 
    "type": "paper"
  }, 
  "2594": {
    "abstract": "Coordinating agents to complete a set of tasks with intercoupled temporal and resource constraints is computationally challenging, yet human domain experts can solve these difficult scheduling problems using paradigms learned through years of apprenticeship. A process for manually codifying this domain knowledge within a computational framework is necessary to scale beyond the one-expert, one-trainee apprenticeship model. However, a human domain expert often has difficulty describing their decision-making process, causing the codification of this knowledge to become laborious. We propose a new approach for capturing domain-expert heuristics through a pairwise ranking formulation. Our approach is model-free and does not require enumerating or iterating through a large state-space. We empirically demonstrate that this approach accurately learns multi-faceted heuristics on both a synthetic data set incorporating job-shop scheduling and vehicle routing problems and a real-world data set consisting of demonstrations of experts solving a variant of the weapon-to-target assignment problem. Our approach is able to learn scheduling policies of superior quality to those generated, on average, by human experts conducting an anti-ship missile defense task.", 
    "authors": [
      {
        "name": "Matthew Gombolay"
      }, 
      {
        "name": "Reed Jensen"
      }, 
      {
        "name": "Jessica Stigile"
      }, 
      {
        "name": "Sung-Hyun Son"
      }, 
      {
        "name": "Julie Shah"
      }
    ], 
    "keywords": "Scheduling, Supervised Learning, Learning from Experts", 
    "title": "Apprenticeship Scheduling: Learning to Schedule from Human Experts", 
    "type": "paper"
  }, 
  "2597": {
    "abstract": "A key component of Mars exploration is the operation of robotic instruments on the surface, such as those on board the Mars Exploration Rovers, the Mars Science Laboratory (MSL), and the planned Mars 2020 Rover. As the instruments carried by these rovers have become more advanced, the area targeted by some instruments becomes smaller, revealing more fine-grained details about the geology and chemistry of rocks on the surface. However, thermal fluctuations, rover settling or slipping, and inherent inaccuracies in pointing mechanisms all lead to pointing error that is on the order of the target size (several millimeters) or larger. We show that given a target located on a previously acquired image, the rover can align this with a new image to visually locate the target and refine the current pointing. Due to round-trip communication constraints, this visual targeting must be done efficiently on board the rover using relatively limited computing hardware. We employ existing ORB features for landmark-based image registration, describe and theoretically justify a novel approach to filtering false landmark matches, and employ a random forest classifier to automatically reject failed alignments. We demonstrate the efficacy of our approach using over 3,800 images acquired by Remote Micro-Imager on board the \"Curiosity\" rover.", 
    "authors": [
      {
        "name": "Gary Doran"
      }, 
      {
        "name": "Tara Estlin"
      }, 
      {
        "name": "David Thompson"
      }
    ], 
    "keywords": "computer vision, machine learning, robotics, space exploration", 
    "title": "Precision Instrument Targeting via Image Registration for the Mars 2020 Rover", 
    "type": "paper"
  }, 
  "2600": {
    "abstract": "Agents learning how to act in new environments can benefit from input of more experienced agents or humans. This paper studies interactive teaching strategies for identifying when a student can benefit from teacher advice in a reinforcement learning framework. In student-teacher learning, a teacher agent can advise the student on which action to take in a given state. Prior works have considered heuristics for the teacher to choose advising opportunities given a limited advice budget. While these approaches effectively accelerate agent training, they assume that the teacher constantly monitors the student. This assumption is not desirable with human teachers, as people incur cognitive costs for monitoring and they might not always pay attention. Therefore, we propose strategies for the teacher and the student to jointly identify advising opportunities so that the teacher is not required to constantly monitor the student. Experimental results show that these approaches reduce the amount of attention required of the teacher while maintaining similar learning gains. Our empirical evaluation also investigates the effect of the information communicated to the teacher and the quality of the initial policy of the student on teaching outcomes.", 
    "authors": [
      {
        "name": "Ofra Amir"
      }, 
      {
        "name": "Ece Kamar"
      }, 
      {
        "name": "Andrey Kolobov"
      }, 
      {
        "name": "Barbara Grosz"
      }
    ], 
    "keywords": "Interactive teaching strategies, Attention budget, Student-teacher learning", 
    "title": "Interactive Teaching Strategies for Agent Training", 
    "type": "paper"
  }, 
  "2602": {
    "abstract": "Building successful recommender systems requires uncovering the underlying dimensions that describe the properties of items as well as users' preferences toward them. In domains like clothing recommendation, explaining users' preferences requires modeling the visual appearance of the items in question. This makes recommendation especially challenging, due to both the complexity and subtlety of people's `visual preferences,' as well as the scale and dimensionality of the data and features involved. Ultimately, a successful model should be capable of capturing considerable variance across different categories and styles, while still modeling the commonalities explained by `global' structures in order to combat the sparsity (e.g. cold-start), variability, and scale of real-world datasets. Here, we address these challenges by building such structures to model the visual dimensions across different product categories. With a novel hierarchical embedding architecture, our method accounts for both high-level (colorfulness, darkness, etc.) and subtle (e.g. casualness) visual characteristics simultaneously.", 
    "authors": [
      {
        "name": "Ruining He"
      }, 
      {
        "name": "Chunbin Lin"
      }, 
      {
        "name": "Jianguo Wang"
      }, 
      {
        "name": "Julian McAuley"
      }
    ], 
    "keywords": "Recommender Systems, Visual Dimensions, Hierarchical Embedding", 
    "title": "Sherlock: Sparse Hierarchical Embeddings for Visually-aware One-class Collaborative Filtering", 
    "type": "paper"
  }, 
  "2603": {
    "abstract": "Peer prediction is the problem of eliciting private, but correlated, information from agents about their environment. The basic technique is to reward an agent for the amount that her report predicts that of another agent, thereby incentivizing effort and truthful reports.  Early mechanisms suffered from multiple equilibria, however, including high-payoff but uninformative equilibria.  Recent approaches seek to promote truthful reporting, e.g., by giving this higher payoff than other strategies.  Adopting replicator dynamics as a model for population learning, we study the extent to which these new designs are robust in promoting truthful reports.  We compare the basin of attraction of the truthful equilibrium on models estimated from real peer evaluations in several massive online courses; among other observations, we confirm that recent mechanisms present a significant robustness improvement over earlier approaches.", 
    "authors": [
      {
        "name": "Victor Shnayder"
      }, 
      {
        "name": "Rafael Frongillo"
      }, 
      {
        "name": "David Parkes"
      }
    ], 
    "keywords": "peer prediction, replicator dynamics, mechanism design, learning dynamics, equilibrium selection", 
    "title": "Measuring performance of peer prediction mechanisms using replicator dynamics", 
    "type": "paper"
  }, 
  "261": {
    "abstract": "Single minded agents have strict preferences, in which a bundle is acceptable if and only if it satisfies a particular demand. Such preferences arise naturally in resource allocation scenarios, such as the problem of allocating computational resources (CPU, memory, bandwidth) among users of an organization, where the goal is to fairly allocate as many requests as possible. The literature on this topic is thin since such valuations are harder to handle due to discontinuity and complementarity. In this paper we explore the problem of fair division among single minded agents.  Our main solution concept -- the ``competitive allocation from equal incomes'' (CAEI) -- is inspired from market equilibria and implements fair outcomes through a pricing mechanism. We study the existence and computation of CAEI for single minded agents in three settings: divisible goods, cake cutting, and discrete goods. For the first two we show existence of CAEI, while for the third we give a succinct characterization of instances that admit this solution; then we give an efficient algorithm to find one in all three cases. Maximizing social welfare turns out to be NP-hard in general, however we obtain efficient algorithms for (i) divisible and discrete goods when the number of different player types is a constant and (ii) cake cutting with contiguous demands.", 
    "authors": [
      {
        "name": "Simina Br\u00e2nzei"
      }, 
      {
        "name": "Yuezhou Lv"
      }, 
      {
        "name": "Ruta Mehta"
      }
    ], 
    "keywords": "fair division, market equilibria, solution concepts, divisible goods, cake cutting, discrete goods, algorithms, complexity", 
    "title": "To Give or Not to Give: Fair Division for Strict Preferences", 
    "type": "paper"
  }, 
  "2612": {
    "abstract": "Team formation is the problem of selecting a group of agents, where each agent has a set of skills; the aim is to accomplish a given mission (a set of tasks), where each task is made precise by a skill necessary for managing it. In a dynamic environment that offers the possibility of losing agents during a mission, e.g., some agents break down, the robustness of a team is crucial. In this paper, the focus is laid on the mission oriented robust multi-team formation problem. A formal framework is defined and two algorithms are provided to tackle this problem, namely, complete and approximate algorithms. In the experiments, these two algorithms are evaluated in RMASBench (a rescue multi-agent benchmarking platform used in the rescue RoboCup competition). We empirically show that (i) the approximate algorithm is more realistic for RMASBench compared to the complete algorithm and (ii) considering the robust mission multi-teams have a better control on the fire spread than the sophisticate solvers provided in RMASBench.", 
    "authors": [
      {
        "name": "Tenda Okimoto"
      }, 
      {
        "name": "Tony Ribeiro"
      }, 
      {
        "name": "Damien Bouchabou"
      }, 
      {
        "name": "Katsumi Inoue"
      }
    ], 
    "keywords": "Multi-agent systems, Team formation, Rescue RoboCup", 
    "title": "Mission Oriented Robust Multi-Team Formation and its Application to Robot Rescue Simulation", 
    "type": "paper"
  }, 
  "2613": {
    "abstract": "The capability to predict changes of spatial regions is important for an intelligent system that interacts with the physical world. For example, in a disaster management scenario, predicting potentially endangered areas and inferring safe zones is essential for planning evacuations and countermeasures. Existing approaches usually predict such spatial changes by simulating the physical world based on specific models. Thus, these simulation-based methods will not be able to provide reliable predictions when the scenario is not similar to any of the models in use or when the input parameters are incomplete.   In this paper, we present a prediction approach that overcomes the aforementioned problem by using a more general model and by analysing the trend of the spatial changes. The method is also flexible to adopt new observations and adapt its prediction to new situations.", 
    "authors": [
      {
        "name": "Xiaoyu Ge"
      }, 
      {
        "name": "Jae Hee Lee"
      }, 
      {
        "name": "Jochen Renz"
      }, 
      {
        "name": "Peng Zhang"
      }
    ], 
    "keywords": "Reasoning about Spatial Change, Evolving (Geo-)Spatial Regions, Prediction, Outer-Approximation", 
    "title": "Trend-Based Prediction of Spatial Change", 
    "type": "paper"
  }, 
  "2615": {
    "abstract": "We present a system based on sequential decision making for the online summarization of massive document streams, such as those found on the web. Given an event of interest (e.g. ``boston marathon bombing''), our system is able to filter the stream for relevance and produce a series of short text updates describing the event as it unfolds over time. Unlike previous work, our approach is able to jointly model the relevance, comprehensiveness, novelty, and timeliness required by time-sensitive queries. We demonstrate a 28.3% improvement in summary F1 and a 43.8% improvement time-sensitive F1 metrics.", 
    "authors": [
      {
        "name": "Chris Kedzie"
      }, 
      {
        "name": "Fernando Diaz"
      }, 
      {
        "name": "Kathleen Mckeown"
      }
    ], 
    "keywords": "summarization, reinforcement learning, sequential decision making, update summarization, streaming summarization", 
    "title": "Real-Time Web Scale Event Summarization Using Sequential Decision Making", 
    "type": "paper"
  }, 
  "2617": {
    "abstract": "Biological adaptation is a powerful mechanism that makes many disorders hard to combat. However, the myopic nature of adaptation means that this adaptation could also be used to improve treatments through careful steering. In this paper we study how adaptation can be exploited through sequential planning for steering the adaptation of a patient's immune system. We propose a general approach where we leverage Monte Carlo tree search to compute a treatment plan, and the biological entity is modeled by a black-box simulator that the planner calls during planning. We apply our framework to a leading T cell simulator (available in the biological modeling package BioNetGen). We run experiments with two alternate goals: developing regulatory T cells or developing effector T cells. The former is important for preventing autoimmune diseases while the latter is associated with better survival rates in cancer patients. We are especially interested in the effect of sequential plans, an approach that has not been explored extensively in the biological literature. We show that for the development of regulatory cells, sequential plans yield significantly higher utility than the best static therapy. In contrast, for developing effector cells, we find that (at least for the given simulator, objective function, action possibilities, and measurement possibilities) single-step plans suffice for optimal treatment.", 
    "authors": [
      {
        "name": "Christian Kroer"
      }, 
      {
        "name": "Tuomas Sandholm"
      }
    ], 
    "keywords": "Monte-Carlo tree search, Steering evolution, Sequential planning", 
    "title": "Sequential planning for steering immune system adaptation", 
    "type": "paper"
  }, 
  "2624": {
    "abstract": "A simple and popular method for estimating counterfactuals and average treatment effects in intervention studies is nearest-neighbor matching. For high-dimensional data, theoretical results show that the bias of a nearest-neighbor matching estimator increases with the dimensionality of the data. To address this problem, we propose a novel estimator that first projects the data to a number of random linear subspaces, and it then estimates the median treatment effect by nearest-neighbor matching in each subspace. We empirically compute the mean square error of the proposed estimator using semi-synthetic data, and we demonstrate the method on real-world digital marketing campaign data. The results show marked improvement over baseline methods.", 
    "authors": [
      {
        "name": "Sheng Li"
      }, 
      {
        "name": "Nikos Vlassis"
      }, 
      {
        "name": "Jaya Kawale"
      }, 
      {
        "name": "Yun Fu"
      }
    ], 
    "keywords": "Nearest-neighbor Matching, Random Projection, Causal Inference, Digital Marketing Campaign", 
    "title": "Matching via Dimensionality Reduction for Estimation of Treatment Effects in Digital Marketing Campaigns", 
    "type": "paper"
  }, 
  "2629": {
    "abstract": "Grounded language learning bridges human language words like `red' and `square' with robot perception. The vast majority of existing work in this space limits perception to robot vision. In this paper, we build perceptual models that use haptic, auditory, and proprioceptive data acquired through robot exploratory behaviors that go beyond vision alone. Our system learns to ground natural language predicates about objects using supervision from an interactive human-robot I, Spy game. In this game, the human and robot take turns describing an object on a table, then trying to guess which object the other has described. All predicate labels were gathered from human subjects physically present to play this game with an embodied robot. We demonstrate that our multi-modal system for grounding natural language outperforms a traditional, vision-only grounding framework by comparing the two on the I, Spy task. We also provide a qualitative analysis of the predicates learned in the game, visualizing groups of predicates that can be understood from the same kinds of robot perception and pointing out predicates for which vision alone is insufficient (e.g. `heavy').", 
    "authors": [
      {
        "name": "Jesse Thomason"
      }, 
      {
        "name": "Jivko Sinapov"
      }, 
      {
        "name": "Maxwell Svetlik"
      }, 
      {
        "name": "Peter Stone"
      }, 
      {
        "name": "Raymond Mooney"
      }
    ], 
    "keywords": "grounded language learning, multi-modal perception, human-robot interaction", 
    "title": "Learning Multi-Modal Grounded Linguistic Semantics by Playing I, Spy", 
    "type": "paper"
  }, 
  "2635": {
    "abstract": "Topic models represent latent topics as probability distributions over words which can be hard to interpret due to the lack of grounded semantics. In this paper, we propose a structured topic representation based on entity taxonomy from a knowledge base. A probabilistic model is developed to infer both hidden topics and entities from text corpora. Each topic is equipped with a random walk over the entity hierarchy to extract semantically grounded and coherent themes. Accurate entity modeling is achieved by leveraging rich textual features from the knowledge base. Experiments show significant superiority of our approach in topic perplexity and key entity identification, indicating potentials of the grounded modeling for semantic extraction and language understanding applications.", 
    "authors": [
      {
        "name": "Zhiting Hu"
      }, 
      {
        "name": "Gang Luo"
      }, 
      {
        "name": "Mrinmaya Sachan"
      }, 
      {
        "name": "Zaiqing Nie"
      }, 
      {
        "name": "Eric Xing"
      }
    ], 
    "keywords": "topic modeling, latent model, grounded semantics, entity taxonomy, knowledge base", 
    "title": "Grounding Topic Models with Knowledge Bases", 
    "type": "paper"
  }, 
  "2639": {
    "abstract": "With the rapid development of data sensing and collection technologies, we can easily obtain large volumes of data (big data). However, big data poses huge challenges to many popular machine learning techniques which take all the data at the same time for processing. To address the big data related challenges, we first partition the data along its feature space, and apply the parallel block coordinate descent algorithm for distributed computation; then, we continue to partition the data along the sample space, and propose a novel matrix decomposition and combination mechanism for distributed processing. The final results from all the entities are guaranteed to be the same as the centralized solution. Extensive experiments performed on Hadoop confirm that our proposed mechanism is superior in terms of both testing errors and convergence rate (computation time) over the canonical distributed machine learning techniques that deal with big data.", 
    "authors": [
      {
        "name": "Hongliang Guo"
      }, 
      {
        "name": "Jie Zhang"
      }
    ], 
    "keywords": "distributed machine learning, big data, least square regression, Hadoop", 
    "title": "A Distributed Large Scale Machine Learning Mechanism for Least Square Problems", 
    "type": "paper"
  }, 
  "2642": {
    "abstract": "This paper proposes a novel framework that enables a robot to learn ordinal object relations. While most related work focuses on classifying objects into discrete categories, such approaches cannot learn object properties (e.g., weight, height, size, etc.) that are context-specific and relative to other objects. To address this problem, we propose that a robot should learn to order objects based on ordinal object relations. In our experiments, the robot explored a set of 32 objects that can be ordered by three properties: height, weight, and width. Next, the robot used unsupervised learning to discover multiple ways that the objects can be ordered based on the haptic and proprioceptive perceptions detected while exploring the objects. Following, the robot's model was presented with labeled object series, allowing it to ground the three ordinal relations in terms of how similar they are to the orders discovered during the unsupervised stage. Finally, the grounded models were used to recognize whether new object series were ordered by any of the three properties as well as to correctly insert additional objects into an existing series.", 
    "authors": [
      {
        "name": "Jivko Sinapov"
      }, 
      {
        "name": "Priyanka Khante"
      }, 
      {
        "name": "Maxwell Svetlik"
      }, 
      {
        "name": "Peter Stone"
      }
    ], 
    "keywords": "Cognitive Robotics, Developmental Robotics, Multi-modal Perception", 
    "title": "Learning to Order Objects using Haptic and Proprioceptive Exploratory Behaviors", 
    "type": "paper"
  }, 
  "2644": {
    "abstract": "Existential rules, also known as data dependencies in Database, have been recently rediscovered as a family of promising languages for Ontology-based Query Answering. In this paper, we prove that disjunctive embedded dependencies exactly capture the class of recursively enumerable ontologies in Ontology-based Conjunctive Query Answering (OCQA). Our expressive completeness result does not rely on any built-in linear order on the database. To establish the result, we introduce a novel semantic definition for OCQA ontologies. We also show that none of the other two kinds of data dependencies: disjunctive tuple-generating dependencies and embedded dependencies, is expressively complete for recursively enumerable OCQA ontologies.", 
    "authors": [
      {
        "name": "Heng Zhang"
      }, 
      {
        "name": "Yan Zhang"
      }, 
      {
        "name": "Jia-Huai You"
      }
    ], 
    "keywords": "Ontology-based Query Answering, Existential Rule Languages, Disjunctive Embedded Dependencies, Expressive Completeness, Expressiveness", 
    "title": "Expressive Completeness of Existential Rule Languages for Ontology-based Query Answering", 
    "type": "paper"
  }, 
  "2646": {
    "abstract": "Multi-Agent Path Finding (MAPF) is a hard combinatorial problem that arises in many important AI applications. Recent work has shown that suboptimal MAPF solvers like Enhanced Conflict-Based Search, ECBS($w_1$), scale better than optimal MAPF solvers. The suboptimality factor $w_1$ stems from the typical use of focal search in such algorithms. In other recent work, Experience Graphs (EGs) have been used to speed up MAPF solvers by incurring a separate suboptimality factor $w_2$ that results from inflating heuristic values. Unfortunately, previous combinations of ECBS and EGs have resulted in a suboptimality bound of $w_1w_2$. In this paper, we present a new suboptimal solver, improved-ECBS($w_1$), that does not inflate the heuristic values but can still use EGs to effectively guide search. Empirical results show that improved-ECBS($w_1$) outperforms ECBS($w_1$). We also present two novel methods for automatically generating EGs for a given MAPF instance. Both these methods are shown to be competitive with human-generated EGs. An empirical evaluation of these MAPF solvers also leads us to the observation of an erratic behavior of the runtime with respect to different EGs and internal orderings of the agents. This prompts us to understand the correlation of the runtime with the size of the minimum vertex cover (MVC) of an underlying agent-interaction graph and thereby improve the runtime by an effective use of randomization and an off-the-shelf MVC solver.", 
    "authors": [
      {
        "name": "Liron Cohen"
      }, 
      {
        "name": "T. K. Satish Kumar"
      }, 
      {
        "name": "Tansel Uras"
      }, 
      {
        "name": "Hong Xu"
      }, 
      {
        "name": "Nora Ayanian"
      }, 
      {
        "name": "Sven Koenig"
      }
    ], 
    "keywords": "Multi-Agent Path Finding, Heuristic Search, Bounded Suboptimal Algorithms", 
    "title": "Improved Bounded-Suboptimal Multi-Agent Path Finding Solvers", 
    "type": "paper"
  }, 
  "2649": {
    "abstract": "Answering science questions posed in natural language is an important AI challenge. Answering such questions often requires non-trivial inference and knowledge that goes beyond factoid retrieval. Yet, most systems for this task are based on relatively shallow Information Retrieval (IR) and statistical correlation techniques operating on large unstructured corpora. We propose a structured inference system for this task, formulated as an Integer Linear Program (ILP), that answers natural language questions using a semi-structured knowledge base derived from text, including questions requiring multi-step inference and a combination of multiple facts. On a dataset of real, unseen science questions, our system significantly outperforms (+14\\%) the best previous attempt at structured reasoning for this task, which used Markov Logic Networks (MLNs). When combined with unstructured inference methods, the ILP system significantly boosts overall performance (+10\\%).  Finally, we show our approach is substantially more robust to question perturbation compared to statistical correlation methods.", 
    "authors": [
      {
        "name": "Daniel Khashabi"
      }, 
      {
        "name": "Tushar Khot"
      }, 
      {
        "name": "Ashish Sabharwal"
      }, 
      {
        "name": "Peter Clark"
      }, 
      {
        "name": "Oren Etzioni"
      }, 
      {
        "name": "Dan Roth"
      }
    ], 
    "keywords": "question answering, ILP, discrete optimization, structured reasoning", 
    "title": "Question Answering via Integer Programming over Semi-Structured Knowledge", 
    "type": "paper"
  }, 
  "266": {
    "abstract": "Multi-label learning has been extensively studied in the area of bioinformatics, information retrieval, multimedia annotation, etc. In multi-label learning, each instance is associated with multiple interdependent class labels, the label information can be noisy and incomplete. In addition, multi-labeled data often has high-dimensional noisy, irrelevant and redundant features. As an effective data preprocessing step, feature selection has shown its effectiveness to prepare high-dimensional data for numerous data mining and machine learning tasks. Most of existing multi-label feature selection algorithms either boil down to solve multiple single-labeled feature selection problems or directly make use of the flawed labels. Therefore, they may not be able to find discriminative features that are shared by multiple labels. In this paper, we propose a novel multi-label informed feature selection framework MIFS, which exploits label correlations to select discriminative features across multiple labels. Specifically, to reduce the negative effects of imperfect label information in finding label correlations, we decompose the multi-label information into a low-dimensional space and then employ the reduced space to steer the feature selection process. Empirical studies on real-world datasets demonstrate the effectiveness and efficiency of the proposed framework.", 
    "authors": [
      {
        "name": "Ling Jian"
      }, 
      {
        "name": "Jundong Li"
      }, 
      {
        "name": "Kai Shu"
      }, 
      {
        "name": "Huan Liu"
      }
    ], 
    "keywords": "Multi-label classification, Sparse feature selection, Supervised learning", 
    "title": "Multi-Label Informed  Feature Selection", 
    "type": "paper"
  }, 
  "2663": {
    "abstract": "Recognizing sources of opinions is an important task in sentiment analysis. Different from previous work which categorize an opinion according to whether the source is the writer or the source is a noun phrase, we propose a new categorization of opinions according to the role that the source plays: a narrator opinion or a participant opinion. The source of a narrator opinion is the narrator of the opinion. The source of a participant opinion is a participant in the event that triggers the opinion. Based on this new categorization, we classify an opinion using phrase-level embeddings. A transductive learning method is used for the classifier since there is no existing annotated corpora of this new categorization. A joint prediction model of Probabilistic Soft Logic then recognizes the sources of different types of opinions in a single model. The experiments have shown that our model improves recognizing sources of opinions over baselines and several state-of-the-art work.", 
    "authors": [
      {
        "name": "Lingjia Deng"
      }, 
      {
        "name": "Janyce Wiebe"
      }
    ], 
    "keywords": "sentiment analysis, opinion mining, natual language processing, joint model, transduction, markov logic network, probabilistic soft logic", 
    "title": "Recognizing Opinion Sources Based on A New Categorization of Opinion Types", 
    "type": "paper"
  }, 
  "2666": {
    "abstract": "We propose solutions for assignment of physical tasks to heterogeneous agents when  the costs of the tasks change over time.  We assume tasks have a natural growth rate which is counteracted by the work applied by agents. As the future cost of a task depends heavily on the assignments of agents, reasoning must be both spatial and temporal to effectively minimize the growth of tasks before they are completed. Our solutions for heterogeneous agents uses a modifications of a Variable-Sized Bin-Packing-Problem to generalize a previous solution to the same problem which works only for homogeneous agents. We analyze and prove optimal solutions for two general categories of growth functions that have not been considered in the literature. Empirical results are given in the RoboCup Rescue Simulator for agents with varying capabilities when the task growth function is an approximation of an unknown growth function.", 
    "authors": [
      {
        "name": "James Parker"
      }, 
      {
        "name": "Maria Gini"
      }
    ], 
    "keywords": "Multi-agent, Task Allocation, RoboCup", 
    "title": "Controlling growing tasks with heterogeneous agents", 
    "type": "paper"
  }, 
  "2672": {
    "abstract": "We consider a controller synthesis problem in turn-based stochastic games with both a qualitative linear temporal logic (LTL) constraint and a quantitative discounted-sum objective. For the cases in which the LTL specification is realizable and can be equivalently transformed into a deterministic Buchi automaton, we show that there always exists a memoryless (possibly randomized) almost-sure winning strategy that is $\\varepsilon$-optimal with respect to the discounted-sum objective for any positive $\\varepsilon$. Building on the idea of the R-MAX algorithm, we propose a probably approximately correct (PAC) learning algorithm that can learn such a strategy efficiently in an online manner with a-priori unknown reward functions and unknown transition distributions. To the best of our knowledge, this is the first result on PAC learning in stochastic games with independent quantitative and qualitative objectives.", 
    "authors": [
      {
        "name": "Min Wen"
      }, 
      {
        "name": "Ufuk Topcu"
      }
    ], 
    "keywords": "reinforcement learning, temporal logic, PAC learning, stochastic two-player games", 
    "title": "Probably Approximately Correct Learning in Stochastic Games with Temporal Logic Specifications", 
    "type": "paper"
  }, 
  "2678": {
    "abstract": "To learn with limited labeled data, active learning tries to query more labels from an oracle, while transfer learning tries to utilize the labeled data from a related source domain. However, in many real cases, there is very few labeled data in both source and target domains, and the oracle is unavailable in the target domain. To solve this practical yet rarely studied problem, in this paper, we jointly perform transfer learning and active learning by querying the most valuable information from the source domain. The computation of importance weights for domain adaptation and the instance selection for active queries are integrated into one unified framework based on distribution matching, which is further solved with alternating optimization. The effectiveness of the proposed method is validated by experiments on 15 datasets for sentiment analysis and text categorization.", 
    "authors": [
      {
        "name": "Sheng-Jun Huang"
      }, 
      {
        "name": "Songcan Chen"
      }
    ], 
    "keywords": "Transfer learning, Active learning, Semi-supervised learning", 
    "title": "Transfer Learning with Active Queries from Source Domain", 
    "type": "paper"
  }, 
  "2679": {
    "abstract": "Mathematical Word Problems (MWPs) are important for training students\u00e2\u0080\u0099 literacy and numeracy skills. They require students to understand and translate the background knowledge represented in the textual descriptions into symbolic notations. Traditionally MWPs have been manually designed. An effective automated MWP generator can significantly benefit education and research. The few existing efforts focus on the natural language aspect of generating MWPs. In contrast, the goal of this work is to efficiently synthesize MWPs that are authentic (i.e., similar to manually written problems), diverse (i.e., covering a wide range of mathematical tasks), and configurable (i.e., varying difficulty levels and solution characteristics). This is challenging because a generated problem needs to both exhibit a well-founded mathematical structure and also an easily understood natural language story. Our key insight to tackle the above challenge is to leverage the important role that dimensional units play in MWPs, both textually and symbolically. In particular, we first synthesize a dimensionally consistent equation and then compose the natural language story via a bottom-up recursive traversal of the equation tree. We have realized our technique and extensively evaluated its efficiency and effectiveness. Results show that the system can generate hundreds of valid problems per second with varying levels of difficulty. More importantly, we show, via a user study with 30 students from a lo- cal middle school, that the generated problems are statistically indistinguishable from actual textbook problems for practice and examination.", 
    "authors": [
      {
        "name": "Ke Wang"
      }, 
      {
        "name": "Zhendong Su"
      }
    ], 
    "keywords": "Mathematics, Computer-Aided Education, Dimensional Units", 
    "title": "Dimensionally Guided Synthesis of Mathematical Word Problems", 
    "type": "paper"
  }, 
  "269": {
    "abstract": "Many conventional statistical machine learning algorithms generalise poorly if distribution bias exists in the datasets. For example, distribution bias arises in the context of domain generalisation, where knowledge acquired from multiple source domains need to be used in a previously unseen target domains. We propose Elliptical Summary Randomisation (ESRand), an efficient domain generalisation approach that comprises of a randomised kernel and elliptical data summarisation. ESRand learns a domain interdependent projection to a latentsubspace that minimises the existing biases to the data while maintaining the functional relationship between domains. In the latent subspace, ellipsoidal summaries replace the samples to enhance the generalisation by further removing bias and noise in the data. Moreover, the summarisation enables large-scale data processing by significantly reducing the size of the data. Through comprehensive analysis, we show that our subspace-based approach outperforms state-of-the-art results on several activity recognition benchmark datasets, while keeping the computational complexity significantly low.", 
    "authors": [
      {
        "name": "Sarah M. Erfani"
      }, 
      {
        "name": "Mahsa Baktashmotlagh"
      }, 
      {
        "name": "Masud Moshtaghi"
      }, 
      {
        "name": "Vinh Nguyen"
      }, 
      {
        "name": "Christopher Leckie"
      }, 
      {
        "name": "James Bailey"
      }, 
      {
        "name": "Kotagiri Ramamohanarao"
      }
    ], 
    "keywords": "Data summarisation, Random projection, Activity recognition", 
    "title": "Elliptical Summary Randomisation for Sensor-based Human Activity Recognition", 
    "type": "paper"
  }, 
  "2690": {
    "abstract": "We present a novel model for movie recommendations using additional visual features extracted from pictural data like posters and still frames, to better understand movies. In particular, several context-based methods for recommendation are shown to be special cases of our proposed framework. Unlike existing context-based approaches, our method can be used to incorporate visual features -- features that are lacking in existing context-based approaches for movie recommendations. In reality, movie posters and still frames provide us with rich knowledge for understanding movies, users' preferences as well. For instance, user may want to watch a movie at the minute when she/he finds some released posters or still frames attractive. Such unique features can not be revealed from rating data or other form of context that being used in most of existing methods. In this paper, we take a step in this direction and investigate both low-level and high-level visual features from the movie posters and  still frames for further improvement of recommendation methods. A comprehensive set of experiments on real world datasets shows that our approach leads to significant improvement over the state-of-the-art methods.", 
    "authors": [
      {
        "name": "Lili Zhao"
      }, 
      {
        "name": "Zhongqi Lu"
      }, 
      {
        "name": "Sinno Jialin Pan"
      }, 
      {
        "name": "Qiang Yang"
      }
    ], 
    "keywords": "recommender system, collaborative filtering, context information, matrix factorization", 
    "title": "Matrix Factorization+ for Movie Recommendation", 
    "type": "paper"
  }, 
  "2708": {
    "abstract": "Current multi-agent navigation approaches typically assume that all agents use the same underlying framework to locally navigate across an environment and avoid collisions with each other. However, such homogeneity assumption does not hold when an agent has to navigate through an unknown environment, since it does not have any information about the types of neighboring agents that will encounter. Different neighbors may employ different navigation routines and try to reach different goals, which prevent the agent from exhibiting collision-free and efficient behavior. In this paper, we address this issue by proposing a Bayesian inference approach to estimate the navigation model and goal of each neighbor, and use this to compute a plan that minimizes collisions and drives the agent to its goal. Simulation experiments performed in many scenarios demonstrate that an agent using our approach computes safer and more time-efficient paths as compared to the ones generated by a state-of-the-art local navigation framework, and by a recently proposed receding-horizon planning technique for multi-agent navigation.", 
    "authors": [
      {
        "name": "Julio Godoy"
      }, 
      {
        "name": "Ioannis Karamouzas"
      }, 
      {
        "name": "Stephen Guy"
      }, 
      {
        "name": "Maria Gini"
      }
    ], 
    "keywords": "Multi-agent navigation, Real-time planning, Plan recognition", 
    "title": "Moving in a Crowd: Safe and Efficient Navigation among Heterogeneous Agents", 
    "type": "paper"
  }, 
  "272": {
    "abstract": "The Mondrian process (MP) produces hierarchical partitions on a product space as a $k$d-tree, which can be served as a flexible yet parsimonious partition prior for relational modeling. Due to the recursive generation of partitions and varying dimensionality of the partition state space, the inference procedure for the MP relational modeling is extremely difficult. The prevalent inference method reversible-jump MCMC for this problem requires a number of unnecessary retrospective steps to transit from one partition state to a very similar one and it is prone to fall into a local optimum. In this paper, we attempt to circumvent these drawbacks by proposing an alternative method for inferring the MP partition structure. Based on the observation that similar cutting rate measures on the partition space lead to similar partition layouts, we propose to impose a nonhomogeneous cutting rate measure on the partition space to control the layouts of the generated partitions -- the original MCMC sampling problem is thus transformed into a Bayesian global optimization problem. The empirical tests demonstrate that Bayesian optimization is able to find better partition structures than MCMC sampling with the same number of partition structure proposals.", 
    "authors": [
      {
        "name": "Yi Wang"
      }, 
      {
        "name": "Bin Li"
      }, 
      {
        "name": "Xuhui Fan"
      }, 
      {
        "name": "Yang Wang"
      }, 
      {
        "name": "Fang Chen"
      }
    ], 
    "keywords": "Mondrian process, Relational modeling, Stochastic partition process", 
    "title": "Bayesian Optimization of Partition Layouts for Mondrian Processes", 
    "type": "paper"
  }, 
  "2723": {
    "abstract": "Phenotyping with electronic health records (EHR) has received much attention in recent years because the phenotyping opens a new way to discover clinically meaningful insights, such as disease progression and disease subtypes without human supervisions. In spite of its potential benefits, the complex nature of EHR often requires more sophisticated methodologies compared with traditional methods. Previous works on EHR-based phenotyping utilized unsupervised and supervised learning methods separately by independently detecting phenotypes and predicting medical risk scores. To improve EHR-based phenotyping by bridging the separated methods, we present Bayesian nonparametric collaborative topic Poisson factorization (BN-CTPF) that is the first nonparametric content-based Poisson factorization and first application of jointly analyzing the phenotye topics and estimating the individual risk scores. BN-CTPF shows better performances in predicting the risk scores when we compared the model with previous matrix factorization and topic modeling methods including a Poisson factorization and its collaborative extensions. Also, BN-CTPF provides faceted views on the phenotype topics by patients\u00e2\u0080\u0099 demographics. Finally, we demonstrate a scalable stochastic variational inference algorithm by applying BN-CTPF to a national-scale EHR dataset.", 
    "authors": [
      {
        "name": "Wonsung Lee"
      }, 
      {
        "name": "Youngmin Lee"
      }, 
      {
        "name": "Heeyoung Kim"
      }, 
      {
        "name": "Il-Chul Moon"
      }
    ], 
    "keywords": "Matrix factorization, Topic modeling, Collaborative filtering, Medical informatics, Bayesian nonparametric model, EHR phenotyping", 
    "title": "Bayesian Nonparametric Collaborative Topic Poisson Factorization for Electronic Health Records-Based Phenotyping", 
    "type": "paper"
  }, 
  "273": {
    "abstract": "Spectral clustering has been playing a vital role in various research areas. Most traditional spectral clustering algorithms comprise two independent stages (i.e., first learning continuous labels and then rounding the learned labels into discrete ones), which may lead to severe information loss and performance degradation. In this work, we study how to achieve discrete clustering as well as reliably generalize to unseen data. We propose a unified spectral clustering scheme which jointly learns discrete clustering labels and robust out-of-sample prediction functions. Specifically, we explicitly enforce a discrete transformation on the intermediate continuous labels, which leads to a tractable optimization problem with a discrete solution. Moreover, to further compensate the unreliability of the learned labels, we integrate an adaptive robust module with $\\ell_{2,p}$ loss to learn prediction function for unseen data. Extensive experiments conducted on various data sets have demonstrated the superiority of our proposal as compared to existing clustering approaches.", 
    "authors": [
      {
        "name": "Yang Yang"
      }, 
      {
        "name": "Fumin Shen"
      }, 
      {
        "name": "Zi Huang"
      }, 
      {
        "name": "Heng Tao Shen"
      }
    ], 
    "keywords": "discrete, spectral clustering, out-of-sample", 
    "title": "A Unified Framework for Discrete Spectral Clustering", 
    "type": "paper"
  }, 
  "2730": {
    "abstract": "In this paper, we consider the popular proportional sharing mechanism and discuss the incentives and opportunities of an agent to lie for personal gains. The main result is a proof that an agent manipulating the proportional sharing mechanism by misreporting its resource amount will not benefit its utility eventually. This result establishes a strategic stability property of the resource exchange protocol. We further illustrate and confirm the result via network examples.", 
    "authors": [
      {
        "name": "Yukun Cheng"
      }, 
      {
        "name": "Xiaotie Deng"
      }, 
      {
        "name": "Qi Qi"
      }, 
      {
        "name": "Xiang Yan"
      }
    ], 
    "keywords": "Resource Exchange, Proportional Sharing Mechanism, Strategic Behavior, Truthfulness", 
    "title": "Truthfulness of a Proportional Sharing Mechanism in Resource Exchange", 
    "type": "paper"
  }, 
  "274": {
    "abstract": "In a multiagent system, a (social) norm describes what the agents may expect from each other. Norms promote autonomy (an agent need not comply with a norm) and heterogeneity (a norm describes interactions at a high level independent of implementation details). Researchers have studied norm emergence through social learning where the agents interact repeatedly in a graph structure. In contrast, we consider norm emergence in an open system, where membership can change, and where no predetermined graph structure exists. We propose Silk, a mechanism wherein a generator monitors interactions among member agents and recommends norms to them for conflict resolution. Each member decides on whether to accept or reject a recommended norm. Upon exiting the system, a member passes its experience along to incoming members. Thus, members develop their norms in a bottom-up manner to resolve conflicts. We evaluate Silk via simulation in the traffic domain. Our results show that social norms promoting conflict resolution emerge in both moderate and selfish society via our hybrid mechanism.", 
    "authors": [
      {
        "name": "Mehdi Mashayekhi"
      }, 
      {
        "name": "Hongying Du"
      }, 
      {
        "name": "George F. List"
      }, 
      {
        "name": "Munindar P. Singh"
      }
    ], 
    "keywords": "multiagent simulation, norms, norm emergence", 
    "title": "Silk: A Simulation Study of Regulating Open Normative Multiagent Systems", 
    "type": "paper"
  }, 
  "2754": {
    "abstract": "Synergistic interactions between task/resource allocation and stochastic planning exist in many environments such as transportation and logistics, UAV task assignment and disaster rescue. Existing research in exploiting these synergistic interactions between the two problems have either only considered domains where tasks/resources are completely independent of each other or have focussed on approaches with limited scalability. In this paper, we address these two limitations by introducing a generic model for task/resource constrained multi-agent stochastic planning, referred to as TasC-MDPs. We provide two scalable greedy algorithms, one of which provides posterior quality guarantees. Finally, we illustrate the high scalability and solution performance of our approaches in comparison with existing work on two benchmark problems from the literature.", 
    "authors": [
      {
        "name": "Pritee Agrawal"
      }, 
      {
        "name": "Pradeep Varakantham"
      }, 
      {
        "name": "William Yeoh"
      }
    ], 
    "keywords": "Task/resource allocation, Stochastic planning, Greedy, Dual decomposition", 
    "title": "Scalable Greedy Algorithms for Task/Resource Constrained Multi-Agent Stochastic Planning", 
    "type": "paper"
  }, 
  "2759": {
    "abstract": "Driven by cognitive film studies and visual perception research, we present a computational framework for the grounding and semantic interpretation of \\emph{dynamic visuo-spatial imagery} consisting of video and eye-tracking data. We demonstrate key technological capabilities aimed at investigating attention and recipient effects vis-a-vis the motion picture; this encompasses high-level analysis of subject's visual fixation patterns and correlating this with semantic analysis of the dynamic visual data (e.g., fixation on movie characters, influence of cinematographic devices such as \\emph{cuts}). As case-study, select examples are presented from an  experiment consisting of $31$ (eye-tracked) subjects. The framework and its application as a general AI-based assistive technology platform  ---integrating vision and KR--- for cognitive film studies is highlighted.", 
    "authors": [
      {
        "name": "Jakob Suchan"
      }, 
      {
        "name": "Mehul Bhatt"
      }
    ], 
    "keywords": "AI and Art, Moving Image - Film, Visual Perception, Eye-Tracking, Cognitive Vision, Semantic Q/A from Video, Cognitive Film Studies", 
    "title": "AI for Cognitive Film Studies - Semantic Q/A with Video and Eye-Tracking Data for Analysing Human Visual Perception", 
    "type": "paper"
  }, 
  "2760": {
    "abstract": "Negotiation is an important focus of artificial intelligence research. While research has emphasized fully-automated negotiations, recent efforts have sought to analyze or engage with human negotiators. Unlike rational frameworks where \u00e2\u0080\u009ctalk is cheap\u00e2\u0080\u009d, information exchange is crucial in human negotiations. By sharing information, negotiators form better models of their opponent, gain insight into the joint structure of the task, and reach more efficient solutions that benefit all. However, some negotiators subvert this process for selfish gain. Recently, an article introduced the misrepresentation game, a game-theoretic analysis of how deceptive agents could manipulate information exchange to gain disproportionate rewards while seeming honest and fair.  Those authors presented a solution to the misrepresentation game and showed it could gain considerable advantage over human negotiators, however the solution and empirical demonstration made strong assumptions that may render the work irrelevant in practice. Here, we evaluate the formalism against a large corpus of human face-to-face negotiations. We confirm that the model is consistent with how people behave in unstructured negotiations. Specifically, deceptive negotiators lie in the ways predicted by the model, and these liars significantly out-performed their honest counterparts. Fortunately, we also show that people give-off hints following these strategies which provides the opportunity for algorithms to detect and defeat this malicious tactic.", 
    "authors": [
      {
        "name": "Zahra Nazari"
      }, 
      {
        "name": "Jonathan Gratch"
      }
    ], 
    "keywords": "Human-Aware AI, Negotiation, Behavioral Game theory", 
    "title": "Predictive models of malicious behavior in human negotiations", 
    "type": "paper"
  }, 
  "2764": {
    "abstract": "We explore the possibility of automatically constructing high-level schemas from low-level sensory data using wearable devices. Here we suggest a hierarchical event network to describe the high-level schemas and a method to learn the network from the low-level data. The proposed network consists of four layers: low-level sensory data, actions, event schemas and scripts. From the low-level sensory data which is collected by wearable devices, the actions are automatically recognized. Spatio-temporal combination of the actions forms the event schemas and the sequence of event schemas form the scripts. The key idea of learning the suggested network is sequential Bayesian integration method which is inspired by the sensory cue integration method of human. For the experiments, we collected sensory data using multiple wearable devices in restaurant situations. The experimental results demonstrate that event schemas can be correctly extracted from the low-level sensory data. Furthermore, the scripts from the highest layer of the network is shown to be analogous to the traditional methods such as SCRIPTs by Schank.", 
    "authors": [
      {
        "name": "Eun-Sol Kim"
      }, 
      {
        "name": "Kyoung-Woon On"
      }, 
      {
        "name": "Byoung-Tak Zhang"
      }
    ], 
    "keywords": "Automatic behavioral schema acquisition, Automatic knowledge extraction, Wearable sensor data, Multimodal deep learning, Hierarchical graphical model", 
    "title": "DeepSchema: Automatic Schema Acquisition from Wearable Sensor Data in Restaurant Situations", 
    "type": "paper"
  }, 
  "2771": {
    "abstract": "With the abundance of video data, the interest in more effective methods for recognizing faces from unconstrained videos has grown. State-of-the-art algorithms for describing an image set use descriptors that are either very high-dimensional and/or sensitive to outliers and image misalignment.  In this paper, we propose to represent image sets as dictionaries of SPD matrices that are more robust to local deformations and outliers. We then learn a tangent map into a lower dimensional Log-Euclidean space such that the atoms of the gallery dictionaries adhere to a more discriminative subspace structure. A probe image set is then classified by first mapping its image SPD descriptors into the computed Log-Euclidean tanget space and using sparse representation over the tangent space to decide a label for the image set. Experiments on three public video datasets show that our method outperforms other state-of-the-art methods.", 
    "authors": [
      {
        "name": "Mohammed E. Fathy"
      }, 
      {
        "name": "Azadeh Alavi"
      }, 
      {
        "name": "Rama Chellappa"
      }
    ], 
    "keywords": "Computer Vision, Image Set Classification, SPD Manifold, Sparse Representation-Based Classification", 
    "title": "Discriminative Log-Euclidean Feature Learning for Sparse Representation-Based Recognition of Faces from Videos", 
    "type": "paper"
  }, 
  "2779": {
    "abstract": "Bike Sharing Systems (BSSs) experience a significant loss in customer demand due to starvation (empty base stations precluding bike pickup) or congestion (full base stations precluding bike return). Therefore, BSSs often reposition bikes between stations on an ad-hoc basis using carrier vehicles. However, due to unpredictable nature of the demand, these myopic repositioning solutions perform unsatisfactorily. We propose an online and robust repositioning approach to minimise the loss in customer demand while considering the possible uncertainty in future demand. We develop a scenario generation approach to compute a strategy of repositioning by assuming that the environment can generate a worse demand scenario (out of the feasible demand scenarios) against the current repositioning solution that maximises the lost demand. Extensive computational results on a simulation built on real world data set of bike sharing company demonstrate that our approaches can significantly reduce the expected lost demand over the existing benchmark approaches.", 
    "authors": [
      {
        "name": "Supriyo Ghosh"
      }, 
      {
        "name": "Michael Trick"
      }, 
      {
        "name": "Pradeep Varakantham"
      }
    ], 
    "keywords": "Robust Repositioning, Bike Sharing System, Optimisation under uncertainty, Online Planning", 
    "title": "Robust Repositioning to Counter Unpredictable Demand in Bike Sharing Systems", 
    "type": "paper"
  }, 
  "2785": {
    "abstract": "In the standard reinforcement learning setting, the agent learns   optimal policy solely from state transitions and rewards from the   environment. We consider an extended setting where a trainer   additionally provides feedback on the actions executed by the   agent. This requires appropriately incorporating the feedback, even   when the feedback is not necessarily accurate. In this paper, we   present a Bayesian approach to this extended reinforcement learning   setting. Specifically, we extend Kalman Temporal Difference learning   to compute the posterior distribution over Q-values given the state   transitions and rewards from the environment as well as the feedback   from the trainer. Through experiments on standard reinforcement   learning tasks, we show that learning performance can be   significantly improved even with inaccurate feedback.", 
    "authors": [
      {
        "name": "Teakgyu Hong"
      }, 
      {
        "name": "Jongmin Lee"
      }, 
      {
        "name": "Kee-Eung Kim"
      }, 
      {
        "name": "Pedro A. Ortega"
      }, 
      {
        "name": "Daniel Lee"
      }
    ], 
    "keywords": "Bayesian reinforcement learning, Kalman filter, behavioral feedback", 
    "title": "Bayesian Reinforcement Learning with Behavioral Feedback", 
    "type": "paper"
  }, 
  "2787": {
    "abstract": "In this paper, we propose the Robust Convex Ensemble Clustering algorithm (RCEC). More specifically, we formulate an ensemble clustering as a nuclear norm and cluster-wise group norm regularization problem, and propose an simple yet efficient optimization algorithm. A key advantage of RCEC over existing algorithms is that it can get rid of anomaly cluster assignments obtained from sub-clustering methods by using group-norm regularization. Moreover, the proposed method is convex and can find a globally optimal solution. Through synthetic experiments, we first illustrate that RCEC can learn a stable cluster assignments from input matrices that includes anomaly clusters. Then, we empirically show that RCEC outperforms state-of-the- art ensemble clustering methods in real-world data sets.", 
    "authors": [
      {
        "name": "Junning Gao"
      }, 
      {
        "name": "Makoto Yamada"
      }, 
      {
        "name": "Samuel Kaski"
      }, 
      {
        "name": "Hiroshi Mamitsuka"
      }, 
      {
        "name": "Shanfeng Zhu"
      }
    ], 
    "keywords": "Unsupervised Learning, Ensemble Clustering, Group norm regularization", 
    "title": "A Robust Convex Formulations for Ensemble Clustering", 
    "type": "paper"
  }, 
  "2792": {
    "abstract": "Neural Machine translation (NMT) has shown promising results in recent years. In order to control the computational complexity, NMT has to employ a small vocabulary, and massive rare words outside the vocabulary are all replaced with a single unk symbol. Besides the inability to translate rare words, this kind of simple approach leads to much increased ambiguity of the sentences since meaningless unks break the structure of sentences, and thus hurts the translation and reordering of the in-vocabulary words. To tackle this problem, we propose a novel substitution-translation-restoration method. In substitution step, the rare words in a testing sentence are replaced with similar in-vocabulary words based on a similarity model learnt from monolingual data. In translation and restoration steps, the sentence will be translated with a model trained on new bilingual data with rare words replaced, and finally the translations of the replaced words will be substituted by that of original ones.  Experiments on Chinese-to-English translation demonstrate that our proposed method can achieve more than 4 BLEU points over the attention-based NMT. When compared to the recently proposed method handling rare words in NMT, our method can also obtain an improvement by nearly 3 BLEU points.", 
    "authors": [
      {
        "name": "Xiaoqing Li"
      }, 
      {
        "name": "Jiajun Zhang"
      }, 
      {
        "name": "Chengqing Zong"
      }
    ], 
    "keywords": "neural machine translation, rare words, similarity", 
    "title": "Towards Zero Unknown Word in Neural Machine Translation", 
    "type": "paper"
  }, 
  "2795": {
    "abstract": "Microblogging platforms can be ideal places for rumors to grow and spread. Automatically debunking rumors is crucial in such online social media contexts. Existing rumor detection approaches typically rely on hand-crafted features when employing machine learning algorithms to differentiate rumors from others. The feature engineering work needs daunting manual effort. When facing a dubious claim, people tend to dispute its truthfulness by posting various cues over time, in which long-distance dependencies of evidence are generated. Yet, these clues are hard to follow using existing approaches. In this research, we present a novel method that learns continuous representations of microblog events for identifying rumors.  We base our model on recurrent neural networks (RNN) for learning the hidden representations and capturing the variation of contextual information of relevant posts over time. Experimental results on microblog datasets indicate that (1) RNN outperforms state-of-the-art rumor detection models that use hand-crafted features; (2) the performance of RNN-based algorithms are further improved by using more sophisticated recurrent units and extra hidden layers; (3) our method can detect rumors more quickly with higher accuracy than the baselines, including the leading online rumor debunking services.", 
    "authors": [
      {
        "name": "Jing Ma"
      }, 
      {
        "name": "Wei Gao"
      }, 
      {
        "name": "Prasenjit Mitra"
      }, 
      {
        "name": "Sejeong Kwon"
      }, 
      {
        "name": "Bernard J. Jansen"
      }, 
      {
        "name": "Kam-Fai Wong"
      }, 
      {
        "name": "Meeyoung Cha"
      }
    ], 
    "keywords": "rumor detection, microblogs, recurrent neural networks, GRU, LSTM", 
    "title": "Detecting Rumors from Microblogs with Recurrent Neural Networks", 
    "type": "paper"
  }, 
  "2801": {
    "abstract": "The models developed to date for knowledge base embedding are all based on the assumption that the relations contained in knowledge bases are binary.For the training and testing of these embedding models, multi-fold (or n-ary) relational data are  converted to triples (e.g., in FB15K dataset) and interpreted as instances of binary relations. This paper presents a canonical representation of knowledge bases containing multi-fold relations. We show that the existing embedding models on the popular FB15K datasets correspond to a sub-optimal modelling framework, resulting in a loss of structural information. We advocate a novel modelling framework, which models multi-fold relations directly using this canonical representation. Using this framework, the existing TransH model is generalized to a new model,  m-TransH. We demonstrate experimentally that  m-TransH outperforms TransH by a large margin, thereby establishing a new state of the art.", 
    "authors": [
      {
        "name": "Jianfeng Wen"
      }, 
      {
        "name": "Jianxin Li"
      }, 
      {
        "name": "Yongyi Mao"
      }, 
      {
        "name": "Shini Chen"
      }, 
      {
        "name": "Richong Zhang"
      }
    ], 
    "keywords": "knowledge graph, embedding, multiple fold relation", 
    "title": "On the Representation and Embedding of Knowledge Bases Beyond Binary Relations", 
    "type": "paper"
  }, 
  "2802": {
    "abstract": "We consider the problem of belief propagation in a network of communicating agents, modeled in the  recently introduced Belief Revision Game (BRG) framework. In this setting, each agent expresses her belief through a propositional formula and revises her own belief at each step by considering the beliefs of her acquaintances, using belief change tools. In this paper, we investigate the extent to which BRGs satisfy some monotonicity properties, i.e., whether \u00e2\u0080\u009cpromoting\u00e2\u0080\u009d some desired piece of belief to a given set of agents is actually always useful for making it accepted by all of them. We formally capture such a concept of promotion by a new family of belief change operators. We show that some basic monotonicity properties are not satisfied by BRGs in general, even when the agent\u00e2\u0080\u0099s merging-based revision policies are \u00e2\u0080\u009cfully rational\u00e2\u0080\u009d (in the AGM sense). We also identify some classes where they hold.", 
    "authors": [
      {
        "name": "Nicolas Schwind"
      }, 
      {
        "name": "Katsumi Inoue"
      }, 
      {
        "name": "Gauvain Bourgne"
      }, 
      {
        "name": "S\u00e9bastien Konieczny"
      }, 
      {
        "name": "Pierre Marquis"
      }
    ], 
    "keywords": "belief change, belief revision, belief merging, belief revision games", 
    "title": "Is Promoting Beliefs Useful to Make Them Accepted in Networks of Agents?", 
    "type": "paper"
  }, 
  "2807": {
    "abstract": "Phrase-based statistical machine translation (SMT) systems have previously been used for the task of grammatical error correction (GEC) to achieve state-of-the-art accuracy. The superiority of SMT systems comes from their ability to learn text transformations from erroneous to corrected text, irrespective of the type of errors. However, phrase-based SMT systems suffer from limitations of discrete word representation, linear mapping, and lack of global context. In this paper, we address these limitations by using two different yet complementary neural network models, namely a neural network global lexicon model and a neural network joint model. These neural networks can generalize better by using continuous space representation of words and learn non-linear mappings. Moreover, they can leverage contextual information from the source sentence more effectively. By adding these two components, we achieve statistically significant improvement in accuracy for grammatical error correction over a state-of-the-art GEC system.", 
    "authors": [
      {
        "name": "Shamil Chollampatt"
      }, 
      {
        "name": "Kaveh Taghipour"
      }, 
      {
        "name": "Hwee Tou Ng"
      }
    ], 
    "keywords": "grammatical error correction, neural network models, educational applications", 
    "title": "Neural Network Translation Models for Grammatical Error Correction", 
    "type": "paper"
  }, 
  "2808": {
    "abstract": "Creating new ties in a social network facilitates transfer of information and gives individuals positional advantage.  In this paper, we study the process, which we call network building, of establishing ties between two existing social networks in order to reach certain structural goals. We investigate the case when one of the two networks consists only of a single member and motivate this case from two perspectives. The first perspective is socialization: we investigate how a newcomer can forge relationships with an existing network to place herself at the center of the network. We prove that obtaining optimal solutions to this problem is NP-complete, and present several efficient algorithms to solve this problem and compare them with each other. The second perspective is network expansion: we investigate how a network may preserve or reduce its diameter through establishing ties with a new node, hence ensuring small distance between its members. We give two algorithms for solving this problem. For both perspectives the experimental analysis demonstrates that a small number of edges is usually required to reach the respective goal.", 
    "authors": [
      {
        "name": "Anastasia Moskvina"
      }, 
      {
        "name": "Jiamou Liu"
      }
    ], 
    "keywords": "social networks, network building, socialization, ties, distance", 
    "title": "How to Build Your Network? A Structural Analysis", 
    "type": "paper"
  }, 
  "2814": {
    "abstract": "Translation ability is known as one of the most difficult language abilities to be measured. A typical way of measuring the translation ability is to ask translators to translate sentences and to request professional evaluators to grade the translations. It imposes a heavy burden on both translators and evaluators. In this paper, we study a practical method for assessing translation ability. Our key idea is to incorporate translators vocabulary knowledge for the translation ability assessment. Our method just asks translators to tell if they know given words or not. Using this vocabulary information, we build a probabilistic model to estimate the translators\u00e2\u0080\u0099 vocabulary ability and translation ability simultaneously. We evaluated our method in a realistic crowdsourcing translation setting in which there is a great need to measure translators\u00e2\u0080\u0099 translation ability to select good translators. The results of our experiments show that the proposed method accurately estimates translation ability and selects translators who have sufficient skills to translate a given sentence. We also find that our method significantly reduces the cost of crowdsourcing translation.", 
    "authors": [
      {
        "name": "Yo Ehara"
      }, 
      {
        "name": "Yukino Baba"
      }, 
      {
        "name": "Masao Utiyama"
      }, 
      {
        "name": "Eiichiro Sumita"
      }
    ], 
    "keywords": "Ability Assessment, Item Response Theory, Crowdsourcing, Translation", 
    "title": "Assessing Translation Ability through Vocabulary Ability Assessment", 
    "type": "paper"
  }, 
  "2820": {
    "abstract": "We propose a new algorithm called DPC+ to enforce partial path consistency (PPC) on qualitative constraint networks. PPC restricts path consistency (PC) to a triangulation of the underlying constraint graph of a network. As PPC retains the sparseness of a constraint graph, it can make reasoning tasks such as consistency checking and minimal labelling of large qualitative constraint networks much easier to tackle than PC. For qualitative constraint networks defined over any distributive subalgebra of well-known spatio-temporal calculi, such as the Region Connection Calculus and the Interval Algebra, we show that DPC+ can achieve PPC very fast. Indeed, the algorithm enforces PPC on a qualitative constraint network by processing each triangle in a triangulation of its underlying constraint graph at most three times. Our experiments demonstrate significant improvements of DPC+ over the state-of-the-art PPC enforcing algorithm.", 
    "authors": [
      {
        "name": "Zhiguo Long"
      }, 
      {
        "name": "Michael Sioutis"
      }, 
      {
        "name": "Sanjiang Li"
      }
    ], 
    "keywords": "qualitative spatial and temporal reasoning, path consistency, chordal graph, distributive subalgebra", 
    "title": "Efficient Path Consistency Algorithm for Large Qualitative Constraint Networks", 
    "type": "paper"
  }, 
  "2821": {
    "abstract": "Market-makers serve an important role as providers of liquidity and order in financial markets, particularly during periods of high volatility. Optimal market-makers solve a sequential decision making problem, where they face an exploration versus exploitation dilemma at each time step. A belief state MDP based solution was presented by Das and Magdon-Ismail (NIPS, 2008). This solution however, was closely tied to the choice of a Gaussian belief state prior and did not take asset inventory into consideration when calculating an optimal policy. In this work we introduce a novel continuous state POMDP framework which is the first to solve, exactly and in closed-form, the optimal market making problem with inventory, arbitrary belief state priors, trader models and reward functions via symbolic dynamic programming. We use this novel model and solution to show that sequentially optimal policies are heavily inventory-dependent and calculate policies that operate with bounded loss guarantees under a variety of market models and conditions.", 
    "authors": [
      {
        "name": "Shamin Kinathil"
      }, 
      {
        "name": "Scott Sanner"
      }, 
      {
        "name": "Sanmay Das"
      }, 
      {
        "name": "Nicolas Della Penna"
      }
    ], 
    "keywords": "Sequential Decision Making, Market Making, Reinforcement Learning", 
    "title": "A Symbolic Closed-form Solution to Sequential Market Making with Inventory", 
    "type": "paper"
  }, 
  "2824": {
    "abstract": "Multi-view learning receives increasing interest in recent years to analyze complex data. Lately, multi-view maximum entropy discrimination (MVMED) and alternative MVMED (AMVMED) were proposed as extensions of maximum entropy discrimination (MED) to the multi-view learning setting, which use the hard margin consistency principle that enforces two view margins to be the same. In this paper, we propose a soft margin consistency based multi-view MED (SMVMED) achieving margin consistency in a less rigorous way that minimizes the relative entropy between the posteriors of two view margins. With a trade-off parameter balancing large margin and margin consistency, SMVMED is more flexible. We also propose a sequential minimal optimization (SMO) algorithm to efficiently train SMVMED and make it scalable to large datasets. We evaluate the performance of SMVMED on multiple real-world datasets and get encouraging results.", 
    "authors": [
      {
        "name": "Liang Mao"
      }, 
      {
        "name": "Shiliang Sun"
      }
    ], 
    "keywords": "maximum entropy discrimination, sequential minimal optimization, multi-view learning, supervised learning", 
    "title": "Soft Margin Consistency Based Scalable Multi-View Maximum Entropy Discrimination", 
    "type": "paper"
  }, 
  "2826": {
    "abstract": "Collaborative filtering has emerged as the de facto approach to personalized recommendation problems. However, a scenario that has proven difficult in practice is the implicit feedback or one-class collaborative filtering case (OC-CF), where one has examples of items that a user prefers, but no examples of items they do not prefer. In such cases, it is desirable to have recommendation algorithms that are personalized, learning-based, and highly scalable. Existing linear recommenders for OC-CF achieve good performance in benchmarking tasks, but they involve solving a large number of a regression subproblems, which limits their applicability to large-scale problems. We demonstrate that it is possible to significantly scale up linear recommenders to big data, by learning an OC-CF model in a randomized low-dimensional embedding of the user-item interaction matrix. Our algorithm, Linear-Flow, achieves state-of-the-art performance in a comprehensive set of experiments on standard benchmarks as well as real data.", 
    "authors": [
      {
        "name": "Suvash Sedhain"
      }, 
      {
        "name": "Hung Bui"
      }, 
      {
        "name": "Jaya Kawale"
      }, 
      {
        "name": "Nikos Vlassis"
      }, 
      {
        "name": "Branislav Kveton"
      }, 
      {
        "name": "Aditya Menon"
      }, 
      {
        "name": "Trung Bui"
      }, 
      {
        "name": "Scott Sanner"
      }
    ], 
    "keywords": "One-Class Collaborative Filtering, Recommender Systems, Randomized SVD", 
    "title": "Practical linear models for large-scale one-class collaborative filtering", 
    "type": "paper"
  }, 
  "2827": {
    "abstract": "With the increasing adoption of Electric Vehicles (EVs) and limited deployment of new charging stations, it is important that existing charging stations are properly utilized. Occupancy by non-EVs (a.k.a. \u00e2\u0080\u009cicing\u00e2\u0080\u009d) and overstaying by EVs (even after charging is complete) are common problems. While enforcement with appropriately heavy fines may help curb icing, a gentler approach is prudent to manage overstaying EVs. Since overstaying is easily detectable by monitoring the power drawn from the charger, one option is to impose an appropriate \u00e2\u0080\u009cpenalty\u00e2\u0080\u009d during the overstaying period. Higher penalties do discourage overstaying; however, due to uncertainty in parking duration, less people would find such penalties acceptable, leading to decreased utilization. This trade-off is the central motivation for this work.  We develop a framework to study this trade-off and locate the optimal penalty from the points of view of utilization and revenue, for different values of the external charging demand. Our framework integrates models for realistic user behavior and queueing dynamics to analyze this trade-off. Next, when the model parameters are unknown, we show how an online learning algorithm such as UCB can be adapted to learn the optimal penalty. To validate the theoretical models, we conduct experiments based on charging data from London. Our experiments show that an appropriate penalty can increase both utilization and revenue while significantly reducing overstaying. Finally, we take a first step towards investigating a combined approach that can leverage partial model-information to \"warm-start\" the learning algorithm to deliver potentially better performance.", 
    "authors": [
      {
        "name": "Arpita Biswas"
      }, 
      {
        "name": "Ragavendran Gopalakrishnan"
      }, 
      {
        "name": "Partha Dutta"
      }
    ], 
    "keywords": "park-and-charge revenue maximization, sustainable behavior, queueing model, multi-armed bandits", 
    "title": "Managing Overstaying Electric Vehicles in Park-and-Charge Facilities", 
    "type": "paper"
  }, 
  "2828": {
    "abstract": "DeepWalk is a typical representation learning method that learns low-dimensional representations for vertices in social networks. Similar to other network representation learning (NRL) models, it encodes the network structure into vertex representations and is learned in unsupervised fashion. However, the learned representations usually lack the ability of discrimination when applied to machine learning tasks, such as vertex classification. In this paper, we overcome this challenge by proposing a novel semi-supervised model, Max-Margin DeepWalk (MMDW). MMDW is a unified NRL framework that jointly optimizes the max-margin classifier and the aimed social representation learning model. Influenced by the max-margin classifier, the learned representations not only contain the network structure, but also have the characteristic of discrimination. The visualizations of learned representations indicate that our model is more discriminative than unsupervised ones, and the experimental results on vertex classification demonstrate that our method achieves a significant improvement than other state-of-the-art methods.", 
    "authors": [
      {
        "name": "Cunchao Tu"
      }, 
      {
        "name": "Weicheng Zhang"
      }, 
      {
        "name": "Zhiyuan Liu"
      }, 
      {
        "name": "Maosong Sun"
      }
    ], 
    "keywords": "network representation learning, max-margin, DeepWalk, vertex classification", 
    "title": "Max-Margin DeepWalk: Discriminative Learning of Network Representation", 
    "type": "paper"
  }, 
  "2830": {
    "abstract": "In this paper, we present a novel task of ordering given concepts (e.g., London, Hawaii, Rome) based on common attribute intensity expressed by a given adjective (e.g., safe) and propose statistical ordering methods that integrate heterogeneous evidence extracted from text on the concept ordering. This study aims at deriving collective wisdom on concept ordering from social media text. Solving this task is not only interesting from a scientific perspective but also beneficial in practical senses for those who want to order unfamiliar entities in terms of subjective attributes that are hard to be quantified in order to make a correct decision. Experiments on real-world concepts revealed a strong correlation between orders given by our method and gold-standard orders.", 
    "authors": [
      {
        "name": "Tatsuya Iwanari"
      }, 
      {
        "name": "Naoki Yoshinaga"
      }, 
      {
        "name": "Nobuhiro Kaji"
      }, 
      {
        "name": "Toshiharu Nishina"
      }, 
      {
        "name": "Masashi Toyoda"
      }, 
      {
        "name": "Masaru Kitsuregawa"
      }
    ], 
    "keywords": "Social-Media Text Mining, Ordering Concepts, Learning for Ranking", 
    "title": "Ordering Concepts Based on Common Attribute Intensity", 
    "type": "paper"
  }, 
  "2831": {
    "abstract": "Grammatical error correction (GEC) is the task of detecting and correcting grammatical errors in texts written by second language learners. The statistical machine translation (SMT) approach to GEC, in which sentences written by second language learners are translated to grammatically correct sentences, has achieved state-of-the-art accuracy. However, the SMT approach is unable to utilize global context. In this paper, we propose a novel approach to improve the accuracy of GEC, by exploiting the n-best hypotheses generated by an SMT approach. Specifically, we build a classifier to score the edits in the n-best hypotheses. The classifier can be used to select appropriate edits or re-rank the n-best hypotheses. We apply these methods to a state-of-the-art GEC system that uses the SMT approach. Our experiments show that our methods achieve statistically significant improvements in accuracy over the best published results on a benchmark test dataset on GEC.", 
    "authors": [
      {
        "name": "Duc Tam Hoang"
      }, 
      {
        "name": "Shamil Chollampatt"
      }, 
      {
        "name": "Hwee Tou Ng"
      }
    ], 
    "keywords": "grammatical error correction, n-best hypotheses, educational applications", 
    "title": "Exploiting N-Best Hypotheses to Improve an SMT Approach to Grammatical Error Correction", 
    "type": "paper"
  }, 
  "2832": {
    "abstract": "A Virtual Best Solver (VBS) is a hypothetical algorithm that selects the best solver from a given portfolio of alternatives on a per-instance basis. The VBS idealizes performance when all solvers in a portfolio are run in parallel, and is also a valuable bound on the performance of portfolio-based algorithm selectors. Typically, VBS performance is measured by running every solver in a portfolio once on a given instance and reporting the best performance over all solvers. In this paper, we argue that this results in a flawed measure that is biased to reporting better performance when randomized solvers are present in an algorithm portfolio. Specifically, this flawed notion of VBS tends to show performance better than that achievable by a perfect selector that for each given instance runs the solver with the best expected running time. We report results from an empirical study using solvers and instances submitted to several SAT Competitions, in which we demonstrate this bias. We also show that the bias increases with the number of randomized solvers and decreases as we average solver performance over many independent runs per instance. We propose an alternative VBS measure by (1) empirically obtaining the best expected solver for each instance and (2) taking bootstrap samples over the best expected solver for every instance, reporting a confidence interval on VBS performance. Our findings shed new light on widely studied algorithm selection benchmarks and help explain performance gaps observed between VBS and state-of-the-art algorithm selection approaches.", 
    "authors": [
      {
        "name": "Chris Cameron"
      }, 
      {
        "name": "Holger Hoos"
      }, 
      {
        "name": "Kevin Leyton-Brown"
      }
    ], 
    "keywords": "Algorithm Selection, Algorithm Portfolios, Virtual Best Solver, Randomized Algorithms", 
    "title": "Bias in Algorithm Portfolio Performance Evaluation", 
    "type": "paper"
  }, 
  "2833": {
    "abstract": "Effective placement of charging stations plays a key role in Electric Vehicle (EV) adoption. In the placement problem, given a set of candidate sites, an optimal subset needs to be selected with respect to the concerns of both (a) the charging station service provider, such as the demand at the candidate sites and the budget for deployment, and (b) the EV user, such as charging station reachability and short waiting times at the station. This work addresses these concerns, making the following three novel contributions: (i) a supervised multi-view learning framework using Canonical Correlation Analysis (CCA) for demand prediction at candidate sites, using multiple datasets such as points of interest information, traffic density, and the historical usage at existing charging stations; (ii) a \u00e2\u0080\u009cmixed-packing-and-covering\u00e2\u0080\u009d optimization framework that models competing concerns of the service provider and EV users, which is also extended to optimize government grant allocation to multiple service providers; (iii) an iterative heuristic to solve these problems by alternately invoking knapsack and set cover algorithms. The performance of the demand prediction model and the placement optimization heuristic are evaluated using real world data. In most cases, and especially when budget is scarce, our heuristic achieves an improvement of 10-20% over a naive heuristic, both in terms of finding feasible solutions and maximizing demand.", 
    "authors": [
      {
        "name": "Ragavendran Gopalakrishnan"
      }, 
      {
        "name": "Arpita Biswas"
      }, 
      {
        "name": "Alefiya Lightwala"
      }, 
      {
        "name": "Skanda Vasudevan"
      }, 
      {
        "name": "Partha Dutta"
      }, 
      {
        "name": "Abhishek Tripathi"
      }
    ], 
    "keywords": "Electric Vehicle Charging Stations, Mixed Packing and Covering Problem, Facility Location, Canonical Correlation Analysis, Multi-View Learning, Multivariate Regression", 
    "title": "Demand Prediction and Placement Optimization for Electric Vehicle Charging Stations", 
    "type": "paper"
  }, 
  "2835": {
    "abstract": "Identifying soon-to-be-popular items as early as possible through social networking services offers important benefits. We propose a machine learning based framework that attempts to identify the users who can find prospective popular items. Such visionary users are called observers. By adding the observes to a favorite user list, they act to find popular items. Our approach uses a classifier to predict item popularity, where the input features are a set of users who adopted an item before others. By training the classifier with sparse and non-negative constraints, only features corresponding to observers become non-zero values. We develop an efficient learning algorithm based on stochastic optimization. In experiments, we test our framework using real social bookmark datasets. The results demonstrate that our approach can find popular items in advance more effectively than the baseline methods.", 
    "authors": [
      {
        "name": "Takuya Konishi"
      }, 
      {
        "name": "Tomoharu Iwata"
      }, 
      {
        "name": "Kohei Hayashi"
      }, 
      {
        "name": "Ken-Ichi Kawarabayashi"
      }
    ], 
    "keywords": "User recommendation, Popularity prediction, Feature selection", 
    "title": "Identifying Key Observers to Find Popular Information in Advance", 
    "type": "paper"
  }, 
  "285": {
    "abstract": "Torso joints can be considered as the landmarks of the human body. An action consists of a series of body poses which are determined by the positions of the joints. With the rapid development of RGB-D camera technique and pose estimation research, the acquisition of the joints position has become much easier than before. Thus, we propose to incorporate joints position information with currently popular deep-learned features for action recognition. In this paper, we present a simple, yet effective method to aggregate convolutional activations of a 3D deep convolutional neural network (3D CNN) into discriminative descriptors based on the constraint of joints position. Two pooling schemes for mapping body joints into convolutional feature maps are discussed. The joints-pooled 3D deep convolutional descriptors (JDDs) are more effective and robust than the original 3D CNN features and other competing features. We evaluate the proposed descriptors on recognizing both short actions and complex activities. Experimental results on real-world datasets show that our method generates promising results, outperforming state-of-the-art results significantly.", 
    "authors": [
      {
        "name": "Congqi Cao"
      }, 
      {
        "name": "Yifan Zhang"
      }, 
      {
        "name": "Chunjie Zhang"
      }, 
      {
        "name": "Hanqing Lu"
      }
    ], 
    "keywords": "body joints, convolutional networks, coordinate mapping, action recognition", 
    "title": "Action Recognition with Joints-Pooled 3D Deep Convolutional Descriptors", 
    "type": "paper"
  }, 
  "2861": {
    "abstract": "The word embedding vectors obtained from neural word embedding methods, such as vLBL models and SkipGram, have now become an important fundamental resource for tackling a wide variety of tasks in artificial intelligence field.  This paper focuses on a fact that the model size of high-quality embedding vectors becomes relatively large, i.e., more than 1GB.  Thus, this paper proposes a learning framework that can provide a set of `compact' embedding vectors for the purpose of enhancing the `usability' in actual use.  Our proposed method incorporates parameter sharing constraints into optimization problem.   By the effect of our additional constraints, the embedding vectors are forced to share the parameter values that allows us to significantly shrink the model.  We investigate the trade-off between quality and model size of embedding vectors on several linguistic benchmark data, and show that our method can significantly reduce the model size while maintaining the task performance in the same level with conventional methods.", 
    "authors": [
      {
        "name": "Jun Suzuki"
      }, 
      {
        "name": "Masaaki Nagata"
      }
    ], 
    "keywords": "neural word embedding, compact modeling, constrained optimization", 
    "title": "Learning Compact Neural Word Embeddings by Parameter Space Sharing", 
    "type": "paper"
  }, 
  "2865": {
    "abstract": "Abstraction heuristics are a popular method to guide optimal search algorithms in classical planning. Cost partitionings, like the recently proposed saturated cost partitioning, allow to sum heuristic estimates that are derived from a collection of Cartesian abstractions admissibly. We present saturated state-dependent cost partitioning, and show that it dominates saturated cost partitioning by taking the context into account in which costs are incurred. Furthermore, we show that it is not dominated by optimal cost partitioning and describe optimal state-dependent cost partitioning that is a combination of both concepts and dominates both.", 
    "authors": [
      {
        "name": "Thomas Keller"
      }, 
      {
        "name": "Florian Pommerening"
      }, 
      {
        "name": "Jendrik Seipp"
      }, 
      {
        "name": "Florian Gei\u00dfer"
      }, 
      {
        "name": "Robert Mattm\u00fcller"
      }
    ], 
    "keywords": "classical planning, abstraction heuristics, cost partitioning, Cartesian abstraction, state-dependent action costs", 
    "title": "State-dependent Cost Partitionings for Cartesian Abstractions in Classical Planning", 
    "type": "paper"
  }, 
  "2873": {
    "abstract": "Object ranking is a problem that involves ordering given objects by aggregating pairwise comparison data collected from one or more evaluators; however, the cost of object evaluations is frequently high. In this paper, we propose an efficient data collection method called progressive comparison, the objective of which is to collect many pairwise comparison data while reducing the number of evaluations. We also propose active learning methods to determine which object should be evaluated next in the progressive comparison; that is, we propose two measures of expected model changes, one considering the changes in the evaluation score distributions and the other the changes in the winning probabilities. The results of experiments using a synthetic dataset and two real crowdsourced datasets demonstrate that the progressive comparison method achieves high estimation accuracy with a smaller number of evaluations than the standard pairwise comparison method, and that the active learning methods further reduce the number of evaluations as compared with a random sampling method.", 
    "authors": [
      {
        "name": "Ryusuke Takahama"
      }, 
      {
        "name": "Toshihiro Kamishima"
      }, 
      {
        "name": "Hisashi Kashima"
      }
    ], 
    "keywords": "Rank Aggregation, Pairwise Comparison, Crowdsourcing, Bradley-Terry Model", 
    "title": "Progressive Comparison for Ranking Estimation", 
    "type": "paper"
  }, 
  "2877": {
    "abstract": "Timed Failure Propagation Graphs (TFPGs) are used in the design of safety-critical systems as a way of modeling failure propagation, and to evaluate and implement diagnostic systems. TFPGs are a very rich formalism: they allow to model Boolean combinations of faults and events, also dependent on the operational modes of the system and quantitative delays between them. TFPGs are mostly produced manually, from a given dynamic system of greater complexity. In this paper we present a technique to automate the construction of TFPGs. It takes as input a set of failure mode and discrepancy nodes and builds the graph on top of them, based on an exhaustive analysis of all system behaviors. The result is a TFPG that accurately represents the sequences of failures and their effects as they appear in the system model. The proposed approach is evaluated on a number of synthetic and industrial benchmarks.", 
    "authors": [
      {
        "name": "Benjamin Bittner"
      }, 
      {
        "name": "Marco Bozzano"
      }, 
      {
        "name": "Alessandro Cimatti"
      }
    ], 
    "keywords": "symbolic model checking, model-based safety assessment, timed failure propagation graphs, diagnosis", 
    "title": "Automated Synthesis of Timed Failure Propagation Graphs", 
    "type": "paper"
  }, 
  "2881": {
    "abstract": "In many domains a generalized plan can only be effectively computed if high-level state features are available, i.e.~features that capture important concepts needed to distinguish between abstract states in order to make accurate decisions. In most applications of generalized planning, such high-level features are hand-coded. This paper introduces a novel method to generate high-level state features in the form of conjunctive queries for solving generalized planning tasks. Our method is an extension of a previous compilation of generalized planning into classical planning that tightly integrates the computation of features with the computation of generalized plans. Experiments show that we are able to compute high-level state features for different domains and hence, generate generalized plans without providing a prior high-level representation of the states. In addition, our approach naturally models binary classification tasks in which both the examples and the classifier are represented in predicate logic, bringing a new landscape of challenging benchmarks to classical planning.", 
    "authors": [
      {
        "name": "Damir Lotinac"
      }, 
      {
        "name": "Javier Segovia"
      }, 
      {
        "name": "Sergio Jimenez"
      }, 
      {
        "name": "Anders Jonsson"
      }
    ], 
    "keywords": "Classical planning, Generalized planning, Planning and learning", 
    "title": "Automatic Generation of High-Level State Features for Generalized Planning", 
    "type": "paper"
  }, 
  "2884": {
    "abstract": "In this paper, we study an expressive fragment, namely $\\mathcal{L}_{\\mu}$, of linear time $\\mu$-calculus as a high-level goal specification language. We define Goal Progression Form (GPF) for $\\mathcal{L}_{\\mu}$ formulas and show that every closed formula can be transformed into this form. Based on GPF, we present the notion of Goal Progression Form Graph (GPG) which can be used to describe models of a formula. Further, we propose a simple and intuitive GPG-based decision procedure for checking satisfiability of $\\mathcal{L}_{\\mu}$ formulas which runs in $2^{O(|\\phi|)}$. This makes the decision problem of $\\mathcal{L}_{\\mu}$ and Linear Temporal Logic (LTL) have the same complexity. However, $\\mathcal{L}_{\\mu}$ is able to express a wider variety of temporal goals compared with LTL.", 
    "authors": [
      {
        "name": "Yao Liu"
      }, 
      {
        "name": "Zhenhua Duan"
      }, 
      {
        "name": "Cong Tian"
      }
    ], 
    "keywords": "linear time $\\mu$-calculus, goal specification language, goal progression form, goal progression form graph, decision procedure, satisfiability", 
    "title": "Decision Procedure for a Fragment of Linear Time Mu-Calculus", 
    "type": "paper"
  }, 
  "2887": {
    "abstract": "Choosing a language for knowledge representation and reasoning involves a trade-off between two competing desiderata: succinctness (the encoding should be small) and tractability (the language should support efficient reasoning algorithms). The area of knowledge compilation is devoted to the systematic study of representation languages along these two dimensions. In particular, it aims to determine the relative succinctness of representations. Showing that one representation is more succinct than another typically involves proving a nontrivial lower bound on the encoding size of a carefully chosen function, and the corresponding arguments increase in difficulty with the succinctness of the target language. In this paper, we introduce a general technique for obtaining lower bounds on Decomposable Negation Normal Form (DNNF), one of the most widely studied and succinct representation languages, by establishing a connection between the size of DNNFs and communication complexity. This connection allows us to prove lower bounds by leveraging  lower bounds from the communication complexity literature. We illustrate the potential of this approach by proving exponential separations of DNNFs from deterministic DNNFs and of CNF formulas from DNNFs.", 
    "authors": [
      {
        "name": "Simone Bova"
      }, 
      {
        "name": "Florent Capelli"
      }, 
      {
        "name": "Stefan Mengel"
      }, 
      {
        "name": "Friedrich Slivovsky"
      }
    ], 
    "keywords": "Knowledge Compilation, DNNF, Communication Complexity", 
    "title": "Knowledge Compilation Meets Communication Complexity", 
    "type": "paper"
  }, 
  "29": {
    "abstract": "Entity recommendation, providing entity suggestions relevant to the query that a user is searching for, has become a key feature of today\u00e2\u0080\u0099s web search engine. Despite the fact that related entities are relevant to users' search queries, sometimes users cannot easily understand the recommended entities without evidences. This paper proposes a statistical model consisting of four sub-models to generate evidences for entities, which can help users better understand each recommended entity, and figure out the connections between the recommended entities and a given query. The experiments show that our method is domain independent, and can generate catchy and interesting evidences in the application of entity recommendation.", 
    "authors": [
      {
        "name": "Jizhou Huang"
      }, 
      {
        "name": "Shiqi Zhao"
      }, 
      {
        "name": "Shiqiang Ding"
      }, 
      {
        "name": "Haiyang Wu"
      }, 
      {
        "name": "Mingming Sun"
      }, 
      {
        "name": "Haifeng Wang"
      }
    ], 
    "keywords": "Entity recommendation, Recommendation Evidences, SMT, Statistical Machine Translation, Translation Models", 
    "title": "Generating Recommendation Evidence using Translation Model", 
    "type": "paper"
  }, 
  "2902": {
    "abstract": "Finite state controllers (FSCs) are an effective way to represent sequential plans compactly. By imposing appropriate conditions on transitions, FSCs can also represent generalized plans that solve a range of planning problems from a given domain. In this paper we introduce the concept of hierarchical FSCs for planning by allowing controllers to call other controllers. We show that hierarchical FSCs can represent generalized plans more compactly than individual FSCs. The call mechanism also makes it possible to generate hierarchical FSCs in a modular fashion, or even to apply recursion. We then introduce a compilation that takes as input a set of planning problems from a given domain and produces as output a single planning problem, whose solution corresponds to an hierarchical FSC. In experiments, this compilation enables an off-the-shelf classical planner to generate hierarchical FSCs that solve challenging generalized planning problems.", 
    "authors": [
      {
        "name": "Javier Segovia"
      }, 
      {
        "name": "Sergio Jimenez"
      }, 
      {
        "name": "Anders Jonsson"
      }
    ], 
    "keywords": "Generalized planning, Classical planning, Finite State controller", 
    "title": "Hierarchical Finite State Controllers for Generalized Planning", 
    "type": "paper"
  }, 
  "2903": {
    "abstract": "Similarly to the classical AI planning, the Atari 2600 games supported in the Arcade Learning Environment all feature a fully observable (RAM) state and actions that have deterministic effect. At the same time, the problems in ALE are given only implicitly, via a simulator, precluding exploiting most of the modern classical planning techniques. Despite that, Lipovetzky et al.~(2015) recently showed how online planning for Atari-like problems can be effectively addressed using $IW(i)$, a blind state-space search algorithm that employs a certain form of similarity-based pruning. We show that the effectiveness of the blind state-space search for Atari-like online planning can be pushed even further by focusing the search based on both structural state similarity and the relative myopic value of the states. We also show that the planning effectiveness can be further improved by considering online planning for the Atari games as a multiarmed bandit style competition between the various actions available at the state planned for, and not purely as a classical planning style action sequence optimization problem.", 
    "authors": [
      {
        "name": "Alexander Shleyfman"
      }, 
      {
        "name": "Alexander Tuisov"
      }, 
      {
        "name": "Carmel Domshlak"
      }
    ], 
    "keywords": "planning, game playing, Atari, iterated width", 
    "title": "Blind Search for Atari-like Online Planning Revisited", 
    "type": "paper"
  }, 
  "2910": {
    "abstract": "The behavior composition problem involves synthesizing a controller to coordinate a set of available modules so as to implement a desired, but non-existent, target behavior. In standard behavior composition, the available modules are limited to sequential execution. In this paper, we extend behavior composition to manufacturing environments in which a controller needs to coordinate a number of available modules (the machines on the shop floor) so as to realize an unbounded number of target components (the products), which must conform to a given specification (blueprint). Critically, the machines in the factory work in parallel, which significantly increases the complexity of the control task. We characterize the fundamental differences of this setting with respect to the standard composition problem, and propose a sound and complete technique for synthesizing manufacturing controllers.", 
    "authors": [
      {
        "name": "Paolo Felli"
      }, 
      {
        "name": "Brian Logan"
      }, 
      {
        "name": "Sebastian Sardina"
      }
    ], 
    "keywords": "behavior composition, simulation, synthesis of controllers", 
    "title": "Parallel Behavior Composition for Manufacturing", 
    "type": "paper"
  }, 
  "292": {
    "abstract": "", 
    "authors": [
      {
        "name": "Martin Aleksandrov"
      }
    ], 
    "keywords": "Multi-agent systems, Social choice, Resource allocation, Fair division, Mechanism design", 
    "title": "Online Fair Division Redux", 
    "type": "poster"
  }, 
  "2948": {
    "abstract": "Operator-counting is a recently developed framework for analysing and integrating many state-of-the-art heuristics for planning using Linear Programming. In cost-optimal planning only the objective value of these heuristics is traditionally used to guide the search. However the primal solution, i.e. the operator counts, contains useful information. We exploit this information using a SAT-based approach which given an operator-count, either finds a valid plan; or generates a generalized landmark constraint violated by that count. We show that these generalized landmarks can be used to encode the perfect heuristic, h*, as a Mixed Integer Program. Our most interesting experimental result is that finding or refuting a sequence for an operator-count is most often empirically efficient, enabling a novel and promising approach to planning based on Logic-Based Benders Decomposition (LBBD).", 
    "authors": [
      {
        "name": "Toby Davies"
      }, 
      {
        "name": "Adrian Pearce"
      }, 
      {
        "name": "Peter J. Stuckey"
      }, 
      {
        "name": "Nir Lipovetzky"
      }
    ], 
    "keywords": "Planning, Operator Sequencing, Logic Based Benders Decomposition", 
    "title": "Sequencing Operator Counts", 
    "type": "talk"
  }, 
  "2949": {
    "abstract": "Datatypes and codatatypes are useful to represent finite and potentially infinite objects. We describe a decision procedure to reason about such types. The procedure has been integrated into CVC4, a modern SMT (satisfiability modulo theories) solver, which can be used both as a constraint solver and as an automatic theorem prover. An evaluation based on formalizations developed in the Isabelle proof assistant shows the potential of the procedure.", 
    "authors": [
      {
        "name": "Andrew Reynolds"
      }, 
      {
        "name": "Jasmin Christian Blanchette"
      }
    ], 
    "keywords": "Decision procedures, Satisfiability modulo theories (SMT), Datatypes, Codatatypes", 
    "title": "A Decision Procedure for (Co)datatypes in SMT Solvers", 
    "type": "talk"
  }, 
  "2951": {
    "abstract": "Designing industrial robot systems for welding, painting, and assembly, is challenging because they must perform with high precision, speed, and endurance. ABB Robotics has specialized in building highly reliable and safe robotized paint systems using an integrated process control system. However, current validation practices are mainly limited to manual test scenarios, which makes it difficult to exercise important aspects of a paint robot system, such as the need to coordinate the timing of paint activation with the robot motion control.  To address these challenges, we have developed and deployed a cost-effective, automated test generation technique aimed at validating the timing behavior of the process control system. The approach is based on a constraint optimization model written in Prolog. This model has been integrated into an automated continuous integration environment, allowing the model to be solved on demand prior to test execution, which allows us to obtain the most optimal and diverse set of test scenarios for the current system configuration.", 
    "authors": [
      {
        "name": "Morten Mossige"
      }, 
      {
        "name": "Arnaud Gotlieb"
      }, 
      {
        "name": "Hein Meling"
      }
    ], 
    "keywords": "Constraint programming, industrial robot, software testin", 
    "title": "Generating Tests for Robotized Painting Using Constraint Programming", 
    "type": "talk"
  }, 
  "2952": {
    "abstract": "Tackling the decision-making problem faced by a prosumer (i.e., a producer that is simultaneously a consumer) when selling and buying energy in the emerging smart electricity grid, is of utmost importance  for the economic profitability of such a business entity. In this work, we model, for the first time, this problem as a factored Markov  Decision Process. By so doing, we are able to represent  the problem compactly, and provide an exact  optimal solution via dynamic programming\u00e2\u0080\u0094 notwithstanding its large size. Our model success- fully captures the main aspects of the business decisions  of a prosumer corresponding to a community microgrid of any size. Moreover, it includes appropriate  sub-models for prosumer production and consumption prediction. Experimental simulations verify the effectiveness of our approach; and show that our exact value iteration solution matches that of a state-of-the-art method for stochastic planning in very large environments, while outperforming it in terms of computation time.", 
    "authors": [
      {
        "name": "Angelos Angelidakis"
      }, 
      {
        "name": "Georgios Chalkiadakis"
      }
    ], 
    "keywords": "energy, smart grid, factored MDPs, decision-making", 
    "title": "Optimal Prosumer Decision-Making using Factored MDPs", 
    "type": "talk"
  }, 
  "2953": {
    "abstract": "", 
    "authors": [
      {
        "name": "Felipe Leno Da Silva"
      }
    ], 
    "keywords": "Transfer Learning, Multiagent Reinforcement Learning, Multiagent Systems, Markov Decision Processes", 
    "title": "Transfer Learning for Multiagent Reinforcement Learning Systems", 
    "type": "poster"
  }, 
  "2955": {
    "abstract": "In this paper, we describe proximal gradient temporal difference learning, which provides a principled way for designing and analyzing true stochastic gradient temporal difference learning algorithms. We show how gradient TD  (GTD) reinforcement learning methods can be formally derived,  not with respect to their original objective functions as previously attempted, but rather with respect to primal-dual saddle-point objective functions. We also conduct a saddle-point error analysis to obtain finite-sample bounds on their performance.  Previous analyses of this class of algorithms use stochastic approximation techniques to prove asymptotic convergence,  and no finite-sample analysis had been  attempted.  An accelerated algorithm is also proposed, namely GTD2-MP, which use proximal ``mirror maps'' to yield acceleration.  The results of our theoretical analysis imply that the GTD family of algorithms are  comparable and may indeed be preferred over existing least squares TD methods for off-policy learning, due to their linear complexity. We provide experimental results showing the improved performance of our accelerated gradient TD methods.", 
    "authors": [
      {
        "name": "Bo Liu"
      }, 
      {
        "name": "Mohammad Ghavamzadeh"
      }, 
      {
        "name": "Ian Gemp"
      }, 
      {
        "name": "Ji Liu"
      }, 
      {
        "name": "Sridhar Mahadevan"
      }
    ], 
    "keywords": "reinforcement learning, temporal difference learning, stochastic optimization, proximal method, mirror descent, saddle point, primal-dual method, operator splitting", 
    "title": "Proximal Gradient Temporal Difference Learning Algorithms", 
    "type": "talk"
  }, 
  "2956": {
    "abstract": "", 
    "authors": [
      {
        "name": "Diana Troanca"
      }
    ], 
    "keywords": "Polyadic Formal Concept Analysis, Answer-Set Programming, Conceptual Navigation", 
    "title": "Conceptual Visualization and Navigation Methods for Polyadic Formal Concept Analysis", 
    "type": "poster"
  }, 
  "2957": {
    "abstract": "A binary CSP instance satisfying the broken-triangle property (BTP) can be solved in polynomial time. Unfortunately, in practice, few instances satisfy the BTP. We show that a local version of the BTP allows the merging of domain values in binary  CSPs, thus providing a novel polynomial-time reduction operation. Experimental trials on benchmark instances demonstrate a significant decrease in instance size for certain classes of problems. We show that BTP-merging can be generalised to instances with constraints of arbitrary arity.   A directional version of the general-arity BTP then allows us to extend the BTP tractable class previously defined only for binary CSP.", 
    "authors": [
      {
        "name": "Martin Cooper"
      }, 
      {
        "name": "Achref El Mouelhi"
      }, 
      {
        "name": "Cyril Terrioux"
      }, 
      {
        "name": "Bruno Zanuttini"
      }
    ], 
    "keywords": "CSP, constraint satisfaction, domain reduction, tractable class, hybrid tractability", 
    "title": "On Broken Triangles", 
    "type": "talk"
  }, 
  "2958": {
    "abstract": "", 
    "authors": [
      {
        "name": "Andrew Cropper"
      }
    ], 
    "keywords": "program synthesis, inductive programming, relational learning", 
    "title": "Logic-based inductive synthesis of efficient programs", 
    "type": "poster"
  }, 
  "296": {
    "abstract": "Constraint answer set programming is a promising research direction that integrates answer set programming with constraint processing. It is often informally related to the field of Satisfiability Modulo Theories. Yet, the exact formal link is obscured as the terminology and concepts used in these two research areas differ. In this paper, we make the link between these two areas precise.", 
    "authors": [
      {
        "name": "Yuliya Lierler"
      }, 
      {
        "name": "Benjamin Susman"
      }
    ], 
    "keywords": "constraint answer set programming, constraint satisfaction processing, satisfiability modulo theories", 
    "title": "Constraint Answer Set Programming versus Satisfiability Modulo Theories Or Constraints versus Theories", 
    "type": "paper"
  }, 
  "2962": {
    "abstract": "", 
    "authors": [
      {
        "name": "Xingyu Su"
      }
    ], 
    "keywords": "Diagnosis, Discrete Event Systems, Diagnosability, On-line Diagnosis, Knowledge Representation and Reasoning", 
    "title": "Extended Abstract: Time Decomposition for Diagnosis of Discrete Event Systems", 
    "type": "poster"
  }, 
  "2966": {
    "abstract": "", 
    "authors": [
      {
        "name": "Evgenii Balai"
      }
    ], 
    "keywords": "Knowledge Representation, Probabilistic Reasoning, Logic programming, Answer Set Programming", 
    "title": "Combining Logic and Probability: P-log Perspective", 
    "type": "poster"
  }, 
  "2967": {
    "abstract": "", 
    "authors": [
      {
        "name": "Bhagya N. Wickramasinghe"
      }
    ], 
    "keywords": "agent based models, integrating, combining, reusing, reusing agent based models", 
    "title": "Integrating Heterogeneous Multi-Agent Component Models to Build Complex Simulations", 
    "type": "poster"
  }, 
  "2968": {
    "abstract": "", 
    "authors": [
      {
        "name": "Fei Mi"
      }
    ], 
    "keywords": "Adaptive machine learning, Personalized sequential recommendation, Context tree recommender, Massive open online courses (MOOCs)", 
    "title": "Adaptive Sequential Recommendation using Context Trees", 
    "type": "poster"
  }, 
  "2969": {
    "abstract": "", 
    "authors": [
      {
        "name": "Sheng Li"
      }
    ], 
    "keywords": "robust data representation, low-rank modeling, data analytics", 
    "title": "Learning Robust Representations for Data Analytics", 
    "type": "poster"
  }, 
  "2970": {
    "abstract": "", 
    "authors": [
      {
        "name": "Dingwen Zhang"
      }
    ], 
    "keywords": "visual understanding, unsupervised learning, weakly-supervised learning", 
    "title": "Towards Intelligent Visual Understanding under Minimal Supervision", 
    "type": "poster"
  }, 
  "2971": {
    "abstract": "", 
    "authors": [
      {
        "name": "Gulnar Mehdi"
      }
    ], 
    "keywords": "Knowledge Modeling, Semantics, Data Analytics, Diagnostics, Complex Systems, Ontology-base Solution", 
    "title": "Semantic Framework for Industrial Analytics and Diagnostics", 
    "type": "poster"
  }, 
  "2972": {
    "abstract": "", 
    "authors": [
      {
        "name": "Zeynep G\u00f6zen Saribatur"
      }
    ], 
    "keywords": "reactive policies, action languages, verification", 
    "title": "Reactive Policy Checking for Action Languages", 
    "type": "poster"
  }, 
  "2973": {
    "abstract": "", 
    "authors": [
      {
        "name": "Elias Khalil"
      }
    ], 
    "keywords": "branch-and-bound, tree search, integer programming, machine learning, multi-armed bandits", 
    "title": "Machine Learning for Integer Programming", 
    "type": "poster"
  }, 
  "2974": {
    "abstract": "", 
    "authors": [
      {
        "name": "Antonela Tommasel"
      }
    ], 
    "keywords": "Machine Learning, Online Feature Selection, Short-text Classification, Social Networks", 
    "title": "Integrating Social Network Structure into Online Feature Selection", 
    "type": "poster"
  }, 
  "2975": {
    "abstract": "", 
    "authors": [
      {
        "name": "John Winder"
      }
    ], 
    "keywords": "transfer learning, lifelong learning, anomaly detection, formal concept analysis, representation learning, conceptual clustering, case-based reasoning", 
    "title": "A Framework for Anomaly Reasoning: Interpretation through Concept Formation for Knowledge Transfer and Lifelong Learning", 
    "type": "poster"
  }, 
  "2976": {
    "abstract": "", 
    "authors": [
      {
        "name": "Alberto Camacho"
      }
    ], 
    "keywords": "Automated Planning, Planning Under Uncertainty, Non-Deterministic Planning, Probabilistic Planning, Temporally Extended Goals", 
    "title": "Planning Under Uncertainty with Temporally Extended Goals", 
    "type": "poster"
  }, 
  "2977": {
    "abstract": "", 
    "authors": [
      {
        "name": "Bilal Kartal"
      }
    ], 
    "keywords": "Monte Carlo tree search, UCT, Stochastic tree search, Multi-agent planning, Multi-robot planning, Multi-robot task allocation, Computer narrative generation", 
    "title": "Stochastic Planning in Large Search Spaces", 
    "type": "poster"
  }, 
  "2978": {
    "abstract": "", 
    "authors": [
      {
        "name": "Decebal Constantin Mocanu"
      }
    ], 
    "keywords": "Network Science, Complex Networks, Artificial Intelligence, Deep Learning, Restricted Boltzmann Machine, Swarm Intelligence, Image/Video quality assessment, Computer Vision", 
    "title": "On the synergy of network science and artificial intelligence", 
    "type": "poster"
  }, 
  "2979": {
    "abstract": "", 
    "authors": [
      {
        "name": "Elaine Short"
      }
    ], 
    "keywords": "Socially Assistive Robotics, Human-Robot Interaction, Multi-Party Interaction, Moderation, Multi-Party Human-Robot Interaction", 
    "title": "Socially Assistive Robot Moderators: Validation and Future Directions", 
    "type": "poster"
  }, 
  "2980": {
    "abstract": "", 
    "authors": [
      {
        "name": "Josep Valls-Vargas"
      }
    ], 
    "keywords": "computational narrative, natural language processing, information extraction", 
    "title": "Automated Narrative Information Extraction Using Non-Linear Pipelines", 
    "type": "poster"
  }, 
  "2981": {
    "abstract": "", 
    "authors": [
      {
        "name": "Ramanuja Simha"
      }
    ], 
    "keywords": "Machine Learning, Multi-label Classification, Bioinformatics, Probabilistic Generative Model, Bayesian Networks", 
    "title": "Improved Multi-label Learning and its Application to Protein Location Prediction", 
    "type": "poster"
  }, 
  "2982": {
    "abstract": "", 
    "authors": [
      {
        "name": "Ping Hou"
      }
    ], 
    "keywords": "Probabilistic Planning, Markov Decision Process, Partially Observable Markov Decision Process, Utility Theory, Risk-Sensitive", 
    "title": "Probabilistic Planning with Risk-Sensitive Criterion", 
    "type": "poster"
  }, 
  "2985": {
    "abstract": "", 
    "authors": [
      {
        "name": "Jasmin Grosinger"
      }
    ], 
    "keywords": "Goal autonomy, Planning, Deliberation, Proactivity, Robotics", 
    "title": "Proactivity in Robots", 
    "type": "poster"
  }, 
  "2986": {
    "abstract": "", 
    "authors": [
      {
        "name": "David Winer"
      }
    ], 
    "keywords": "partial-order planning, narrative reasoning, discourse plan", 
    "title": "BiPOCL: A Discourse-Driven Story Planner for Procedural Narrative Generation (Extended Abstract)", 
    "type": "poster"
  }, 
  "2988": {
    "abstract": "", 
    "authors": [
      {
        "name": "Ankit Anand"
      }
    ], 
    "keywords": "Symmetries, MDPs, Monte Carlo Tree Search, Inference", 
    "title": "Lifting Techniques for Sequential Decision Making and Probabilistic Inference", 
    "type": "poster"
  }, 
  "2989": {
    "abstract": "", 
    "authors": [
      {
        "name": "Sam Snodgrass"
      }
    ], 
    "keywords": "Procedural Content Generation, PCG, Markov chains, Machine Learning", 
    "title": "General Statistical Approaches to Procedural Map Generation", 
    "type": "poster"
  }, 
  "2990": {
    "abstract": "We study online boosting, the task of converting any weak online learner into a strong online learner. Based on a novel and natural definition of weak online learnability, we develop two online boosting algorithms. The first algorithm is an online version of boost-by-majority. By proving a matching lower bound, we show that this algorithm is essentially optimal in terms of the number of weak learners and the sample complexity needed to achieve a specified accuracy. The second algorithm is adaptive and parameter-free, albeit not optimal.", 
    "authors": [
      {
        "name": "Alina Beygelzimer"
      }, 
      {
        "name": "Satyen Kale"
      }, 
      {
        "name": "Haipeng Luo"
      }
    ], 
    "keywords": "online learning, boosting, online boosting, classification", 
    "title": "Optimal and Adaptive Algorithms for Online Boosting", 
    "type": "talk"
  }, 
  "2991": {
    "abstract": "", 
    "authors": [
      {
        "name": "Aaron Isaksen"
      }
    ], 
    "keywords": "human-like artificial intelligence, player modeling, game design, computational creativity, survival analysis", 
    "title": "Computer-Aided Game Design: Doctoral Consortium Research Abstract", 
    "type": "poster"
  }, 
  "2993": {
    "abstract": "", 
    "authors": [
      {
        "name": "Przemys\u0142aw Andrzej Wa\u0142\u0119ga"
      }
    ], 
    "keywords": "Spatial Reasoning, Spatio-Temporal Reasoning, Answer Set Programming Modulo Theories, Commonsense Reasoning", 
    "title": "Reasoning about Space and Change with Answer Set Programming Modulo Theories", 
    "type": "poster"
  }, 
  "2995": {
    "abstract": "", 
    "authors": [
      {
        "name": "Claudia Perez-D'Arpino"
      }
    ], 
    "keywords": "Motion Prediction, Human-Robot Interaction, Classification, Robotics", 
    "title": "Fast Motion Prediction For Collaborative Robotics", 
    "type": "poster"
  }, 
  "2996": {
    "abstract": "", 
    "authors": [
      {
        "name": "Ciaran McCreesh"
      }
    ], 
    "keywords": "combinatorial search, constraint programming, parallel search, variable and value ordering heuristics, graph algorithms, subgraph isomorphism, maximum clique, maximum common subgraph", 
    "title": "Solving Hard Subgraph Problems in Parallel", 
    "type": "poster"
  }, 
  "2997": {
    "abstract": "", 
    "authors": [
      {
        "name": "Faiza Khan Khattak"
      }
    ], 
    "keywords": "Human Computation, Multiple labels, Crowd-labeling, Expert-labels, Frequentist approach, Bayesian approach", 
    "title": "Toward a Robust and Universal Crowd-labeling Framework", 
    "type": "poster"
  }, 
  "2999": {
    "abstract": "", 
    "authors": [
      {
        "name": "Peter Krafft"
      }
    ], 
    "keywords": "collective behavior, computational social science, multiagent systems", 
    "title": "Multiagent Models of Human Collective Behavior", 
    "type": "poster"
  }, 
  "3001": {
    "abstract": "", 
    "authors": [
      {
        "name": "Liron Cohen"
      }
    ], 
    "keywords": "Heuristic Search, Multi-Agent Path Finding, Bounded Suboptimal Algorithms", 
    "title": "Bounded Suboptimal Multi-Agent Path Finding Using Highways", 
    "type": "poster"
  }, 
  "3002": {
    "abstract": "", 
    "authors": [
      {
        "name": "Julio Godoy"
      }
    ], 
    "keywords": "multi agent navigation, multi agent motion planning, multi agent planning, multi agent coordination", 
    "title": "Action Selection Methods for Multi-Agent Navigation in Crowded Environments", 
    "type": "poster"
  }, 
  "3003": {
    "abstract": "", 
    "authors": [
      {
        "name": "Zohreh Alavi"
      }
    ], 
    "keywords": "planning, goal reasoning, cognitive systems", 
    "title": "Rational-based Visual Planning Monitors", 
    "type": "poster"
  }, 
  "3005": {
    "abstract": "", 
    "authors": [
      {
        "name": "Dustin Dannenhauer"
      }
    ], 
    "keywords": "goal driven autonomy, anomaly detection, metacognition", 
    "title": "Self Monitoring, Goal Driven Autonomy Agents", 
    "type": "poster"
  }, 
  "3006": {
    "abstract": "", 
    "authors": [
      {
        "name": "Caner Komurlu"
      }
    ], 
    "keywords": "Dynamic Bayesian Networks, Probabilistic Graphical Models, Active Inference, Uncertainty Measurement, Spatio-temporal Models", 
    "title": "Active Inference on Dynamic Bayesian Networks", 
    "type": "poster"
  }, 
  "3007": {
    "abstract": "", 
    "authors": [
      {
        "name": "Chris Kedzie"
      }
    ], 
    "keywords": "summarization, abstractive text generation, natural language processing, stream processing", 
    "title": "Extractive and Abstractive Event Summarization over Streaming Web Text", 
    "type": "poster"
  }, 
  "3009": {
    "abstract": "", 
    "authors": [
      {
        "name": "Banafsheh Rekabdar"
      }
    ], 
    "keywords": "Spiking Neuron Network, Synaptic Plasticity, Unsupervised Learning, Classification, Early Detection, Real-time recognition", 
    "title": "Are Spiking Neural Networks Useful for Classifying and Early Recognition of Spatio-Temporal Patterns?", 
    "type": "poster"
  }, 
  "301": {
    "abstract": "Policy advice is a transfer learning method where a student agent is able to learn faster via advice from a teacher agent. However, both this and other current reinforcement learning transfer methods have little theoretical analysis. This paper formally defines a setting where multiple teacher agents can provide advice to a student and introduces an algorithm to leverage both autonomous exploration and the teacher's advice. Regret bounds justify the intuition that \u00e2\u0080\u009cgood teachers help while bad teachers hurt.\u00e2\u0080\u009d Using our formalization, we are also able to quantify, for the first time, when negative transfer can occur within such a reinforcement learning setting.", 
    "authors": [
      {
        "name": "Yusen Zhan"
      }, 
      {
        "name": "Haitham Bou Ammar"
      }, 
      {
        "name": "Matthew Taylor"
      }
    ], 
    "keywords": "Transfer Learning, Reinforcement Learning, Teaching", 
    "title": "Theoretically-Grounded Policy Advice from Multiple Teachers in Reinforcement Learning Settings with Applications to Negative Transfer", 
    "type": "paper"
  }, 
  "3010": {
    "abstract": "", 
    "authors": [
      {
        "name": "Arpit Sharma"
      }
    ], 
    "keywords": "Natural Language Understanding, Semantic Parsing, Commonsense knowledge Acquisition, Winograd Schema Challenge", 
    "title": "Towards Understanding Natural Language: Semantic Parsing, Commonsense Knowledge Acquisition and Applications", 
    "type": "poster"
  }, 
  "3011": {
    "abstract": "", 
    "authors": [
      {
        "name": "Valentin Mayer-Eichberger"
      }
    ], 
    "keywords": "Boolean Satisfiability, Constraint Programming, SAT Solving, Pseudo-Boolean Constraint, CNF", 
    "title": "Application for Doctoral Consortium: Modelling Satisfiability Problems ; Theory and Practice", 
    "type": "poster"
  }, 
  "3012": {
    "abstract": "", 
    "authors": [
      {
        "name": "Negar Ghourchian"
      }
    ], 
    "keywords": "Human mobility modelling, Unsupervised learning, Hierarchical Dirichlet Processes", 
    "title": "Location Data Mining with Hierarchical Dirichlet Process", 
    "type": "poster"
  }, 
  "3013": {
    "abstract": "", 
    "authors": [
      {
        "name": "Daqing Yi"
      }
    ], 
    "keywords": "Multi-objective path-planning, Homotopy-based optimal path-planning, Human-robot interaction, Language understanding", 
    "title": "Instructing Quantitative Path-Planning by Qualitative Language Expression", 
    "type": "poster"
  }, 
  "3015": {
    "abstract": "", 
    "authors": [
      {
        "name": "Steven Damer"
      }
    ], 
    "keywords": "Game Theory, Cooperation, Machine Learning", 
    "title": "An Approach to Cooperation in General-sum Normal Form Games", 
    "type": "poster"
  }, 
  "3018": {
    "abstract": "We present an original approach to compute efficient mid-term fleet configurations at the request of a Queensland-based long-haul trucking carrier. Our approach considers one year\u00e2\u0080\u0099s worth of demand data, and employs a constraint programming (CP) model and an adaptive large neighbourhood search (LNS) scheme to solve the underlying multi-day multi-commodity split delivery capacitated vehicle routing problem.", 
    "authors": [
      {
        "name": "Tommaso Urli"
      }, 
      {
        "name": "Philip Kilby"
      }
    ], 
    "keywords": "Vehicle Routing Problem, Fleet Size and Mix, Large Neighbourhood Search, Constraint Programming, Pre-processing", 
    "title": "Fleet Design Optimisation From Historical Data Using Constraint Programming and Large Neighbourhood Search", 
    "type": "talk"
  }, 
  "3020": {
    "abstract": "We present the AIX platform - an AI eXperimentation platform built on top of the popular computer game Minecraft, and designed to support fundamental research in artificial intelligence. As the AI research community pushes for artificial general intelligence (AGI), experimentation platforms are needed that support the development of flexible agents that learn to solve diverse tasks in complex environments. Minecraft is an ideal foundation for such a platform, as it exposes agents to complex 3D worlds, coupled with infinitely varied game-play.   The AIX platform provides a sophisticated abstraction layer on top of Minecraft that supports a wide range of experimentation scenarios, ranging from navigation and survival to collaboration and problem solving tasks. In this demo we present the AIX platform and its capabilities. The platform is publicly released as open source software at IJCAI, to support openness and collaboration in AI research.", 
    "authors": [
      {
        "name": "Katja Hofmann"
      }, 
      {
        "name": "Matthew Johnson"
      }, 
      {
        "name": "David Bignell"
      }, 
      {
        "name": "Tim Hutton"
      }
    ], 
    "keywords": "artificial intelligence, experimentation, benchmarks, reinforcement learning", 
    "title": "AI eXperimentation with the AIX platform", 
    "type": "demo"
  }, 
  "3021": {
    "abstract": "Because planning with a long horizon (i.e., looking far into the future) is computationally expensive, it is common in practice to save time by using reduced horizons. This is usually understood to come at the expense of computing suboptimal plans, which is the case when the planning model is exact. However, when the planning model is estimated from data, as is frequently true in the real world, the policy found using a shorter planning horizon can actually be better than a policy learned with the true horizon. In this paper we provide a precise explanation for this phenomenon based on principles of learning theory. We show formally that the planning horizon is a complexity control parameter for the class of policies available to the planning algorithm, having an intuitive, monotonic relationship with a simple measure of complexity. We prove a planning loss bound predicting that shorter planning horizons can reduce overfitting and improve test performance, and we confirm these predictions empirically.", 
    "authors": [
      {
        "name": "Nan Jiang"
      }, 
      {
        "name": "Alex Kulesza"
      }, 
      {
        "name": "Satinder Baveja"
      }, 
      {
        "name": "Richard Lewis"
      }
    ], 
    "keywords": "reinforcement learning, over-fitting, discount factor", 
    "title": "The Dependence of Effective Planning Horizon on Model Accuracy", 
    "type": "talk"
  }, 
  "3022": {
    "abstract": "We present a method and an associated system, called MATHCHECK, that embeds the functionality of a computer algebra system (CAS) within the inner loop of a conflict-driven clause-learning SAT solver. SAT+CAS systems, a la MATHCHECK, can be used as an assistant by mathematicians to either counterexample or finitely verify open universal conjectures on any mathematical topic (e.g., graph and number theory, algebra, geometry, etc.) supported by the underlying CAS system. Such a SAT+CAS system combines the efficient search routines of modern SAT solvers, with the expressive power of CAS, thus complementing both. The key insight behind the power of the SAT+CAS combination is that the CAS system can help cut down the search-space of the SAT solver, by providing learned clauses that encode theory-specific lemmas, as it searches for a counterexample to the input conjecture. We demonstrate the efficacy of our approach on a long-standing open conjecture regarding matchings of hypercubes.", 
    "authors": [
      {
        "name": "Ed Zulkoski"
      }, 
      {
        "name": "Vijay Ganesh"
      }, 
      {
        "name": "Krzysztof Czarnecki"
      }
    ], 
    "keywords": "SAT, SMT, CAS, Graphs, Hypercubes, Matchings", 
    "title": "MathCheck: A Math Assistant based on a Combination of Computer Algebra Systems and SAT Solvers", 
    "type": "talk"
  }, 
  "3023": {
    "abstract": "The fast advancement in sensor data acquisition and communication technology greatly facilitates the collection of data from taxis, and thus enables analyzing the citywide taxi service system. In this paper, we present a novel and practical system for taxi service monitoring, analytics and visualization. By utilizing both of the buffered streaming and the large-size historical taxi data, the system focuses on wait time estimation (for both passengers and taxi drivers), citywide taxi pickup/dropoff hotspots, as well as the taxi trip distributions. The three-dimensional (3D) visualization is designed for users to access the analytics results and understand the characteristics of the taxi service.", 
    "authors": [
      {
        "name": "Yu Lu"
      }, 
      {
        "name": "Gim Guan Chua"
      }, 
      {
        "name": "Huayu Wu"
      }, 
      {
        "name": "Clement Shi Qi Ong"
      }
    ], 
    "keywords": "Taxi Service, Data Mining, Data Analytics, Smart City", 
    "title": "An Intelligent System for Taxi Service Monitoring, Analytics and Visualization", 
    "type": "demo"
  }, 
  "3024": {
    "abstract": "The logic programming language Prolog uses a resource-efficient SLD resolution strategy for query answering. Yet, its propensity for nontermination seriously detracts from the language\u00e2\u0080\u0099s declarative nature. This problem is remedied by tabling, a modified execution strategy that allows a larger class of programs to terminate. Unfortunately, few Prolog systems provide tabling, because the documented implementation techniques are complex, low-level and require a prohibitive engineering effort.  To enable more widespread adoption, this paper presents a novel implementation of tabling for Prolog that is both high-level and compact. It comes in the form of a Prolog library that weighs in at under 600 lines of code, is based on delimited control and delivers reasonable performance.", 
    "authors": [
      {
        "name": "Benoit Desouter"
      }, 
      {
        "name": "Marko van Dooren"
      }, 
      {
        "name": "Tom Schrijvers"
      }, 
      {
        "name": "Alexander Vandenbroucke"
      }
    ], 
    "keywords": "tabling, tabulation, delimited continuation, Prolog, logic programming", 
    "title": "Tabling as a Library with Delimited Control", 
    "type": "talk"
  }, 
  "3025": {
    "abstract": "We investigate the effects of market making on market performance, focusing on allocative efficiency as well as gains from trade accrued by background traders. We employ empirical simulation-based methods to evaluate heuristic strategies for market makers as well as background investors in a variety of complex trading environments. Our market model incorporates private and common valuation elements, with dynamic fundamental value and asymmetric information. In this context, we compare the surplus achieved by background traders in strategic equilibrium, with and without a market maker. Our findings indicate that the presence of the market maker strongly tends to increase total welfare across a variety of environments. Market-maker profit may or may not exceed the welfare gain, thus the effect on background investor surplus is ambiguous. We find that market making tends to benefit investors in relatively thin markets, and situations where background traders are impatient, due to limited trading opportunities. Introducing additional market makers increases these benefits, as competition drives market makers to provide liquidity at lower price spreads.", 
    "authors": [
      {
        "name": "Elaine Wah"
      }, 
      {
        "name": "Mason Wright"
      }, 
      {
        "name": "Michael Wellman"
      }
    ], 
    "keywords": "market maker, agent-based simulation, allocative efficiency", 
    "title": "Welfare Effects of Market Making in Continuous Double Auctions (Extended Abstract)", 
    "type": "talk"
  }, 
  "3026": {
    "abstract": "In recent years, there has been considerable progress on fast randomized algorithms that approximate probabilistic inference with tight tolerance and confidence guarantees. The idea here is to formulate inference as a counting task over an annotated propositional theory, called weighted model counting (WMC), which can be partitioned into smaller tasks using universal hashing. An inherent limitation of this approach, however, is that it only admits the inference of discrete probability distributions. In this work, we consider the problem of approximating inference tasks for a probability distribution defined over discrete and continuous random variables. Building on a notion called weighted model integration, which is a strict generalization of WMC and is based on annotating Boolean and arithmetic constraints, we show how probabilistic inference in hybrid domains can be put within reach of  hashing-based WMC solvers. Empirical evaluations demonstrate the applicability and promise of the proposal.", 
    "authors": [
      {
        "name": "Vaishak Belle"
      }, 
      {
        "name": "Guy Van den Broeck"
      }, 
      {
        "name": "Andrea Passerini"
      }
    ], 
    "keywords": "approximate model counting, probabilistic inference, hybrid graphical models, universal hash functions", 
    "title": "Hashing-Based Approximate Probabilistic Inference in Hybrid Domains:  An Abridged Report", 
    "type": "talk"
  }, 
  "3027": {
    "abstract": "We present a new domain model acquisition algorithm, LOP,  that  induces  static  predicates  by  using a combination of the generalised output from LOCM2 and a set of optimal plans as input to thelearning system.  We observe that static predicates can  be  seen as  restrictions  on  the  valid  groundings  of  actions.   Without  the  static  predicates restricting possible groundings, the domains induced by LOCM2 produce plans that are typically shorterthan the true optimal solutions. LOP works by finding a set of minimal static predicates for each operator that preserves the length of the optimal plan.", 
    "authors": [
      {
        "name": "Peter Gregory"
      }, 
      {
        "name": "Stephen Cresswell"
      }
    ], 
    "keywords": "domain model acquisition, planning, learning in planning", 
    "title": "Domain Model Acquisition in the Presence of Static Relations in the LOP System", 
    "type": "talk"
  }, 
  "3028": {
    "abstract": "Projection can be seem as a unifying concept that underlies inference in logic and consistency maintenance in constraint programming. This perspective allows one to import projection methods into both areas, resulting in deeper insight as well as faster solution methods. We show that inference in propositional logic can be achieved by Benders decomposition, an optimization method based on projection. In constraint programming, viewing consistency maintenance as projection suggests a new but natural concept of consistency that is achieved by projection onto a subset of variables. We show how to solve this combinatorial projection problem for some global constraints frequently used in constraint programming. The resulting projections are useful when propagated through decision diagrams rather than the traditional domain store.", 
    "authors": [
      {
        "name": "John Hooker"
      }
    ], 
    "keywords": "projection, consistency, inference, constraint programming", 
    "title": "Projection, Inference and Consistency", 
    "type": "talk"
  }, 
  "3029": {
    "abstract": "We present a novel approach to enrich classification trees with the representation learning ability of deep (neural) networks within an end-to-end trainable architecture. We combine these two worlds via a stochastic and differentiable decision tree model, which steers the formation of latent representations within the hidden layers of a deep network. The proposed model differs from conventional deep networks in that a decision forest provides the final predictions and it differs from conventional decision forests by introducing a principled, joint and global optimization of split and leaf node parameters. Our approach compares favourably to other state-of-the-art deep models on a large-scale image classification task like ImageNet.", 
    "authors": [
      {
        "name": "Peter Kontschieder"
      }, 
      {
        "name": "Madalina Fiterau"
      }, 
      {
        "name": "Antonio Criminisi"
      }, 
      {
        "name": "Samuel Rota Bul\u00f2"
      }
    ], 
    "keywords": "random forests, deep neural network, image classification", 
    "title": "Deep Neural Decision Forests", 
    "type": "talk"
  }, 
  "3030": {
    "abstract": "Understanding when equilibria are guaranteed to exist is a central theme in economic theory, seemingly unrelated to computation. We show that the existence of equilibria in markets is inextricably connected to the computational complexity of related optimization problems, such as revenue or welfare maximization. We demonstrate how this relationship implies, under suitable complexity assumptions, a host of impossibility results. We also suggest a complexity-theoretic explanation for the lack of useful extensions of the Walrasian equilibrium concept: such extensions seem to require the invention of novel polynomial-time algorithms for welfare maximization.", 
    "authors": [
      {
        "name": "Tim Roughgarden"
      }, 
      {
        "name": "Inbal Talgam-Cohen"
      }
    ], 
    "keywords": "Market Equilibrium, Algorithmic Game Theory, Computational Complexity", 
    "title": "Why Prices Need Algorithms", 
    "type": "talk"
  }, 
  "3031": {
    "abstract": "Most of the key computational ideas in classical planning assume a simple planning language where action preconditions and goals are conjunctions of propositional atoms. Preconditions and goals that do not fit into this form are manually or automatically converted into such form for the derivation of heuristics that guide the search for plans. In this work, we show that this modeling choice hides important structural information, resulting in poorer heuristics and weaker planning performance. To address this, we show how relaxed plan heuristics can be lifted to a variable-free first-order planning language, Functional STRIPS, where atomic formulas can involve arbitrary terms. The key idea is to regard the set of atoms in a propositional layer of the relaxed planning graph as encoding a set of logical first-order interpretations. A precondition or goal formula is then regarded as reachable in a propositional layer, adding potentially new atoms to the next layer, when the set of atoms in the layer make the formula satisfiable according to the rules of first-order logic. While this satisfiability test and the resulting heuristics turn out to be intractable, we show how a meaningful polynomial approximation can be obtained by formulating the satisfiability problem as a CSP and applying constraint propagation techniques. Experiments illustrating the computational value of planning with more expressive languages are also reported.", 
    "authors": [
      {
        "name": "Guillem Franc\u00e8s"
      }, 
      {
        "name": "Hector Geffner"
      }
    ], 
    "keywords": "Classical planning, Functional STRIPS, Language expressiveness, Relaxed planning graph", 
    "title": "Effective Planning with More Expressive Languages", 
    "type": "talk"
  }, 
  "3032": {
    "abstract": "In answer set programming, knowledge involving sets of objects collectively is naturally represented by aggregates, which are rewritten into simpler forms known as monotone aggregates by current implementations. However, there is a complexity gap between general and monotone aggregates. In this paper, this gap is filled by means of a polynomial, faithful, and modular translation function, which can introduce disjunction in rule heads. The translation function is now part of the recent version 4.5 of the grounder GRINGO . This paper focuses on the key points of the translation function, and in particular on the mapping from non-convex sums to monotone sums.", 
    "authors": [
      {
        "name": "Mario Alviano"
      }, 
      {
        "name": "Wolfgang Faber"
      }, 
      {
        "name": "Martin Gebser"
      }
    ], 
    "keywords": "answer set programming, aggregates, transformation", 
    "title": "From Non-Convex Aggregates to Monotone Aggregates in ASP", 
    "type": "talk"
  }, 
  "3033": {
    "abstract": "Affect detection is a key component of intelligent educational interfaces that can respond to the affective states of students. We use computer vision, learning analytics, and machine learning to detect students\u00e2\u0080\u0099 affect in the real-world environment of a school computer lab that contained as many as thirty students at a time. Students moved around, gestured, and talked to each other, making the task quite difficult. Despite these challenges, we were moderately successful at detecting boredom, confusion, delight, frustration, and engaged concentration in a manner that generalized across students, time, and demographics. Our model was applicable 98% of the time despite operating on noisy real-world data.", 
    "authors": [
      {
        "name": "Nigel Bosch"
      }, 
      {
        "name": "Sidney D'Mello"
      }, 
      {
        "name": "Ryan Baker"
      }, 
      {
        "name": "Jaclyn Ocumpaugh"
      }, 
      {
        "name": "Valerie Shute"
      }, 
      {
        "name": "Matthew Ventura"
      }, 
      {
        "name": "Lubin Wang"
      }, 
      {
        "name": "Weinan Zhao"
      }
    ], 
    "keywords": "affect detection, naturalistic facial expressions, classroom data, in the wild", 
    "title": "Detecting Student Emotions in Computer-Enabled Classrooms", 
    "type": "talk"
  }, 
  "3034": {
    "abstract": "We establish connections from optimizing Bellman Residual and Temporal Difference Loss to worst- case long-term predictive error. In the online learning framework, learning takes place over a sequence of trials with the goal of predicting a future discounted sum of rewards. Our first analysis shows that, together with a stability assumption, any no-regret online learning algorithm that minimizes Bellman error ensures small prediction error. Our second analysis shows that applying the family of online mirror descent algorithms on temporal difference loss also ensures small prediction error. No statistical assumptions are made on the sequence of observations, which could be non-Markovian or even adversarial. Our approach thus establishes a broad new family of provably sound algorithms and provides a generalization of previous worst-case result for minimizing predictive error. We investigate the potential advantages of some of this family both theoretically and empirically on benchmark problems.", 
    "authors": [
      {
        "name": "Wen Sun"
      }, 
      {
        "name": "J. Andrew Bagnell"
      }
    ], 
    "keywords": "No-regret Online Learning, Bellman Residual Learning, Temporal Difference Learning", 
    "title": "Online Bellman Residual and Temporal Difference Algorithms with Predictive Error Guarantees", 
    "type": "talk"
  }, 
  "3035": {
    "abstract": "We consider the problem of robots following natural language commands through previously unknown outdoor environments. A robot receives commands in natural language, such as ``Navigate around the building to the car left of the fire hydrant and near the tree''. The robot needs first to classify its surrounding objects into categories, using images obtained from its sensors. The result of this classification is a map of the environment, where each object is given a list of semantic labels, such as ``tree'' and ``car'', with varying degrees of confidence. Then, the robot needs to ground the nouns in the command, i.e. mapping each noun in the command into a physical object in the environment. We use a probabilistic model for interpreting the spatial relations, such as ``left of'' and ``near''. The model is learned from examples provided by humans. For each noun in the command, a distribution on the objects in the environment is computed by combining spatial constraints with a prior given as the semantic classifier's confidence values. The robot needs also to ground a specified navigation mode, such as ``navigate quickly'' and ``navigate covertly'', as a cost map. The cost map is also learned from examples, using Inverse Optimal Control (IOC). The cost map and the grounded goal are used to generate a path for the robot.", 
    "authors": [
      {
        "name": "Abdeslam Boularias"
      }, 
      {
        "name": "Felix Duvallet"
      }, 
      {
        "name": "Jean Oh"
      }, 
      {
        "name": "Anthony Stentz"
      }
    ], 
    "keywords": "Spatial relations, mobile robot, navigation, learning", 
    "title": "Learning Qualitative Spatial Relations for Robotic Navigation", 
    "type": "talk"
  }, 
  "3036": {
    "abstract": "We introduce a framework for sparsity structures defined via graphs. Our approach is flexible and generalizes several previously studied sparsity models. Moreover, we provide efficient projection algorithms for our sparsity model that run in nearly-linear time. In the context of sparse recovery, our framework achieves an information-theoretically optimal sample complexity for a wide range of parameters. We complement our theoretical analysis with experiments showing that our algorithms also improve on prior work in practice.", 
    "authors": [
      {
        "name": "Chinmay Hegde"
      }, 
      {
        "name": "Piotr Indyk"
      }, 
      {
        "name": "Ludwig Schmidt"
      }
    ], 
    "keywords": "Sparsity, Structured sparsity, Sparse recovery, Compressive sensing, Graphs", 
    "title": "A Nearly-Linear Time Framework for Graph-Structured Sparsity", 
    "type": "talk"
  }, 
  "3037": {
    "abstract": "Topic modeling has become a ubiquitous topic analysis tool for text exploration.  Most of the existing work on topic modeling focuses on fitting topic models to input data. They however ignore an important aspect that is closely related to the end user experience: stability.  In this study, we investigate the stability problem in topic modeling. We first conduct experiments to quantify the severity of the problem. We then propose a new learning framework to mitigate the problem by explicitly incorporating topic stability constraints in model training. We also perform user study to demonstrate the advantages of the proposed method.", 
    "authors": [
      {
        "name": "Yi Yang"
      }, 
      {
        "name": "Shimei Pan"
      }, 
      {
        "name": "Yangqiu Song"
      }, 
      {
        "name": "Jie Lu"
      }, 
      {
        "name": "Mercan Topkara"
      }
    ], 
    "keywords": "topic modeling, stability, user experience, dynamic content", 
    "title": "Improving Topic Model Stability for Effective Document Exploration", 
    "type": "talk"
  }, 
  "3038": {
    "abstract": "Gelfond and Zhang recently proposed a new stable model semantics based on Vicious Circle Principle in order to improve the interpretation of logic programs with aggregates. A detailed complexity analysis of coherence testing and cautious reasoning under the new semantics highlighted similarities and differences versus mainstream stable model semantics for aggregates, which eventually led to the design of compilation techniques for implementing the new semantics on top of existing ASP solvers.", 
    "authors": [
      {
        "name": "Mario Alviano"
      }, 
      {
        "name": "Nicola Leone"
      }
    ], 
    "keywords": "answer set programming, aggregates, compilation", 
    "title": "On the Properties of GZ-Aggregates in Answer Set Programming", 
    "type": "talk"
  }, 
  "3039": {
    "abstract": "There are different approaches for proving that a program satisfies a correctness property. The deductive approach uses logics and proof systems and is automated using decision procedures and proof assistants. The automata-theoretic approach reduces questions about programs to algorithmic questions about automata and their languages. In the abstract interpretation approach, programs and their properties are expressed in terms of fixed points in lattices and reasoning uses fixed point approximation techniques. We describe a research programme to establish precise, mathematical correspondences between these approaches. The main theoretical tools we use are a construction of Lindenbaum and Tarski for generating lattices from logics, and the theorem of B\u00c3\u00bcchi that relate automata and logic. This research has lead to improvements in existing tools and we anticipate further theoretical and practical consequences.", 
    "authors": [
      {
        "name": "Vijay D'Silva"
      }, 
      {
        "name": "Caterina Urban"
      }
    ], 
    "keywords": "deductive verification, automata-theoretic model checking, abstract interpretation", 
    "title": "B\u00fcchi, Lindenbaum, Tarski: A Program Analysis Appetizer", 
    "type": "talk"
  }, 
  "3040": {
    "abstract": "We describe a tool for generating Euler diagrams from a set of region connection calculus formulas. The generation is based on a variant of local search capturing default reasoning for improving aesthetic appearance of Euler diagrams. We also describe an optimization for diagrams to be interactive: the user can modify the diagram with the mouse while formulas are still satisfied. We also discuss how such a tool may propose new relevant formulas to add to the specification using an approximation algorithm based on the satisfiability of Horn clauses.", 
    "authors": [
      {
        "name": "Francois Schwarzentruber"
      }
    ], 
    "keywords": "Euler diagrams, Local search, Default reasoning, Interaction", 
    "title": "A Tool for generating Interactive Euler diagrams", 
    "type": "demo"
  }, 
  "3041": {
    "abstract": "We demonstrate Eddy, a new tool for designing ontologies specified in the Graphol language. Graphol is completely visual and fully captures OWL 2. Thus Eddy is the first ontology editor  that allows to create OWL 2 ontologies by only using simple graphical editing features.", 
    "authors": [
      {
        "name": "Domenico Lembo"
      }, 
      {
        "name": "Daniele Pantaleone"
      }, 
      {
        "name": "Valerio Santarelli"
      }, 
      {
        "name": "Domenico Fabio Savo"
      }
    ], 
    "keywords": "Ontologies, Description Logics, OWL, Visual Languages, Graphical Conceptual Models, Usability Evaluation", 
    "title": "Eddy: A Graphical Editor for OWL 2 Ontologies", 
    "type": "demo"
  }, 
  "3042": {
    "abstract": "We present LTS++, an interactive development environment for planning-based hypothesis generation in applications with unreliable observations.", 
    "authors": [
      {
        "name": "Shirin Sohrabi"
      }, 
      {
        "name": "Octavian Udrea"
      }, 
      {
        "name": "Anton Riabov"
      }, 
      {
        "name": "Oktie Hassanzadeh"
      }
    ], 
    "keywords": "Planning, Knowledge engineering, High-quality plans", 
    "title": "Interactive Planning-based Hypothesis Generation with LTS++", 
    "type": "demo"
  }, 
  "3043": {
    "abstract": "We demonstrate a family of novel, standards-based, online application for promoting  tourist events with minimal operational impact using AI methods.  The capabilities span collection of events using  standards and crowd, their analysis to promote discovery by future tourists as well as manage impact by city managers, and its dissemination.  The solution offers benefits to citizens, travelers,  city managers and businesses and can be rolled out to cities around the world.", 
    "authors": [
      {
        "name": "Vishalaksh Aggarwal"
      }, 
      {
        "name": "Biplav Srivastava"
      }, 
      {
        "name": "Srikanth Tamilselvam"
      }
    ], 
    "keywords": "Tourism Promotion, Machine Learning, Open Data, City Comparison, Semantics", 
    "title": "Data-based Promotion of Tourist Events with Minimal Operational Impact", 
    "type": "demo"
  }, 
  "3046": {
    "abstract": "An increasing number of structured knowledge bases have become available on the Web, enabling many new forms of analytics and applications. However, the fact these are being published by different parties with heterogeneous vocabularies and ontologies also leads to formidable data integration challenges. This paper presents Klint, a web-based system that automatically creates mappings to transform knowledge from original data sources into a large unified schema, and allows them to be reviewed and edited by users with a streamlined interface. In this way, it allows human-level accuracy with minimum human effort.", 
    "authors": [
      {
        "name": "Jacobo Rouces"
      }, 
      {
        "name": "Gerard de Melo"
      }, 
      {
        "name": "Katja Hose"
      }
    ], 
    "keywords": "knowledge bases, data integration, heterogeneous knowledge", 
    "title": "Klint: Assisting Integration and Querying of Heterogeneous Knowledge", 
    "type": "demo"
  }, 
  "3048": {
    "abstract": "Tracking of objects in 3D space in general is not an easy task. There exist solutions involving hi-tech cameras and powerful computer systems capable of tracking many objects (and changing number of them) in large dynamic space simultaneously in real time. On the other hand, there are situations where such functionality is not necessary and the conditions may be specified in more detail, which makes the task significantly easier. This paper falls in this second category. It shows the possibility to track a single object using low-cost cameras on an ordinary laptop in a small-scale and mostly static environment. This solution is useful for tracking a single object of interest in mobile robotics and particularly in the debugging phases, where the user needs to judge the robot movement system independently on what the robot claims.", 
    "authors": [
      {
        "name": "Roman Bartak"
      }, 
      {
        "name": "Michal Koutn\u00fd"
      }, 
      {
        "name": "David Obdrzalek"
      }
    ], 
    "keywords": "tracking, localization, computer vision, robotics", 
    "title": "Practical 3D tracking using low-cost cameras", 
    "type": "demo"
  }, 
  "3050": {
    "abstract": "We analyze the observability of 3-D position and orientation from the fusion of visual and inertial sensors. Because the model contains unknown parameters, such as sensor biases, the problem is usually cast as a mixed filtering/identification, with the resulting observability analysis providing necessary conditions for convergence to a unique point estimate. Most models treat sensor bias rates as \"noise\", independent of other states, including biases themselves, an assumption that is violated in practice. We show that, when this assumption is lifted, the resulting model is not observable, and therefore existing analyses cannot be used to conclude that the set of states that are indistinguishable from the measurements is a singleton. We re-cast the analysis as one of sensitivity: Rather than attempting to prove that the set of indistinguishable trajectories is a singleton, we derive bounds on its volume, as a function of characteristics of the sensor and other sufficient excitation conditions. This provides an explicit characterization of the indistinguishable set that can be used for analysis and validation purposes.", 
    "authors": [
      {
        "name": "Joshua Hernandez"
      }, 
      {
        "name": "Konstantine Tsotsos"
      }, 
      {
        "name": "Stefano Soatto"
      }
    ], 
    "keywords": "observability, identifiability, navigation, visual-inertial", 
    "title": "Observability, Identifiability and Sensitivity of Vision-Aided Inertial Navigation", 
    "type": "talk"
  }, 
  "3052": {
    "abstract": "Widely adoption of GPS-enabled devices generates massive trajectory data every minute. The trajectory data not only describes the movement history of moving objects but also can generate many meaningful traffic patterns. In this demo, we present a system called PARecommender, which can predict traffic conditions and provide a reliable route recommendation to users based on previously generated traffic patterns, especially when the real-time traffic data source is unavailable. In addition, the patterns can also increase the accuracy when the route planning and estimated time arrival calculation are required. In this paper, we first introduce the technical details of PARecommender and then show several real cases that how PARecommender works.", 
    "authors": [
      {
        "name": "Feiyi Tang"
      }, 
      {
        "name": "Jia Zhu"
      }, 
      {
        "name": "Yang Cao"
      }, 
      {
        "name": "Sanli Ma"
      }, 
      {
        "name": "Yulong Chen"
      }, 
      {
        "name": "Jing He"
      }, 
      {
        "name": "Changqin Huang"
      }, 
      {
        "name": "Gansen Zhao"
      }, 
      {
        "name": "Yong Tang"
      }
    ], 
    "keywords": "Pattern Mining, Route Recommendation, Trajectory Data Processing", 
    "title": "PARecommender:A Pattern-based System for Route Recommendation", 
    "type": "demo"
  }, 
  "3053": {
    "abstract": "We will demonstrate a tabletop robotic agent that learns new tasks through interactive natural language instruction. The tasks to be demonstrated are simple puzzles and games, such as Tower of Hanoi, Eight Puzzle, Tic-Tac-Toe, Three Men\u00e2\u0080\u0099s Morris, and the Frog and Toads puzzle. We will include a live, interactive simulation of a mobile robot that learns new tasks using the same system.", 
    "authors": [
      {
        "name": "James Kirk"
      }, 
      {
        "name": "Aaron Mininger"
      }, 
      {
        "name": "John Laird"
      }
    ], 
    "keywords": "task learning, robotics, game learning, human-robot interaction", 
    "title": "A Demonstration of Interactive Task Learning", 
    "type": "demo"
  }, 
  "3054": {
    "abstract": "Analysis, pattern discovery, and decision support processes can benefit greatly from informative and interpretable visualizations, especially when facing high-dimensional data. \\textit{Informative Projection Ensemble} (IPE) methodology has proven to be an effective tool for projecting high-dimensional data into interpretable renderings that can reveal hidden low-dimensional structures embedded in data if such structures exist.  In this demonstration, we present a powerful analysis tool that uses Informative Projections in support of fundamental machine learning tasks: regression, classification, and clustering. Our tool comprises of an interactive web application operating on 2D and 3D projections of data, automatically selected by IPE algorithms as informative for the user-specified data and task. We also introduce RESTful APIs that offer remote users the ability to seamlessly integrate our service with other tools and to easily extend its functionality.   We showcase the capacity of our tool to discover informative elements by leveraging hidden structure in high-dimensional data.", 
    "authors": [
      {
        "name": "Donghan Wang"
      }, 
      {
        "name": "Madalina Fiterau"
      }, 
      {
        "name": "Artur Dubrawski"
      }
    ], 
    "keywords": "feature selection, informative projections, visualization, ensemble, interpretability", 
    "title": "VIPR: An Interactive Tool for Meaningful Visualization of High-Dimensional Data", 
    "type": "demo"
  }, 
  "3055": {
    "abstract": "The extraction of the relevant and debated opinions from online social media and commercial websites is an emerging task in the opinion mining research field. Its growing relevance is mainly due to the impact of exploiting such techniques in different application domains from social science analysis to personal advertising. In this demo, we present our opinion summary application built on top of an argumentation framework, a standard AI framework whose value is to exchange, communicate and resolve possibly conflicting viewpoints in distributed scenarios. We show how our application is able to extract relevant and debated opinions from a set of documents containing user generated content from online commercial websites.", 
    "authors": [
      {
        "name": "Mauro Dragoni"
      }, 
      {
        "name": "Serena Villata"
      }, 
      {
        "name": "Andrea Tettamanzi"
      }, 
      {
        "name": "C\u00e9lia Da Costa Pereira"
      }
    ], 
    "keywords": "Opinion Mining, Argumentation Theory, User Generated Content Analysis", 
    "title": "SMACk: An argumentation framework for opinion mining", 
    "type": "demo"
  }, 
  "3056": {
    "abstract": "In recent years, many different types of intelligent mobile robots have been developed in research and industrial labs. Although there are significant differences in both hardware and software over these robots, many of them share a common set of AI capabilities, e.g., planning, learning, vision and natural language processing. At the same time, almost all of them are equipped with traditional robotic capabilities such as mapping, localization, and navigation. However, to date it has been difficult to compare and contrast their capabilities in any controlled way. The main goal of the Robot Scavenger Hunt is to provide a standardized framework that includes a set of standardized tasks for evaluating the AI and robotic capabilities of medium-sized intelligent mobile robots. Compared to existing benchmarks, e.g., RoboCup@Home, Robot Scavenger Hunt aims at evaluations in larger spaces (multi-floor buildings vs. rooms) over longer periods of time (hours vs. minutes) while interacting with real human residents.", 
    "authors": [
      {
        "name": "Shiqi Zhang"
      }, 
      {
        "name": "Dongcai Lu"
      }, 
      {
        "name": "Xiaoping Chen"
      }, 
      {
        "name": "Peter Stone"
      }
    ], 
    "keywords": "intelligent mobile robots, artificial intelligence, robotics, standardized evaluation", 
    "title": "Robot Scavenger Hunt: a Standardized Framework for Evaluating Intelligent Mobile Robots", 
    "type": "demo"
  }, 
  "3058": {
    "abstract": "The combination of data, semantics, and the Web has led to an ever growing and increasingly complex body of semantic data. Accessing such structured data requires learning formal query languages, such as SPARQL, which poses significant difficulties for non-expert users. To date, many interfaces for querying Ontologies have been developed. However, such interfaces often rely on predefined templates and require expensive customization. To avoid the pitfalls of existing approaches, while at the same time retaining the ability to capture users' complex information needs, we have developed a simple keyword-based search interface to the Semantic Web. In this demonstration, we will present ASQFor, a systematic framework for automated SPARQL query formulation and execution over RDF repository using simple concept-based search primitives. Allowing end-users to express simple queries based on a list of \"key-value\" pairs that are then translated on-the-fly into SPARQL queries is a hard problem. In this demonstration, we will discuss the challenges that we have addressed to bring ASQFor to real practice, and also the difficult problems that remain to be solved in future work. During our demonstration, we will show how ASQFor can be used for decision support as well as an intelligent Q/A System.", 
    "authors": [
      {
        "name": "Muhammad Rizwan Saeed"
      }, 
      {
        "name": "Charalampos Chelmis"
      }, 
      {
        "name": "Viktor Prasanna"
      }
    ], 
    "keywords": "Semantic Web, SPARQL Queries, RDF", 
    "title": "Thou Shalt ASQFor And Shalt Receive The Semantic Answer", 
    "type": "demo"
  }, 
  "3059": {
    "abstract": "In this demonstration paper, we introduce an adaptive Process Management System implementation that combines business process execution monitoring, unanticipated exception detection and automated resolution strategies leveraging on well-established formalisms developed for reasoning about actions in Artificial Intelligence, including the Situation Calculus, IndiGolog and classical planning. Such formalisms provide a natural framework for the formal specification of explicit mechanisms to model world changes and responding to anomalous situations, exceptions, exogenous events in an automated way during process execution. The adaptation mechanisms of the proposed system allows to deviate from the execution path prescribed by the original process without altering its process model, by making it particularly suitable for managing complex processes in cyber-physical domains.", 
    "authors": [
      {
        "name": "Andrea Marrella"
      }, 
      {
        "name": "Massimo Mecella"
      }, 
      {
        "name": "Sebastian Sardina"
      }
    ], 
    "keywords": "Business Process Management, Adaptive Process Management System, Situation Calculus, IndiGolog, Classical Planning, PDDL", 
    "title": "An Adaptive Process Management System Implementation based on Situation Calculus, Indigolog and Classical Planning", 
    "type": "demo"
  }, 
  "3060": {
    "abstract": "Question answering (QA) has become a popular way for humans to access billion-scale knowledge bases. QA systems over knowledge bases give out accurate and concise answers. The key of QA over knowledge bases is to map the question to a knowledge base structure. To do this, KBQA uses a new kind of question representation: templates, learned from a million scale QA corpora. For example, for questions about a city's population, KBQA learns templates such as What's the population of $city?, How many people are there in $city?. It learns overall 27 million templates for 2782 relations. Based on these templates, KBQA effectively and efficiently supports binary factoid questions or complex questions.", 
    "authors": [
      {
        "name": "Wanyun Cui"
      }, 
      {
        "name": "Yanghua Xiao"
      }, 
      {
        "name": "Wei Wang"
      }
    ], 
    "keywords": "question answering, knowledge base, template", 
    "title": "KBQA: An Online Template Based Question Answering System over Freebase", 
    "type": "demo"
  }, 
  "3061": {
    "abstract": "Demand Response (DR) allows utilities to curtail electricity consumption during peak demand periods. Real time automated DR can offer utilities a scalable solution for fine grained control of curtailment over small intervals for the duration of the entire DR event. In this work, we demonstrate a system for a real time automated Dynamic DR (D 2 R). Our system has already been integrated with the electrical infrastructure of University X, which offers a unique environment to study the impact of automated DR in a complex social and cultural environment including 170 buildings in a \u00e2\u0080\u009ccity-within-a-city\u00e2\u0080\u009d scenario. Our large scale information processing system coupled with accurate forecasting models for sparse data and fast polynomial time optimization algorithms for curtailment maximization provide the ability to adapt and respond to changing curtailment requirements in near real-time. Our D2R algorithms automatically and dynamically select customers for load curtailment to guarantee the achievement of a curtailment target over a given DR interval.", 
    "authors": [
      {
        "name": "Sanmukh Rao Kuppannagari"
      }, 
      {
        "name": "Rajgopal Kannan"
      }, 
      {
        "name": "Charalampos Chelmis"
      }, 
      {
        "name": "Viktor K. Prasanna"
      }
    ], 
    "keywords": "Demand Response, Smart Grid, Influence based learning, Customer selection algorithms", 
    "title": "Implementation of Learning-Based Dynamic Demand Response on a Campus Micro-grid", 
    "type": "demo"
  }, 
  "3063": {
    "abstract": "For millions of people with swallowing disorders, preventing potentially deadly aspiration pneumonia requires following pre-scribed safe eating strategies.  But adherence is poor, and care-givers\u00e2\u0080\u0099 ability to encourage adherence is limited by the onerous and socially aversive need to monitoring another\u00e2\u0080\u0099s eating.  We have developed an early prototype for an intelligent assistant that monitors adherence and provides feedback to the patient, and tested monitoring precision with healthy subjects for one strategy called a \u00e2\u0080\u009cchin tuck.\u00e2\u0080\u009d  Results indicate that adaptations of current generation machine vision and personal assistant technologies could effectively monitor chin tuck adherence, and suggest the feasibility of a more general assistant that encourages adherence to a wide range of safe eating strategies.", 
    "authors": [
      {
        "name": "Michael Freed"
      }, 
      {
        "name": "Brian Burns"
      }, 
      {
        "name": "Aaron Heller"
      }, 
      {
        "name": "Daniel Sanchez"
      }, 
      {
        "name": "Sharon Beaumont-Bowman"
      }
    ], 
    "keywords": "Intelligent Assistant, Machine Vision, Monitoring", 
    "title": "A Virtual Assistant to Help Dysphagia Patients Eat Safely At Home", 
    "type": "demo"
  }, 
  "3064": {
    "abstract": "Imperfect-information games, where players have private information, pose a unique challenge in artificial intelligence. In recent years, Heads-Up No-Limit Texas Hold'em poker, a popular version of poker, has emerged as the primary benchmark for evaluating game-solving algorithms for imperfect-information games. We demonstrate a winning agent from the 2016 Annual Computer Poker Competition, Baby Tartanian8.", 
    "authors": [
      {
        "name": "Noam Brown"
      }, 
      {
        "name": "Tuomas Sandholm"
      }
    ], 
    "keywords": "poker, cfr, regret minimization, no-regret learning, abstraction", 
    "title": "Baby Tartanian8: Winning Agent from the 2016 Annual Computer Poker Competition", 
    "type": "demo"
  }, 
  "3066": {
    "abstract": "Research in Artificial Intelligence, Robotics and Computer Vision has recently made great strides in improving indoor localization. Publicly available technology now allows for indoor localization with very small margins of error. In this demo, we show a system that uses state-of the-art technology to assist visually impaired people navigate indoors. Our system takes advantage of spatial representations from CAD files, or floor plan images, to extract valuable information that later can be used to improve navigation and human-computer interaction. Using depth information, our system is capable of detecting obstacles and guiding the user to avoid them.", 
    "authors": [
      {
        "name": "J. Pablo Mu\u00f1oz"
      }, 
      {
        "name": "Jizhong Xiao"
      }, 
      {
        "name": "Bing Li"
      }, 
      {
        "name": "Xuejian Rong"
      }, 
      {
        "name": "Yingli Tian"
      }, 
      {
        "name": "Aries Arditi"
      }
    ], 
    "keywords": "Assistive Technology, Human-Computer Interaction, Navigation", 
    "title": "Demo: Assisting visually impaired people navigate indoors", 
    "type": "demo"
  }, 
  "3067": {
    "abstract": "This demonstration presents a tag-based statistical English math word problem (MWP) solver with understanding, reasoning, and explanation. It analyzes the text and transforms both body and question parts into their tag-based logic forms, and then performs inference on them. The proposed tag provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information, which can be used to identify the desired operand and filter out irrelevant quantities (so that the answer can be obtained precisely). It also provides the explanation text to explain how the answer obtained from the reasoning chain. Many MWP solvers only handle addition and subtraction operations. In contrast, the proposed approach could handle 16 different problem types (including division, summation, etc.).", 
    "authors": [
      {
        "name": "Chao-Chun Liang"
      }, 
      {
        "name": "Kuang-Yi Hsu"
      }, 
      {
        "name": "Chien-Tsung Huang"
      }, 
      {
        "name": "Chung-Min Li"
      }, 
      {
        "name": "Shen-Yun Miao"
      }, 
      {
        "name": "Keh-Yih Su"
      }
    ], 
    "keywords": "Math Word Problem Solver, Machine Reading, Natural Language Understanding", 
    "title": "A Tag-based English Math Word Problem Solver with Understanding, Reasoning and Explanation", 
    "type": "demo"
  }, 
  "3069": {
    "abstract": "General-purpose speech engines are trained on large corpus. However, studies and experiments have shown that when such engines are used to recognize spoken sentences in specific domains they may not produce accurate ASR output. Further, the accent and the environmental conditions in which the speaker speaks a sentence may induce the speech engine to inaccurately recognize certain words/ sets of words. Thus, the speech engine\u00e2\u0080\u0099s output may need to be repaired for a domain before any further natural language processing is carried out. We present an artificial development (Art-Dev) based mechanism for such a repair to make the subsequent natural language processing better. Our approach considers an erroneous sentence as a zygote and grows it through an artificial development approach, with evolution and development of the partial gene present in the input sentence with respect to the genes in the domain. Once the genotypes are identified, we \u00e2\u0080\u0098grow\u00e2\u0080\u0099 them into phenotypes that fill the missing gaps and replace erroneous words with appropriate domain words in the sentence. We demonstrate our approach on the outputs of standard ASR engines such as Google Now and IBM Watson and show how it improves the accuracy.", 
    "authors": [
      {
        "name": "C. Anantaram"
      }, 
      {
        "name": "Sunil Kumar Kopparapu"
      }, 
      {
        "name": "Chirag Patel"
      }, 
      {
        "name": "Aditya Mittal"
      }
    ], 
    "keywords": "Artificial Development, Evo-Devo, Speech, ASR, repair, accuracy", 
    "title": "Repairing general-purpose ASR output to improve accuracy of spoken sentences in specific domains using artificial development approach", 
    "type": "demo"
  }, 
  "310": {
    "abstract": "Model sampling has proved to be a practically viable method for decision-making under uncertainty, for example in imperfect-information games with large state spaces. In this paper, we examine the logical foundations of sampling-based belief revision. We show that it satisfies six of the standard AGM postulates but not Vacuity nor Subexpansion. We provide a corresponding representation theorem that generalises the standard result from single to multiple entrenchment relations for a given belief set. We also provide a formal axiomatisation of sampling-based belief revision in the Situation Calculus as an alternative way of reasoning about actions, sensing, and beliefs.", 
    "authors": [
      {
        "name": "Michael Thielscher"
      }
    ], 
    "keywords": "Model sampling, Belief revision, Reasoning about actions and beliefs", 
    "title": "Sampling-Based Belief Revision", 
    "type": "paper"
  }, 
  "32": {
    "abstract": "Stochastic local search (SLS) algorithms have proven to be very competitive in solving hard computational problems.This paper investigates the foundations of SLS algorithms. We develop a simple SLS algorithm, MarkovSLS, and study two special cases of it: SoftSLS and AdaptiveSLS.  SoftSLS uses soft (probabilistic) rather than hard (deterministic) restarts.  This formulation enables analysis using standard homogeneous  Markov chains. We study the interaction between the restart and noise parameters in SoftSLS, and optimize them  analytically in addition to the traditional empirical approach.  Experimentally, we investigate the dependency of SoftSLS's performance on its noise and restart parameters, validating the analytical results. AdaptiveSLS dynamically adjusts its noise and restart parameters during search. Experimentally, on synthetic and feature selection problems, we compare AdaptiveSLS with other algorithms including an analytically optimized version of SoftSLS, and find that it performs well while not requiring prior knowledge of the search space.", 
    "authors": [
      {
        "name": "Ole Mengshoel"
      }, 
      {
        "name": "Youssef Ahres"
      }, 
      {
        "name": "Tong Yu"
      }
    ], 
    "keywords": "Stochastic local search, Markov chains, Adaptive search", 
    "title": "Markov Chain Analysis of Noise and Restart in Stochastic Local Search", 
    "type": "paper"
  }, 
  "320": {
    "abstract": "Answering questions in a university's entrance examination like Gaokao in China challenges AI technology. As a preliminary attempt to take up this challenge, we focus on multiple-choice questions in Gaokao, and propose a three-stage approach that simulates typical human mental processes for answering a question by exploiting and extending information retrieval techniques. Taking Wikipedia as the source of knowledge, our approach obtains knowledge relevant to the question by retrieving pages via string matching and disambiguation, and then ranks and filters pages using multiple strategies to draw critical evidence, based on which the truth of each option is assessed via relevance-based entailment. It achieves encouraging results on real-life questions in recent history tests, significantly outperforming baseline approaches.", 
    "authors": [
      {
        "name": "Gong Cheng"
      }, 
      {
        "name": "Weixi Zhu"
      }, 
      {
        "name": "Ziwei Wang"
      }, 
      {
        "name": "Jianghui Chen"
      }, 
      {
        "name": "Yuzhong Qu"
      }
    ], 
    "keywords": "entrance examination, Gaokao, problem solving, question answering, information retrieval", 
    "title": "Taking up the Gaokao Challenge: An Information Retrieval Approach", 
    "type": "paper"
  }, 
  "343": {
    "abstract": "Semantic matching, which aims to determine the matching degree between two texts, is a fundamental problem for many NLP applications. Recently, deep learning approach has been applied to this problem and significant improvements have been achieved. In this paper, we propose to view the generation of the global interaction between two texts as a recursive process: i.e.~the interaction of two texts at each position is a composition of the interactions between their prefixes as well as the word level interaction at the current position. Based on this, we propose a novel deep architecture, namely Match-SRNN, to model the recursive matching structure. Firstly, a tensor is constructed to capture the word level interactions. Then a spatial RNN is applied to the tensor to integrate the local interactions recursively, with importance determined by four types of gates. Finally, the matching score is calculated based on the global interaction. We show that, after degenerated to the exact matching scenario, Match-SRNN will approximate the dynamic programming process of longest common subsequence. Thus, there exists a clear interpretation for Match-SRNN. Our experiments on question answering, a typical semantic matching task, showed the effectiveness of Match-SRNN and its ability of visualizing the learned matching structure.", 
    "authors": [
      {
        "name": "Shengxian Wan"
      }, 
      {
        "name": "Yanyan Lan"
      }, 
      {
        "name": "Jun Xu"
      }, 
      {
        "name": "Jiafeng Guo"
      }, 
      {
        "name": "Liang Pang"
      }, 
      {
        "name": "Xueqi Cheng"
      }
    ], 
    "keywords": "Semantic Matching, Spatial RNN, Longest Common Subsequence, Question Answering", 
    "title": "Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN", 
    "type": "paper"
  }, 
  "345": {
    "abstract": "We consider entailment problems involving powerful constraint languages such as guarded existential rules, in which additional semantic restrictions are put on a set of distinguished relations. We consider restricting a relation to be transitive,  restricting a relation  to be the transitive closure of another relation, and restricting a relation to be a linear order. We give some natural generalizations of guardedness that allow inference to be decidable in each case, and isolate the complexity of the corresponding decision problems. Finally we show that slight changes in our conditions lead to undecidability.", 
    "authors": [
      {
        "name": "Antoine Amarilli"
      }, 
      {
        "name": "Michael Benedikt"
      }, 
      {
        "name": "Pierre Bourhis"
      }, 
      {
        "name": "Michael Vanden Boom"
      }
    ], 
    "keywords": "existential rules, query answering, transitivity", 
    "title": "Query answering with transitive and linear-ordered data", 
    "type": "paper"
  }, 
  "348": {
    "abstract": "We study the problem of map-matching, or finding the route on a road network from a trace of noisy and sparse observed points, particularly when a huge number of points are given.  The algorithms based on Hidden Markov Models (HMMs) are known to achieve high accuracy for noisy and sparse data but suffer from high computational cost.  We find that the bottleneck of the HMM-based map-matching is in the shortest path search for calculating transition probabilities.  We propose a technique to truncate the shortest path search before finding all the shortest paths in the HMM-based map-matching without losing accuracy.  We run the one-to-many shortest path searches on the reversed network and terminate the searches based on the log likelihood of the Viterbi algorithm.  Computational experiments show that the proposed approaches can  reduce the computational cost by a factor of at least 5.4.", 
    "authors": [
      {
        "name": "Takashi Imamichi"
      }, 
      {
        "name": "Takayuki Osogami"
      }, 
      {
        "name": "Rudy Raymond"
      }
    ], 
    "keywords": "Map-matching, Hidden Markov model, Shortest path search", 
    "title": "Truncating Shortest Path Search for Efficient Map-matching", 
    "type": "paper"
  }, 
  "350": {
    "abstract": "We describe a sketch interpretation system that analyzes drawings created by subjects taking the Clock Drawing Test, a clinical tool widely used to screen for cognitive impairments (e.g., dementia). We describe how it balances appearance and context, and document its performance on some 2000 drawings (about 24K clock numerals) produced by a wide spectrum of patients. We calibrate the utility of different forms of context, describing experiments with CRFs trained and tested using a variety of features. We identify context that contributes to interpreting otherwise ambiguous or incomprehensible strokes. We describe ST-slices, a novel representation that enables \u00e2\u0080\u009cunpeeling\u00e2\u0080\u009d the layers of ink that result when people overwrite, which often produces ink impossible to analyze if only the final drawing is examined. We characterize when ST slices work, calibrate their impact on performance, and consider their breadth of applicability.", 
    "authors": [
      {
        "name": "Randall Davis"
      }, 
      {
        "name": "Yale Song"
      }, 
      {
        "name": "Kaichen Ma"
      }, 
      {
        "name": "Dana L. Penney"
      }
    ], 
    "keywords": "Sketch understanding, Clock drawing test, Human computer interaction", 
    "title": "Balancing Appearance and Context in Sketch Interpretation", 
    "type": "paper"
  }, 
  "354": {
    "abstract": "Consider an event organizer who is trying to schedule a group meeting. Availability of agents is unknown to the organizer a priori,  but the organizer may have probability estimates on availability of each agent for each date/time option.  The organizer can ask an agent to reveal her availability,  but it causes inconvenience for the agent, and thus the organizer wishes to find an agreeable outcome at a minimum number of such queries. Motivated by this example, we study the Probabilistic Matrix Inspection problem in which we are given a matrix of Bernoulli random variables that are mutually independent,  and the objective is to determine whether the matrix contains a column consisting only of 1's. We are allowed to inspect an arbitrary entry at unit cost, which reveals the realization of the entry, and we wish to find an inspection policy whose expected number of inspections is minimum. We first show that an intuitive greedy algorithm exists for 1-row and 1-column matrices, and we generalize this to design an algorithm that finds an optimal policy in polynomial time for the general case.", 
    "authors": [
      {
        "name": "Hooyeon Lee"
      }, 
      {
        "name": "Ashish Goel"
      }
    ], 
    "keywords": "group scheduling, event scheduling, probabilistic matrix inspection, polynomial time algorithm", 
    "title": "Probabilistic Matrix Inspection and Group Scheduling", 
    "type": "paper"
  }, 
  "355": {
    "abstract": "In recent literature, several approaches have been developed to solve over-constrained travel planning problems, which are often framed as conditional temporal problems with discrete choices. These approaches are able to explain the causes of failure and recommend alternative solutions by suspending or weakening temporal constraints. While helpful, they may not be practical in many situations, as we often cannot compromise on time. \t In this paper, we present an approach for solving such over-constrained problems, by also relaxing non-temporal variable domains through the consideration of additional options that are semantically similar. Our solution, called Conflict-Directed Semantic Relaxation (CDSR), integrates a knowledge base and a semantic distance calculator, and is able to simultaneously enumerate both temporal and domain relaxations in best-first order. When evaluated empirically on a range of urban trip planning scenarios, CDSR demonstrates a substantial improvement in flexibility and solution quality compared to temporal relaxation only approaches.", 
    "authors": [
      {
        "name": "Peng Yu"
      }, 
      {
        "name": "Jiaying Shen"
      }, 
      {
        "name": "Peter Yeh"
      }, 
      {
        "name": "Brian Williams"
      }
    ], 
    "keywords": "Temporal Problems, Over-constrained problems, Constraint Relaxations, Knowledge Representation and Reasoning", 
    "title": "Resolving Over-constrained Conditional Temporal Problems using Semantically Similar Alternatives", 
    "type": "paper"
  }, 
  "362": {
    "abstract": "Distributed knowledge representation (KR) encodes both entities and relations in a low-dimensional semantic space, which have significantly promoted the performance of relation extraction and knowledge reasoning. In many knowledge graphs (KG), some relations indicate attributes of entities (attributional relations) and others indicate relations between entities (relational relations). Existing KR models regard all relations equally, and usually suffer from poor accuracies when modeling 1-to-many and many-to-1 relations, mostly composed of attributional relations. In this paper, we distinguish existing KG-relations into attributes and relations, and propose a new KR model with entities, attributes and relations (KR-EAR). The experiment results show that, by special modeling of attributional relations, KR-EAR can significantly outperform existing state-of-the-art KR models in prediction of entities, attributes and relations.", 
    "authors": [
      {
        "name": "Yankai Lin"
      }, 
      {
        "name": "Zhiyuan Liu"
      }, 
      {
        "name": "Maosong Sun"
      }
    ], 
    "keywords": "Knowledge Representation, Knowledge Completion, Relation Extraction", 
    "title": "Knowledge Representation Learning with Entities, Attributes and Relations", 
    "type": "paper"
  }, 
  "364": {
    "abstract": "Recently, Word2Vec tool has attracted a lot of interest for its promising performances in a variety of natural language processing (NLP) tasks. However, a critical issue is that the dense word representations learned in Word2Vec are lacking of interpretability. It is natural to ask if one could improve their interpretability while keeping their performances. Inspired by the success of sparse models in enhancing interpretability, we propose to introduce sparse constraint into Word2Vec. Specifically, take the Continuous Bag of Words (CBOW) model as an example in our study and add the L1 regularizer into its learning objective. One challenge of optimization lies in that: stochastic gradient descent (SGD) cannot directly produce sparse solutions with L1 regularizer in online training. To solve this problem, we employ the Regularized Dual Averaging (RDA) method, an online optimization algorithm for regularized stochastic learning. In this way, the learning process is very efficient and our model can scale up to very large corpus to derive sparse word representations. The proposed model is evaluated on both expressive power tasks and interpretability task. The results show that, compared with origin CBOW model, the proposed model can obtain state-of-the-art results with better interpretability using less than 10% non-zero elements.", 
    "authors": [
      {
        "name": "Fei Sun"
      }, 
      {
        "name": "Jiafeng Guo"
      }, 
      {
        "name": "Yanyan Lan"
      }, 
      {
        "name": "Jun Xu"
      }, 
      {
        "name": "Xueqi Cheng"
      }
    ], 
    "keywords": "Word Representation, Sparse Word Representation, Dense Representation, Interpretability", 
    "title": "Sparse Word Embeddings Using L1 Regularized Online Learning", 
    "type": "paper"
  }, 
  "374": {
    "abstract": "This paper presents a real application system to automatically recommend the most suitable makeup for a female and synthesis the makeup on her face. Given a before-makeup face, her most suitable makeup is determined automatically. Then, both the before-makeup and the reference faces are fed into the  proposed Deep Localized Makeup Transfer Network to generate the  after-makeup face. Our end-to-end makeup transfer network  bears several nice properties including it is:  1) with  complete functions: including  foundation, lip gloss and eye shadow transfer; 2) cosmetic specific: different cosmetic are transferred in different manners;  3) localized: different cosmetics are applied on different facial regions; 4) producing naturally looking results without obvious artifacts. 5) controllable makeup lightness: various results from light makeup to heavy makeup can be generated.   Qualitative and quantitative experiments show that our network performs much better than the state-of-the-art methods.", 
    "authors": [
      {
        "name": "Si Liu"
      }, 
      {
        "name": "Xinyu Ou"
      }, 
      {
        "name": "Ruihe Qian"
      }, 
      {
        "name": "Wei Wang"
      }, 
      {
        "name": "Xiaochun Cao"
      }
    ], 
    "keywords": "deep learning, makeup transfer, Deep Localized Makeup Transfer Network", 
    "title": "Makeup like a superstar: Deep Localized Makeup Transfer Network", 
    "type": "paper"
  }, 
  "38": {
    "abstract": "Robust principal component analysis (PCA) is one of the most important dimensionality reduction techniques to handle high-dimensional data with outliers. However, the existing robust PCA either suppose the mean of data is zero and incorrectly utilize the optimal mean based on Euclidean distance for robust PCA with l_1-norm, or integrate the estimation of optimal mean into the dimensionality reduction objective and lead to expensive computation. In this paper, we equivalently reformulate the maximization of variances such that the optimal projection directions are learned by maximizing the sum of projection difference between each pair of instances rather than the difference between each instance and the mean of the data. In this sense, we propose a novel robust PCA to avoid the calculation of the optimal mean on the basis of l_1-norm distance automatically and make the assumption of centered data unnecessary.An intuitive extension to tensor version of robust 2DPCA is also developed for image recognition. We exploit an efficient non-greedy algorithm to maximize the objectives with l_1-norm which converges fast in practice. Finally, some experimental results on benchmark data sets demonstrate the effectiveness and superiority of the proposed approaches on the tasks of reconstruction and recognition.", 
    "authors": [
      {
        "name": "Minnan Luo"
      }, 
      {
        "name": "Feiping Nie"
      }, 
      {
        "name": "Xiaojun Chang"
      }, 
      {
        "name": "Yi Yang"
      }, 
      {
        "name": "Qinghua Zheng"
      }
    ], 
    "keywords": "Robust principal component analysis, dimensionality reduction, L1-norm Maximization", 
    "title": "Avoiding Optimal Mean Robust PCA/2DPCA with Non-greedy L1-norm Maximization", 
    "type": "paper"
  }, 
  "381": {
    "abstract": "We introduce the logic of Here-and-There with Constraints in order to capture constraint theories in the non-monotonic setting known from Answer Set Programming (ASP). This allows for assigning default values to constraint variables or to leave them undefined. Also, it provides us with a semantic framework integrating ASP and Constraint Processing in a uniform way. We put some emphasis on logic programs dealing with linear constraints on integer variables, where we further introduce a directional assignment operator. We elaborate upon the formal relation and implementation of these programs in terms of Constraint ASP, sketching an existing system.", 
    "authors": [
      {
        "name": "Pedro Cabalar"
      }, 
      {
        "name": "Roland Kaminski"
      }, 
      {
        "name": "Max Ostrowski"
      }, 
      {
        "name": "Torsten Schaub"
      }
    ], 
    "keywords": "Answer Set Programming, Logic of Here-and-There, Constraint Answer Set Programming, Constraint Satisfaction Problem", 
    "title": "An ASP Semantics for Default Reasoning with Constraints", 
    "type": "paper"
  }, 
  "386": {
    "abstract": "The field of iterated belief change has focused mainly on revision, with the other main operator of AGM belief change theory, i.e., contraction receiving relatively little attention. In this paper we extend the Harper Identity from single-step change to define iterated contraction in terms of iterated revision. Specifically, just as the Harper Identity provides a recipe for defining the belief set resulting from contracting A in terms of (i) the initial belief set and (ii) the belief set resulting from revision by \\neg A, we look at ways to define the plausibility ordering over worlds resulting from contracting A in terms of (iii) the initial plausibility ordering, and (iv) the plausibility ordering resulting from revision by \\neg A. After noting that the most straightforward such extension leads to a trivialisation of the space of permissible orderings, we provide a family of operators for combining plausibility orderings that avoid such a result. These operators are characterised in our domain of interest by a pair of intuitively compelling properties, which turn out to enable the derivation of a number of iterated contraction postulates from postulates for iterated revision. We finish by observing that a salient member of this family allows for the derivation of counterparts for contraction of some well known iterated revision operators, as well as for defining new iterated contraction operators.", 
    "authors": [
      {
        "name": "Richard Booth"
      }, 
      {
        "name": "Jake Chandler"
      }
    ], 
    "keywords": "belief revision, iterated belief change, Harper Identity, contraction", 
    "title": "Extending the Harper Identity to Iterated Belief Change", 
    "type": "paper"
  }, 
  "39": {
    "abstract": "Deep learning has been one of the most prominent machine learning techniques nowadays, being the state-of-the-art on a broad range of applications where automatic feature extraction is needed. Many such applications also demand varying costs for different types of mis-classification errors, but it is not clear whether or how such cost information can be incorporated into deep learning to improve performance. In this work, we first design a novel loss function that embeds the cost information for the training stage of cost-sensitive deep learning. We then show that the loss function can also be integrated into the pre-training stage to conduct cost-aware feature extraction more effectively. Extensive experimental results justify the validity of the novel loss function for making existing deep learning models cost-sensitive, and demonstrate that our proposed model with cost-aware pre-training and training outperforms non-deep models and other deep models that digest the cost information in other stages.", 
    "authors": [
      {
        "name": "Yu-An Chung"
      }, 
      {
        "name": "Hsuan-Tien Lin"
      }, 
      {
        "name": "Shao-Wen Yang"
      }
    ], 
    "keywords": "Cost-sensitive, Deep learning, Auto-encoder, De-noising auto-encoder, Pre-training, Multiclass classification, Multiclass cost-sensitive classification, Deep neural networks", 
    "title": "Cost-aware Pre-training for Multiclass Cost-sensitive Deep Learning", 
    "type": "paper"
  }, 
  "391": {
    "abstract": "Matrix factorization has been recently utilized for the task of multi-modal hashing for cross-modality visual search, where basis functions are learned to map data from different modalities to the same Hamming embedding.   %However, it remains as an open problem to integrate the semantic information sufficiently, during the process of binary quantization across different modalities. In this paper, we propose a novel cross-modality hashing algorithm termed Supervised Matrix Factorization Hashing (SMFH)  which tackles the multi-modal hashing problem with a collective non-matrix factorization across the different modalities.In particular, SMFH employs a well-designed binary code learning algorithm to preserve the similarities among multi-modal original features through a graph regularization. At the same time, semantic labels, when available, are incorporated into the learning procedure.We conjecture that all these would facilitate to preserve the most relevant information during the binary quantization process, and hence improve the retrieval accuracy. We demonstrate the superior performance of SMFH on three cross-modality visual search benchmarks, \\textit{i.e.}, the PASCAL-Sentence, Wiki, and NUS-WIDE, with quantitative comparison to various state-of-the-art methods.", 
    "authors": [
      {
        "name": "Hong Liu"
      }, 
      {
        "name": "Rongrong Ji"
      }, 
      {
        "name": "Yongjian Wu"
      }, 
      {
        "name": "Wei Liu"
      }, 
      {
        "name": "Gang Hua"
      }
    ], 
    "keywords": "large-scale visual search, cross-modality retrieval, hashing, binary code learning", 
    "title": "Supervised Matrix Factorization for Cross-Modality Hashing", 
    "type": "paper"
  }, 
  "402": {
    "abstract": "Real-world data are seldom unstructured, yet traditional Matrix Factorization (MF) models, as one of the most powerful collaborative filtering approaches, generally rely on this assumption to recover the low-rank structures for recommendation. However, few of them are able to explicitly consider structured constraint with the underlying low-rank assumption to model complex user interests. To solve this problem, we propose a unified MF framework with generalized Laplacian constraint for collaborative filtering. We investigate the connection between the recently proposed Laplacian constraint and the classical normalized cut problem, and make it possible to extend the original non-overlapping prior, to capture the overlapping case via learning the decomposed multi-facet graphs. Experiments on real-world datasets demonstrate the effectiveness of the proposed method.", 
    "authors": [
      {
        "name": "Qing Zhang"
      }, 
      {
        "name": "Houfeng Wang"
      }
    ], 
    "keywords": "Collaborative Filtering, Laplacian Constraint, Overlapping Decomposition", 
    "title": "Collaborative Filtering with Generalized Laplacian Constraint via Overlapping Decomposition", 
    "type": "paper"
  }, 
  "412": {
    "abstract": "Control applications often feature tasks with similar, but not identical, dynamics. We introduce the Hidden Parameter Markov Decision Process (HiP-MDP), a framework that parametrizes a family of related dynamical systems with a low-dimensional set of latent factors, and introduce a semiparametric regression approach for learning its structure from data. We show that a learned HiP-MDP rapidly identifies the dynamics of new task instances in several settings, flexibly adapting to task variation.", 
    "authors": [
      {
        "name": "Finale Doshi-Velez"
      }, 
      {
        "name": "George Konidaris"
      }
    ], 
    "keywords": "Reinforcement learning, Transfer, Model learning", 
    "title": "Hidden Parameter Markov Decision Processes: A Semiparametric Regression Approach for Discovering Latent Task Parametrizations", 
    "type": "paper"
  }, 
  "417": {
    "abstract": "How should one aggregate ordinal preferences expressed by voters into a measurably superior social choice? A well-established approach -- which we refer to as implicit utilitarian voting -- assumes that voters have latent utility functions that induce the reported rankings, and seeks voting rules that approximately maximize utilitarian social welfare. We extend this approach to the design of rules that select a subset of alternatives. We derive analytical bounds on the performance of optimal rules in terms of two measures, distortion and regret. Empirical results show that regret-based rules are more compelling than distortion-based rules, leading us to focus on developing a scalable implementation for the optimal (deterministic) regret-based rule. Our methods underlie the design and implementation of an upcoming social choice website.", 
    "authors": [
      {
        "name": "Ioannis Caragiannis"
      }, 
      {
        "name": "Swaprava Nath"
      }, 
      {
        "name": "Ariel Procaccia"
      }, 
      {
        "name": "Nisarg Shah"
      }
    ], 
    "keywords": "Voting, Approximation, Real-world applications", 
    "title": "Subset Selection Via Implicit Utilitarian Voting", 
    "type": "paper"
  }, 
  "421": {
    "abstract": "Learning of low-rank matrices is fundamental to many machine learning applications. A state-of-the-art is the rank-one matrix pursuit (R1MP) algorithm, which is based on greedy approximation. However, it is only designed for matrix completion with the square loss. In this paper, we develop a more flexible greedy algorithm that can be applied to generalized low-rank models. The objective of the corresponding optimization problem can be smooth or nonsmooth, general convex or strongly convex. Analysis show that the proposed algorithm has low per-iteration time complexity and fast convergence rates. Experimental results show that it achieves comparable or better prediction performance as the state-of-the-art, but much faster.", 
    "authors": [
      {
        "name": "Quanming Yao"
      }, 
      {
        "name": "James.T Kwok"
      }
    ], 
    "keywords": "generalized low-rank models, low-rank optimization, greedy algorithm, matrix completion", 
    "title": "Greedy Learning of Generalized Low-Rank Models", 
    "type": "paper"
  }, 
  "426": {
    "abstract": "Consider the following data fusion scenario: two datasets/peers contain the same real-world entities described using partially shared features, \\emph{e.g.} banking and insurance company records of the same customer base. Our goal is to learn a classifier in the cross product space of the two domains, in the hard case in which no shared ID is available --\\emph{e.g.} due to anonymization. Traditionally, the problem is approached by first addressing entity matching and subsequently learning the classifier in a standard manner. We present an end-to-end solution which bypasses matching entities,  based on the recently introduced concept of \\emph{Rademacher observations} (rados). Informally, we replace the minimisation of a loss over examples, which requires to solve entity resolution, by the \\textit{equivalent} minimisation of a (different) loss over rados. Among others, key properties we show are (i) a potentially huge subset of these rados \\textit{does not require} to perform entity matching, and (ii) the algorithm that provably minimizes the rado loss over these rados has time and space complexities \\textit{smaller} than the algorithm minimizing the equivalent example loss. Last, we relax a key assumption of the model, that the data is vertically partitioned among peers --- in this case, we would not even know the \\textit{existence} of a solution to entity resolution. In this more general setting, experiments validate the possibility of significantly beating even the \\textit{optimal} peer in hindsight.", 
    "authors": [
      {
        "name": "Giorgio Patrini"
      }, 
      {
        "name": "Richard Nock"
      }, 
      {
        "name": "Stephen Hardy"
      }, 
      {
        "name": "Tiberio Caetano"
      }
    ], 
    "keywords": "Learning from distributed data, Entity resolution, Statistical learning, Supervised learning, Rademacher observations", 
    "title": "Fast Learning from Distributed Datasets without Entity Matching", 
    "type": "paper"
  }, 
  "429": {
    "abstract": "Words are central to text classification. It has been shown that simple Naive Bayes models with word and bigram features can give highly competitive accuracies when compared to more sophisticated models with part-of-speech, syntax and semantic features. Embeddings offer distributional features about words. We study a conceptually simple classification model by exploiting multi-prototype word embeddings based on text classes. The key assumption is that word exhibit different distributional characteristics under different text classes. Based on this assumption, we train multi-prototype distributional word representations for different text classes. Given a new document, its text class is predicted by maximizing the probabilities of embedding vectors of its words under the class. In two standard classification benchmark datasets, one is balance and the other is imbalance, our model outperforms state-of-the-art systems, using both accuracy and macro-average f-1 score.", 
    "authors": [
      {
        "name": "Peng Jin"
      }, 
      {
        "name": "Yue Zhang"
      }, 
      {
        "name": "Xingyuan Chen"
      }, 
      {
        "name": "Yunqing Xia"
      }
    ], 
    "keywords": "multi-prototype word embedding, text classification, Naive Bayes", 
    "title": "Bag-of-Embeddings for Text Classification", 
    "type": "paper"
  }, 
  "437": {
    "abstract": "Conventional location recommendation models rely on users' visit history, geographical influence, temporal influence, etc. to infer users' preference for locations. However, systematically modeling a location's context (i.e., a set of locations visited before and after this location) is relatively unexplored. In this paper, by leveraging Skip-gram model, we learn latent representation for a location to capture the influence of its context. Weighted Approximately Ranked Pairwise loss is then applied to learn latent representations for users for personalized top-N recommendation. We also extend our model by taking into account temporal influence. Stochastic Gradient Descent based optimization algorithms are developed to fit the models. We conduct comprehensive experiments over 4 real datasets. Experimental results demonstrate that our approach significantly outperforms the state-of-the-art location recommendation methods.", 
    "authors": [
      {
        "name": "Xin Liu"
      }, 
      {
        "name": "Yong Liu"
      }
    ], 
    "keywords": "location recommendation, skip-gram, context, top-N recommendation, temporal-aware recommendation", 
    "title": "Exploring the Context of Locations for Personalized Location Recommendations", 
    "type": "paper"
  }, 
  "438": {
    "abstract": "In this paper, we develop a novel approach to aspect term extraction based on unsupervised learning of distributed representations of words and dependency paths. The basic idea is to connect two words (w1 and w2) with the dependency path (r) between them in the embedding space. Specifically, our method optimizes the objective w1 + r \u00e2\u0089\u0088 w2 in the low-dimensional space, where the multi-hop dependency paths are treated as a sequence of grammatical relations and modeled by a recurrent neural network. Then, we design the embedding features that consider linear context and dependency context information, for the conditional random field (CRF) based aspect term extraction. Experimental results on the SemEval datasets show that, (1) with only embedding features, we can achieve state-of-the-art results; (2) our embedding method which incorporates the syntactic information among words yields better performance than other representative ones in aspect term extraction.", 
    "authors": [
      {
        "name": "Yichun Yin"
      }, 
      {
        "name": "Furu Wei"
      }, 
      {
        "name": "Li Dong"
      }, 
      {
        "name": "Kaimeng Xu"
      }, 
      {
        "name": "Ming Zhang"
      }, 
      {
        "name": "Ming Zhou"
      }
    ], 
    "keywords": "Aspect term extraction, Word embedding, Dependency path embedding", 
    "title": "Unsupervised Word and Dependency Path Embeddings for Aspect Term Extraction", 
    "type": "paper"
  }, 
  "440": {
    "abstract": "Heterogeneous data with complex feature dependencies is common in real-world applications. Clustering algorithms for mixed -- continuous and discrete valued -- features often do not adequately model dependencies and are limited to modeling meta--Gaussian distributions. Copulas, that provide a modular parameterization of joint distributions, can model a variety of dependencies but their use with discrete data remains limited due to challenges in parameter inference. In this paper we use Gaussian mixture copulas, to model complex dependencies beyond those captured by meta--Gaussian distributions, for clustering. We design a new, efficient, semiparametric algorithm to approximately estimate the parameters of the copula that can fit continuous, ordinal and binary data. We analyze the conditions for obtaining consistent estimates and empirically demonstrate performance improvements over state-of-the-art methods of correlation clustering on synthetic and benchmark datasets.", 
    "authors": [
      {
        "name": "Vaibhav Rajan"
      }, 
      {
        "name": "Sakyajit Bhattacharya"
      }
    ], 
    "keywords": "Dependency clustering, Gaussian mixture copula, Extended rank likelihood", 
    "title": "Dependency Clustering of Mixed Data with Gaussian Mixture Copulas", 
    "type": "paper"
  }, 
  "448": {
    "abstract": "Determining the direction in which a person is looking is an important problem in a wide range of HCI applications.  In this paper we describe a highly accurate algorithm that performs gaze estimation using an affordable and widely available device such as Kinect. The method we propose starts by performing accurate head pose estimation achieved by fitting a person specific morphable model of the face to depth data.  The ordinarily competing requirements of high accuracy and high speed are met concurrently by formulating the fitting objective function as a combination of terms which excel either in accurate or fast fitting, and then by adaptively adjusting their relative contributions throughout fitting.  Follow pose estimation, pose normalization is done by re-rendering the fitted model as a frontal face.  Finally gaze estimates are obtained through regression from the appearance of the eyes in synthetic, normalized images.  Using a standard public dataset for for the evaluation of gaze estimation algorithms from Kinect data we demonstrate that our method greatly outperforms the state of the art.", 
    "authors": [
      {
        "name": "Reza Ghiass"
      }, 
      {
        "name": "Ognjen Arandjelovic"
      }
    ], 
    "keywords": "gaze, HCI, pose, eyes, interaction", 
    "title": "Highly Accurate Gaze Estimation using a Consumer RGB+Depth Sensor", 
    "type": "paper"
  }, 
  "45": {
    "abstract": "Outliers are ubiquitous in modern data sets. Distance-based techniques are a popular non-parametric approach to outlier detection as they require no prior assumptions on the data generating distribution and are simple to implement. Scaling these techniques to massive data sets without sacrificing accuracy is a challenging task. We propose a novel algorithm based on the intuition that outliers have a significant influence on the quality of distance-based clustering solutions. We propose sensitivity - the worst-case impact of a data point on the clustering objective - as a measure of outlierness. We then prove that influence - a (non-trivial) upper-bound on the sensitivity can be computed by a simple linear time algorithm. To scale beyond a single machine, we propose a communication efficient distributed algorithm. In an extensive experimental evaluation we demonstrate the effectiveness and establish the statistical significance of the proposed approach. In particular, it outperforms the most popular distance-based approaches while being several orders of magnitude faster.", 
    "authors": [
      {
        "name": "Mario Lucic"
      }, 
      {
        "name": "Olivier Bachem"
      }, 
      {
        "name": "Andreas Krause"
      }
    ], 
    "keywords": "Outlier detection, Scalability, Sensitivity, Clustering", 
    "title": "Linear-time Outlier Detection via Sensitivity", 
    "type": "paper"
  }, 
  "453": {
    "abstract": "Cross-modal learning tries to find various types of heterogeneous data (e.g., image) from a given query (e.g., text). Most cross-modal algorithms heavily rely on semantic labels and benefit from a semantic-preserving aggregation of pairs of heterogeneous data. However, the semantic labels are not readily obtained in many real-world applications. This paper studies the aggregation of these pairs in an unsupervised way. Apart from lower pairwise correspondences that force the data from one pair to be close to each other, we propose a novel concept, referred as \\textbf{groupwise correspondences}, supposing that each paired heterogeneous data are from an identical latent group. We incorporate this groupwise correspondences into \\emph{canonical correlation analysis} (CCA) model, and seek a latent common subspace where data are naturally clustered into several latent groups. To simplify this nonconvex and nonsmooth problem, we introduce a non-negative orthogonal variable to represent the soft group membership, then two coupled computationally efficient subproblems (a generalized ratio-trace problem and a non-negative problem) are alternatively minimized to guarantee the proposed algorithm converges locally. Experimental results on two benchmark datasets demonstrate that the proposed unsupervised algorithm even achieves comparable performance to some state-of-the-art supervised cross-modal algorithms.", 
    "authors": [
      {
        "name": "Jian Liang"
      }, 
      {
        "name": "Ran He"
      }, 
      {
        "name": "Zhenan Sun"
      }, 
      {
        "name": "Tieniu Tan"
      }
    ], 
    "keywords": "cross-modal, unsupervised learning, latent subspace, groupwise correspondences", 
    "title": "Group-Invariant Cross-Modal Subspace Learning", 
    "type": "paper"
  }, 
  "46": {
    "abstract": "Recent proposed machine comprehension(MC) application is an effort to deal with natural language understanding problem. However, the small size of machine comprehension labeled data confines the application of deep neural network architecture that has shown advantage in semantic inference tasks. Previous methods use a lot of NLP tools to extract linguistic features, but gain little improvement over simple baseline. In this paper, we build an attention-based recurrent neural network model, train it with the help of external knowledge which is semantically relevant to machine comprehension, and achieves a new state-of-art result.", 
    "authors": [
      {
        "name": "Bingning Wang"
      }, 
      {
        "name": "Shangmin Guo"
      }, 
      {
        "name": "Kang Liu"
      }, 
      {
        "name": "Shizhu He"
      }, 
      {
        "name": "Jun Zhao"
      }
    ], 
    "keywords": "machine comprehension, attention based model, question answering, Answer selection, recognizing textual entailment", 
    "title": "Employing External Rich Knowledge for Machine Comprehension", 
    "type": "paper"
  }, 
  "462": {
    "abstract": "This paper studies the problem of RGB-D object recognition. Inspired by the great success of deep convolutional neural networks (DCNN) in AI, researchers have tried to apply it to improve the performance of RGB-D object recognition. However, DCNN always requires a large-scale annotated dataset to supervise its training. Manually labeling such a large RGB-D dataset is expensive and time consuming, which prevents DCNN from quickly promoting this research area. To address this problem, we propose a semi-supervised multimodal deep learning framework to train DCNN effectively based on very limited labeled data and massive unlabeled data. The core of our framework is a novel diversity preserving co-training algorithm, which can successfully guide DCNN to learn from the unlabeled RGB-D data by making full use of the complementary cues of the RGB and depth data in object representation. Experiments on the benchmark RGB-D dataset demonstrate that, with only 5% labeled training data, our approach achieves competitive performance for object recognition compared with those state-of-the-art results reported by fully-supervised methods.", 
    "authors": [
      {
        "name": "Yanhua Cheng"
      }, 
      {
        "name": "Xin Zhao"
      }, 
      {
        "name": "Rui Cai"
      }, 
      {
        "name": "Zhiwei Li"
      }, 
      {
        "name": "Kaiqi Huang"
      }, 
      {
        "name": "Yong Rui"
      }
    ], 
    "keywords": "co-training, multimodal, diversity preserving, RGB-D object recognition", 
    "title": "Semi-Supervised Multimodal Deep Learning for RGB-D Object Recognition", 
    "type": "paper"
  }, 
  "463": {
    "abstract": "Unsupervised word representations have demonstrated improvements in predictive generalization on various NLP tasks. Much effort has been devoted to effectively learning word representations, but little attention has been given to distributed character representations, although such character-level representations could be very useful for a variety of NLP applications in intrinsically \"character-based\" languages (e.g. Chinese and Japanese). On the other hand, most of existing representation models create a single-prototype representation per word, which is problematic because many words are in fact polysemous, and a single-prototype model is incapable of capturing phenomena of homonymy and polysemy. In this paper, we present a novel neural network architecture to jointly learn character embeddings and induce context representations from large data sets. The explicitly produced context representations are further used to learn context-specific and multiple-prototype character embeddings, particularly capturing their polysemous variants. Our character embeddings were evaluated on three NLP tasks of character similarity, word segmentation, and named entity recognition, and the experimental results demonstrated the proposed model outperforms other competing ones on all the three tasks.", 
    "authors": [
      {
        "name": "Xiaoqing Zheng"
      }, 
      {
        "name": "Jiangtao Feng"
      }, 
      {
        "name": "Wenqiang Zhang"
      }
    ], 
    "keywords": "Distributed character representations, Multi-prototype model, Deep neural network", 
    "title": "Context-Specific and Multi-Prototype Character Representations", 
    "type": "paper"
  }, 
  "467": {
    "abstract": "We propose Neural Enquirer -- a neural network architecture for answering natural language (NL) questions given a knowledge base (KB) table. Unlike previous work on end-to-end training of semantic parsers, Neural Enquirer is fully \"neuralized\": it gives distributed representations of queries and KB tables, and executes queries through a series of differentiable operations. Operations are modeled by layers of \"executors'', which compute intermediate results in the form of table annotations at different levels. Neural Enquirer can be trained with gradient descent, with which not only the parameters of the semantic parsing component and query execution component, but also the embeddings of query words and tables can be learned from scratch. The training can be done in an end-to-end fashion, and it can also be carried out with stronger guidance, e.g., step-by-step supervision for complex queries. Neural Enquirer is an important step towards building neural network systems that can understand natural language in real-world tasks. Our experiments show that the model can learn to execute complex NL queries on KB tables with rich structures.", 
    "authors": [
      {
        "name": "Pengcheng Yin"
      }, 
      {
        "name": "Zhengdong Lu"
      }, 
      {
        "name": "Hang Li"
      }, 
      {
        "name": "Ben Kao"
      }
    ], 
    "keywords": "Deep Learning, Natural Language Understanding, Semantic Parsing, Learning to Execute", 
    "title": "Neural Enquirer: Learning to Query Tables in Natural Language", 
    "type": "paper"
  }, 
  "470": {
    "abstract": "The popularity of social media creates a large amount of user-generated content, playing an important role in addressing cold-start problems in recommendation. Although much effort has been devoted to incorporating this information into recommendation, past work mainly targets explicit feedback. There is still no general framework tailored to implicit feedback, such as views, listens, or visits. To this end, we propose a sparse Bayesian content-aware collaborative filtering framework especially for implicit feedback, and develop a scalable optimization algorithm to jointly learn latent factors and hyperparameters. Due to the adaptive update of hyperparameters, automatic feature selection is naturally embedded in this framework. Convincing experimental results on three different implicit feedback datasets indicate the superiority of the proposed algorithm to state-of-the-art content-aware recommendation methods.", 
    "authors": [
      {
        "name": "Defu Lian"
      }, 
      {
        "name": "Yong Ge"
      }, 
      {
        "name": "Nicholas Jing Yuan"
      }, 
      {
        "name": "Xing Xie"
      }, 
      {
        "name": "Hui Xiong"
      }
    ], 
    "keywords": "content-aware collaborative filtering, sparse Bayesian learning, implicit feedback", 
    "title": "Sparse Bayesian Content-Aware Collaborative Filtering for Implicit Feedback", 
    "type": "paper"
  }, 
  "478": {
    "abstract": "This paper studies a decision-making problem for heterogeneous multi-agent systems, in  the presence of safety density constraints. An individual agent's decision-making problem is modeled by the standard Markov Decision Process (MDP) formulation. However, the MDP states may have limited capacities, hence upper bounds on the expected  number of agents in each state are imposed. We refer to  these upper bound constraints  as ``safety\"  constraints.  If  agents follow  unconstrained policies (policies that do not impose the safety constraints), the safety constraints might be violated. In this paper, we devise algorithms that provide safe decision-making policies. The set of safe decision policies can be shown to be convex, and hence the policy synthesis is tractable via reliable and fast Interior Point Method (IPM) algorithms.  We evaluate the effectiveness of the proposed algorithms using  a simple MDP problem first, and then using a dynamic traffic assignment problem. The numerical results demonstrate  that safe decision-making  policies of this paper significantly outperform other baseline decision algorithms.", 
    "authors": [
      {
        "name": "Ruohan Zhang"
      }, 
      {
        "name": "Yue Yu"
      }, 
      {
        "name": "Mahmoud El Chamie"
      }, 
      {
        "name": "Behcet Acikmese"
      }, 
      {
        "name": "Dana Ballard"
      }
    ], 
    "keywords": "safe density constraints, Markov decision process, heterogeneous multiagent system, dynamic traffic assignment", 
    "title": "Decision-Making Policies for Heterogeneous Autonomous Multi-Agent Systems with Safety Constraints", 
    "type": "paper"
  }, 
  "480": {
    "abstract": "The k-support-norm regularized minimization has recently been applied with success to sparse prediction problems. The proximal gradient method is conventionally used to minimize this composite model. Although converges fast, the proximal method tends to suffer from expensive iteration cost as the proximity operator associated with k-support-norm needs exhaustive searching operations and thus could be time consuming in large scale settings. In this paper, we reformulate the k-support-norm regularized formulation into an identical constrained formulation and propose a fully corrective Frank-Wolfe algorithm to minimize the constrained model. Our method is inspired by an interesting observation that the convex hull structure of the $k$-support-norm ball allows the application of Frank-Wolfe-type algorithms with low iteration complexity. The convergence behavior and estimation error of the proposed algorithm are analyzed. Extensive numerical results in  learning tasks including logistic regression and matrix pursuit demonstrate the substantially improved computational efficiency of our algorithm over the state-of-the-art proximal gradient algorithms.", 
    "authors": [
      {
        "name": "Bo Liu"
      }, 
      {
        "name": "Xiaotong Yuan"
      }, 
      {
        "name": "Shaoting Zhang"
      }, 
      {
        "name": "Qingshan Liu"
      }, 
      {
        "name": "Dimitris Metaxas"
      }
    ], 
    "keywords": "Sparsity, k-Support Norm, Frank-Wolfe Algorithm, Regularized Minimization", 
    "title": "Efficient k-Support-Norm Regularized Minimization via Fully Corrective Frank-Wolfe Method", 
    "type": "paper"
  }, 
  "488": {
    "abstract": "We investigate learning heuristics for domain-specific planning. Prior work framed learning a heuristic as an ordinary regression problem. However, in a greedy best-first search, the it ordering of states induced by a heuristic is more indicative of the resulting planner's performance than mean squared error. Thus, we instead frame learning a heuristic as an ordinal regression problem which we solve using a RankSVM formulation. Additionally, we introduce new methods for computing features that capture temporal interactions in an approximate plan. Our experiments on recent International Planning Competition problems show that the RankSVM learned heuristics outperform both the original heuristics and heuristics learned through ordinary regression.", 
    "authors": [
      {
        "name": "Caelan Garrett"
      }, 
      {
        "name": "Leslie Kaelbling"
      }, 
      {
        "name": "Tomas Lozano-Perez"
      }
    ], 
    "keywords": "Automated Planning, Heuristic Search, Ordinal Regression", 
    "title": "Ordinal Regression for Learning Planning Heuristics", 
    "type": "paper"
  }, 
  "489": {
    "abstract": "Learning the representations of a knowledge graph has attracted significant research interest in the field of intelligent Web. By regarding each relation as one translation from head entity to tail entity, translation-based methods including TransE, TransH and TransR are simple, effective and achieving the state-of-the-art performance. However, they still suffer the following issues: (i) low performance when modeling 1-to-N, N-to-1 and N-to-N relations. (ii) limited performance due to the structure sparseness of the knowledge graph. In this paper, we propose a novel knowledge graph representation learning method by taking advantage of the rich context information in a text corpus. The rich textual context information is incorporated to expand the semantic structure of the knowledge graph and each relation is enabled to own different representations for different head and tail entities to better handle 1-to-N, N-to-1 and N-to-N relations. Experiments on multiple benchmark datasets show that our proposed method successfully addresses the above issues and significantly outperforms the state-of-art methods.", 
    "authors": [
      {
        "name": "Zhigang Wang"
      }, 
      {
        "name": "Juanzi Li"
      }, 
      {
        "name": "Zhiyuan Liu"
      }, 
      {
        "name": "Jie Tang"
      }
    ], 
    "keywords": "Representation Learning, Knowledge Graph, Text Information", 
    "title": "Text-enhanced Representation Learning for Knowledge Graph", 
    "type": "paper"
  }, 
  "495": {
    "abstract": "We introduce the Maximum Sustainable Yield problem for a multi-robot foraging and construction system, inspired by the relationship between the natural resource growth and harvesting behaviors in an ecosystem. The resources spawn according to the logistic model and are vulnerable to overharvesting. The robots must maintain sustainability while maximizing productivity. The foraging robots harvest different types of resources, which enable a construction robot to build new foraging robots. We design and implement several algorithms to perform robot construction, assignment, and scheduling. However, a critical issue is that resource growth model is often unknown in practice. We propose an adaptive algorithm that overcomes this problem. We also show that our algorithms are robust to noises in the actuation and environment. The case where the observation noise could harm sustainability is discussed.", 
    "authors": [
      {
        "name": "Ruohan Zhang"
      }, 
      {
        "name": "Zhao Song"
      }
    ], 
    "keywords": "multi-robot foraging system, maximum sustainable yield, information incompleteness and noise", 
    "title": "Maximum Sustainable Yield Problem for Robot Foraging and Construction System", 
    "type": "paper"
  }, 
  "50": {
    "abstract": "With the rise of social media such as Twitter, people are more willing to convey their stressful life events via these platforms. In a sense, it is feasible to detect stress from social media data for proactive health care. In psychology, stress is composed of stressor and stress level, where stressor further comprises of stressor event and subject. By far, little attention has been paid to estimate exact stressor and stress level from social media data, due to the following challenges: 1) stressor subject identification, 2) stressor event detection, and 3) data collection and representation. To address these problems, we devise a comprehensive scheme to measure a user\u00e2\u0080\u0099s stress level from his/her social media data. In particular, we first build a benchmark dataset and extract a rich set of stress-oriented features. We then propose a novel hybrid multi-task model to detect the stressor event and subject, which is capable of modeling the relatedness among stressor events as well as stressor subjects. At last, we lookup an expert-defined stress table with the detected subject and event to estimate the stressor and stress level. Extensive experiments on real-world datasets well verify the effectiveness of our scheme.", 
    "authors": [
      {
        "name": "Huijie Lin"
      }, 
      {
        "name": "Jia Jia"
      }, 
      {
        "name": "Liqiang Nie"
      }, 
      {
        "name": "Guangyao Shen"
      }, 
      {
        "name": "Tat-Seng Chua"
      }
    ], 
    "keywords": "social media, health care, emotion analysis, affective computing", 
    "title": "What does Social Media Say about Your Stress?", 
    "type": "paper"
  }, 
  "5001": {
    "abstract": "", 
    "authors": [
      {
        "name": "Dafna Shahaf"
      }
    ], 
    "title": "A Hard Look at Soft Concepts", 
    "type": "talk"
  }, 
  "5002": {
    "abstract": "", 
    "authors": [
      {
        "name": "Ruslan Salakhutdinov"
      }
    ], 
    "title": "Recent Advances in Deep Learning: Learning Structured, Robust, and Multimodal Deep Models", 
    "type": "talk"
  }, 
  "5003": {
    "abstract": "", 
    "authors": [
      {
        "name": "Yejin Choi"
      }
    ], 
    "title": "Language and Vision: Learning Knowledge about the World", 
    "type": "talk"
  }, 
  "5004": {
    "abstract": "", 
    "authors": [
      {
        "name": "Abhinav Gupta"
      }
    ], 
    "title": "Supersizing Self-Supervision: Never Ending Learning from Images, Text and Physical Interactions", 
    "type": "talk"
  }, 
  "5005": {
    "abstract": "", 
    "authors": [
      {
        "name": "Amanda Coles"
      }
    ], 
    "title": "Planning with Expressive World Models", 
    "type": "talk"
  }, 
  "5006": {
    "abstract": "", 
    "authors": [
      {
        "name": "Mausam"
      }
    ], 
    "title": "Open Information Extraction Systems and Downstream Applications", 
    "type": "talk"
  }, 
  "5007": {
    "abstract": "", 
    "authors": [
      {
        "name": "Finale Velez-Doshi"
      }
    ], 
    "title": "Unsupervised Methods for Disease Trajectory Subtyping", 
    "type": "talk"
  }, 
  "5008": {
    "abstract": "", 
    "authors": [
      {
        "name": "Stefano Ermon"
      }
    ], 
    "title": "Machine Learning and Decision Making for Sustainability", 
    "type": "talk"
  }, 
  "5009": {
    "abstract": "", 
    "authors": [
      {
        "name": "Edith Elkind"
      }
    ], 
    "title": "Preference Restrictions in Computational Social Choice: Recent Progress", 
    "type": "talk"
  }, 
  "5010": {
    "abstract": "", 
    "authors": [
      {
        "name": "Matti Jarvisalo"
      }
    ], 
    "title": "Boolean Satisfiability and Beyond: Algorithms, Analysis, and AI Applications", 
    "type": "talk"
  }, 
  "5011": {
    "abstract": "", 
    "authors": [
      {
        "name": "Shivani Agarwal"
      }
    ], 
    "title": "On Ranking and Choice Models", 
    "type": "talk"
  }, 
  "5012": {
    "abstract": "", 
    "authors": [
      {
        "name": "Sven Seuken"
      }
    ], 
    "title": "AI, Game Theory and Economics: A \u201cDream Team\u201d for the Design of Combinatorial Auctions", 
    "type": "talk"
  }, 
  "5013": {
    "abstract": "", 
    "authors": [
      {
        "name": "Suchi Saria"
      }
    ], 
    "title": "Towards an Individualized Reasoning Engine (for Healthcare)", 
    "type": "talk"
  }, 
  "5014": {
    "abstract": "", 
    "authors": [
      {
        "name": "Yevgeniy Vorobeychik"
      }
    ], 
    "title": "Adversarial Artificial Intelligence", 
    "type": "talk"
  }, 
  "5015": {
    "abstract": "", 
    "authors": [
      {
        "name": "Ece Kamar"
      }
    ], 
    "title": "Directions in Hybrid Intelligence: Complementing AI Systems with Human Intelligence", 
    "type": "talk"
  }, 
  "5016": {
    "abstract": "", 
    "authors": [
      {
        "name": "Haris Aziz"
      }
    ], 
    "title": "Computer Science and Social Choice: A Fruitful Symbiosis", 
    "type": "talk"
  }, 
  "5017": {
    "abstract": "", 
    "authors": [
      {
        "name": "Julie A Shah"
      }
    ], 
    "title": "Humans and Machine of Like Mind: Augmenting Humans through Collaboration in Decision-Making Tasks", 
    "type": "talk"
  }, 
  "5018": {
    "abstract": "", 
    "authors": [
      {
        "name": "Steven Schockaert"
      }
    ], 
    "title": "Plausible reasoning based on qualitative entity embeddings", 
    "type": "talk"
  }, 
  "5019": {
    "abstract": "", 
    "authors": [
      {
        "name": "Meghyn Bienvenu"
      }
    ], 
    "title": "Ontology-Mediated Query Answering: Harnessing Knowledge to Get More From Data", 
    "type": "talk"
  }, 
  "502": {
    "abstract": "In highly anonymous environments such as the Internet, many applications suffer from the fact that a single user can pose as multiple users. Indeed, presumably many potential applications do not even get off the ground as a result. Consider the example of an online vote. Requiring voters to provide identifying information, to the extent that this is even feasible, can significantly deter participation. On the other hand, not doing so makes it possible for a single individual to vote more than once, so that the result may become almost meaningless (as many examples of bizarre outcomes of online votes demonstrate). CAPTCHAs may prevent running a program that votes many times, but they do nothing to prevent a single user from voting many times by hand. In this paper, we propose ATUCAPTS (Automated Tests That a User Cannot Pass Twice Simultaneously) as a solution. ATUCAPTS are automatically generated tests such that it is (1) easy for a user to pass one instance, but (2) extremely difficult for a user to pass two instances at the same time. Thus, if it is feasible to require all users to take such a test at the same time, we can verify that no user holds more than one account. We propose a specific class of ATUCAPTS and present the results of a human subjects study to validate that they satisfy the two properties above. We also introduce several theoretical models of how well an attacker might perform and show that these models still allow for good performance on both (1) and (2) with reasonable test lengths.", 
    "authors": [
      {
        "name": "Garrett Andersen"
      }, 
      {
        "name": "Vincent Conitzer"
      }
    ], 
    "keywords": "anonymity, online voting and rating, Sybil attacks, CAPTCHAs", 
    "title": "ATUCAPTS: Automated Tests That a User Cannot Pass Twice Simultaneously", 
    "type": "paper"
  }, 
  "5020": {
    "abstract": "", 
    "authors": [
      {
        "name": "Guy Van den Broeck"
      }
    ], 
    "title": "First-Order Probabilistic Reasoning: Successes and Challenges", 
    "type": "talk"
  }, 
  "5021": {
    "abstract": "", 
    "authors": [
      {
        "name": "Sonia Chernova"
      }
    ], 
    "title": "Crowds and Robots: Leveraging the Web to Advance Robot Autonomy", 
    "type": "talk"
  }, 
  "5022": {
    "abstract": "", 
    "authors": [
      {
        "name": "Pradeep Varakantham"
      }
    ], 
    "title": "Sequential Decision Making for Improving Efficiency in Urban Environments", 
    "type": "talk"
  }, 
  "5023": {
    "abstract": "", 
    "authors": [], 
    "title": "Workshops Highlights1", 
    "type": "talk"
  }, 
  "5024": {
    "abstract": "", 
    "authors": [], 
    "title": "Workshops Highlights2", 
    "type": "talk"
  }, 
  "5025": {
    "abstract": "", 
    "authors": [], 
    "title": "Opening Remarks", 
    "type": "talk"
  }, 
  "5026": {
    "abstract": "", 
    "authors": [
      {
        "name": "David Silver (Google DeepMind)"
      }
    ], 
    "title": "AlphaGo - Mastering the Game of Go with Deep Neural Networks and Tree Search", 
    "type": "talk"
  }, 
  "5027": {
    "abstract": "", 
    "authors": [
      {
        "name": "Wolfram Burgard"
      }
    ], 
    "title": "Probabilistic Techniques for Robot Navigation", 
    "type": "talk"
  }, 
  "5028": {
    "abstract": "", 
    "authors": [
      {
        "name": "Sheila McIlraith"
      }
    ], 
    "title": "Do as I say and as I do:  The future of automated programming", 
    "type": "talk"
  }, 
  "5029": {
    "abstract": "", 
    "authors": [
      {
        "name": "Kevin Leyton-Brown"
      }
    ], 
    "title": "Incentive Auctions and Spectrum Repacking: A Case Study for \"Deep Optimization\"", 
    "type": "talk"
  }, 
  "5030": {
    "abstract": "", 
    "authors": [
      {
        "name": "Robert Holte"
      }
    ], 
    "title": "Heuristic Search: Something Old and Something New", 
    "type": "talk"
  }, 
  "5031": {
    "abstract": "", 
    "authors": [
      {
        "name": "James A. Hendler"
      }
    ], 
    "title": "Knowledge Representation in the Era of Deep Learning, Watson and the Semantic Web", 
    "type": "talk"
  }, 
  "5032": {
    "abstract": "", 
    "authors": [
      {
        "name": "Jonathan Gratch"
      }
    ], 
    "title": "AI\u2019s Final Frontier? Buildings machines that understand and shape human emotion", 
    "type": "talk"
  }, 
  "5033": {
    "abstract": "", 
    "authors": [
      {
        "name": "Barbara Grosz"
      }
    ], 
    "title": "From the Turing Test to Smart Partners: or \u201cIs Your System Smart Enough to Work with Us?\u201d", 
    "type": "talk"
  }, 
  "5034": {
    "abstract": "", 
    "authors": [
      {
        "name": "Percy Liang"
      }
    ], 
    "title": "Computers and Thought Award Talk", 
    "type": "talk"
  }, 
  "5035": {
    "abstract": "", 
    "authors": [
      {
        "name": "Moshe Tennenholtz"
      }
    ], 
    "title": "AI with GT Flavor", 
    "type": "talk"
  }, 
  "5036": {
    "abstract": "", 
    "authors": [
      {
        "name": "Michael I. Jordan"
      }
    ], 
    "title": "Research Excellence Award Talk", 
    "type": "talk"
  }, 
  "5037": {
    "abstract": "", 
    "authors": [], 
    "title": "Closing Ceremony", 
    "type": "talk"
  }, 
  "5038": {
    "abstract": "", 
    "authors": [], 
    "title": "Industry Day Talks1", 
    "type": "talk"
  }, 
  "5039": {
    "abstract": "", 
    "authors": [], 
    "title": "Industry Day Talks2", 
    "type": "talk"
  }, 
  "5040": {
    "abstract": "", 
    "authors": [], 
    "title": "Preparing for the Future of AI", 
    "type": "talk"
  }, 
  "523": {
    "abstract": "Matrix approximation (MA) is one of the most popular techniques for collaborative filtering (CF). Most existing MA methods train user/item latent factors based on a user-item rating matrix and then use the global latent factors to model all users/items. However, globally optimized latent factors may not reflect the unique interests shared among only subsets of users/items, without which unique interests of users may not be accurately modelled. As a result, existing MA methods, which cannot capture the uniqueness of different user/item, cannot provide optimal recommendation.  In this paper, a mixture probabilistic matrix approximation (MPMA) method is proposed, which unifies globally optimized user/item feature vectors (on the entire rating matrix) and locally optimized user/item feature vectors (on subsets of user/item ratings) to improve recommendation accuracy. More specifically, in MPMA, a method is developed to find both globally and locally optimized user/item feature vectors. Then, a Gaussian mixture model is adopted to combine global predictions and local predictions to produce accurate rating predictions. Experimental study using MovieLens and Netflix datasets demonstrates that MPMA outperforms five state-of-the-art MA based CF methods in recommendation accuracy with good scalability.", 
    "authors": [
      {
        "name": "Chao Chen"
      }, 
      {
        "name": "Dongsheng Li"
      }, 
      {
        "name": "Qin Lv"
      }, 
      {
        "name": "Junchi Yan"
      }, 
      {
        "name": "Stephen Chu"
      }, 
      {
        "name": "Li Shang"
      }
    ], 
    "keywords": "recommendation, collaborative filtering, matrix approximation", 
    "title": "MPMA: Mixture Probabilistic Matrix Approximation for Collaborative Filtering", 
    "type": "paper"
  }, 
  "524": {
    "abstract": "Semantic attributes have been proposed to bridge the semantic gap between low-level feature representation and high-level semantic understanding of visual objects. Obtaining a good representation of semantic attributes usually requires learning from high-dimensional low-level features, which not only significantly increases the time and space requirement but also degrades the performance due to numerous irrelevant features. Since multi-attribute prediction can be generalized as an multi-task learning problem, sparse-based multi-task feature selection approaches have been introduced, utilizing the relatedness among multiple attributes. However, such approaches either do not investigate the pattern of the relatedness among attributes, or require prior knowledge about the pattern. In this paper, we propose a novel feature selection approach which embeds attribute correlation modeling in multi-attribute joint feature selection. Experiments on both synthetic dataset and multiple public benchmark datasets demonstrate that the proposed approach effectively captures the correlation among multiple attributes and significantly outperforms the state-of-the-art approaches.", 
    "authors": [
      {
        "name": "Lin Chen"
      }, 
      {
        "name": "Baoxin Li"
      }
    ], 
    "keywords": "Semantic attribute, Feature selection, Multi-task learning", 
    "title": "Clustering-based Joint Feature Selection for Semantic Attribute Prediction", 
    "type": "paper"
  }, 
  "529": {
    "abstract": "Diagnosability is the property a Discrete-Event System (DES) exhibits if every fault can be detected and isolated within a finite number of (observable) events that have taken place after its occurrence. In the literature, diagnosability of DESs relies on the availability of a certain observation, that is, an observation that equals the sequence of observable events that have taken place in the DES. But can diagnosability be achieved even if the observation is uncertain? The present paper provides an answer to this question when the observation is temporally or logically uncertain, that is, when the order of the observed events or their (discrete) values are partially unknown. The original notion of compound observable event enables a smooth extension of both the definition of DES diagnosability in the literature and the twin plant method to check such a property. The intuition is to deal with a compound observable event the same way as with a single event. In case a DES is diagnosable even if its observation is uncertain, the diagnosis task can be performed (without any loss in the ability to identify every fault) by adopting a measuring equipment cheaper than that needed to gather a certain observation.", 
    "authors": [
      {
        "name": "Xingyu Su"
      }, 
      {
        "name": "Marina Zanella"
      }, 
      {
        "name": "Alban Grastien"
      }
    ], 
    "keywords": "diagnosis, diagnosability, Discrete-Event Systems, uncertain observations", 
    "title": "Diagnosability of Discrete-Event Systems with Uncertain Observations", 
    "type": "paper"
  }, 
  "538": {
    "abstract": "In this paper, we present a novel approach for relation extraction using only term pairs as the input without textual features. We aim to build a single joint space for each relation which is then used to produce relation specific term embeddings. The proposed method fits particularly well for domains in which similar arguments are often associated with similar relations. It can also handle the situation when the labeled data is limited. The proposed method is evaluated both theoretically with a proof for the closed-form solution and experimentally with promising results on both DBpedia and medical relations.", 
    "authors": [
      {
        "name": "Chang Wang"
      }, 
      {
        "name": "Liangliang Cao"
      }, 
      {
        "name": "James Fan"
      }
    ], 
    "keywords": "relation extraction, semi-supervised learning, medical informatics, embedding", 
    "title": "Building Joint Spaces for Relation Extraction", 
    "type": "paper"
  }, 
  "54": {
    "abstract": "In real-world applications, multi-view data often cannot be collected in a single time due to temporal and spatial constrictions. However, few efforts have been made to develop online multi-view learning algorithms. In this paper, we propose an online Bayesian method to learn predictive subspace from multi-view data with max-margin principle. Specifically, we first define the latent margin loss for multiclass classification in the subspace, and then cast the learning problem into a variational Bayesian framework by exploiting the pseudolikelihood and data augmentation idea. With the variational approximate posterior inferred from the past samples, we can naturally combine historical knowledge with new arrival data, in a Bayesian Passive-Aggressive style. Experiments on various classification tasks show that our model have superior performance.", 
    "authors": [
      {
        "name": "Jia He"
      }, 
      {
        "name": "Changying Du"
      }, 
      {
        "name": "Fuzhen Zhuang"
      }, 
      {
        "name": "Xin Yin"
      }, 
      {
        "name": "Qing He"
      }, 
      {
        "name": "Guoping Long"
      }
    ], 
    "keywords": "Multi-view learning, Max-margin subspace learning, Bayesian online learning, Variational approximate inference", 
    "title": "Online Bayesian Max-margin Subspace Multi-view Learning", 
    "type": "paper"
  }, 
  "554": {
    "abstract": "We present an approach and a system that explores the application of interactive machine learning to a branching program-based boosting algorithm- Martingale Boosting. Typically, its performance is based on the ability of a learner to meet a fixed objective and does not account for preferences ({\\em e.g.}, low FPs) arising from an underlying classification problem. A learner might, therefore, benefit from human guidance on what might be an acceptable performance and then incorporate this preference while learning. While arbitrary preferences might be difficult to meet for a single classifier, a non-linear ensemble of classifiers as the one constructed by martingale boosting, might do better. It is this thesis that we explore here, and show its viability through extensive experiments.", 
    "authors": [
      {
        "name": "Ashish Kulkarni"
      }, 
      {
        "name": "Pushpak Burange"
      }, 
      {
        "name": "Ganesh Ramakrishnan"
      }
    ], 
    "keywords": "martingale boosting, interactive machine learning, asymmetric classification", 
    "title": "Interactive Martingale Boosting", 
    "type": "paper"
  }, 
  "555": {
    "abstract": "The paper presents an Ackermann-based approach for forgetting concept and role symbols in ALCOIH\\mu+(top,and,or)-ontologies. The method is one of the only few approaches that can eliminate role symbols, that can handle role inverse, ABox statements (via nominals) and the only approach so far providing support for forgetting in description logics with nominals. Despite the method not being complete, performance results with a prototypical implementation have shown high success rates on real-life ontologies. Our implementation makes use of a heuristic for determining the order of eliminating concept and roles symbols. This provides more control over the forgetting process and leads to less non-determinism and a small search space.", 
    "authors": [
      {
        "name": "Yizheng Zhao"
      }, 
      {
        "name": "Renate A. Schmidt"
      }
    ], 
    "keywords": "Description logics, Ontologies, Forgetting and uniform interpolation", 
    "title": "Forgetting Concept and Role Symbols in ALCOIHmu+(top, and, or)-Ontologies", 
    "type": "paper"
  }, 
  "565": {
    "abstract": "Non-negative Matrix Factorization (NMF) has received considerable attentions in various areas for its psychological and physiological interpretation of naturally occurring data whose representation may be parts-based in the human brain. Despite its good practical performance, one shortcoming of original NMF is that it ignores intrinsic structure of data set. On one hand, samples might be on a manifold and thus one may hope that geometric information can be exploited to improve NMF's performance. On the other hand, features might correlate with each other, thus conventional $L_2$ distance can not well measure the distance between samples. Although some works have been proposed to solve these problems, rare connects them together. In this paper, we propose a novel method which exploits knowledge in both data manifold and features correlation. We adopt an approximation of Earth Mover's Distance(EMD) as metric and add a graph regularized term based on EMD to NMF. Furthermore, we propose an efficient multiplicative iteration algorithm to solve it. Our empirical study shows the encouraging results of the proposed algorithm comparing with other NMF methods.", 
    "authors": [
      {
        "name": "Wei Qian"
      }, 
      {
        "name": "Bin Hong"
      }, 
      {
        "name": "Deng Cai"
      }, 
      {
        "name": "Xiaofei He"
      }, 
      {
        "name": "Xuelong Li"
      }
    ], 
    "keywords": "Data Representation, Non-negative Matrix Factorization, Sinkhorn Distance", 
    "title": "Non-negative Matrix Factorization with Sinkhorn Distance", 
    "type": "paper"
  }, 
  "571": {
    "abstract": "Sentiment analysis is one of the key challenges for mining online user generated content. In this work, we focus on customer reviews which are an important form of opinionated content. The goal is to identify each sentence's semantic orientation (e.g. positive or negative) of a review. Traditional sentiment classification methods often involve substantial human efforts, e.g. lexicon construction, feature engineering. In recent years, deep learning has emerged as an effective means for solving sentiment classification problems. A neural network intrinsically learns a useful representation automatically without human efforts. However, the success of deep learning highly relies on the availability of large-scale training data. In this paper, we propose a novel deep learning framework for review sentiment classification which employs prevalently available ratings as weak supervision signals. The framework consists of two steps: (1) learn a high level representation (embedding space) which captures the general sentiment distribution of sentences through rating information; (2) add a classification layer on top of the embedding layer and use labeled sentences for supervised fine-tuning. Experiments on review data obtained from Amazon show the efficacy of our method and its superiority over baseline methods.", 
    "authors": [
      {
        "name": "Ziyu Guan"
      }, 
      {
        "name": "Long Chen"
      }, 
      {
        "name": "Wei Zhao"
      }, 
      {
        "name": "Yi Zheng"
      }, 
      {
        "name": "Shulong Tan"
      }, 
      {
        "name": "Deng Cai"
      }
    ], 
    "keywords": "Sentiment Classification, Opinion mining, Deep Learning", 
    "title": "Weakly-supervised Deep Learning for Customer Review Sentiment Classification", 
    "type": "paper"
  }, 
  "572": {
    "abstract": "Recognizing multiple mixed group activities from one still image is not a hard problem for humans but remains highly challenging for computer recognition systems. When modelling interactions among multiple units (i.e., more than two groups or persons), the existing approaches tend to divide them into interactions between pairwise units. However, there is no mathematical evidence to support this transformation. Therefore, these approaches' performance is limited in real applications that capture images containing multiple activities. In this paper, we propose a generative model to provide a more reasonable interpretation for the multiple mixed group activities contained in one image. We design a four level structure and convert the original intra-level interactions into inter-level interactions, in order to implement both interactions among multiple groups and interactions among multiple persons within a group. The proposed four-level structure makes our model more be robust against the occlusion and overlap of the visible poses in images. Experimental results demonstrate that our model makes good interpretations for mixed group activities and outperforms the-state-of-art methods on the Collective Activity Classification dataset.", 
    "authors": [
      {
        "name": "Zheng Zhou"
      }, 
      {
        "name": "Kan Li"
      }, 
      {
        "name": "Xiangjian He"
      }, 
      {
        "name": "Mengmeng Li"
      }
    ], 
    "keywords": "group activity recognition, image-based, multiple groups, generative model", 
    "title": "A Generative Model for Recognizing Mixed Group Activities in Still Images", 
    "type": "paper"
  }, 
  "575": {
    "abstract": "Many sensors have been deployed in the physical world, generating massive geo-tagged time series data. In reality, we usually lose readings of sensors at some unexpected moments because of sensor or communication errors. Those missing readings do not only affect real-time monitoring but also com-promise the performance of further data analysis. In this paper, we propose a spatio-temporal multi-view-based learning (ST-MVL) method to collectively fill missing readings in a collection of geo-sensory time series data, considering 1) the temporal correlation between readings at different timestamps in the same series and 2) the spatial correlation between different time series. Our meth-od combines empirical statistic models, consisting of Inverse Distance Weighting and Simple Exponential Smoothing, with data-driven algorithms, comprised of User-based and Item-based Collaborative Filtering. The former models handle the general missing cases based on empirical assumptions derived from history data over a long period, standing for two global views from a spatial and temporal perspective respectively. The latter algorithms deal with special cases where empirical assumptions may not hold, based on recent contexts of data, denoting two local views from a spatial and temporal perspective respectively. The predictions of the four views are aggregated to a final value in a multi-view learning algorithm. We evaluate our method based on Beijing air quality and meteorological data, finding our model\u00e2\u0080\u0099s advantages beyond ten baseline approaches.", 
    "authors": [
      {
        "name": "Yi Xiuwen"
      }, 
      {
        "name": "Zheng Yu"
      }, 
      {
        "name": "Zhang Junbo"
      }, 
      {
        "name": "Li Tianrui"
      }
    ], 
    "keywords": "Missing value, Spatio-temporal data, Multi-view learning, Urban Computing", 
    "title": "ST-MVL: Filling Missing Values in Geo-sensory Time Series Data", 
    "type": "paper"
  }, 
  "586": {
    "abstract": "Effectiveness and robustness are two essential aspects of supervised learning studies. To achieve the learning effectiveness, ensemble methods are developed to build a strong effective model from ensemble of weak models. To enhance the learning robustness, self-paced learning (SPL) is proposed to learn in a self-controlled pace from easy samples to complex ones. Motivated by simultaneously ensuring the learning effectiveness and robustness, in this paper, we propose a unified framework, Self-Paced Boost Learning (SPBL). With an adaptive from-easy-to-hard pace in boosting process, SPBL asymptotically guides the model to focus more on the insufficiently learned samples with higher reliability. Via a self-paced max-margin boosting optimization with sample selection, SPBL is capable of capturing the intrinsic inter-class discriminative patterns while ensuring the reliability of the samples involved in learning. We formulate SPBL as a fully-corrective optimization for classification task. The experiments on several real-world datasets show the superiority of SPBL in terms of both effectiveness and robustness.", 
    "authors": [
      {
        "name": "Te Pi"
      }, 
      {
        "name": "Xi Li"
      }, 
      {
        "name": "Zhongfei Zhang"
      }, 
      {
        "name": "Deyu Meng"
      }, 
      {
        "name": "Fei Wu"
      }, 
      {
        "name": "Jun Xiao"
      }, 
      {
        "name": "Yueting Zhuang"
      }
    ], 
    "keywords": "boost learning, self-paced learning, classification, effectiveness, robustness", 
    "title": "Self-Paced Boost Learning for Classification", 
    "type": "paper"
  }, 
  "587": {
    "abstract": "Over the past decades, numerous theories and studies have demonstrated that salient objects in different scenes often share some properties in common that make them visually stand out from their surroundings, and thus can be processed in finer details. In this paper, we propose a novel method for salient object detection that involves the transfer of the annotations from an existing example onto an input image. Our method, which is based on the low-level saliency features of each pixel, estimates dense pixel-wise correspondences between the input image and an example image, and then integrates high-level concepts to produce an initial saliency map. Finally, a coarse-to-fine optimization framework is proposed to generate uniformly highlighted salient objects. Qualitatively and quantitatively experiments on six popular benchmark datasets validate that our approach greatly outperforms the state-of-the-art algorithms and recently published works.", 
    "authors": [
      {
        "name": "Xin Li"
      }, 
      {
        "name": "Fan Yang"
      }, 
      {
        "name": "Leiting Chen"
      }, 
      {
        "name": "Hongbin Cai"
      }
    ], 
    "keywords": "computer vision, object detection, feature extraction, image segmentation", 
    "title": "Saliency Transfer: An Example-Based Method for Salient Object Detection", 
    "type": "paper"
  }, 
  "591": {
    "abstract": "We propose a model of interdependent scheduling games in which each player has a set of services that they schedule independently. Each of these services only begin to accrue reward for the player when all predecessor services, which may or may not be in the set of services for the player, have been activated. This model, where players have interdependent services, is motivated by the problems faced in planning and coordinating large-scale infrastructures, e.g., restoring electricity and gas to residents after a natural disaster or providing medical care after a disaster when different agencies are responsible for the delivery of staff, equipment, and medicine. We undertake a detailed game-theoretic analysis of this setting and in particular consider the issues of welfare maximization, computing best responses, Nash dynamics, and existence and computation of Nash equilibria.", 
    "authors": [
      {
        "name": "Andres Abeliuk"
      }, 
      {
        "name": "Haris Aziz"
      }, 
      {
        "name": "Gerardo Berbeglia"
      }, 
      {
        "name": "Serge Gaspers"
      }, 
      {
        "name": "Petr Kalina"
      }, 
      {
        "name": "Nicholas Mattei"
      }, 
      {
        "name": "Dominik Peters"
      }, 
      {
        "name": "Paul Stursberg"
      }, 
      {
        "name": "Pascal Van Hentenryck"
      }, 
      {
        "name": "Toby Walsh"
      }
    ], 
    "keywords": "game theory, scheduling, coordination, multi-agent systems", 
    "title": "Interdependent Scheduling Games", 
    "type": "paper"
  }, 
  "602": {
    "abstract": "Over the past decade, computer-automated barter exchange has become one of the most successful applications at the intersection of AI and economics. Standard exchange models, such as house allocation and kidney exchange cannot be applied to an emerging industrial application, coined *digital good exchange*, where an agent still possesses her initial endowment after exchanging with others. However, her valuation toward the endowment decreases as it is possessed by more agents.  We put forward game theoretical models tailored for digital good exchange. In the first part of the paper, we first consider a natural class of games where agents can choose either a subset of other participants' items or no participation at all. It turns out that this class of games can be modeled as a variant of congestion games. We prove that it is in general NP-complete to determine whether there exists a non-trivial pure Nash equilibrium where at least some agent chooses a nonempty subset of items. However, we show that in a subset of games for single-minded agents with unit demand, there exist non-trivial Pure Nash equilibria and put forward an efficient algorithm to find such equilibria.  In the second part of the paper, we investigate digital good exchange from a mechanism design perspective. We ask if there is a truthful mechanism in this setting that can achieve good social welfare guarantee. To this end, we design a randomized fixed-price-exchange mechanism that is individually rational and truthful, and for two-player case yields a tight log-approximation with respect to any individually rational allocation.", 
    "authors": [
      {
        "name": "Wenyi Fang"
      }, 
      {
        "name": "Pingzhong Tang"
      }, 
      {
        "name": "Song Zuo"
      }
    ], 
    "keywords": "barter exchange, digital good exchange, congestion games", 
    "title": "Digital Good Exchange", 
    "type": "paper"
  }, 
  "603": {
    "abstract": "Word cloud is a visualization form for text that is recognized for its aesthetic, social, and analytical values.  Here, we are concerned with deepening its analytical value for visual comparison of documents.  To aid comparative analysis of two or more documents, users need to be able to perceive similarities and differences among documents through their word clouds.  However, as we are dealing with text, approaches that treat words independently may impede accurate discernment of similarities among word clouds containing different words of related meanings.  We therefore motivate the principle of displaying related words in a coherent manner, and propose to realize it through modeling the latent aspects of words.  Our Word Flock solution brings together latent variable analysis for embedding and aspect modeling, and calibrated layout algorithm within a synchronized word cloud generation framework.  We present the quantitative and qualitative results on real-life text corpora, showcasing how the word clouds are useful in preserving the information content of documents so as to allow more accurate visual comparison of documents.", 
    "authors": [
      {
        "name": "Tuan Le"
      }, 
      {
        "name": "Hady Lauw"
      }
    ], 
    "keywords": "word cloud, visual comparison, latent aspect", 
    "title": "Word Clouds with Latent Variable Analysis for Visual Comparison of Documents", 
    "type": "paper"
  }, 
  "626": {
    "abstract": "Capturing place semantics is critical for enabling location-based applications. Techniques for assigning semantic labels (e.g., \"bar\" or \"office\") to unlabeled places mainly resort to mining user activity logs by exploiting visiting patterns. However, existing approaches focus on inferring place labels with a static user activity dataset, and ignore the visiting pattern dynamics in user activity streams, leading to the rapid decrease of labeling accuracy over time. In this paper, we tackle the problem of semantic place labeling over user activity streams. We formulate this problem as a classification problem by characterizing each place through its fine-grained visiting patterns, which encode the visiting frequency of each user in each typical time slot. However, with the incoming activities of new users in data streams, such fine-grained visiting patterns constantly grow, leading to a continuously expanding feature space. To solve this issue, we propose an updatable sketching technique that creates and incrementally updates a set of compact and fixed-size sketches to approximate the similarity between fine-grained visiting patterns of ever-growing size. We further consider the discriminative weights of user activities in place labeling, and seamlessly incorporate them into our sketching method. Our empirical evaluation on real-world datasets demonstrates the validity of our approach and shows that sketches can be efficiently and effectively used to infer place labels over user activity streams.", 
    "authors": [
      {
        "name": "Dingqi Yang"
      }, 
      {
        "name": "Bin Li"
      }, 
      {
        "name": "Philippe Cudr\u00e9-Mauroux"
      }
    ], 
    "keywords": "Data sketching, Semantic place labeling, User activity, Data streams", 
    "title": "POISketch: Semantic Place Labeling over User Activity Streams", 
    "type": "paper"
  }, 
  "63": {
    "abstract": "Random forests are one type of the most effective ensemble methods popularly used to solve many learning tasks. Despite their sound empirical performance, the study on their theoretical properties has been left far behind. Recently, several random forests variants with nice theoretical basis have been proposed, but they all suffer from poor empirical performance. In this paper, we propose a Bernoulli random forests model (BRF), which intends to close the gap between the theoretical consistency and the empirical soundness of random forests classification. Compared to Breiman's original random forests, BRF makes two simplifications in tree construction by using two independent Bernoulli distributions. The first Bernoulli distribution is used to control the selection of candidate attributes for each node of the tree, and the second one controls the splitting point used by each node. As a result, BRF enjoys proved theoretical consistency, so its accuracy can converge to optimum (i.e., the Bayes risk) as the training data grow infinitely large. Empirically, BRF demonstrates the best performance among all theoretical random forests, and is very comparable to Breiman's original random forests (which do not have the proved consistency yet). The theoretical and experimental studies advance the research one step further in closing the gap between the theory and the practical performance of random forests classification.", 
    "authors": [
      {
        "name": "Yisen Wang"
      }, 
      {
        "name": "Qingtao Tang"
      }, 
      {
        "name": "Shu-Tao Xia"
      }, 
      {
        "name": "Jia Wu"
      }, 
      {
        "name": "Xingquan Zhu"
      }
    ], 
    "keywords": "Theoretical Consistency, Empirical Soundness, Random Forests, Bernoulli Distribution", 
    "title": "Bernoulli Random Forests: Closing the Gap between Theoretical Consistency and Empirical Soundness", 
    "type": "paper"
  }, 
  "630": {
    "abstract": "Large-scale clustering has found wide applications in many fields and received much attention in recent years. However, most existing large-scale clustering methods can only achieve mediocre performance, because they are sensitive to the unavoidable presence of noise in the large-scale data. To address this challenging problem, we thus propose a large-scale sparse clustering (LSSC) algorithm. In this paper, we choose a two-step optimization strategy for large-scale sparse clustering: 1) k-means clustering over the large-scale data to obtain the initial clustering results; 2) clustering refinement over the initial results by developing a spare coding algorithm. To guarantee the scalability of the second step, we also utilize nonlinear approximation and dimension reduction techniques to speed up the sparse coding algorithm. Experimental results on both synthetic and real-world datasets demonstrate the promising performance of our LSSC algorithm.", 
    "authors": [
      {
        "name": "Ruqi Zhang"
      }, 
      {
        "name": "Zhiwu Lu"
      }
    ], 
    "keywords": "large-scale clustering, sparse coding, nonlinear approximation, dimension reduction", 
    "title": "Large Scale Sparse Clustering", 
    "type": "paper"
  }, 
  "639": {
    "abstract": "Recent years have witnessed wide application of hashing for large-scale image retrieval. However, most existing hashing methods are based on hand-crafted features which might not be optimally compatible with the hashing procedure. Recently, deep hashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown better performance than traditional hashing methods with hand-crafted features. Most of these deep hashing methods are supervised whose supervised information is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this paper, we propose a novel deep hashing method, called deep pairwise-supervised hashing(DPSH), to perform simultaneous feature learning and hash-code learning for applications with pairwise labels. Experiments on real datasets show that our DPSH method can outperform other methods to achieve the state-of-the-art performance in image retrieval applications.", 
    "authors": [
      {
        "name": "Wu-Jun Li"
      }, 
      {
        "name": "Sheng Wang"
      }, 
      {
        "name": "Wang-Cheng Kang"
      }
    ], 
    "keywords": "Learning to hash, Image retrieval, Deep hashing", 
    "title": "Feature Learning based Deep Supervised Hashing with Pairwise Labels", 
    "type": "paper"
  }, 
  "640": {
    "abstract": "In this article we address the problem of semantic region analysis in structured/unstructured crowded video scenes.  A new tracklet representation for motion flows in crowded scenes is introduced. Every tracklet is represented as a directed line segment, and a novel tracklets similarity measure is formulated based on line segment geometry. For analysis, we used non-parametric clustering of the extracted tracklets. Particularly, we adopted the Distance Dependent Chinese Restaurant Process (DD-CRP). Tracklet similarities are integrated as priors to DD-CRP in order to enforce the spatial coherence between tracklets in the same cluster. Using the resulting clustering, our approach can   identify the semantic regions in crowded scenes including common pathways and  motion  flow directions, sources, and sinks, without any prior information about the scene layout. Qualitative  experiments on multiple crowded scene datasets, and a novel qualitative analysis on the New York Grand Central Station video, demonstrate the effectiveness of our method in the semantic analysis of crowded scenes.", 
    "authors": [
      {
        "name": "Allam S. Hassanein"
      }, 
      {
        "name": "Mohamed E. Hussein"
      }, 
      {
        "name": "Walid Gomaa"
      }
    ], 
    "keywords": "non-parametric clustering, DDCRP, Crowded Scenes, Tracklet, semantic regions", 
    "title": "Semantic Analysis for Crowded Scenes Based on Non-Parametric Tracklet Clustering", 
    "type": "paper"
  }, 
  "641": {
    "abstract": "In recent years, convolutional neural networks (CNNs) have achieved remarkable successes in various applications such as image classification, object detection, object parsing and face alignment. Such CNNs models are extremely powerful to deal with massive amounts of training data through tens of millions of parameters. However, these models are typically deficient due to the heavy cost in model storage, which prohibits the usage of CNNs on resource-limited hardwares like mobile phones or embedded devices. In this paper, we target at compressing CNN models to an extreme without significantly reducing their discriminability. Our main idea is to explicitly model the reconstruction error between the outputs of the original and compressed CNNs, which is minimized to pursuit a satisfactory \\textit{rate-distortion} during the model compression. In particular, a global error reconstruction method, termed GER, is presented, which leverages an SVD-based low rank approximation to firstly compress the parameters in the fully connected layers in a layer-wise manner. Subsequently, such layer-wise initial compressions are jointly optimized in a global perspective via back-propagation. The proposed GER method is evaluated on the ILSVRC2012 image classification benchmark, testing on two widely-adopted convolutional neural networks, \\textit{i.e.}, AlexNet and VGGNet-19.  Comparing to several state-of-the-art methods in CNN compression, the proposed compression scheme has demonstrated drastically better rate-distortion performance.", 
    "authors": [
      {
        "name": "Shaohui Lin"
      }, 
      {
        "name": "Rongrong Ji"
      }, 
      {
        "name": "Yongjian Wu"
      }, 
      {
        "name": "Xuelong Li"
      }
    ], 
    "keywords": "Convolutional neural networks, Image classification, Object detection, Object parsing, Face alignment, SVD", 
    "title": "Towards Convolutional Neural Networks Compressing via Global Error Reconstruction", 
    "type": "paper"
  }, 
  "643": {
    "abstract": "Almost all the existing representation based classifiers represent a query sample as a linear combination of training samples, and their time and memory cost will increase rapidly with the number of training samples. We investigate the representation based classification problem from a rather different perspective in this paper, that is, we learn how each feature (i.e., each element) of a sample can be represented by the features of itself. Such a self-representation property of sample features can be readily employed for pattern classification and a novel self-representation induced classifier (SRIC) is proposed. SRIC learns a self-representation matrix for each class. Given a query sample, its self-representation residual can be computed by each of the learned self-representation matrices, and classification can then be performed by comparing these residuals. In light of the principle of SRIC, a discriminative SRIC (DSRIC) method is developed. For each class, a discriminative self-representation matrix is trained to minimize the self-representation residual of this class while representing little the features of other classes. Experimental results on different pattern recognition tasks show that DSRIC achieves comparable or superior recognition rate to state-of-the-art representation based classifiers, however, it is much more efficient and needs much less storage space.", 
    "authors": [
      {
        "name": "Pengfei Zhu"
      }, 
      {
        "name": "Lei Zhang"
      }, 
      {
        "name": "Wangmeng Zuo"
      }, 
      {
        "name": "Xiangchu Feng"
      }, 
      {
        "name": "Qinghua Hu"
      }
    ], 
    "keywords": "sparse representation, self-representation, nearest subspace classifier", 
    "title": "A Self-representation induced Classifier", 
    "type": "paper"
  }, 
  "650": {
    "abstract": "In this paper, we present a probabilistic approach to explicitly infer containment relations between objects in 3D scenes. Given an input RGB-D video, our algorithm quantizes the perceptual space of a 3D scene by reasoning about containment relations over time. At each frame, we represent the containment relations in space by a containment graph, where each vertex represents an object and each edge represents a containment relation. In time, we assume that human action is the only cause that leads to containment relation changes, and consequently classify human actions into four types of events: move-in, move-out, no-change and paranormal-change. Here, paranomal-change refers to the event that is not feasible, thus are ruled out through reasoning. A standard dynamic programming algorithm is adopted to find both the optimal sequence of containment graphs across the video, and containment relation changes between adjacent frames. In experiments, we test the proposed method on our dataset with 1326 video clips taken in 9 indoor scenes, including some challenging cases, such as heavy occlusions and diverse changes of containment relations.", 
    "authors": [
      {
        "name": "Wei Liang"
      }, 
      {
        "name": "Yixin Zhu"
      }, 
      {
        "name": "Songchu Zhu"
      }
    ], 
    "keywords": "computer vision, spatial reasoning, containment relations", 
    "title": "What are Where: Inferring Containment Relations from Videos", 
    "type": "paper"
  }, 
  "655": {
    "abstract": "The attentional mechanism has proven to be effective in improving end-to-end neural machine translation. However, due to the structural divergence between natural languages, unidirectional attention-based models might only capture partial aspects of attentional regularities. We propose agreement-based joint training for bidirectional attention-based end-to-end neural machine translation. Instead of training source-to-target and target-to-source translation models independently, our approach encourages the two complementary models to agree on word alignment matrices on the same training data. Experiments on Chinese-English and English-French translation tasks show that joint training significantly improves both alignment and translation quality over independent training.", 
    "authors": [
      {
        "name": "Yong Cheng"
      }, 
      {
        "name": "Shiqi Shen"
      }, 
      {
        "name": "Zhongjun He"
      }, 
      {
        "name": "Wei He"
      }, 
      {
        "name": "Hua Wu"
      }, 
      {
        "name": "Maosong Sun"
      }, 
      {
        "name": "Yang Liu"
      }
    ], 
    "keywords": "neural machine translation, agreement-based joint training, attentional mechanism", 
    "title": "Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation", 
    "type": "paper"
  }, 
  "659": {
    "abstract": "Real-world norm monitors have limited capabilities for observing agents. This paper proposes a novel mechanism to take full advantage of limited observation capabilities by selecting the agents to be monitored. Our evaluation shows this significantly increases the number of violations detected.", 
    "authors": [
      {
        "name": "Natalia Criado"
      }, 
      {
        "name": "Jose M. Such"
      }
    ], 
    "keywords": "Norms, Monitoring, Agents", 
    "title": "Norm Monitoring with Incomplete Observations", 
    "type": "paper"
  }, 
  "677": {
    "abstract": "Graph-based approaches have been successful in unsupervised and semi-supervised learning. In this paper, we focus on the real-world applications where the same instance can be represented by multiple heterogeneous features. The key point of utilizing the graph-based knowledge to deal with this kind of data is to reasonably integrate the different representations and obtain the mostly consistent manifold with the real data distributions. To address this problem, in this paper, we propose a novel framework via the reformulation to the standard spectral learning model paired with iterative optimization process to automatically incorporate a variety of represented graphs for clustering and semi-supervised tasks. Unlike other methods in the literature, the proposed method has no free parameter, obtains the weight for each graph automatically and can be solved easily. Furthermore, our objective under semi-supervised learning is convex and the global optimal result will be obtained. Extensive empirical results on different real-world data sets demonstrate that the proposed method achieves the comparable performance with the state-of-the-art approaches and is more practical to use.", 
    "authors": [
      {
        "name": "Feiping Nie"
      }, 
      {
        "name": "Jing Li"
      }, 
      {
        "name": "Xuelong Li"
      }
    ], 
    "keywords": "Parameter-free', 'Graph-based', 'Multi-view', 'Clustering', 'Semi-supervised'", 
    "title": "Parameter-free Auto-weighted Multiple Graph Learning: A Framework for Clustering and Semi-supervised Classification", 
    "type": "paper"
  }, 
  "680": {
    "abstract": "In this paper, we propose an end-to-end deep correspondence structure learning (DCSL) approach to address the cross-camera person-matching problem in the person re-identification task. The proposed DCSL approach captures the intrinsic structural information on persons by learning a semantics-aware image representation  based on convolutional neural networks, which adaptively learns discriminative features for person identification. Furthermore, the proposed DCSL approach seeks to adaptively learn a hierarchical data-driven feature matching function which outputs the matching correspondence results between the learned semantics-aware image representations for a person pair. Finally, we set up a unified end-to-end deep learning scheme to jointly optimize the processes of semantics-aware image representation learning and cross-person correspondence structure learning, leading to more reliable and robust person re-identification results in complicated scenarios. Experimental results on several benchmark datasets demonstrate the effectiveness of our approach against the state-of-the-art approaches.", 
    "authors": [
      {
        "name": "Yaqing Zhang"
      }, 
      {
        "name": "Xi Li"
      }, 
      {
        "name": "Liming Zhao"
      }, 
      {
        "name": "Zhongfei Zhang"
      }
    ], 
    "keywords": "person re-identification, semantics-aware image representation, correspondence structure learning, deep learning", 
    "title": "Semantics-aware Deep Correspondence Structure Learning for Robust Person Re-identification", 
    "type": "paper"
  }, 
  "683": {
    "abstract": "When engineering an automated planning model, domain authors   typically assume a static, unchanging ground-truth   world. Unfortunately, this assumption can clash with reality, where   domain changes often rapidly occur in best practices, effectors, or   known conditions. In these cases, re-modeling the domain causes   domain experts to ensure newly captured requirements integrate well   with the current model. In this work, we address this model   maintenance problem in a system called Marshal. Marshal   assists model maintainers by reasoning about their model as a   (hidden) stochastic process. It issues queries, and learns models by   observing query answers, plan solutions and direct changes to the   model. In this paper, we present Marshal and investigate its use   in solving the model maintenance problem through simulated user   experiments.  Our results indicate that anticipating model evolution   leads to more accurate learned models over traditional   non-anticipatory approaches.", 
    "authors": [
      {
        "name": "Daniel Bryce"
      }, 
      {
        "name": "J. Benton"
      }, 
      {
        "name": "Michael Boldt"
      }
    ], 
    "keywords": "Planning, Stochastic Process, Particle Filter", 
    "title": "Maintaining Evolving Domain Models", 
    "type": "paper"
  }, 
  "691": {
    "abstract": "We study a rating system in which a set of individuals (e.g., the customers  of a restaurant) evaluate a given service (e.g, the food provided), with their aggregated opinion determining the probability of all individuals to use the service and thus its generated revenue. We explicitly model the influence relation by a social network,  with individuals being influenced by the evaluation of their trusted peers. On top of that we allow a malicious service provider (e.g., the restaurant owner) to bribe a subset of the potential users, i.e., to invest a part of his or her expected income to modify their opinion, therefore influencing his or her final gain. We analyse the effect of bribing strategies under various constraints, and we show under what conditions the system is bribery-proof, i.e., there is no bribing strategy yielding a strictly positive expected gain to the service provider.  We show under what conditions the system is bribery-proof, i.e., there is no bribing strategy yielding a strictly positive expected gain to the service provider.", 
    "authors": [
      {
        "name": "Umberto Grandi"
      }, 
      {
        "name": "Paolo Turrini"
      }
    ], 
    "keywords": "Rating Systems, Social Networks, Mechanism Design", 
    "title": "A network-based rating system and its resistance to bribery", 
    "type": "paper"
  }, 
  "7": {
    "abstract": "Contextual bandit is a popular framework for recommender systems to provide personalized recommendations to new users. Existing contextual bandit algorithms generally rely on features alone to capture user variability. Such methods are inefficient in learning new users\u00e2\u0080\u0099 interests. In this paper we propose Latent Class Contextual Bandits. We consider both the benefit of leveraging a set of learned latent user classes for new users, and how we can learn such latent classes from prior users. We show that our approach achieves a better regret bound than existing algorithms. We also demonstrate the benefit of our approach using a large real world dataset and a preliminary user study.", 
    "authors": [
      {
        "name": "Li Zhou"
      }, 
      {
        "name": "Emma Brunskill"
      }
    ], 
    "keywords": "contextual bandits, user modeling, sequential decision making, cold start", 
    "title": "Latent Contextual Bandits and their Application to Personalized Recommendations for New Users", 
    "type": "paper"
  }, 
  "700": {
    "abstract": "Embeddings or vector representations of objects have used with remarkable success in various machine learning and AI tasks, from dimensionality reduction and data visualization, to vision and natural language processing. Although point vectors are popular, probabilistic embeddings are arguably more useful; they provide uncertainties surrounding embedded points and can capture non-linear relationships between elements (e.g., via divergence measures). In this work, we seek probabilistic embeddings that faithfully represent observed relationships between objects (e.g., physical distances, preferences). We derive a variational Bayesian variant of multidimensional scaling that (i) provides a posterior distribution over latent points without computationally-heavy MCMC sampling, and (ii) can leverage existing side information using sparse Gaussian Processes (GPs) to learn a nonlinear mapping to the embedding. Furthermore, the derived approximate lower-bounds can be used to discover the intrinsic dimensionality of the data and limit embedding complexity. We demonstrate the effectiveness of our methods empirically on three synthetic problems and on the real-world tasks of political unfolding analysis and multi-sensor localization.", 
    "authors": [
      {
        "name": "Harold Soh"
      }
    ], 
    "keywords": "Probabilistic Embedding, Manifold Learning, Gaussian Process, Variational Inference, Multimensional Scaling", 
    "title": "Distance-Preserving Probabilistic Embeddings with Side Information: Variational Bayesian Multidimensional Scaling Gaussian Process", 
    "type": "paper"
  }, 
  "702": {
    "abstract": "Researchers have used from 30 days to several years of daily returns as source data for clustering financial time series based on their correlations. This paper sets up a statistical framework to study the validity of such practices. We first show that clustering correlated random variables from their observed values is statistically consistent. Then, we also give a first empirical answer to the much debated question: How long should the time series be? If too short, the clusters found can be spurious; if too long, dynamics can be smoothed out.", 
    "authors": [
      {
        "name": "Gautier Marti"
      }, 
      {
        "name": "S\u00e9bastien Andler"
      }, 
      {
        "name": "Frank Nielsen"
      }, 
      {
        "name": "Philippe Donnat"
      }
    ], 
    "keywords": "consistency, clustering, random variables, correlation, financial time series, empirical convergence rate", 
    "title": "Clustering Financial Time Series: How Long is Enough?", 
    "type": "paper"
  }, 
  "704": {
    "abstract": "In this paper, we propose a novel visual tracking framework that intelligently discovers reliable patterns from a wide range of video to resist drift error for long-term tracking tasks. First, we design a Discrete Fourier Transform (DFT) based tracker which is able to exploit a large number of tracked samples while still ensures real-time performance. Second, we propose a clustering method with temporal constraints to explore and memorize consistent patterns from previous frames, named as \"reliable memories\". By virtue of this method, our tracker can utilize uncontaminated information to alleviate drifting issues. Experimental results show that our tracker performs favorably against other state-of-the-art methods on benchmark datasets. Furthermore, it is significantly competent in handling drifts and able to robustly track challenging long videos over 4000 frames, while most of others lose track at early frames.", 
    "authors": [
      {
        "name": "Shu Wang"
      }, 
      {
        "name": "Shaoting Zhang"
      }, 
      {
        "name": "Wei Liu"
      }, 
      {
        "name": "Dimitris Metaxas"
      }
    ], 
    "keywords": "Visual Tracking, Single Object Tracking, Temporally Constrained Clustering, Deep Neural Networks", 
    "title": "Visual Tracking with Reliable Memories", 
    "type": "paper"
  }, 
  "715": {
    "abstract": "Planning in MDPs often uses a smaller planning horizon than specified in the problem to save computational expense at the risk of a loss due to suboptimal plans. Jiang et al. recently showed that smaller than specified planning horizons can in fact be beneficial in cases where the MDP model is learned from data and therefore not accurate. In this paper, we consider planning with accurate models and investigate structural properties of MDPs that bound the loss incurred by using smaller than specified planning horizons. We identify a number of structural parameters some of which depend on the reward function alone, some on the transition dynamics alone, and some that depend on the interaction between rewards and transition dynamics. We provide planning loss bounds in terms of these structural parameters and, in some cases, also show tightness of the upper bounds. Empirical results with randomly generated MDPs are used to validate qualitative properties of our theoretical bounds for shallow planning.", 
    "authors": [
      {
        "name": "Nan Jiang"
      }, 
      {
        "name": "Satinder Singh"
      }, 
      {
        "name": "Ambuj Tewari"
      }
    ], 
    "keywords": "Markov Decision Processes, planning, horizon, discount factor", 
    "title": "On Structural Properties of MDPs that Bound Loss due to Shallow Planning", 
    "type": "paper"
  }, 
  "721": {
    "abstract": "Because of the complementarity of multiple visual cues (features) in appearance modeling, many tracking algorithms attempt to fuse multiple features to improve the tracking performance from two aspects: increasing the representation accuracy against appearance variations and enhancing the discriminability between the tracked target and its background. Since both these two aspects simultaneously contribute to the success of a visual tracker, how to fully unleash the capabilities of multiple features from these two aspects in appearance modeling is a key issue for feature fusion-based visual tracking. To address this problem, different from other feature fusion-based trackers which consider one of these two aspects only, this paper proposes an unified feature learning framework which simultaneously exploits both the representation capability and the discriminability of multiple features for visual tracking. In particular, the proposed feature learning framework is capable of: 1) learning robust features by separating out corrupted features for accurate feature representation, 2) seamlessly imposing the discriminabiltiy of multiple visual cues into feature learning, and 3) fusing features by exploiting their shared and feature-specific discriminative information. Extensive experiment results on challeging videos demonstrate the effectiveness of the proposed method.", 
    "authors": [
      {
        "name": "Xiangyuan Lan"
      }, 
      {
        "name": "Shengping Zhang"
      }, 
      {
        "name": "Pong Chi Yuen"
      }
    ], 
    "keywords": "Object tracking, Appearance modeling, Feature fusion", 
    "title": "Robust Joint Discriminative Feature Learning for Visual Tracking", 
    "type": "paper"
  }, 
  "730": {
    "abstract": "We introduce a model of preference diffusion in which agents in a social network update their preferences based on those of their influencers in the network, and we study the dynamics of this model. Preferences are modelled as ordinal rankings over a finite set of alternatives. At each time step, some of the agents update the relative ordering of two alternatives adjacent in their current ranking with the majority view of their influencers. We consider both a synchronous and an asynchronous variant of this model. Our results show how the graph-theoretic structure of the social network and the structure of the agents' preferences affect the termination of the diffusion process and the properties of the preference profile at the time of termination.", 
    "authors": [
      {
        "name": "Markus Brill"
      }, 
      {
        "name": "Edith Elkind"
      }, 
      {
        "name": "Ulle Endriss"
      }, 
      {
        "name": "Umberto Grandi"
      }
    ], 
    "keywords": "social influence, computational social choice, voter models", 
    "title": "Pairwise Diffusion of Preference Rankings", 
    "type": "paper"
  }, 
  "734": {
    "abstract": "We study FO-rewritability of conjunctive queries in the presence of ontologies formulated in a description logic  between EL and Horn-SHIF, along with related query containment problems. Apart from providing useful characterizations, we establish complexity results ranging from ExpTime via NExpTime to \n735\tMain Track\tAn Online Mechanism for Ridesharing in   Autonomous Mobility-on-Demand Systems\tWen Shen, Cristina V. Lopes and Jacob W. Crandall\tMechanism Design, Ridesharing, Cost-sharing, Autonomous Mobility-on-Demand\"", 
    "authors": [
      {
        "name": "Meghyn Bienvenu"
      }, 
      {
        "name": "Peter Hansen"
      }, 
      {
        "name": "Carsten Lutz"
      }, 
      {
        "name": "Frank Wolter"
      }
    ], 
    "keywords": "Description Logic, Query Rewriting, Deciding FO-Rewritability", 
    "title": "First Order-Rewritability of Conjunctive Queries in Horn Description Logics", 
    "type": "paper"
  }, 
  "737": {
    "abstract": "In a connected graph, spanning tree centralities of a vertex and an edge measure how crucial they are for the graph to be connected. In this paper, we propose efficient algorithms for estimating spanning tree centralities with theoretical guarantees on their accuracy. We experimentally demonstrate that our methods are orders of magnitude faster than previous methods. Then, we propose a novel centrality notion of a vertex, called aggregated spanning tree centrality, which also considers the number of connected components obtained by removing the vertex. We also give an efficient algorithm for estimating aggregated spanning tree centrality. Finally, we experimentally show that those spanning tree centralities are useful to identify vulnerable edges and vertices in infrastructure networks.", 
    "authors": [
      {
        "name": "Takanori Hayashi"
      }, 
      {
        "name": "Takuya Akiba"
      }, 
      {
        "name": "Yuichi Yoshida"
      }
    ], 
    "keywords": "graphs, social networks, infrastructure networks, centrality measures, graph connectivity", 
    "title": "Efficient Algorithms for Spanning Tree Centrality", 
    "type": "paper"
  }, 
  "738": {
    "abstract": "Two important requirements when aggregating the preferences of multiple agents are that the outcome should be economically efficient and the aggregation mechanism should not be manipulable. In this paper, we provide a computer-aided proof of a sweeping impossibility using these two conditions for randomized aggregation mechanisms. More precisely, we show that every efficient aggregation mechanism can be manipulated for all expected utility representations of the agents' preferences. This settles a conjecture by Aziz et al. [2013] and strengthens both, theorems in social choice [Aziz et al., 2013b; Aziz et al., 2014; Brandl et al., 2016] and related statements from the special domain of assignment. Our proof is obtained by formulating the claim as a satisfiability problem over predicates from real-valued arithmetic, which is then checked using an SMT (satisfiability modulo theories) solver. To the best of our knowledge, this is the first application of SMT solvers in computational social choice.", 
    "authors": [
      {
        "name": "Florian Brandl"
      }, 
      {
        "name": "Felix Brandt"
      }, 
      {
        "name": "Christian Geist"
      }
    ], 
    "keywords": "randomized social choice, social decision schemes, efficiency, strategyproofness, computer-aided theorem proving, SMT", 
    "title": "Proving the Incompatibility of Efficiency and Strategyproofness via SMT Solving", 
    "type": "paper"
  }, 
  "739": {
    "abstract": "Algorithms for computing game-theoretic solutions have recently been applied to a number of security domains.  However, many of the techniques developed for compact representations of security games do not extend to {\\em Bayesian} security games, which allow us to model uncertainty about the attacker's type.  In this paper, we introduce a general framework of {\\em catcher-evader} games that can capture Bayesian security games as well as other game families of interest. We show that computing Stackelberg strategies is NP-hard, but give an algorithm for computing a Nash equilibrium that performs well in simulations.  We also prove that the Nash equilibria of these games satisfy the {\\em interchangeability} property, so that equilibrium selection is not an issue.", 
    "authors": [
      {
        "name": "Yuqian Li"
      }, 
      {
        "name": "Vincent Conitzer"
      }, 
      {
        "name": "Dmytro Korzhyk"
      }
    ], 
    "keywords": "Nash equilibrium, Stackelberg strategies, Bayesian security games", 
    "title": "Catcher-Evader Games", 
    "type": "paper"
  }, 
  "744": {
    "abstract": "Nowadays multi-modal visual data are much easier to access as the technology develops. Nevertheless, there is an underlying problem hidden behind the emerging multi-modality techniques: What if one/more modal data fail? Motivated by this question, we propose an unsupervised method which well handles the incomplete multimodal data by transforming the original and incomplete data into a new and complete representation in a latent space. Different from the existing efforts that simply project data from each modality into a common subspace, a novel graph Laplacian term with a good probabilistic interpretation is proposed to couple the incomplete multimodal samples. In such way, a compact global structure over the entire heterogeneous data is well preserved, leading to a strong grouping discriminability. As a non-trivial contribution, we provide the optimization solution to the proposed model. In experiments, we make extensive experiments on one synthetic data, two RGB-D video datasets and two image datasets. The superior results, especially when multimodal data suffer from a large incompleteness, validate the benefits of the proposed method.", 
    "authors": [
      {
        "name": "Handong Zhao"
      }, 
      {
        "name": "Hongfu Liu"
      }, 
      {
        "name": "Yun Fu"
      }
    ], 
    "keywords": "multi-view clustering, incomplete view, graph Laplacian", 
    "title": "Incomplete Multimodal Visual Data Grouping", 
    "type": "paper"
  }, 
  "750": {
    "abstract": "Complex collaborative activities such as treating patients, co-authoring documents and developing software are often characterized by teamwork that is loosely coupled and extends in time. To remain coordinated and avoid conflicts, team members need to identify dependencies between their activities --- which though loosely coupled may interact --- and share information appropriately. The loose-coupling of tasks increases the difficulty of identifying dependencies, with the result that team members often lack important information or are overwhelmed by irrelevant information. This paper formalizes a new multi-agent systems problem, Information Sharing in Loosely-Coupled Extended-Duration Teamwork (ISLET). It defines a new representation, Mutual Influence Potential Networks (MIP-Nets) and an algorithm, MIP-DOI, that uses this representation to determine the information that is most relevant to each team member. Importantly, because the extended duration of the teamwork precludes team-members developing complete plans in advance. The MIP-Nets approach, unlike prior work to information sharing, does not rely on a priori knowledge of a team\u00e2\u0080\u0099s possible plans. Instead, it models collaboration patterns and dependencies among activities based on team-members\u00e2\u0080\u0099 interactions. Empirical evaluations show that this approach is able to learn collaboration patterns and identify relevant information to share with team members.", 
    "authors": [
      {
        "name": "Ofra Amir"
      }, 
      {
        "name": "Barbara Grosz"
      }, 
      {
        "name": "Krzysztof Gajos"
      }
    ], 
    "keywords": "Information sharing, Loosely-coupled extended-duration teamwork, Mutual Influence Potential Networks", 
    "title": "Mutual Influence Potential Networks: Enabling Information Sharing in Loosely-Coupled Extended-Duration Teamwork", 
    "type": "paper"
  }, 
  "755": {
    "abstract": "Computational Drug Repositioning (CDR) is the knowledge discovery process of finding new indications for existing drugs leveraging heterogeneous drug-related data. Longitudinal Observational Data (LOD) such as Electronic Health Records (EHRs) have become an emerging data source for CDR. To address the high-dimensional, irregular, subject- and time-heterogeneous nature of EHRs, we propose Baseline Regularization (BR) that extends the one-way fixed effect model, which is a standard approach to analyze small-scale LOD. For evaluation, we use BR to search for drugs that can lower Fasting Blood Glucose (FBG) level in a real-world EHR (identity withheld for double-blind review). Experimental results suggest that BR is capable of rediscovering drugs that can lower FBG level as well as identifying some potential blood sugar lowering drugs in the literature.", 
    "authors": [
      {
        "name": "Zhaobin Kuang"
      }, 
      {
        "name": "James Thomson"
      }, 
      {
        "name": "Michael Caldwell"
      }, 
      {
        "name": "Peggy Peissig"
      }, 
      {
        "name": "Ron Stewart"
      }, 
      {
        "name": "C. David Page"
      }
    ], 
    "keywords": "Longitudinal Data Analysis, Generalized Lasso, Computational Drug Repositioning", 
    "title": "Baseline Regularization for Computational Drug Repositioning with Longitudinal Observational Data", 
    "type": "paper"
  }, 
  "758": {
    "abstract": "We consider an assignment problem that has aspects of fair division as well as social choice. In particular, we investigate the problem of assigning a small subset from a set of indivisible items to multiple players so that the chosen subset is \\emph{agreeable} to all players, i.e., every player weakly prefers the chosen subset to any subset of its complement. For an arbitrary number of players, we derive tight upper bounds on the size for which a subset of that size that is agreeable to all players always exists when preferences are monotonic. We then present polynomial-time algorithms that find an agreeable subset of approximately half of the items when there are two or three players and preferences are responsive. Our results translate to a 2-approximation on the individual welfare of every player when preferences are subadditive.", 
    "authors": [
      {
        "name": "Warut Suksompong"
      }
    ], 
    "keywords": "fair division, envy-freeness, social choice", 
    "title": "Assigning a Small Agreeable Set of Indivisible Items to Multiple Players", 
    "type": "paper"
  }, 
  "766": {
    "abstract": "The popularity of social media shatters the barrier for online users to create and share information at any place at any time. As a consequence,  it has become increasing difficult to locate relevance information about an entity.  Timeline has been proven to provide an effective and efficient access to understand an entity by displaying a list of episodes about the entity in chronological order. However, summarizing the timeline about an entity with social media data faces new challenges. First, key timeline episodes about the entity are typically unavailable in existing social media services. Second, the short, noisy and informal nature of social media posts determines that only content-based summarization could be insufficient. In this paper, we investigate the problem of timeline summarization and propose a novel framework Timeline-Sumy, which consists of episode detecting and summary ranking. In episode detecting, we explicitly model temporal information with life cycle models to detect timeline episodes since episodes usually exhibit sudden-rise-and-heavy-tail patterns on time-series. In summary ranking, we rank social media posts in each episode via a learning-to-rank approach. The experimental results on social media datasets demonstrate the effectiveness of the proposed framework.", 
    "authors": [
      {
        "name": "Yi Chang"
      }, 
      {
        "name": "Jiliang Tang"
      }, 
      {
        "name": "Dawei Yin"
      }, 
      {
        "name": "Yan Liu"
      }
    ], 
    "keywords": "Timeline, Timeline Summarization, Life Cycle Model", 
    "title": "Timeline Summarization with Life Cycle Models", 
    "type": "paper"
  }, 
  "775": {
    "abstract": "Recently, much attention has been devoted to discrete preference games, which model the formation of opinions in social networks. More specifically, they model the agents' strategic decision of expressing publicly an opinion, which is a result of an interplay between the agent's private belief and the social pressure. However, these games have very limited expressive power; they can model very simple social relations and assume that agents respond to social pressure in the same way. In this paper, we define and study the novel class of {\\em generalized discrete preference games}. These games have additional characteristics and can model social relations to allies or competitors, complex relations among more than two agents, introduce different levels of strength for each relation, and personalize the dependence of each agent to its neighborhood.  We show that these novel games admit generalized ordinal potential functions and, more importantly,  that every two-strategy game that admits a generalized ordinal potential function  is structurally equivalent to a generalized discrete preference game.  This implies that the games in the novel class capture the full generality of two-strategy games in which the existence of (pure) equilibria is guaranteed by topological arguments.", 
    "authors": [
      {
        "name": "Vincenzo Auletta"
      }, 
      {
        "name": "Ioannis Caragiannis"
      }, 
      {
        "name": "Diodato Ferraioli"
      }, 
      {
        "name": "Clemente Galdi"
      }, 
      {
        "name": "Giuseppe Persiano"
      }
    ], 
    "keywords": "social networks, opinion formation, potential games, social influence", 
    "title": "Generalized Discrete Preference Games", 
    "type": "paper"
  }, 
  "791": {
    "abstract": "The problem of belief tracking in the presence of stochastic actions and observations is pervasive and yet computationally intractable. In this work, we show that probabilistic beliefs can be maintained in factored form exactly and efficiently across a number of causally closed beams, when the state variables that appear in more than one beam obey a form of backward determinism. Since computing marginals from the factors is computationally intractable in general and variables appearing in several beams are not always backward-deterministic, the basic formulation is extended with two approximations: forms of belief propagation for computing marginals from factors, and sampling of non-backward-deterministic variables for making such variables backward-deterministic given their sampled history. Unlike, Rao-Blackwellized particle-filtering, the sampling is not used for making the inference tractable but for making the possibly intractable factorization sound. The resulting algorithm involves sampling and belief propagation or just one of them as determined by the structure of the model, and it is tested on domains that illustrate these various possibilities.", 
    "authors": [
      {
        "name": "Blai Bonet"
      }, 
      {
        "name": "Hector Geffner"
      }
    ], 
    "keywords": "Belief tracking, Inference in DBNs, Probabilistic reasoning, Particle filters, Graphical models, Probabilistic planning", 
    "title": "Factored Probabilistic Belief Tracking", 
    "type": "paper"
  }, 
  "793": {
    "abstract": "Agent supervision is a form of control/customization where a supervisor restricts the behavior of an agent to enforce certain requirements, while leaving the agent as much autonomy as possible. In this work, we investigate supervision of an agent that may acquire new knowledge about her environment during execution, for example, by sensing. Thus we consider an agent's online executions, where, as she executes the program, at each time point she must make decisions on what to do next based on what her current knowledge is. This is done in a setting based on the situation calculus and a variant of the ConGolog programming language.  The main results of this paper are (i) a formalization of the online maximally permissive supervisor, (ii) a sound and complete technique for execution of the agent as constrained by such a supervisor, and (iii) a new type of lookahead search construct that ensures nonblockingness over such online executions.", 
    "authors": [
      {
        "name": "Bita Banihashemi"
      }, 
      {
        "name": "Giuseppe De Giacomo"
      }, 
      {
        "name": "Yves Lesperance"
      }
    ], 
    "keywords": "Agent Supervision and Control, Situation Calculus, Online Execution, Autonomous Agents, Reasoning about action, Reasoning about processes", 
    "title": "Online Agent Supervision in the Situation Calculus", 
    "type": "paper"
  }, 
  "798": {
    "abstract": "Online multi-object tracking (MOT) is challenging: frame-by-frame matching of detection hypotheses to the correct tracker can be difficult. The Hungarian algorithm is the most commonly used online MOT data association method due to its rapid assignment; however, the Hungarian algorithm simply considers associations based on an affinity model. For crowded scenarios, frequently occurring interactions between objects complicate associations, and affinity-based methods usually fail in these scenarios. Here we introduce quadratic pseudo-Boolean optimization (QPBO) to an online MOT model to analyze frequent interactions. Specifically, we formulate two useful interaction types as pairwise potentials in QPBO, a design that benefits our model by exploiting informative interactions and allowing our online tracker to handle complex scenes. The auxiliary interactions result in a non-submodular QPBO, so we accelerate our online tracker by solving the model with a graph cut combined with a simple heuristic method. This combination achieves a reasonable local optimum and, importantly, implements the tracker efficiently. Extensive experiments on publicly available datasets from both static and moving cameras demonstrate the superiority of our method.", 
    "authors": [
      {
        "name": "Long Lan"
      }, 
      {
        "name": "Dacheng Tao"
      }, 
      {
        "name": "Chen Gong"
      }, 
      {
        "name": "Naiyang Guan"
      }, 
      {
        "name": "Zhigang Luo"
      }
    ], 
    "keywords": "multi-object tracking, QPBO, online tracking", 
    "title": "Online Multi-object Tracking by Quadratic Pseudo-Boolean Optimization", 
    "type": "paper"
  }, 
  "799": {
    "abstract": "Predictive models are finding an increasing number of applications in many industries. As a result, a practical means for trading-off the cost of deploying a model versus its effectiveness is needed. Our work is motivated by risk prediction problems in healthcare. Specifically, cost-structures in domains such as healthcare are more complex and challenging than our existing approaches can handle. We propose a novel framework for designing cost-sensitive structured regularizers that is suitable for problems with complex cost dependencies. We draw upon a surprising connection to boolean circuits. In particular, we represent the problem costs as a multi-layer boolean circuit, and then use properties of boolean circuits to define an extended feature vector and a group regularizer that exactly captures the underlying cost structure. The resulting regularizer may then be combined with a fidelity function to perform model prediction, for example. On a challenging real-world application of risk prediction for sepsis in intensive care units, the use of our regularizer leads to models that are in harmony with the underlying cost structure and thus provide an excellent prediction accuracy versus cost tradeoff.", 
    "authors": [
      {
        "name": "Suchi Saria"
      }, 
      {
        "name": "Daniel Robinson"
      }
    ], 
    "keywords": "Structured Regularizer, Practice-Cost Sensitive Prediction, Healthcare", 
    "title": "Incorporating User Preferences in Predictive Models Via Structured Regularizers", 
    "type": "paper"
  }, 
  "800": {
    "abstract": "Attribute based object recognition performs object recognition using the semantic properties of the object. Unlike the existing approaches that treat attributes as a middle level representation and require to estimate the attributes during testing, we propose to incorporate the hidden attributes, which are the attributes used only during training to improve model learning and are not needed during testing. To achieve this goal, we develop two different approaches to incorporate hidden attributes. The first approach utilizes hidden attributes as additional features to improve the object classification model. The second approach further exploits the semantic relationships between the objects and the hidden attributes. Experiments on benchmark data sets demonstrate that both approaches can effectively improve the learning of the object classifiers over the baseline models that do not use attributes, and their combination reaches the best performance. Experiments also show that the proposed approaches outperform both state of the art methods that use attributes as middle level representation and the approaches that learn the classifiers with hidden information.", 
    "authors": [
      {
        "name": "Xiaoyang Wang"
      }, 
      {
        "name": "Qiang Ji"
      }
    ], 
    "keywords": "Object recognition, Object attributes, Classification", 
    "title": "Object Recognition with Hidden Attributes", 
    "type": "paper"
  }, 
  "801": {
    "abstract": "Data-driven approaches for learning models for Bayesian filtering often try to maximize the likelihood given parametric forms for the transition and observation dynamics models. However, this objective is usually nonconvex in the parametrization and can only be locally optimized, losing performance guarantees on the filtering task. In this work, we propose to use inference machines to directly optimize the filtering performance by using supervised learning to mimic the inference procedure on a graphical model. Our procedure is capable of handling both partially-observable systems with known and unknown state representations. To do this, we adapt Predictive State Inference Machines (PSIM) by introducing the concept of \"hints\" to accompany the predictive state representation. This allows PSIM to be applied to a new family of filtering problems. Our adaptation enjoys similar theoretical advantages, and we showcase the method's performance on a variety of benchmarks.", 
    "authors": [
      {
        "name": "Arun Venkatraman"
      }, 
      {
        "name": "Wen Sun"
      }, 
      {
        "name": "Byron Boots"
      }, 
      {
        "name": "J. Andrew Bagnell"
      }, 
      {
        "name": "Martial Hebert"
      }
    ], 
    "keywords": "Filter Learning, Dynamics Learning, Inference Machines", 
    "title": "Inference Machines for Nonparametric Filter Learning", 
    "type": "paper"
  }, 
  "808": {
    "abstract": "Dataless text classification (Chang et al., 2008) is a classification paradigm which maps documents into a given label space without requiring any annotated training data. This paper explores a cross-lingual variant of this paradigm, where documents in multiple languages are classified into an English label space. Consequently, a person who only understands English should nevertheless be able to know \u00e2\u0080\u009cwhat is this document about?\u00e2\u0080\u009d when given a document in any of these languages. We use CLESA (Cross-Lingual Explicit Semantic Analysis) to embed both foreign language documents and an English label space into a shared semantic space, and select the best label(s) for a document using the similarity between the corresponding ESA representations. We exhibit the performance of our approach by experimenting with classifying documents in 88 different languages into the same English label space. In particular, we show that CLESA is better than using a monolingual ESA on the target foreign language and translating the English labels into that language. Moreover, the evaluation on two benchmarks, TED and RCV2, showed that cross-lingual dataless classification outperforms supervised learning methods when a large collection of annotated documents is not available.", 
    "authors": [
      {
        "name": "Yangqiu Song"
      }, 
      {
        "name": "Shyam Upadhyay"
      }, 
      {
        "name": "Haoruo Peng"
      }, 
      {
        "name": "Dan Roth"
      }
    ], 
    "keywords": "Dataless Classification, Text Classification, Unsupervised Learning", 
    "title": "Cross-lingual Dataless Classification for Many Languages", 
    "type": "paper"
  }, 
  "809": {
    "abstract": "While multitask learning has been extensively studied for years, most existing methods rely on linear base models (e.g. linear regression, logistic regression), which may fail in dealing with more general (nonlinear) problems. In this paper, we present a new approach that combines \\emph{dictionary learning} with \\emph{gradient boosting} to achieve multitask learning with general (nonlinear) basis functions. Specifically, for each task we learn a sparse representation in a nonlinear dictionary that is shared across the set of tasks. Each atom of the dictionary is a nonlinear feature mapping of the original input space, learned in function space by gradient boosting. The resulting model is a hierarchical ensemble where the top layer of the hierarchy is the task-specific sparse coefficients and the bottom layer is the boosted models common to all tasks. The proposed method takes the advantages of both dictionary learning and boosting for multitask learning: knowledge across tasks can be shared via the dictionary, and flexibility and generalization performance are guaranteed by boosting. More important, this general framework can can be used to adopt any (nonlinear) base learning algorithm to multitask learning. Experimental results on both synthetic and benchmark datasets confirm the effectiveness of the proposed approach and show performance improvements compared to state-of-the-art multitask learning algorithms.", 
    "authors": [
      {
        "name": "Boyu Wang"
      }, 
      {
        "name": "Joelle Pineau"
      }
    ], 
    "keywords": "Multitask learning, Boosting, Dictionary learning", 
    "title": "Generalized Dictionary for Multitask Learning with Boosting", 
    "type": "paper"
  }, 
  "810": {
    "abstract": "To learn users' preference, their feedback information is commonly modeled as scalars and integrated into matrix factorization (MF) based algorithms. Based on MF techniques, the preference degree is computed by the product of user and item vectors, which is also represented by scalars. On the contrary, in this paper, we express users' feedback as constrained vectors, and call the idea constrained preference embedding (CPE); it means that we regard users, items and all users' behavior as vectors. We will see that this viewpoint is more flexible and powerful than traditional MF for item recommendation. For example, by the proposed assumption, users' heterogeneous actions can be coherently mined because all entities and actions can be transferred to a space of the same dimension. In addition, CPE is able to model the feedback of uncertain preference degree. To test our assumption, we propose two models called CPE-s and CPE-ps based on CPE for item recommendation, and show that the popular pair-wise ranking model BPR-MF can be deduced by some restrictions and variations on CPE-s. In the experiments, we will study CPE and the proposed algorithms, and prove their effectiveness.", 
    "authors": [
      {
        "name": "Xin Wang"
      }, 
      {
        "name": "Yunhui Guo"
      }, 
      {
        "name": "Congfu Xu"
      }
    ], 
    "keywords": "Recommender systems, Item recommendation, Preference embedding, Matrix factorization, Bayesian personalized ranking", 
    "title": "Constrained Preference Embedding for Item Recommendation", 
    "type": "paper"
  }, 
  "817": {
    "abstract": "Curling is an adversarial 2-player game with a continuous state and action space, and stochastic action outcomes. This paper focuses on one aspect of the full game, namely, finding the optimal \"hammer shot\", which is the last action taken before a score is tallied. We survey existing methods for finding an optimal action in a continuous low-dimensional space with stochastic outcomes, and adapt a method based on Delaunay Triangulation to our application. Experiments using our curling physics simulator show that our new method's shot selection exceeds Olympic-level human performance and outperforms other algorithms.", 
    "authors": [
      {
        "name": "Zaheen Farraz Ahmad"
      }, 
      {
        "name": "Robert Holte"
      }, 
      {
        "name": "Michael Bowling"
      }
    ], 
    "keywords": "Heuristic Search, Optimization, Games", 
    "title": "Action Selection of Hammer Shots in Curling", 
    "type": "paper"
  }, 
  "820": {
    "abstract": "Large-scale customer service call records contain lots of valuable information for business intelligence, the analysis on which, however, comes to reality until today, in big data era. There are two fundamental problems before mining and analyses: 1) The telephone conversation is mixed with both agent\u00e2\u0080\u0099s and user\u00e2\u0080\u0099s words which have to be recognized before analysis; 2) The speakers in conversation are not in a pre-defined set. These problems are new challenges, which haven\u00e2\u0080\u0099t been well studied in previous work. In this paper, we propose a four-phase framework for role labeling in real customer service telephone conversation, with the benefit of integrating multi-modal features, i.e., both low-level acoustic features and semantic-level textual features. Firstly, we conduct \u00e2\u0088\u0086Bayesian Information Criterion (\u00e2\u0088\u0086BIC) based speaker diarization to get two segments clusters from audio stream. Secondly, the segments are transferred into text in Automatic Speech Recognition (ASR) phase with deep learning model DNN-HMM. Thirdly, by integrating acoustic and textual features, dialog level role labeling is proposed to map the two clusters into agent and user. Finally, sentence level roles correction is designed to correct labeling results in a fine-grained notion, which reduce the errors made in previous phases. The proposed framework is tested in two real datasets: mobile and bank customer service calls datasets. The precision of dialog level labeling is over 99.0%. On sentence level, the accuracy of labeling reaches 90.4%, greatly outperforms traditional acoustic features based model, which gets only 78.5% in accuracy. Notice that our study is the fundamental work of customer service call analysis, which is useful in various business intelligence analyses, such as automatic service quality evaluation, customer satisfaction, hot issues finding, market research, etc.", 
    "authors": [
      {
        "name": "Weizhi Ma"
      }, 
      {
        "name": "Min Zhang"
      }, 
      {
        "name": "Yiqun Liu"
      }, 
      {
        "name": "Shaoping Ma"
      }
    ], 
    "keywords": "Role labeling, Multi-modality, Multi-grained", 
    "title": "Multi-grained Role Labeling Based on Multi-modality Information for Real Customer Service Telephone Conversation", 
    "type": "paper"
  }, 
  "821": {
    "abstract": "The essence of domain adaptation is to explore common latent factors shared by the involved domains. These factors can be specific features or geometric structures. Most of previous methods exploit either the shared features or the shared geometric structures separately. However, the two strategies are complementary with each other and jointly exploring them is more optimal. This paper proposes a novel approach, named joint Feature Selection and Structure Preservation (FSSP), for unsupervised domain adaptation. FSSP smoothly integrates structure preservation and feature selection into a unified optimization problem. Intensive experiments on text categorization, image classification and video event recognition demonstrate that our method performs better, even with up to 30% improvement in average, compared with the state-of-the-art methods.", 
    "authors": [
      {
        "name": "Jingjing Li"
      }, 
      {
        "name": "Jidong Zhao"
      }, 
      {
        "name": "Ke Lu"
      }
    ], 
    "keywords": "Domain Adaptation, Feature Selection, Structure Preservation", 
    "title": "Joint Feature Selection and Structure Preservation for Domain Adaptation", 
    "type": "paper"
  }, 
  "822": {
    "abstract": "Deep feature learning has recently emerged with demonstrated effectiveness in domain adaptation. In this paper, we propose a Deep Nonlinear Feature Coding framework (DNFC) for unsupervised domain adaptation. Our DNFC builds on the marginalized stacked denoising autoencoder (mSDA) to extract rich deep features. We introduce two new elements to mSDA: domain divergence minimization by Maximum Mean Discrepancy (MMD), and nonlinear coding by kernelization. These two elements are essential for domain adaptation as they ensure the extracted deep features to have a small distribution discrepancy and encode data nonlinearity. The effectiveness of our proposed method is verified by extensive experiments on benchmark datasets. Specifically, our DNFC attains much higher prediction accuracy than state-of-the-art domain adaptation methods. Compared to its basis mSDA, DNFC is able to achieve remarkable prediction improvement and meanwhile converges much faster with a small number of stacked layers.", 
    "authors": [
      {
        "name": "Pengfei Wei"
      }, 
      {
        "name": "Yiping Ke"
      }, 
      {
        "name": "Chi Keong Goh"
      }
    ], 
    "keywords": "Unsupervised domain adaptation, deep feature learning, domain divergence minimization, nonlinear coding", 
    "title": "Deep Nonlinear Feature Coding for Unsupervised Domain Adaptation", 
    "type": "paper"
  }, 
  "823": {
    "abstract": "Markowitz's modern portfolio theory as a definitive investment guideline for institutions and individuals is ubiquitous in financial industry. However, its noticeably poor out-of-sample performance due to the inaccurate estimation of parameters evokes unremitting efforts of investigating efficacious remedies. One common retrofit that blends portfolios from disparate investment perspectives has received growing attention. While even a naive portfolio blending strategy can possibly outperform Markowitz's portfolio, how to effectually and robustly blend portfolios to generate stable performance improvement remains less explored. In this paper, we present a machine learning algorithm that leverages Thompson sampling into the sequential decision making process for portfolio blending. By modeling blending coefficients as probabilities of choosing basis portfolios and utilizing Bayes decision rules to update the corresponding distribution functions, our online algorithm sequentially determines the optimal coefficients to blend multiple portfolios that embody different criteria of investment and market views. Comparing with competitive trading strategies across representative datasets, we illustrate that our blending portfolios are superior and robust in terms of standard practical evaluation metrics.", 
    "authors": [
      {
        "name": "Weiwei Shen"
      }, 
      {
        "name": "Jun Wang"
      }
    ], 
    "keywords": "Bandit Learning, Portfolio Choices, Thompson Sampling", 
    "title": "Portfolio Blending via Thompson Sampling", 
    "type": "paper"
  }, 
  "830": {
    "abstract": "We study dynamic trial-offer markets, in which participants first try a product and later decide whether to purchase it or not. In these markets, social influence and position biases have a greater effect on the decisions taken in the sampling stage than those in the buying stage.  We consider a myopic policy that maximizes the market efficiency for each incoming participant, taking into account the inherent quality of products, position biases, and social influence. We prove that this myopic policy is optimal and predictable asymptotically.", 
    "authors": [
      {
        "name": "Andres Abeliuk"
      }, 
      {
        "name": "Gerardo Berbeglia"
      }, 
      {
        "name": "Felipe Maldonado"
      }, 
      {
        "name": "Pascal Van Hentenryck"
      }
    ], 
    "keywords": "Optimization, Marketing, Social influence, Discrete choice models, Position bias, Discrete dynamical systems", 
    "title": "Asymptotic Optimality of Myopic Optimization in Trial-Offer Markets with Social Influence", 
    "type": "paper"
  }, 
  "831": {
    "abstract": "In this paper, we present an approach for robot learning of affordance from human activity videos. We consider the problem particularly in the context of human-robot interaction: Our approach learns structural representations of human-human (and human-object-human) interactions, describing how body-parts of each agent move with respect to each other and what spatial relations they should maintain to complete each sub-event (i.e., sub-goal). This enables the robot to infer its own movement in reaction to the human body motion, allowing it to naturally replicate such interactions.  We introduce the representation of interactive affordance and propose a generative model for its weakly supervised learning from human demonstration videos. Our approach discovers critical steps (i.e., latent sub-events) in an interaction and the typical motion associated with them, learning what body-parts should be involved and how. The experimental results demonstrate that our Markov Chain Monte Carlo (MCMC) based learning algorithm automatically discovers semantically meaningful interactive affordance from RGB-D videos, which allows us to generate appropriate full body motion for an agent.", 
    "authors": [
      {
        "name": "Tianmin Shu"
      }, 
      {
        "name": "Michael S. Ryoo"
      }, 
      {
        "name": "Song-Chun Zhu"
      }
    ], 
    "keywords": "human-robot interaction, Interactive affordance, latent sub-event, hierarchical generative model", 
    "title": "Learning Interactive Affordance for Human-Robot Interaction", 
    "type": "paper"
  }, 
  "835": {
    "abstract": "Multi-task learning attempts to simultaneously leverage data from multiple domains in order to estimate related functions on each domain. For example, a special case of multi-task learning, transfer learning, is often employed when one has a good estimate of a function on a source domain, but is unable to estimate a related function well on a target domain using only target data. Multi-task/transfer learning problems are usually solved by imposing some kind of \u00e2\u0080\u009csmooth\u00e2\u0080\u009d relationship among/between tasks. In this paper, we study how different smoothness assumptions on task relations affect the upper bounds of algorithms proposed for these problems under different settings. For general multi-task learning, we study a family of algorithms which utilize a reweighting matrix on task weights to capture the smooth relationship among tasks, which has many instantiations in existing literature. Furthermore, for multi-task learning in a transfer learning framework, we study the recently proposed algorithms for the \u00e2\u0080\u009cmodel shift\u00e2\u0080\u009d, where the conditional distribution P(Y|X) is allowed to change across tasks but the change is assumed to be smooth. In addition, we illustrate our results with experiments on both simulated and real data.", 
    "authors": [
      {
        "name": "Xuezhi Wang"
      }, 
      {
        "name": "Junier Oliva"
      }, 
      {
        "name": "Jeff Schneider"
      }, 
      {
        "name": "Barnabas Poczos"
      }
    ], 
    "keywords": "Nonparametric Risk Analysis, Stability Analysis, Multi-Task Learning, Transfer Learning", 
    "title": "Nonparametric Risk and Stability Analysis for \ufffcMulti-Task Learning Problems", 
    "type": "paper"
  }, 
  "837": {
    "abstract": "Representation learning of knowledge graphs aims to encode both entities and relations into a continuous low-dimensional vector space. Most existing methods only concentrate on learning representations with structural information located in triples, regardless of the rich information located in hierarchical types of entities which could be collected in most knowledge graphs. In this paper, we propose TKRL, a novel representation learning method of knowledge graphs, to take advantages of hierarchical entity type information. We suggest that entities should have multiple representations in different types. More specifically, we consider hierarchical types as projection matrices for entities, with two type encoders designed to model hierarchical structures. Meanwhile, type information is also utilized as relation-specific type constraints. We evaluate our models on two tasks including knowledge graph completion and triplet classification, and further explore the performances on long tail dataset. Experimental results show that our models significantly outperforms all baselines on both tasks, especially with long tail distribution, which indicates that our models are capable of capturing hierarchical type information which is significant when constructing representations of knowledge graphs.", 
    "authors": [
      {
        "name": "Ruobing Xie"
      }, 
      {
        "name": "Zhiyuan Liu"
      }, 
      {
        "name": "Maosong Sun"
      }
    ], 
    "keywords": "representation learning, knowledge graph, entity type, hierarchical type", 
    "title": "Representation Learning of Knowledge Graphs with Hierarchical Types", 
    "type": "paper"
  }, 
  "844": {
    "abstract": "Monte-Carlo tree search (MCTS) has been shown to be effective in solving many online planning problems modelled as MDPs. However the scalability of this approach remains limited in MDPs with large branching factors. Prior approaches have explored the use of state abstractions to overcome this limitation but they require ad hoc fixes to resolve the non-Markovian nature of models resulting from abstraction. We propose a new approach for utilizing abstraction in efficiently solving large MDPs while preserving the Markovian nature of models. We model the ground MDP with state abstraction as a POMDP, introduce action abstraction by defining abstract actions as transitions between abstract states, and develop a hierarchical MCTS algorithm that operates in the space of possible histories of the resulting POMDP. We show that the search tree in this history space has a lower branching factor than in the ground space and prove that the performance loss in terms of action values due to state abstraction is bounded by a constant multiple of a state aggregation error. The resulting algorithm converges to the recursively optimal policy consistent with the input state and action abstractions. Empirical results show that the hierarchical MCTS algorithm outperforms ground MCTS significantly by orders of magnitude.", 
    "authors": [
      {
        "name": "Aijun Bai"
      }, 
      {
        "name": "Siddharth Srivastava"
      }, 
      {
        "name": "Stuart Russell"
      }
    ], 
    "keywords": "Monte-Carlo Tree Search, State Abstraction, Action Abstraction, Markov Decision Process, Partially Observable Markov Decision Process", 
    "title": "Markovian State and Action Abstractions in Monte-Carlo Tree Search", 
    "type": "paper"
  }, 
  "848": {
    "abstract": "Non-negative matrix factorization (NMF) and non-negative matrix tri-factorization (NMTF) have been widely researched these years. Many different models are proposed. Most of these models are based on multiplicative updating rules which involves intensive computation. Also, previous algorithms converge very slowly. On the other hand, outliers widely exist in real world applications. When there exist outliers in data sets, most previous NMF/NMTF models fail to achieve good clustering performance. To address the above challenges, in this paper, we propose three new NMF/NMTF models which are robust to outliers. Efficient algorithms are proposed, which converge as fast as k-means algorithm, and thus scalable to large data sets. Experimental results on both synthetic and real world data sets show that our methods outperform other popular NMF/NMTF methods in most cases, and in the meanwhile, takes much less computational time.", 
    "authors": [
      {
        "name": "De Wang"
      }, 
      {
        "name": "Feiping Nie"
      }, 
      {
        "name": "Heng Huang"
      }
    ], 
    "keywords": "Non-negative Matrix Factorization, Non-negative Matrix Tri-factorization, Robust Learning", 
    "title": "Fast Robust Non-negative Matrix Factorization for Large Scale Data Clustering", 
    "type": "paper"
  }, 
  "859": {
    "abstract": "Exploring crowd dynamics is essential in understanding crowd scenes, which still remains as a challenging task due to the nonlinear characteristics and coherent spatio-temporal motion patterns in crowd behaviors. To address these issues, we present a Coherent Long Short Term Memory (cLSTM) network to  capture the nonlinear crowd dynamics by learning an informative representation of crowd motions, which facilitates the critical tasks in crowd scene analysis. By describing the crowd motion patterns with a cloud of keypoint tracklets, we explore the nonlinear crowd dynamics embedded in the tracklets with a stacked LSTM model, which is further improved to capture the collective properties by introducing a coherent regularization term; and finally, we adopt an unsupervised encoder-decoder framework to learn a hidden feature for each input tracklet that embeds its inherent dynamics. With the learnt features properly harnessed, crowd scene understanding is conducted effectively in predicting the future paths of agents, estimating group states, and classifying crowd events. Extensive experiments on hundreds of public crowd videos demonstrate that our method is state-of-the-art performance by exploring the coherent spatio-temporal structures in crowd behaviors.", 
    "authors": [
      {
        "name": "Hang Su"
      }, 
      {
        "name": "Yinpeng Dong"
      }, 
      {
        "name": "Jun Zhu"
      }, 
      {
        "name": "Haibin Ling"
      }, 
      {
        "name": "Bo Zhang"
      }
    ], 
    "keywords": "Crowd Behavior Analysis, LSTM, Coherent Motion", 
    "title": "Understanding Crowd Scene based on Coherent Recurrent Neural Networks", 
    "type": "paper"
  }, 
  "863": {
    "abstract": "Complex objects are usually with multiple modal features. In multi-modal learning, modalities closely related to the target tasks are known as strong modalities. While collecting strong modalities of all instances is often expensive, and current multi-modal learning techniques hardly take the strong modal feature extraction expenses into consideration. On the other hand, active learning is proposed to reduce the labeling expenses by querying the ground truths for specific selected instances. In this paper, we propose a training strategy, ACQUEST (ACtive QUErying STrong modalities), which exploits strong modal information by actively querying the strong modal feature values of ``selected'' instances rather than their corresponding ground truths. In ACQUEST, only the informative instances are selected for strong modal feature acquiring. An inverse prediction technique is also proposed to make the ACQUEST a unified optimization form. Experiments on image datasets show that ACQUEST achieves better classification performance than conventional active learning and multi-modal learning methods with less feature acquisition costs and labeling expenses.", 
    "authors": [
      {
        "name": "Yang Yang"
      }, 
      {
        "name": "De-Chuan Zhan"
      }, 
      {
        "name": "Yuan Jiang"
      }
    ], 
    "keywords": "Multi-Modal Learning, Active Feature Querying, Strong/Weak Modalities", 
    "title": "Learning by Actively Querying Strong Modal Features", 
    "type": "paper"
  }, 
  "868": {
    "abstract": "Distributed representation learned with neural networks has recently showed to be very effective in modeling human languages at fine granularities, including words, phrases, and sentences. The success of such an approach on larger spans of text, i.e., documents, is much less certain due to the inherent challenges. A typical problem of document-level modeling is automatic summarization, which aims to understand a document and generate a summary for it. In this work, we proposed a distraction-enhanced neural models for the problem. Instead of focusing on attention as in much recent work, which is a nature choice to grasp local context and correspondence, we explicitly force our models to learn distraction in order to be able to traverse between different subtopics and aspects of a document, and hence better grasp the meaning of the entire document. Without any feature engineering, we show our model achieve the state-of-the-art results on two public benchmarks.", 
    "authors": [
      {
        "name": "Qian Chen"
      }, 
      {
        "name": "Xiaodan Zhu"
      }, 
      {
        "name": "Zhenhua Ling"
      }, 
      {
        "name": "Si Wei"
      }, 
      {
        "name": "Hui Jiang"
      }
    ], 
    "keywords": "Document modeling, Abstractive summarization, Recurrent neural network", 
    "title": "A Neural Network for Document Summarization", 
    "type": "paper"
  }, 
  "872": {
    "abstract": "The automation of hiring decisions is a well-studied topic in crowdsourcing.  Existing hiring algorithms make a common assumption---that each worker has a level of task competence that is static and does not vary over time.  In this work, we explore the question of how to hire workers who can learn over time.  Using a medical time series classification task as a case study, we conducted experiments to show that workers\u00e2\u0080\u0099 performance does improve with experience and that it is possible to model and predict their learning rate.  Furthermore, we propose a dynamic hiring mechanism that accounts for workers\u00e2\u0080\u0099 learning potential.  Through both simulation and real-world crowdsourcing data, we show that our hiring procedure can lead to high-accuracy outcomes at lower cost compared to other mechanisms.", 
    "authors": [
      {
        "name": "Shengying Pan"
      }, 
      {
        "name": "Kate Larson"
      }, 
      {
        "name": "Josh Bradshaw"
      }, 
      {
        "name": "Edith Law"
      }
    ], 
    "keywords": "Crowdsourcing, Task Routing, Markov Decision Process", 
    "title": "Dynamic Task Allocation Algorithm for Hiring Workers that Learn", 
    "type": "paper"
  }, 
  "877": {
    "abstract": "In this paper, we choose to learn useful cues from object recognition mechanisms of the human visual cortex, and propose a DCNN performance improvement method without the need for increasing the network complexity.  Inspired by the category-selective property of the neuron population in the IT layer of the human visual cortex, we enforce the neuron responses at the top DCNN layer to be category selective. To achieve this, we propose the Sparse Category-Selective Objective Function to modulate the neuron outputs of the top DCNN layer. The proposed method is generic and can be applied to any DCNN models. As experimental results show, when applying the proposed method to the CIFAR10-Quick model CIFAR100-Quick model and NIN models, image classification performances are remarkably improved on four widely used benchmark datasets: CIFAR-10, CIFAR-100, MNIST and SVHN, which demonstrate the effectiveness of the presented method.", 
    "authors": [
      {
        "name": "Shizhou Zhang"
      }, 
      {
        "name": "Yihong Gong"
      }, 
      {
        "name": "Jinjun Wang"
      }
    ], 
    "keywords": "DCNN, Category-Selective, Improving, group sparse", 
    "title": "Improving DCNN Performance with Sparse Category-Selective Objective Function", 
    "type": "paper"
  }, 
  "89": {
    "abstract": "We present a new preprocessing technique for propositional model counting.  This technique leverages definability, i.e., the ability to determine that some gates are implied  by the input formula \\Sigma. Such gates can be exploited to simplify \\Sigma without modifying its number of models. Unlike previous approaches, we only take advantage of the existence of gates, but we do not need to make the gates explicit. Our preprocessing technique thus consists of two phases:  computing a bipartition <I, O> of the variables of \\Sigma where the variables from O are defined in \\Sigma in terms of I, then eliminating some variables of O in \\Sigma. Our large scale experiments clearly show the computational benefits which can be achieved by taking advantage of our preprocessing technique for model counting.", 
    "authors": [
      {
        "name": "Jean Marie Lagniez"
      }, 
      {
        "name": "Emmanuel Lonca"
      }, 
      {
        "name": "Pierre Marquis"
      }
    ], 
    "keywords": "propositional logic, model counting, preprocessing, definability, #SAT", 
    "title": "Improving Model Counting by Leveraging Definability", 
    "type": "paper"
  }, 
  "894": {
    "abstract": "In this paper, we investigate model checking algorithms for variants of strategy logic over pushdown multi-agent systems, modeled by pushdown game structures (PGSs). We consider various fragments of strategy logic, i.e., SL[CG], SL[DG], SL[1G] and $\\bsil$. We show that the model checking problems on PGSs for SL[CG], SL[DG] and SL[1G] are 3EXTIME-complete, which are not harder than the problem for the subsumed logic ATL$^*$. When $\\bsil$ is concerned, the model checking problem becomes 2EXPTIME-complete. Our algorithms are automata-theoretic and based on the saturation technique, which are amenable to implementations.", 
    "authors": [
      {
        "name": "Taolue Chen"
      }, 
      {
        "name": "Fu Song"
      }, 
      {
        "name": "Zhilin Wu"
      }
    ], 
    "keywords": "Pushdown Multi-Agent Systems, Concurrent (Infinite-state) Game Structures, Strategy Logic, Model Checking, Verification", 
    "title": "Verifying Pushdown Multi-Agent Systems against Strategy Logics", 
    "type": "paper"
  }, 
  "90": {
    "abstract": "Video-based facial expression recognition (FER) has recently received increased attention as a result of its widespread application. Many kinds of features have been proposed to represent different properties of facial expressions in videos. However the dimensionality of these features is usually high. In addition, due to the complexity of the available information in video sequences, using only one type of feature is often inadequate. How to effectively reduce the dimensionality and combine multi-view features thus becomes a challenging problem and is barely considered in FER. In this paper, motivated by the recent success in exclusive feature selection, we first introduce exclusive group LASSO (EG-LASSO) to unsupervised dimension reduction (UDR). This leads to the proposed exclusive UDR (EUDR) framework, which allows arbitrary sparse structures on the feature space. To properly combine multiple kinds of features, we further extend EUDR to multi-view EUDR(MEUDR), where the structured sparsity is enforced at both intra- and inter-view levels to explore the correlations on and between the feature spaces of different views. In addition, combination weights are learned for all views to allow them to contribute differently to the final consensus presentation. The complementary nature of multiple views tends to be well-leveraged, and it is possible to obtain a reliable solution. Experiments on two challenging video-based FER datasets demonstrate the effectiveness of the proposed method.", 
    "authors": [
      {
        "name": "Liping Xie"
      }, 
      {
        "name": "Dacheng Tao"
      }, 
      {
        "name": "Haikun Wei"
      }
    ], 
    "keywords": "Video based FER, multiple features, dimension reduction, exclusive group LASSO", 
    "title": "Multi-view Exclusive Unsupervised Dimension Reduction for Video-based Facial Expression Recognition", 
    "type": "paper"
  }, 
  "902": {
    "abstract": "This paper studies the question of external memory bidirectional search. That is, how bidirectional search algorithms can be adapted to efficiently run using external memory resources such as hard drives or solid state drives. While external memory algorithms have been broadly studied in unidirectional search, work has not been done in bidirectional search. We show that the primary bottleneck in bidirectional search is the question of solution detection -- knowing when the two search frontiers have met. We propose a method of delayed solution detection that makes external bidirectional search more efficient. Experimental results show the effectiveness of the approach.", 
    "authors": [
      {
        "name": "Nathan Sturtevant"
      }, 
      {
        "name": "Jingwei Chen"
      }
    ], 
    "keywords": "search, external memory, parallel, breadth-first search, heuristic search", 
    "title": "External Memory Bidirectional Search", 
    "type": "paper"
  }, 
  "903": {
    "abstract": "Coral reefs are valuable and fragile ecosystems which are under threat from human activities like coral mining or blast fishing. Many countries have built marine protected areas (MPAs) and protect their ecosystems through boat patrol. However, it remains a significant challenge to efficiently patrol the MPAs given the limited patrol resources of the protection agency and potential destructors' strategic actions, e.g., the destructor can take a covert path in the MPA to arrive at some specific area and determine a time duration to perform damaging activities in the area. In this paper, we view the problem of efficient patrol for protecting coral reef ecosystems from a game-theoretic perspective and propose 1) a new Stackelberg game model to formulate the problem of protecting MPAs, 2) two algorithms to compute the efficient protection agency's strategies: a compact linear program CLP in which the protection agency's strategies are compactly represented as fractional flows in a network, and a compact-strategy double-oracle algorithm on graphs CDOG which combines the techniques of compactly representing defender strategies and incrementally generating strategies. Experimental results show that our model and algorithms lead to significantly better solution quality than that of previous works, and that the CDOG algorithm can efficiently solve large scale games.", 
    "authors": [
      {
        "name": "Yue Yin"
      }, 
      {
        "name": "Bo An"
      }
    ], 
    "keywords": "Environmental protection, Resource allocation, Double oracle and compact representation", 
    "title": "Efficient Resource Allocation for Protecting Coral Reef Ecosystems", 
    "type": "paper"
  }, 
  "910": {
    "abstract": "Plan recognition algorithms infer agents' plans from their observed   actions. Due to imperfect knowledge about the agent's behavior and   the environment, it is often the case that there are multiple   hypotheses about an  agent's plans that are consistent with the   observations, though only one of these hypotheses is correct.  This   paper addresses the problem of how to disambiguate between   hypotheses, by querying the acting agent about whether a candidate   plan in one of the hypotheses matches its intentions.  This process is   performed sequentially and used to update the set of possible   hypotheses during the recognition process.  The paper defines the   sequential plan recognition process (SPRP), which seeks to reduce the number of hypotheses   using a minimal number of queries.   We propose a number of  policies  for the SPRP which use maximum likelihood and information gain to choose which plan to query. We show this approach works well in practice on two domains from the literature, significantly reducing the number of hypotheses  using  fewer queries than  a baseline approach.", 
    "authors": [
      {
        "name": "Reuth Mirsky"
      }, 
      {
        "name": "Kobi Gal"
      }, 
      {
        "name": "Roni Stern"
      }, 
      {
        "name": "Meir Kalech"
      }
    ], 
    "keywords": "plan recognition, activity recognition, human aware AI", 
    "title": "Sequential Plan Recognition", 
    "type": "paper"
  }, 
  "919": {
    "abstract": "Consider a buyer with independent additive valuations for a set of goods, and a seller who is constrained to sell one item at a time in an online fashion. If the seller is constrained to run independent auctions for each item, then he would run Myerson's optimal auction for each item. If the seller is allowed to use the full power of dynamic mechanism design and have the auction for each item depend on the outcome of the previous auctions, he is able to perform much better. The main issues in implementing such strategies in online settings where items arrive over time are that the auction might be too complicated or it makes too strong assumptions on the buyer's rationality or seller's commitment over time. This motivates us to explore a restricted family of dynamic auctions that can be implemented in an online fashion and without too much commitment from the seller ahead of time. In particular, we study a set of auction in which the space of single-shot auctions is augmented with a structure that we call {\\em bank account}, a real number for each node that summarizes the history so far. This structure allows the seller to store deficits or surpluses of buyer utility from each individual auction and even them out on the long run. This is akin to enforcing individual rationality constraint on average rather than per auction. We also study the effect of enforcing a maximum limit to the values that bank account might grow, which means that we enforce that besides the auction being individually rational on average it is also not far from being individually rational at any given interval. Interestingly, even with these restrictions, we can achieve significantly better revenue and social welfare compared to separate Myerson auctions.", 
    "authors": [
      {
        "name": "Vahab Mirrokni"
      }, 
      {
        "name": "Renato Paes Leme"
      }, 
      {
        "name": "Pingzhong Tang"
      }, 
      {
        "name": "Song Zuo"
      }
    ], 
    "keywords": "dynamic mechanism design, dynamic auctions, bank account, revenue maximization", 
    "title": "Dynamic Auctions with Bank Accounts", 
    "type": "paper"
  }, 
  "92": {
    "abstract": "Urban water quality is of great importance to our daily lives. Prediction of urban water quality help control water pollution and protect human health. In this work, we forecast the water quality of a station over the next few hours, using a multi-task multi-view learning method to fuse multiple datasets from different domains. In particular, our learning model comprises two alignments. The first alignment is the spaio-temporal view alignment, which combines local spatial and temporal information of each station. The second alignment is the prediction alignment among stations, which captures their spatial correlations and performs co-predictions by incorporating these correlations. Extensive experiments on real-world datasets demonstrate the effectiveness of our approach.", 
    "authors": [
      {
        "name": "Ye Liu"
      }, 
      {
        "name": "Yu Zheng"
      }, 
      {
        "name": "Yuxuan Liang"
      }, 
      {
        "name": "Shuming Liu"
      }, 
      {
        "name": "David Rosenblum"
      }
    ], 
    "keywords": "Urban water quality prediction, Urban computing, Multi-task learning, Spatial-temporal analysis", 
    "title": "Urban Water Quality Prediction based on Multi-task Multi-view Learning", 
    "type": "paper"
  }, 
  "927": {
    "abstract": "The Distributed Breakout Algorithm (DBA) is a local search algorithm that was originally designed to solve DisCSPs and DisMaxCSPs. Extending it to general-valued DCOPs requires three design choices: manner of modifying base costs (multiplicative weights or additive penalties); definition of constraint violation (non-zero cost, non-minimum cost, and maximum cost); and scope of modifying cost tables during breakout (entry, row, column, or table).  We propose Generalized DBA (GDBA) to span the \n931\tMain Track\tBeyond object recognition: Deep visual sentiment analysis via affective concept supervision and transfer\tJingwen Wang, Jianlong Fu, Yong Xu and Tao Mei\tvisual sentiment analysis, affective concepts, deep learning\"", 
    "authors": [
      {
        "name": "Steven Okamoto"
      }, 
      {
        "name": "Roie Zivan"
      }, 
      {
        "name": "Aviv Nahon"
      }
    ], 
    "keywords": "Distributed Constraint Optimization, Distributed Local Search, Breaking Out of Local Optima", 
    "title": "Distributed Breakout: Beyond Satisfaction", 
    "type": "paper"
  }, 
  "943": {
    "abstract": "Identification of module or community structures is important for characterizing and understanding complex systems. While designed with different objectives, i.e., stochastic models for regeneration and modularity maximization models for discrimination, both these two types of model look for low-rank embeddings to best represent and reconstruct network topology. However, the mapping through such embedding is linear, whereas real networks have various nonlinear features, making these models less effective in practice. Inspired by the strong representation power of deep neural networks, we propose a novel deep nonlinear reconstruction method by adopting deep neural networks for representation. We then extend the method to a semi-supervised community detection algorithm by incorporating pairwise constraints among graph nodes. Extensive experimental results on synthetic and real networks show that the new methods are effective, outperforming most state-of-the-art methods for community detection.", 
    "authors": [
      {
        "name": "Liang Yang"
      }, 
      {
        "name": "Xiaochun Cao"
      }, 
      {
        "name": "Dongxiao He"
      }, 
      {
        "name": "Chuan Wang"
      }, 
      {
        "name": "Xiao Wang"
      }, 
      {
        "name": "Weixiong Zhang"
      }
    ], 
    "keywords": "Community detection, nonlinear reconstruction, deep learning, modularity based algorithm, Semi-supervised algorithm", 
    "title": "Modularity based Community Detection with Deep Learning", 
    "type": "paper"
  }, 
  "946": {
    "abstract": "Many natural language processing tasks can be generalized into segmentation problem. In this paper, we combine semi-CRF with neural network to solve this. Our model represents a segment both by composing the input and embedding the segment. We thoroughly study different composition functions and different segment embeddings and conduct extensive experiments on two typical segmentation tasks: named entity recognition (NER) and Chinese word segmentation (CWS). Experimental results show that our neural semi-CRF model benefits from multi-level segment representation and achieves the state-of-the-art performance on CWS benchmark dataset and competitive results on the CoNLL03 NER dataset.", 
    "authors": [
      {
        "name": "Yijia Liu"
      }, 
      {
        "name": "Wanxiang Che"
      }, 
      {
        "name": "Jiang Guo"
      }, 
      {
        "name": "Bing Qin"
      }, 
      {
        "name": "Ting Liu"
      }
    ], 
    "keywords": "natural language processing, segmentation, neural network, named entity recognition, Chinese word segmentation", 
    "title": "Neural Segmentation Models Leveraging Segment Representations", 
    "type": "paper"
  }, 
  "948": {
    "abstract": "Recommender systems rely on techniques of predicting the ratings that users would give to yet unconsumed items. Probabilistic matrix factorization (PMF) is a standard technique for such prediction and makes a prediction on the basis of an underlying probabilistic generative model of the behavior of users. We investigate a new model of users' consumption and rating, where a user tends to consume an item that emphasizes those features that the user seeks to enjoy, and the ratings of the users are more strongly affected by those features than others. We incorporate this new user model into PMF and show that the resulting method, {\\em Gated PMF} (GPMF), improves the predictive accuracy by several percent on standard datasets. GPMF is widely applicable, as it is trained only with the ratings given by users and does not rely on any auxiliary data.", 
    "authors": [
      {
        "name": "Shohei Ohsawa"
      }, 
      {
        "name": "Yachiko Obara"
      }, 
      {
        "name": "Takayuki Osogami"
      }
    ], 
    "keywords": "recommendation systems, probabilistic matrix factorization, attention, missing values", 
    "title": "Gated Probabilistic Matrix Factorization: Learning Users' Attention from Missing Values", 
    "type": "paper"
  }, 
  "952": {
    "abstract": "This paper addresses the problem of geometric scene parsing, i.e. simultaneously labeling geometric surfaces (e.g. sky, ground and vertical plane) and determining the interaction relations (e.g. layering, supporting, siding and affinity) between main regions. Furthermore, we aim at 3D scene reconstruction based on the geometric parsing of a single image. This problem is more challenging than the traditional semantic scene labeling, as recovering geometric structures necessarily requires the rich and diverse contextual information. To achieve these goals, we propose a novel recurrent neural network model, named Hierarchical Long Short-term Memory (H-LSTM). It contains two coupled sub-networks: the Pixel LSTM (P-LSTM) and the Multi-scale Super-pixel LSTM (MS-LSTM) for handling the surface labeling and relation prediction, respectively. The two sub-networks provide complementary information to each other to exploit hierarchical scene contexts, and they are jointly optimized for boosting the performance.  Extensive experiments demonstrate that our model is capable of parsing scene geometric structures and advancing the state-of-the-art on SIFT-Flow, LM+SUN and Geometric Context dataset significantly. Several impressive results of 3D reconstruction from a single image are also presented.", 
    "authors": [
      {
        "name": "Zhanglin Peng"
      }, 
      {
        "name": "Ruimao Zhang"
      }, 
      {
        "name": "Xiaodan Liang"
      }, 
      {
        "name": "Liang Lin"
      }, 
      {
        "name": "Xiaobai Liu"
      }
    ], 
    "keywords": "Geometric Scene Parsing, Hierarchical LSTM, Multi-task Learning", 
    "title": "Geometric Scene Parsing with Hierarchical LSTM", 
    "type": "paper"
  }, 
  "96": {
    "abstract": "Learning compact hash codes has been a vibrant research topic for large-scale similarity search owing to the low storage cost and expedited search operation. A recent research thrust aims to learn compact codes jointly from multiple sources, referred to as cross-view (or cross-modal) hashing in the literature. The main theme of this paper is to develop a novel formulation and optimization scheme for cross-view hashing. As a key differentiator, our proposed method directly conducts optimization on discrete binary hash codes, rather than relaxed continuous variables as in existing cross-view hashing methods. This way relaxation-induced search accuracy loss can be avoided. We attack the cross-view hashing problem by simultaneously capturing semantic neighboring relations and maximizing the generative probability of the learned hash codes in each view. Specifically, to enable effective optimization on discrete hash codes, the optimization proceeds in a block coordinate descent fashion. Each iteration sequentially updates a single bit with others clamped. We transform the resultant sub-problem into an equivalent, more tractable quadratic form and devise an active set based solver on the discrete codes. Rigorous theoretical analysis is provided for the convergence and local optimality condition. Comprehensive evaluations are conducted on three image benchmarks. The clearly superior experimental results faithfully prove the merits of the proposed method.", 
    "authors": [
      {
        "name": "Yadong Mu"
      }, 
      {
        "name": "Zongting Lv"
      }, 
      {
        "name": "Wei Liu"
      }, 
      {
        "name": "Cheng Deng"
      }, 
      {
        "name": "Xinbo Gao"
      }
    ], 
    "keywords": "image hashing, coordinate descent, cross-view retrieval", 
    "title": "Coordinate Discrete Optimization for Efficient Cross-View Image Retrieval", 
    "type": "paper"
  }, 
  "961": {
    "abstract": "This paper presents a stochastic  grammar for fine-grained 3D scene reconstruction from a single image.    At the heart of our model is a small number  of grammar rules that can describe the most common geometric structures, e.g.  two straights lines being  co-linear or orthogonal, or that a line lying on a planar region etc.   With these rules, we re-frame single-view 3D reconstruction problem as jointly solving two coupled sub-tasks: i) segmenting of  image entities, e.g. planar regions and straight edge segments,  and ii) optimizing pixel-wise 3D scene model through the application of grammar rules over image entities.  We develop a stage-wise  method to automatically estimate model parameters  from training images without  supervisions. To reconstruct a new image, we design an efficient hybrid Monte Carlo (HMC) algorithm to  simulate Markov Chain using two major dynamics: i) \\emph{Hamiltonian Dynamics}  that makes proposals along the gradient direction for the  continuous  pixel-wise 3D scene model;  and ii) \\emph{Cluster  Dynamics}, that  flip the colors of  clusters of pixels to form planar region partition. Following the Metropolis-hasting principle, these  dynamics not only make distant proposals but also guarantee detail-balance and fast convergence.  Results with comparisons on public image dataset show that our method clearly outperforms the alternate state-of-the-art methods.", 
    "authors": [
      {
        "name": "Xiaobai Liu"
      }, 
      {
        "name": "Yadong Mu"
      }, 
      {
        "name": "Liang Lin"
      }
    ], 
    "keywords": "Stochastic grammar, Scene Reconstruction, Single View", 
    "title": "A Stochastic Grammar for Fine-grained 3D Scene Reconstruction", 
    "type": "paper"
  }, 
  "963": {
    "abstract": "Region-optimal algorithms are local search algorithms for the solution of Distributed Constraint Optimization Problems (DCOPs). In each iteration of the search in such algorithms, every agent selects a group of agents that comply with some selection criteria (each algorithm specifies different criteria). Then, the agent who selected the group, called the mediator, collects assignment information from the group and neighboring agents outside the group, in order to find an optimal set of assignments for its group's agents. A contest between mediators of adjacent groups determines which groups will replace their assignments in that iteration to the found optimal ones. In this work we present a framework called RODA (Region-Optimal DCOP Algorithm) that encompasses the algorithms in the region-optimality family, and in particular any method for selecting groups. We devise a secure implementation of RODA, called P-RODA, which preserves constraint privacy and partial decision privacy. The two main cryptographic means that enable this privacy preservation are secret sharing and homomorphic encryption. We estimate the computational overhead of P-RODA with respect to RODA and give an upper bound that depends on the group and domain sizes and the graph topology but not on the number of agents. The estimations are backed with experimental results.", 
    "authors": [
      {
        "name": "Tamir Tassa"
      }, 
      {
        "name": "Roie Zivan"
      }, 
      {
        "name": "Tal Grinshpoun"
      }
    ], 
    "keywords": "DCOP, Incomplete algorithms, Privacy, Region optimality", 
    "title": "Preserving Privacy in Region Optimal DCOP Algorithms", 
    "type": "paper"
  }, 
  "964": {
    "abstract": "A framework is presented that hierarchically integrates symbolic and sub-symbolic representations in a cognitive robotic architecture. The framework consist of a sense-act directed acyclic graph of nodes. The world-model belief state in each node is updated by action predictions and observations. Node behaviour actions are generated from the belief state by policies selected from parent nodes. We formalise the cognitive architecture abstractly as an explanatory aid and to allow seamless linkages of hybrid node representations. We instantiate the architecture with three levels of nodes where the top node policy function is a concurrent multi-tasking teleo-reactive program. The middle node world-model includes a physics simulator with 3D common-sense spatial knowledge. Lower level nodes process sensors and control the limbs of a real robot. In our experiments a humanoid robot is tasked to build towers of blocks, even in the face of human help and hindrance.", 
    "authors": [
      {
        "name": "Keith Clark"
      }, 
      {
        "name": "Bernhard Hengst"
      }, 
      {
        "name": "Maurice Pagnucco"
      }, 
      {
        "name": "David Rajaratnam"
      }, 
      {
        "name": "Peter Robinson"
      }, 
      {
        "name": "Claude Sammut"
      }, 
      {
        "name": "Michael Thielscher"
      }
    ], 
    "keywords": "Cognitive Robotic Architecture, Symbolic and Sub-symbolic Representations, Robotics, Formal Framework, Teleo-Reactive Program, Physics Simulation, Hierarchical Control", 
    "title": "A Framework for Integrating Symbolic and Sub-symbolic Representations", 
    "type": "paper"
  }, 
  "967": {
    "abstract": "In this paper, we study synthesis under partial observability form logical specifications over finite traces expressed in LTLf/LDLf. This form of synthesis can be seen as a generalization of planning under partial observability in nondeterministic domains, which is known to be 2EXPTIME-complete. We start by showing that the usual ``belief-state construction'' used in planning under partial observability works also for general LTLf/LDLf synthesis, though with a jump in computational complexity from 2EXPTIME to 3EXPTIME. Then we show that the belief-state construction can be avoided in favor of a direct automata construction which exploits projection to hide unobservable propositions. This allow us to prove that the problem remains 2EXPTIME-complete. The new synthesis technique proposed is effective and readily implementable.", 
    "authors": [
      {
        "name": "Giuseppe De Giacomo"
      }, 
      {
        "name": "Moshe Vardi"
      }
    ], 
    "keywords": "Planning for temporary extended goals, Partial observability, LTL and LDL on finite traces, Synthesis from logical specifications, Reasoning about actions", 
    "title": "LTLf and LDLf Synthesis Under Partial Observability", 
    "type": "paper"
  }, 
  "976": {
    "abstract": "Selecting discriminative features in positive unlabelled (PU) learning tasks is a challenging problem due to lack of negative class information. Traditional supervised and semi-supervised feature selection methods are not able to be applied directly in this scenario, and unsupervised feature selection algorithms are designed to handle unlabelled data while neglecting the available information from positive class. To leverage the partially observed positive class information, we propose to encode the weakly supervised information in PU learning tasks into pairwise constraints between training instances. Violation of pairwise constraints are measured and incorporated into a partially supervised graph embedding model. Extensive experiments on different benchmark databases and a real-world cyber security application demonstrate the effectiveness of our algorithm.", 
    "authors": [
      {
        "name": "Yufei Han"
      }, 
      {
        "name": "Yun Shen"
      }
    ], 
    "keywords": "Graph Embedding, Positive Unlabelled Learning, Feature Selection", 
    "title": "Partially Supervised Graph Embedding for Positive Unlabelled Feature Selection", 
    "type": "paper"
  }, 
  "984": {
    "abstract": "The performance of deep neural networks is well-known to be sensitive to the setting of their hyperparameters. Recent advances in reverse-mode automatic differentiation allow for optimizing hyperparameters with gradients. The standard way of computing these gradients involves a forward and backward pass of computations. However, the backward pass usually needs to consume unaffordable memory to store all the intermediate variables to exactly reverse the forward training procedure. In this work we propose a new method, DrMAD, to distill the knowledge of the forward pass into a shortcut path, through which we approximately reverse the training trajectory. Experiments on several image benchmark datasets show that DrMAD is at least 45 times faster and consumes 100 times less memory compared to state-of-the-art methods for optimizing hyperparameters with minimal compromise to its effectiveness. To the best of our knowledge, DrMAD is the first research attempt to make it practical to automatically tune thousands of hyperparameters of deep neural networks.", 
    "authors": [
      {
        "name": "Jie Fu"
      }, 
      {
        "name": "Hongyin Luo"
      }, 
      {
        "name": "Jiashi Feng"
      }, 
      {
        "name": "Kian Hsiang Low"
      }, 
      {
        "name": "Tat-Seng Chua"
      }
    ], 
    "keywords": "deep learning, hyperparameter optimization, automatic differentiation", 
    "title": "DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing Hyperparameters of Deep Neural Networks", 
    "type": "paper"
  }, 
  "985": {
    "abstract": "We revisit the problem of designing strategyproof (SP) mechanisms for allocating divisible items among two agents, where payments are disallowed, and there is no prior information on the agents' preferences. The objective is to design SP mechanisms which are competitive against the most efficient (but not SP) mechanism.  For the case with two items: - We use an analytic approach to derive SP mechanisms which are more competitive than all prior SP mechanisms, and to provide a characterization of SP mechanisms. - We improve the linear-program-based proof of Guo and Conitzer to show new upper bounds on competitive ratios. - We provide the first \"mathematical\" upper bound proof.  For the cases with any number of items, we build on the Partial Allocation mechanisms introduced by Cole et al. to design a SP mechanism which is 0.67776-competitive, breaking the 2/3 barrier.  We propose a new class of SP mechanisms, Dynamical-Increasing-Price mechanisms, where each agent purchases the items using virtual money, and the prices of the items depend on other agents' preferences.", 
    "authors": [
      {
        "name": "Yun Kuen Cheung"
      }
    ], 
    "keywords": "mechanism design, resource allocation, payment-free, prior-free, strategyproofness", 
    "title": "Better Strategyproof Mechanisms without Payments or Prior -- An Analytic Approach", 
    "type": "paper"
  }, 
  "998": {
    "abstract": "We study a combinatorial problem formulated in terms of the following team-formation scenario. Given a group of agents, where each agent has preferences over the set of potential team leaders, the task is to partition the agents into teams and assign a team leader to each of them, so that the team leaders have as high support as possible from the groups they are assigned to lead. We model this scenario as a voting problem, where the goal is to partition a set of voters into a prescribed number of groups so that each group elects its leader, i.e., their leader is a unique winner in the corresponding election. We study the computational complexity of this problem (and several of its variants) for Approval elections.", 
    "authors": [
      {
        "name": "Piotr Faliszewski"
      }, 
      {
        "name": "Arkadii Slinko"
      }, 
      {
        "name": "Nimrod Talmon"
      }
    ], 
    "keywords": "elections, approval voting, combinatorial algorithms, parameterized complexity", 
    "title": "Voting-Based Team Formation", 
    "type": "paper"
  }
}