{
  "PODS104": {
    "title": "2-3 Cuckoo Filters for Faster Triangle Listing and Set Intersection", 
    "abstract": "We introduce new dynamic set intersection data structures,\nwhich we call 2-3 cuckoo filters and hash tables. These\nstructures differ from the standard cuckoo hash tables and\ncuckoo filters by choosing two out of three locations to store\neach item, instead of one out of two, ensuring that any\nitem in an intersection of two structures will have at least\none common location in both structures. We demonstrate\nthe utility of these structures by using them in improved\nalgorithms for listing triangles and answering set intersection\nqueries in internal or external memory. For a graph G of n\nvertices and m edges, our internal-memory triangle listing\nalgorithm runs in O(m(alpha(G) log w)/w + k) expected time,\nwhere alpha(G) is the arboricity of G, w is the number of bits in\na machine word, and k is the number of output triangles.\nOur external-memory algorithm uses O(sort(n alpha(G)) + sort(m(alpha(G) log w)/w) + sort(k)) expected number of I/Os.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "David Eppstein"
      }, 
      {
        "name": "Michael Goodrich"
      }, 
      {
        "name": "Michael Mitzenmacher"
      }, 
      {
        "name": "Manuel Torres"
      }
    ], 
    "type": "paper", 
    "id": "PODS104"
  }, 
  "SIGMOD567": {
    "title": "A Cost-based Optimizer for Gradient Descent Optimization", 
    "abstract": "Due to the increasing importance of machine learning (ML) in a large number of domains, there is an urgent need to free users from algorithm and system concerns. Ideally, one could replicate the success of relational database systems for ML applications, where an optimizer takes care of the ML algorithm selection. \nIn this paper, we go towards this direction and propose a cost-based optimizer for solving optimization problems using gradient descent algorithms. Our optimizer frees users from the burden of gradient descent algorithm selection and low-level implementation details. In particular, our optimizer uses a new abstraction that allows for easy parallelization of GD algorithms and further optimizations that lead to performance speedup. We implemented our optimizer in a prototype built on top of Spark and carry out an extensive evaluation using several real and synthetic datasets. The results show that our optimizer is able to always choose the best gradient descent algorithm, while our optimizations speeds-up our prototype by more than 2 orders of magnitude when comparing it with state-of-the-art ML systems on top of Spark.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Zoi Kaoudi"
      }, 
      {
        "name": "Jorge Arnulfo Quiane Ruiz"
      }, 
      {
        "name": "Saravanan Thirumuruganathan"
      }, 
      {
        "name": "Sanjay Chawla"
      }, 
      {
        "name": "Divy Agrawal"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD567"
  }, 
  "SIGMOD327": {
    "title": "A General-Purpose Counting Filter: Making Every Bit Count", 
    "abstract": "Approximate Membership Query (AMQ) data structures, such as the\n  Bloom filter, quotient filter, and cuckoo filter, have found\n  numerous applications in databases, storage systems, networks,\n  computational biology, and other domains.  However, many\n  applications must work around limitations in the capabilities or\n  performance of current AMQs, making these applications more complex\n  and less performant.  For example, many current AMQs cannot delete\n  or count the number of occurrences of each input item, take up large\n  amounts of space, are slow, cannot be resized or merged, or have\n  poor locality of reference and hence perform poorly when stored on\n  SSD or disk.\n\n  This paper proposes a new general purpose AMQ that is small and\n  fast, has good locality of reference, scales out of RAM to\n  SSD, and supports deletions, counting (even on skewed data sets),\n  resizing, merging, and highly concurrent access.  The paper reports\n  on the structure's performance on both manufactured and\n  applications-generated data sets.\n\n  Specifically, this paper introduces the counting quotient filter\n  (CQF), a data structure for approximate membership testing and for\n  counting the occurrences of items in a data set.  \n\n  In our experiments, the CQF performs in-memory inserts and queries\n  up to an order-of-magnitude faster than the original quotient\n  filter, is several times faster than a Bloom filter, and is\n  competitive with the recently proposed cuckoo filter, even though\n  none of these other data structures support counting.  Indeed, on an\n  SSD, the CQF outperforms the cuckoo filter by a factor of 2 because\n  the CQF has better cache locality.\n\n  The CQF achieves these performance gains by restructuring the\n  metadata bits of the quotient filter to obtain fast lookups at high\n  load factors (i.e., even when the data structure is almost full).\n  As a result, the CQF offers good lookup performance even up to a\n  load factor of 95%.  Counting is essentially free in the CQF in the\n  sense that the structure is comparable or more space efficient even\n  than non-counting data structures (e.g., Bloom, quotient, and cuckoo\n  filters).\n\n  The paper also shows  how to speed up CQF operations by using new x86\n  bit-manipulation instructions introduced in Intel's Haswell line of\n  processors.  The restructured metadata transforms many quotient\n  filter metadata operations into rank-and-select bit-vector\n  operations.  Thus, our efficient implementations of rank and select\n  may be useful for other rank-and-select-based data structures.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Prashant Pandey"
      }, 
      {
        "name": "Michael A. Bender"
      }, 
      {
        "name": "Rob Johnson"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD327"
  }, 
  "SIGMOD569": {
    "title": "A Memory Bandwidth-Efficient Hybrid Radix Sort on GPUs", 
    "abstract": "Sorting is at the core of many database operations, such as index creation, sort-merge joins and user-requested output sorting. As GPUs are emerging as a promising platform to accelerate various operations, sorting on GPUs becomes a viable endeavour. Over the past few years, several improvements have been proposed for sorting on GPUs, leading to the first radix sort implementations that achieve a sorting rate of over one billion 32-bit keys per second. Yet, state-of-the-art approaches are heavily memory bandwidth-bound, as they require substantially more memory transfers than their CPU-based counterparts. \nOur work proposes a novel approach that almost halves the amount of memory transfers and, therefore, considerably lifts the memory bandwidth limitation. Being able to sort two gigabytes of eight byte records in as little as 50 milliseconds, our approach achieves a 2.32-fold improvement over the state-of-the-art GPU-based radix sort for uniform distributions, sustaining a minimum speed-up of no less than a factor of 1.66 for skewed distributions. \nTo address inputs that either do not reside on the GPU or exceed the available device memory, we build on our efficient GPU sorting approach with a pipelined heterogeneous sorting algorithm that mitigates the overhead associated with PCIe data transfers. Comparing the end-to-end sorting performance to the state-of-the-art CPU-based radix sort running 16 threads, our heterogeneous approach achieves a 2.06-fold and a 1.53-fold improvement for sorting 64 GB key-value pairs with a skewed and a uniform distribution, respectively.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Elias Stehle"
      }, 
      {
        "name": "Hans-Arno Jacobsen"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD569"
  }, 
  "PODS47": {
    "title": "A Relational Framework for Classifier Engineering", 
    "abstract": "In the design of analytical procedures and machine-learning solutions,\na critical and time-consuming task is that of feature engineering, for\nwhich various recipes and tooling approaches have been developed. In\nthis framework paper, we embark on the establishment of database\nfoundations for feature engineering. We propose a formal framework for\nclassification, in the context of a relational database, where the\nultimate goal is to assist developers with the task of feature\nengineering by utilizing the database's modeling and understanding of\ndata and queries, and by deploying the well studied principles of\ndatabase management.  We demonstrate the usefulness of this framework\nby formally defining three key algorithmic challenges. The first\nchallenge is that of separability, which is the problem of determining\nthe existence of feature queries that agree with the training\nexamples. The second is that of evaluating the VC dimension of the\nmodel class with respect to a given sequence of feature queries.  The\nthird challenge is identifiability, which is the task of testing for\nthe property of independence among features that are represented as\nqueries on entities. We give preliminary results on these challenges\nfor the case where features are defined by means of conjunctive\nqueries, and in particular study the implication of various\ntraditional syntactic restrictions on the inherent computational\ncomplexity.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Benny Kimelfeld"
      }, 
      {
        "name": "Christopher Re"
      }
    ], 
    "type": "paper", 
    "id": "PODS47"
  }, 
  "SIGMOD384": {
    "title": "A Top-Down Approach to Achieving Performance Predictability in Database Systems", 
    "abstract": "While much of the research on transaction processing has focused on improving overall performance in terms of throughput and mean latency, surprisingly less attention has been given to performance predictability: how often individual transactions exhibit execution latency far from the mean. Performance predictability is increasingly important when transactions lie on the critical path of latency-sensitive applications, enterprise software, or interactive web services.\nIn this paper, we focus on understanding and mitigating the sources of performance unpredictability in today\u00cds transactional databases. First, we propose a profiling framework called VProfiler that, given the source code of a database system and programmer annotations indicating the start and end of a transaction, is able to identify the dominant sources of variance in transaction latency. Second, we conduct the first quantitative study of major sources of variance in MySQL and Postgres (two of the largest and most popular open-source products on the market), and VoltDB (a more modern database). Finally, based on our findings, we investigate alternative algorithms, implementations, and tuning strategies to reduce latency variance without compromising mean latency or throughput. Most notably, we propose a new lock scheduling algorithm, called VarianceAware Transaction Scheduling (VATS), and a lazy buffer pool replacement policy. In particular, our modified MySQL exhibits significantly lower variance and 99th percentile latencies by upto 5.6_ and 6.3_, respectively. Our proposal has been welcomed by the open-source community, and our VATS algorithm has already been adopted and made the default scheduling policy as of MySQL\u00cds 5.7.17 release (including MariaDB and Percona distributions).", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Jiamin Huang"
      }, 
      {
        "name": "Barzan Mozafari"
      }, 
      {
        "name": "Thomas Wenisch"
      }, 
      {
        "name": "Grant Schoenebeck"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD384"
  }, 
  "PODS14": {
    "title": "A Worst-Case Optimal Multi-Round Algorithm for Parallel Computation of Conjunctive Queries", 
    "abstract": "We study the optimal communication cost for computing a full conjunctive query over p distributed servers. Two prior results were known. First, for one-round algorithms over skew-free data the optimal communication cost per server is M/p^tau*, where M is the size of the largest input relation, and tau_ is the fractional vertex covering number of the query hypergraph. Second, for multi-round algorithms and unrestricted database instances, it was shown that any algorithm requires at least M/p^rho* communication cost per server, where rho_ is the fractional edge covering number of the query hypergraph; but no matching algorithm were known for this case (except for two restricted queries: chains and cycles).\nIn this paper we describe a multi-round algorithm that computes any query with load M/p^rho_ per server, in the case when all input relations are binary. Thus, we prove this to be the optimal load for all queries over binary input relations. Our algorithm represents a non-trivial extension of previous algorithms for chains and cycles, and exploits some unique properties of graphs, which no longer hold for hypergraphs.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Bas Ketsman"
      }, 
      {
        "name": "Dan Suciu"
      }
    ], 
    "type": "paper", 
    "id": "PODS14"
  }, 
  "SIGMOD528": {
    "title": "ACIDRain: Concurrency-Related Attacks on Database-Backed Web Applications", 
    "abstract": "In theory, database transactions protect application data from cor-\nruption and integrity violations. In practice, database transactions\nfrequently execute under weak isolation that exposes programs to\na range of concurrency anomalies, and programmers may fail to\ncorrectly employ transactions. While low transaction volumes mask\nmany potential concurrency-related errors under normal operation,\ndetermined adversaries can exploit them programmatically for fun\nand profit. In this paper, we formalize a new kind of attack on\ndatabse-backed applications called an ACIDRain attack, in which\nan adversary systematically exploits concurrency-related vulnerabil-\nities via programmatically accessible APIs. These attacks are not\ntheoretical: ACIDRain attacks have already occurred in a handful\nof applications in the wild, including one attack which bankrupted\na popular Bitcoin exchange. To proactively detect the potential for\nACIDRain attacks, we extend the theory of weak isolation to analyze\nlatent potential for non-serializable behavior under concurrent web\nAPI calls. We introduce a language-agnostic method for detecting\npotential isolation anomalies in web applications, called Abstract\nAnomaly Detection (2AD), that uses dynamic traces of database\naccesses to efficiently reason about the space of possible concurrent\ninterleavings. We apply a prototype 2AD analysis tool to 12 popular\nself-hosted eCommerce applications written in four languages and\nwith a total deploy base of over 2M websites. We identify and verify\n22 critical ACIDRain attacks that allow attackers to corrupt store\ninventory, over-spend gift cards, and steal inventory", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Todd Warszawski"
      }, 
      {
        "name": "Peter Bailis"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD528"
  }, 
  "SIGMOD213": {
    "title": "Accelerating Pattern Matching Queries in Hybrid CPU-FPGA Architectures", 
    "abstract": "Relational databases provide a wealth of functionality to a wide range of applications. Yet, there are tasks for which they are less than optimal, e.g., when the processing becomes more complex (e.g., matching regular expressions) or the data is less structured (e.g., text or long strings). \n\nTaking advantage of recently released hybrid multicore architectures, such as the Intel's Xeon+FPGA machine, where the FPGA has coherent access to the main memory through the QPI bus, we explore the benefits of specializing operators to hardware. For this work we focus on two commonly used SQL operators: LIKE, and REGEXP_LIKE, and provide a novel and efficient implementation of these operators in reconfigurable hardware. We integrate the hardware accelerator into MonetDB, a main-memory column store, and demonstrate a significant improvement in response time and throughput. Given the CPU-FPGA shared-memory architecture, our hardware operator can be integrated with negligible communication overhead into the database. As a result it can be used efficiently for any data size or query complexity. Our hardware user defined function (HUDF) can speed up complex pattern matching by an order of magnitude in comparison to the database running on a 10-core CPU. We consider the insights gained from integrating hardware based string operators into MonetDB to be useful for future designs combining hardware specialization and databases.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "David Sidler"
      }, 
      {
        "name": "Zsolt Istvan"
      }, 
      {
        "name": "Muhsen Owaida"
      }, 
      {
        "name": "Gustavo Alonso"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD213"
  }, 
  "SIGMOD609": {
    "title": "Access Path Selection in Main-Memory Optimized Data Systems: Should I Scan or Should I Probe?", 
    "abstract": "The advent of columnar data analytics engines fueled a series of optimizations on the scan operator. New designs include column-group storage, vectorized execution, shared scans, working directly over compressed data, and operating using SIMD and multi-core execution. Combined with large main memories and deeper cache hierarchies modern scans form a very efficient access path leading to the need to revisit the question of access path selection.\n\nIn this paper, we present a detailed model that captures the behavior of sequential scans and secondary index scans in modern main-memory-optimized analytical systems. We show both analytically and experimentally that while scans have indeed become useful in more cases than before, there is still a strong case for secondary index scans and access path selection, and we show how to perform access path selection (APS). In particular, contrary to the way traditional systems choose between scans and secondary indexes, we find that in addition to the query selectivity, the underlying hardware, and the system design, modern optimizers also need to take into account query concurrency. We show that access path selection should no longer rely on a fixed selectivity threshold as in existing systems; rather, there is a curve dividing the two access methods that depends on the number of concurrent queries and their total selectivity. \n\nWe discuss the implications of integrating access path selection in a modern analytical data system. We demonstrate, both theoretically and experimentally, that using the proposed model a system can quickly perform access path selection, outperforming solutions that rely on a single access path or traditional access path models. We outline a light-weight mechanism to integrate APS into main-memory analytical systems that does not interfere with low latency queries. We also use the APS model to explain how the division between sequential scan and secondary index scan has historically changed due to hardware and workload changes, which allows for future projections based on hardware advancements.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Michael Kester"
      }, 
      {
        "name": "Manos Athanassoulis"
      }, 
      {
        "name": "Stratos Idreos"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD609"
  }, 
  "SIGMOD141": {
    "title": "All-in-One: Graph Processing in RDBMSs Revisited", 
    "abstract": "To support various massive graphs for real applications over online social networks, RDF, Semantic Web, etc. many new graph algorithms are designed to query graphs for a specific problem, and many distributed graph processing systems are developed to support graph querying by programming. In this paper, we focus on RDBMS, which has been well studied over decades to manage large datasets and revisit the issue how RDBMS can support graph processing at the SQL level. Our work is motivated by the fact that there are many relations stored in RDBMS that are closely related to a graph in real applications and need to be used together to query the graph, and RDBMS is a system that can query data and manage data while data may be updated over time. To support graph processing, in this work, we propose 4 new relational algebra operations, MM-join, MV-join, anti-join, and union-by-update. Here, MM-join and MV-join are to join between two matrices, and to join between a matrix and a vector, respectively, followed by aggregation computing over groups, given a matrix/vector can be represented by a relation. Both deal with the semiring by which many graph algorithms can be supported. The anti-join is to remove nodes/edges in a graph when they are unnecessary for the following computing. The union-by-update is to address value updates to compute PageRank, for example. The 4 new relational algebra operations can be defined by the 6 basic relational algebra operations with group-by & aggregation. We revisit SQL recursive queries and show that the 4 operations with others are ensured to have a fixpoint, following the techniques studied in DATALOG, and enhance the recursive with clause in SQL\u00cd99. We conduct extensive performance studies to test 10 graph algorithms using 9 large real graphs in 3 major RDBMSs. We show that RDBMSs are capable of dealing with graph processing in reasonable time. The focus of this work is at SQL level. There is high potential to improve the efficiency by main-memory RDBMSs, efficient join processing in parallel, and new storage management.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Kangfei Zhao"
      }, 
      {
        "name": "Jeffrey Xu Yu"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD141"
  }, 
  "SIGMOD338": {
    "title": "An Experimental Study of Bitmap Compression vs. Inverted List Compression", 
    "abstract": "Bitmap compression has been studied extensively in the database area and many efficient compression schemes were proposed, e.g., BBC, WAH, EWAH, and Roaring. Inverted list compression is also a well-studied topic in the information retrieval community and many inverted list compression algorithms were developed as well, e.g., VB, PforDelta, GroupVB, Simple8b, and SIMDPforDelta. We observe that they essentially solve the same problem, i.e., how to store a collection of sorted integers with as few as possible bits and support query processing as fast as possible. Due to historical reasons, bitmap compression and inverted list compression were developed as two separated lines of research in the database area and information retrieval area. Thus, a natural question is: Which one is better between bitmap compression and inverted list compression?\n\nTo answer the question, we present the first comprehensive experimental study to compare a series of 9 bitmap compression methods  and 15 inverted list  compression methods. Out of which 3 inverted list  compression methods are proposed in this work. We compare these 24 algorithms on synthetic datasets with different distributions (uniform and zipf) as well as 8 real-life datasets in terms of the space overhead, decompression time, intersection time, and union time. Based on the results, we provide many lessons and guidelines for practitioners to decide which technique to adopt in future systems and also for researchers to develop new algorithms.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Jianguo Wang"
      }, 
      {
        "name": "Chunbin Lin"
      }, 
      {
        "name": "Yannis Papakonstantinou"
      }, 
      {
        "name": "Steven Swanson"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD338"
  }, 
  "SIGMOD42": {
    "title": "AnonSys: Analytic Monitoring for Internet of Things Data Streams", 
    "abstract": "An increasing proportion of data today is generated by automated processes, sensors, and devices---collectively, the Internet of Things (IoT). IoT applications' rising data volume, demands for time-sensitive analysis, and heterogeneity exacerbate the challenge of identifying and highlighting important trends in IoT deployments. In response, we present AnonSys, a data analytics engine that performs statistically-informed analytic monitoring of IoT data streams by identifying deviations within streams and generating potential explanations for each. AnonSys is the first analytics engine to combine streaming outlier detection and streaming explanation operators, allowing cross-layer optimizations that deliver order-of-magnitude speedups over existing, primarily non-streaming alternatives. As a result, AnonSys can deliver accurate results at speeds of up to 2M events per second per query on a single core. AnonSys has delivered meaningful analytic monitoring results in production at an IoT company monitoring hundreds of thousands of vehicles and at a web service with hundreds of millions of users.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Peter Bailis"
      }, 
      {
        "name": "Edward Gan"
      }, 
      {
        "name": "Samuel Madden"
      }, 
      {
        "name": "Deepak Narayanan"
      }, 
      {
        "name": "Kexin Rong"
      }, 
      {
        "name": "Sahaana Suri"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD42"
  }, 
  "PODS27": {
    "title": "Answering Conjunctive Queries under Updates", 
    "abstract": "We consider the task of enumerating and counting answers to k-ary conjunctive queries against relational databases that may be updated\nby inserting or deleting tuples. We exhibit a new notion of q-hierarchical conjunctive queries and show that these can be\nmaintained efficiently in the following sense. During a linear time preprocessing phase, we can build a data structure that enables\nconstant delay enumeration of the query results; and when the database is updated, we can update the data structure and restart\nthe enumeration phase within constant time. For the special case of self-join free conjunctive queries we obtain a dichotomy: if a query\nis not q-hierarchical, then query enumeration with sublinear delay and sublinear update time (and arbitrary preprocessing time) is\nimpossible.\n\nFor Boolean conjunctive queries we obtain a complete dichotomy: if the query's homomorphic core is q-hierarchical, then the query result can\nbe computed in linear time and maintained with constant   update time; otherwise the query result cannot be maintained with\nsublinear update time.\n\nRegarding the problem of counting the number of solutions of a query, we obtain the following dichotomy for \nk-ary join queries: if the query is q-hierarchical, then the size of the query result can be computed in linear time and maintained with\nconstant update time. Otherwise,  the size of the query result cannot be maintained with sublinear update time.\n\nAll our lower bounds rely on the OMv-conjecture, a conjecture on the hardness of online matrix-vector multiplication that has recently\nemerged in the field of fine-grained complexity to characterize the hardness of dynamic problems.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Christoph Berkholz"
      }, 
      {
        "name": "Jens Keppeler"
      }, 
      {
        "name": "Nicole Schweikardt"
      }
    ], 
    "type": "paper", 
    "id": "PODS27"
  }, 
  "SIGMOD488": {
    "title": "Automatic Database Management System Tuning Through Large-scale Machine Learning", 
    "abstract": "Database management system (DBMS) configuration tuning is an essential aspect of any data-intensive application effort. But this is historically a difficult task because DBMSs have hundreds of configuration \"knobs\" that control everything in the system, such as the amount of memory to use for caches and how often data is written to storage. The problem with these knobs is that they are not standardized (i.e., two DBMSs use a different name for the same knob), not independent (i.e., changing one knob can impact others), and not universal (i.e., what works for one application may be sub-optimal for another). Worse, information about the effects of the knobs typically comes only from (expensive) experience.\n\nTo overcome these challenges, we present an automated approach that leverages past experience and collects new information to tune DBMS configurations: we use a combination of supervised and unsupervised machine learning methods to (1) select the most impactful knobs, (2) map unseen database workloads to previous workloads from which we can transfer experience, and (3) recommend knob settings. We implemented our techniques in a new tool called TunerX and tested it on two DBMSs. Our evaluation shows that TunerX recommends configurations that are as good as or better than ones generated by existing tools or a human expert.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Dana Van Aken"
      }, 
      {
        "name": "Andrew Pavlo"
      }, 
      {
        "name": "Geoff Gordon"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD488"
  }, 
  "SIGMOD519": {
    "title": "BLOCKBENCH: A Framework for Analyzing Private Blockchains", 
    "abstract": "Blockchain technologies are taking the world by storm. Public\nblockchains, such as Bitcoin and Ethereum, enable secure\npeer-to-peer applications like cryptocurrency or smart contracts.\nTheir security and performance are well studied.\nThis paper concerns recent private blockchain systems designed\nwith stronger security (trust) assumption and performance\nrequirement. These systems target and aim to disrupt\napplications which have so far been implemented on top\nof database systems, for example banking, finance and trading\napplications. Multiple platforms for private blockchains\nare being actively developed and ne tuned. However, there\nis a clear lack of a systematic framework with which different\nsystems can be analyzed and compared against each other.\nSuch a framework can be used to assess blockchains' viability\nas another distributed data processing platform, while\nhelping developers to identify bottlenecks and accordingly\nimprove their platforms.\n\nIn this paper, we first describe Blockbench, the first\nevaluation framework for analyzing private blockchains. It\nserves as a fair means of comparison for different platforms\nand enables deeper understanding of different system design\nchoices. Any private blockchain can be integrated to\nBlockbench via simple APIs, then it is driven by data\nprocessing workloads based on both real and synthetic smart\ncontracts. Blockbench measures overall and component wise\nperformance in terms of throughput, latency, scalability\nand fault-tolerance. Next, we use Blockbench to\nconduct comprehensive evaluation of two popular private\nblockchains: Ethereum and Hyperledger Fabric. The results\ndemonstrate that these systems are still far from displacing\ncurrent database systems in traditional data processing\nworkloads. Furthermore, there are gaps in performance between\nthe two systems which are attributed to the differences\nin their consensus protocols.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Anh Dinh"
      }, 
      {
        "name": "Ji Wang"
      }, 
      {
        "name": "Gang Chen"
      }, 
      {
        "name": "Rui Liu"
      }, 
      {
        "name": "Beng Chin Ooi"
      }, 
      {
        "name": "Kian-Lee Tan"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD519"
  }, 
  "PODS49": {
    "title": "BPTree: an L2 heavy hitters algorithm using constant memory", 
    "abstract": "The task of finding heavy hitters is one of the best known and well studied problems in the area of data streams. In sub-polynomial space, the strongest guarantee available is the L2 guarantee, which requires finding all items that occur at least eps*||f||_2 times in the stream, where the ith coordinate of the\nvector f is the number of occurrences of i in the stream. The first algorithm to achieve the L2 guarantee was the CountSketch, which for constant eps requires O(log n) words of memory and O(log n) update time, and is known to be space-optimal if the stream allows for deletions. The recent work of Braverman, Chestnut, Ivkin and Woodruff (STOC 2016) gave an improved algorithm for insertion-only streams, using only O(log log n) words of memory. In this work, we give an algorithm BPTree for L2 heavy hitters in insertion-only streams that achieves O(1) words of memory and O(1) update time for constant eps, which is optimal. In addition, we describe an algorithm for tracking ||f||_2 at all times with O(1) memory and update time. Our analyses rely on bounding the expected supremum of a Bernoulli process involving Rademachers with limited independence, which we accomplish via a Dudley-like chaining argument that may have applications elsewhere.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Vladimir Braverman"
      }, 
      {
        "name": "Stephen Chestnut"
      }, 
      {
        "name": "Nikita Ivkin"
      }, 
      {
        "name": "Jelani Nelson"
      }, 
      {
        "name": "Zhengyu Wang"
      }, 
      {
        "name": "David Woodruff"
      }
    ], 
    "type": "paper", 
    "id": "PODS49"
  }, 
  "SIGMOD267": {
    "title": "BatchDB: Efficient Isolated Execution of Hybrid OLTP+OLAP Workloads", 
    "abstract": "In this paper we present BatchDB, an in-memory database engine designed for hybrid OLTP and OLAP workloads. BatchDB provides high performance, high level of data freshness, and unlike existing systems no load interaction between the transactional and analytical parts. Therefore, BatchDB enables applications which are driven by real time analysis of latest data, and are under tight SLAs for both the OLTP and OLAP workloads. \nBatchDB relies on primary-secondary replication, with replicas tailored to a particular purpose (OLTP, OLAP) and a number of optimizations for each type of workload (batch scheduling for analytical queries, light weight update extraction for transactions). The experimental results show that using a hybrid workload based on standard TPC-C and TPC-H benchmarks, BatchDB achieves competitive performance to specialized engines for the corresponding transactional and analytical workloads, while providing complete performance isolation and predictable runtime for hybrid workload mixes, otherwise unmet by existing solutions.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Darko Makreshanski"
      }, 
      {
        "name": "Jana Giceva"
      }, 
      {
        "name": "Claude Barthels"
      }, 
      {
        "name": "Gustavo Alonso"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD267"
  }, 
  "SIGMOD197": {
    "title": "BePI: Fast and Memory-Efficient Method for Billion-Scale Random Walk with Restart", 
    "abstract": "How can we measure similarity between nodes quickly and accurately on large graphs? Random walk with restart (RWR) provides a good measure, and has been used in various data mining applications including ranking, recommendation, link prediction and community detection. However, existing methods for computing RWR do not scale to large graphs containing billions of edges; iterative methods are slow in query time, and preprocessing methods require too much memory.\n\nIn this paper, we propose BePI, a fast, memory-efficient, and scalable method for computing RWR on billion-scale graphs. BePI exploits the best properties from both preprocessing methods and iterative methods. BePI uses a block elimination approach, which is a preprocessing method, to enable fast query time. Also, BePI uses a preconditioned iterative method to decrease memory requirement. The performance of BePI is further improved by decreasing non-zeros of the matrix for the iterative method. Through extensive experiments, we show that BePI processes 100 times larger graphs, and requires up to 80 times less memory space than other preprocessing methods. In the query phase, BePI computes RWR scores up to 7 times faster than existing methods.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Jinhong Jung"
      }, 
      {
        "name": "Namyong Park"
      }, 
      {
        "name": "Sael Lee"
      }, 
      {
        "name": "U Kang"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD197"
  }, 
  "PODS43": {
    "title": "Benchmarking the chase", 
    "abstract": "The chase is a family of algorithms proposed for use in a number of data\nmanagement tasks, such as data exchange, answering queries under dependencies,\nquery reformulation with constraints, and data cleaning. The chase is well\nestablished as a theoretical tool for understanding these tasks, and in\naddition a number of prototype implementations have appeared. While individual\nchase-based systems and particular optimizations of the chase have been\nexperimentally evaluated in the past, we provide the first comprehensive and\npublicly-available benchmark---test infrastructure and a set of test\nscenarios---for evaluating chase implementations across a wide range of\nassumptions about the constraints and the data. We apply our benchmarks to test\nchase implementations against one another for data exchange, both in terms of\ncorrectness and scalability. In addition, we extend our benchmark to support\ntesting query answering with dependencies. This allows us to not only compare\nchase implementations to one another, but also compare them with other\napproaches for query answering. Our evaluation provides us with a number of new\ninsights concerning the factors that impact the performance of chase\nimplementations.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Michael Benedikt"
      }, 
      {
        "name": "George Konstantinidis"
      }, 
      {
        "name": "Giansalvatore Mecca"
      }, 
      {
        "name": "Boris Motik"
      }, 
      {
        "name": "Paolo Papotti"
      }, 
      {
        "name": "Donatello Santoro"
      }, 
      {
        "name": "Efthymia Tsamoura"
      }
    ], 
    "type": "paper", 
    "id": "PODS43"
  }, 
  "SIGMOD455": {
    "title": "Beta Probabilistic Databases: A Scalable Approach to Belief Updating and Parameter Learning", 
    "abstract": "Tuple-independent probabilistic databases (TI-PDBs) handle uncertainty by annotating each tuple with a probability parameter; when the user submits a query, the database derives the marginal probabilities of each output-tuple, assuming input-tuples are statistically independent. While query processing in TI-PDBs has been studied extensively, limited research has been dedicated to the problems of updating or deriving the parameters from observations of queries\u00cd results. Addressing this problem is the main focus of this paper. We introduce Beta Probabilistic Databases (B-PDBs), a generalization of TI-PDBs designed to support both (i) belief updating and (ii) parameter learning in a principled and scalable way. The key idea of B-PDBs is to treat each parameter as a latent, Beta-distributed random variable. We show how this simple expedient enables both belief updating and parameter learning in a principled way, without imposing any burden on regular query processing. We use this model to provide the following key contributions: (i) we show how to scalably compute the posterior densities of the parameters given new evidence;(ii) we study the complexity of performing Bayesian belief updates, devising efficient algorithms for tractable classes of queries; (iii) we propose a soft-EM algorithm for computing maximum-likelihood estimates of the parameters; (iv) we show how to embed the proposed algorithms into a standard relational engine; (v) we support our conclusions with extensive experimental results.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Niccolo' Meneghetti"
      }, 
      {
        "name": "Oliver Kennedy"
      }, 
      {
        "name": "Wolfgang Gatterbauer"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD455"
  }, 
  "SIGMOD60": {
    "title": "Big Wide Table Layout Optimization based on Column Ordering and Duplication", 
    "abstract": "Modern data analytical tasks often witness very wide tables, from a few hundred columns to a few thousand. While it is commonly agreed that column stores are an appropriate data format for wide tables and analytical workloads, the physical order of columns has not been investigated. Column ordering plays a critical role in I/O performance, because in wide tables accessing the columns in a single horizontal partition may involve multiple disk seeks. An optimal column ordering will incur minimal cumulative disk seek costs for the set of queries applied to the data. In this paper, we aim to find such an optimal column layout to maximize I/O performance. Specifically, we study two problems for column stores on HDFS: column ordering and column duplication. Column ordering seeks an approximately optimal order of columns; column duplication complements column ordering in that some columns may be duplicated multiple times to reduce contention among the queries' diverse requirements on the column order. We consider an actual fine-grained cost model for column accesses and propose algorithms that take a query workload as input and output a column ordering strategy with or without storage redundancy that significantly improves the overall I/O performance. Experimental results over real-life data and production query workloads confirm the effectiveness of the proposed algorithms in diverse settings.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Haoqiong Bian"
      }, 
      {
        "name": "Ying Yan"
      }, 
      {
        "name": "Wenbo Tao"
      }, 
      {
        "name": "Liang Chen"
      }, 
      {
        "name": "Yueguo Chen"
      }, 
      {
        "name": "Xiaoyong Du"
      }, 
      {
        "name": "Thomas Moscibroda"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD60"
  }, 
  "SIGMOD502": {
    "title": "Bringing Modular Concurrency Control to the Next Level", 
    "abstract": "This paper presents Tebaldi, a distributed key-value store that explores new ways to harness the performance opportunity of combining different specialized concurrency control mechanisms (CCs) within the same database. Tebaldi partitions conflicts at a fine granularity and matches them to specialized CCs within a hierarchical framework that is modular, extensible, and able to support a wide variety of concurrency control techniques, from single-version to multiversion and from lock-based to timestamp-based. When running the TPC-C benchmark, Tebaldi yields more than 20_ the throughput of the basic two-phase locking protocol, and over 3.7_ the throughput of Callas, a state-of-the-art system that, like Tebaldi, aims to combine different CCs.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Chunzhi Su"
      }, 
      {
        "name": "Chao Xie"
      }, 
      {
        "name": "Natacha Crooks"
      }, 
      {
        "name": "Cong Ding"
      }, 
      {
        "name": "Lorenzo Alvisi"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD502"
  }, 
  "SIGMOD526": {
    "title": "CDB: A Crowd-Powered Database System", 
    "abstract": "Crowdsourcing database systems have been proposed to leverage crowd-powered operations to encapsulate the complexities of interacting with the crowd. Existing systems suffer from two major limitations. Firstly, in order to optimize a query, they often adopt the traditional tree model to select an optimized table-level join order. However, the tree model provides a coarse-grained optimization, which generates the same order for different joined tuples and limits the optimization potential that different joined tuples can be optimized by different orders. Secondly, they mainly focus on optimizing the monetary cost. In fact, there are three optimization goals (i.e., smaller monetary cost, lower latency, and higher quality) in crowdsourcing, and it calls for a system to enable multi-goal optimization.\n\nTo address these limitations, we develop a crowd-powered database system CDB that supports crowd-based query optimizations. CDB has the following fundamental differences compared with the existing systems. Firstly, CDB employs a graph-based query model that provides more fine-grained query optimization. Secondly, CDB adopts a unified framework to perform the multi-goal optimization based on the graph model. We have implemented our system and deployed it on AMT and CrowdFlower. We have also created a benchmark for evaluating crowd-powered databases. We have conducted both simulated and real experiments, and the experimental results demonstrate the performance superiority of CDB on cost, latency and quality.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Guoliang Li"
      }, 
      {
        "name": "Chengliang Chai"
      }, 
      {
        "name": "Xueping Weng"
      }, 
      {
        "name": "Ju Fan"
      }, 
      {
        "name": "Jian Li"
      }, 
      {
        "name": "Yudian Zheng"
      }, 
      {
        "name": "Yuanbing Li"
      }, 
      {
        "name": "Xiang Yu"
      }, 
      {
        "name": "Xiaohang Zhang"
      }, 
      {
        "name": "Haitao Yuan"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD526"
  }, 
  "SIGMOD383": {
    "title": "Cicada: Dependably Fast Multi-Core In-Memory Transactions", 
    "abstract": "Multi-core in-memory databases promise high-speed online transaction processing.  However, the performance of individual designs suffers when the workload characteristics miss their small sweet spot of a desired contention level, read-write ratio, record size, processing rate, and so forth.\n\nCicada is a single-node multi-core in-memory transactional database with serializability.  To provide high performance under diverse workloads, Cicada reduces overhead and contention at several levels of the system by leveraging optimistic and multi-version concurrency control schemes and multiple loosely synchronized clocks while mitigating their drawbacks.  On the TPC-C and YCSB benchmarks, Cicada outperforms Silo, TicToc, FOEDUS, MOCC, two-phase locking, Hekaton, and ERMIA in most scenarios, achieving up to 3X higher throughput than the next fastest design.  It handles up to 2.07 M TPC-C transactions per second and 56.5 M YCSB transactions per second, and scans up to 356 M records per second on a single 28-core machine.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Hyeontaek Lim"
      }, 
      {
        "name": "Michael Kaminsky"
      }, 
      {
        "name": "David G. Andersen"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD383"
  }, 
  "PODS10": {
    "title": "Circuit Treewidth, Sentential Decision, and Query Compilation", 
    "abstract": "The evaluation of a query over a probabilistic database consists of computing the probability of a suitable Boolean function, the lineage of the query over the database. The method of query compilation approaches the task in two stages: first, by implementing (compiling) the query lineage in a circuit form where probability computation is tractable; and second, by computing the desired probability over the compiled circuit. A basic theoretical quest in query compilation is to characterize classes of queries whose lineages admit compact representations over increasingly succinct, tractable circuit classes. \n\nFostering previous work by Jha and Suciu and Petke and Razgon, we focus on queries whose lineages admit circuit implementations of small treewidth, and investigate their compilability within tame classes of decision diagrams. In perfect analogy with the characterization of bounded circuit pathwidth by bounded OBDD width by Jha and Suciu, we show that a class of Boolean functions has bounded circuit treewidth if and only if it has bounded SDD width. Sentential decision diagrams (SDDs) are central in knowledge compilation, being essentially as tractable as OBDDs but exponentially more succinct. By incorporating constant width (linear size) SDDs and polynomial size SDDs in the picture, we refine the panorama of query compilation for unions of conjunctive queries with and without inequalities.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Simone Bova"
      }, 
      {
        "name": "Stefan Szeider"
      }
    ], 
    "type": "paper", 
    "id": "PODS10"
  }, 
  "SIGMOD577": {
    "title": "Coarsening Massive Influence Networks for Scalable Diffusion Analysis", 
    "abstract": "Fueled by the increasing popularity of online social networks,\nsocial influence analysis has attracted a great deal of research attention in the past decade.\nThe diffusion process is often modeled using influence graphs, and\nthere has been a line of research that involves algorithmic problems in influence graphs.\nHowever, the vast size of today's real-world networks\nraises a serious issue with regard to computational efficiency.\n\nIn this paper,\nwe propose a new algorithm for reducing influence graphs.\nGiven an input influence graph,\nthe proposed algorithm produces a vertex-weighted influence graph,\nwhich is compact and approximates the diffusion properties of the input graph.\nThe central strategy of influence graph reduction is coarsening,\nwhich has the potential to greatly reduce the number of edges by\nmerging a vertex set into a single weighted vertex.\nWe provide two implementations;\na speed-oriented implementation which runs in linear time with linear space and\na scalability-oriented implementation which runs in practically linear time with sublinear space.\nFurther, we present general frameworks using our compact graphs\nthat accelerate existing algorithms for\ninfluence maximization and influence estimation problems,\nwhich are motivated by practical applications, such as viral marketing.\nUsing these frameworks,\nwe can quickly obtain solutions that have accuracy guarantees under a reasonable assumption.\nExperiments with real-world networks\ndemonstrate that\nthe proposed algorithm can scale to billion-edge graphs and reduce the graph size to up to 4%.\nIn addition,\nour influence maximization framework achieves\nfour times speed-up of a state-of-the-art D-SSA algorithm, and\nour influence estimation framework cuts down\nthe computation time of a simulation-based method to 3.5%.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Naoto Ohsaka"
      }, 
      {
        "name": "Tomohiro Sonobe"
      }, 
      {
        "name": "Sumio Fujita"
      }, 
      {
        "name": "Ken-Ichi Kawarabayashi"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD577"
  }, 
  "PODS131": {
    "title": "Combined Tractability of Probabilistic Evaluation for Conjunctive Queries on Binary Signatures", 
    "abstract": "Query evaluation over probabilistic databases is known to be intractable in many\ncases, even in data complexity, i.e., when the query is fixed. Although some\nrestrictions of the queries \\cite{dalvi2012dichotomy} and instances\n\\cite{amarilli2015provenance} have been proposed to lower the complexity, these\nknown tractable cases usually do not apply to combined complexity, i.e., when\nthe query is not fixed. This leaves open the question of which query and\ninstance languages ensure the tractability of probabilistic query evaluation in\ncombined complexity.\n\nThis paper proposes the first general study of the combined complexity of\nconjunctive query evaluation on probabilistic instances over binary signatures,\nwhich we can alternatively phrase as a probabilistic version of the graph\nhomomorphism problem, or of a constraint satisfaction problem (CSP) variant. We\nstudy the complexity of this problem depending on whether instances and queries\ncan use features such as edge labels, disconnectedness, branching, and edges in\nboth directions. We show that the complexity landscape is surprisingly rich,\nusing a variety of technical tools: automata-based compilation to d-DNNF\nlineages as in~\\cite{amarilli2015provenance}, $\\beta$-acyclic lineages\nusing~\\cite{brault2015understanding}, the $\\underline{X}$-property for tractable\nCSP from~\\cite{gutjahr1992polynomial}, graded DAGs~\\cite{odagiri2014greatest}\nand various coding techniques for hardness proofs.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Antoine Amarilli"
      }, 
      {
        "name": "Mikael Monet"
      }, 
      {
        "name": "Pierre Senellart"
      }
    ], 
    "type": "paper", 
    "id": "PODS131"
  }, 
  "SIGMOD165": {
    "title": "Complete Event Trend Detection in High-Rate Event Streams", 
    "abstract": "Event processing applications from financial fraud detection to health care analytics continuously execute event queries with Kleene+ patterns to extract event sequences of finite yet unbounded length, called Complete Event Trends (CETs). State-of-the-art techniques do not satisfy the low-latency requirements of CET detection in high-rate event streams. To overcome this limitation, we propose a compact data structure, called CET graph, that effectively encodes relevant events and their CET relationships. Based on this graph structure, we define the spectrum of CET detection algorithms from CPU-optimal to memory-optimal. Our CET solution finds the middle ground between these two extremes by partitioning the CET graph into time-centric graphlets. Our CET executor caches partial CETs per graphlet --  enabling effective reuse of these intermediate results both within one window and across multiple overlapping sliding windows. Our analysis reveals cost monotonicity properties of the search space of candidate partitioning plans. We thus design the CET optimizer that leverages these properties to prune significant portions of the search to produce a partitioning plan with minimal CPU costs yet within the given memory limit. Our experimental study demonstrates that our CET detection solution achieves up to 42--fold speed-up even under rigid memory constraints compared to the state-of-the-art techniques in diverse scenarios.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Olga Poppe"
      }, 
      {
        "name": "Chuan Lei"
      }, 
      {
        "name": "Salah Ahmed"
      }, 
      {
        "name": "Elke Rundensteiner"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD165"
  }, 
  "SIGMOD127": {
    "title": "Computing A Near-Maximum Independent Set in Linear Time by Reducing-Peeling", 
    "abstract": "This paper studies the problem of efficiently computing a maximum independent set from a large graph, a fundamental problem in graph analysis. Due to the hardness results of computing an exact maximum independent set or an approximate maximum independent set with accuracy guarantee, the existing techniques resort to heuristic algorithms for approximately computing a maximum independent set with good performance in practice but no accuracy guarantee theoretically. Observing that the existing techniques have various limits, in this paper, we aim to develop efficient (with linear or near-linear time complexity) algorithms that can generate a high-quality (large-size) independent set from a graph in practice. In particular, firstly we develop a Reducing-Peeling framework which iteratively reduces the graph size by applying reduction rules on vertices with very low degrees (Reducing) and temporarily removing the vertex with the highest degree (Peeling) if the reduction rules cannot be applied. Secondly, based on our framework we design two baseline algorithms, BDOne and BDTwo, by utilizing the existing reduction rules for handling degree-one and degree-two vertices, respectively. Both algorithms can generate higher-quality (larger-size) independent sets than the existing techniques. Thirdly, we propose a linear-time algorithm, LinearTime, and a near-linear time algorithm, NearLinear, by designing new reduction rules and developing techniques for efficiently and incrementally applying reduction rules. In practice, LinearTime takes similar time and space to BDOne but computes a higher quality independent set, similar in size to that of an independent set generated by BDTwo. Moreover, in practice NearLinear has a good chance to generate a maximum independent set and it often generates near-maximum independent sets. Fourthly, we extend our techniques to accelerate the existing iterated local search algorithms. Extensive empirical studies show that all our algorithms output much larger independent sets than the existing linear-time algorithms while having a similar running time, as well as achieve significant speedup against the existing iterated local search algorithms.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Lijun Chang"
      }, 
      {
        "name": "Wei Li"
      }, 
      {
        "name": "Wenjie Zhang"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD127"
  }, 
  "SIGMOD497": {
    "title": "Concerto: A High Concurrency Key-value Store with Integrity", 
    "abstract": "Verifying integrity of outsourced data is a classic, well-studied problem. However current techniques have fundamental limitations handling update heavy workloads and incur a performance overhead of multiple orders-of-magnitude. In this paper, we investigate the potential advantages of deferred and batched verification rather than per-operation verification used in prior work. We present Concerto, a comprehensive key-value store designed around this idea. Using Concerto, we argue that our deferred verification formulation preserves most of the utility of online verification; more importantly, this formulation enables enormous concurrency that translates to orders-of-magnitude performance improvements. On standard benchmarks, the performance of Concerto is within a factor of 2 of state-of-the-art key-value stores without integrity.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Arvind Arasu"
      }, 
      {
        "name": "Ken Eguro"
      }, 
      {
        "name": "Raghav Kaushik"
      }, 
      {
        "name": "Donald Kossmann"
      }, 
      {
        "name": "Pingfan Meng"
      }, 
      {
        "name": "Vineet Pandey"
      }, 
      {
        "name": "Ravi Ramamurthy"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD497"
  }, 
  "SIGMOD408": {
    "title": "Controlling False Discoveries During Interactive Data Exploration", 
    "abstract": "Recent tools for interactive data exploration significantly increase the chance that users make false discoveries. The crux is that these tools implicitly allow the user to test a large body of different hypotheses with just a few clicks thus incurring in the issue commonly known in statistics as the \u00f1multiple hypothesis testing error\u00ee. In this paper, we propose solutions to integrate multiple hypothesis testing control into interactive data exploration tools. A key insight is that existing methods for controlling the false discovery rate (such as FDR) are not directly applicable for interactive data exploration. We therefore discuss a set of new control procedures that are better suited and integrated them in our system called AWARE. By means of extensive experiments using both real-world and synthetic data sets we demonstrate how AWA R E can help experts and novice users alike to efficiently control false discoveries.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Zheguang Zhao"
      }, 
      {
        "name": "Lorenzo De Stefani"
      }, 
      {
        "name": "Emanuel Zgraggen"
      }, 
      {
        "name": "Carsten Binnig"
      }, 
      {
        "name": "Eli Upfal"
      }, 
      {
        "name": "Tim Kraska"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD408"
  }, 
  "PODS71": {
    "title": "Counting and Enumerating (Preferred) Database Repairs", 
    "abstract": "In the traditional sense, a subset repair of an inconsistent database refers to a consistent subset of facts (tuples) that is maximal under set containment. Preferences between pairs of facts allow to distinguish a set of preferred repairs based on relative reliability (source credibility, extraction quality, recency, etc.) of data items. Previous studies explored the problem of categoricity, where one aims to determine whether preferences suffice to repair the database unambiguously, or in other words, whether there is precisely one preferred repair. In this paper we study the ability to quantify ambiguity, by investigating two classes of problems. The first is that of counting the number of subset repairs, both preferred (under various common semantics) and traditional. We establish dichotomies in data complexity for the entire space of (sets of) functional dependencies. The second class of problems is that of enumerating the preferred repairs. We devise enumeration algorithms with efficiency guarantees on the delay between generated repairs, even for constraints represented as general conflict graphs or hypergraphs.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Ester Livshits"
      }, 
      {
        "name": "Benny Kimelfeld"
      }
    ], 
    "type": "paper", 
    "id": "PODS71"
  }, 
  "SIGMOD663": {
    "title": "CrowdDQS: Dynamic Question Selection in Crowdsourcing Systems", 
    "abstract": "In this paper, we present CrowdDQS, a system that uses the most recent set of crowdsourced voting evidence to dynamically issue questions to workers on Amazon Mechanical Turk (AMT). CrowdDQS posts all questions to AMT in a single batch, but delays the decision of the exact question to issue a worker until the last moment, concentrating votes on uncertain questions to maximize accuracy. Unlike previous works, CrowdDQS also (1) optionally can also decide when it is more beneficial to issue gold standard questions with known answers than to solicit new votes (both can help us estimate worker accuracy, but gold standard questions provide a less noisy estimate of worker accuracy at the expense of not obtaining new votes), (2) estimates worker accuracies in real-time even with limited evidence (with or without gold standard questions), and (3) infers the distribution of worker skill levels to actively block poor workers. We deploy our system live on AMT to over 1000 crowdworkers, and find that using our techniques, CrowdDQS can accurately answer questions using up to 6x fewer votes than standard approaches. We also find there are many non-obvious practical challenges involved in deploying such a system seamlessly to crowdworkers, and discuss techniques to overcome these challenges.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Asif Khan"
      }, 
      {
        "name": "Hector Garcia-Molina"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD663"
  }, 
  "SIGMOD210": {
    "title": "Crowdsourced Top-k Queries by Confidence-Aware Pairwise Judgments", 
    "abstract": "Crowdsourced query processing is an emerging processing technique that tackles computationally challenging problems by human intelligence. The basic idea is to decompose a computationally challenging problem into a set of human friendly microtasks (e.g., pairwise comparisons) that are distributed to and answered by the crowd. The solution of the problem is then computed (e.g., by aggregation) based on the crowdsourced answers to the microtasks. In this work, we attempt to revisit the crowdsourced processing of the top-k queries, aiming at (1) securing the quality of crowdsourced comparisons by a certain condence level and (2) minimizing the total monetary cost. To secure the quality of each paired comparison, we employ two statistical tools, Student's t-distribution estimation and Stein's estimation, to estimate the condence interval of the underlying mean value, which is then used to draw a conclusion to the comparison. Based on the pairwise comparison process, we attempt to minimize the monetary cost of the top-k processing within a Select-Partition-Rank framework. Our experiments, conducted on four real datasets, demonstrate that our stochastic method outperforms other existing top-k processing techniques by a visible dierence.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Ngai Meng Kou"
      }, 
      {
        "name": "Yan Li"
      }, 
      {
        "name": "Hao Wang"
      }, 
      {
        "name": "Leong Hou U"
      }, 
      {
        "name": "Zhiguo Gong"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD210"
  }, 
  "SIGMOD180": {
    "title": "Cryptanalysis of Comparable Encryption in SIGMOD\u00cd16", 
    "abstract": "Comparable encryption proposed by Furukawa (ESORICS 2013, CANS 2014) is a variant of order-preserving encryption (OPE) and order-revealing encryption (ORE); we cannot compare a ciphertext of v and another ciphertext of v', but we can compare a ciphertext of v and a token of b and compare a token of b and another token of b'. Comparable encryption allows us to implement range and point queries while keeping the order of v's as secret as possible. \n\nRecently, Karras, Malhotra, Bhatt, Nikitin, Antyukhov, and Idreos independently re-define comparable encryption and propose two schemes, a basic one and an ``ambiguous'' one, based on linear algebra~(SIGMOD 2016). The basic scheme is just comparable encryption. To hide the order revealed by tokens, they also proposed an ambiguous scheme where each ciphertext has two interpretations v and v_{dummy}. In the context of an indexed database, this means that every encryption has two places in the database corresponding to the two interpretations, masking the correct placement in the database unless the dummy value is detectable.\n\nThis paper cryptanalyzes their comparable encryption schemes by using simple linear algebra. Our attacks are summarized as follows:\n\nAttacks against the basic scheme: \n* A ciphertext-only attack using two tokens orders the ciphertexts correctly.\n* A known-plaintext attack using two tokens and two plaintexts reveals exact value of v. \n\nAttacks against the ambiguous scheme:\n* A ciphertext-only attack using two tokens orders the ciphertexts with a constant probability. \n* A known-plaintext attack using two tokens and three plaintexts reveals exact value of v. \n\nWe additionally give variants of the attacks exploiting the parameters of the distributions of v.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Caleb Horst"
      }, 
      {
        "name": "Ryo Kikuchi"
      }, 
      {
        "name": "Keita Xagawa"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD180"
  }, 
  "SIGMOD41": {
    "title": "DAG Reduction: Fast Answering Reachability Queries", 
    "abstract": "Answering reachability queries is one of the fundamental graph operations, which asks whether a node $v$ is reachable from a node $u$ over a directed graph $\\bigG$, and has been extensively studied. The existing approaches build indexes on a directed acyclic graph (\\DAG) $\\G$ which is constructed by coalescing each strongly connected component of $\\bigG$ into a node, and answer reachability queries using $\\G$. However, $\\G$ can still be large to be processed efficiently.\n\nIn this paper, we study \\DAG reduction, which reduces the size of $\\G$ by computing transitive reduction (TR) followed by computing equivalence reduction (ER). For TR, we firstly reduce the size of $G$ into a small graph $G'$ by removing redundant edges starting from a set of nodes identified based on LPM tree, and then remove all redundant edges of $G'$ with new optimization techniques. The result of \\tr is a graph $\\G^t$. For ER, we reduce the computing cost by showing that the equivalent relation between two nodes in $G^t$ is the same as that in $G$. The result of ER is a graph $G^{\\varepsilon}$. Our DAG reduction approach significantly improves the  cost of time and space, and is able to deal with large graphs. We confirm the  efficiency of our approaches by extensive experimental studies for both DAG  reduction (TR and ER) and reachability query processing using 20 real datasets.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Junfeng Zhou"
      }, 
      {
        "name": "Shijie Zhou"
      }, 
      {
        "name": "Jeffrey Xu Yu"
      }, 
      {
        "name": "Hao Wei"
      }, 
      {
        "name": "Ziyang Chen"
      }, 
      {
        "name": "Xian Tang"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD41"
  }, 
  "SIGMOD693": {
    "title": "DEX: Query Execution in a Delta-based Storage System", 
    "abstract": "The increasing reliance on robust data-driven decision-making has made it necessary for data management systems to manage many thousands to millions of versions of datasets, acquired or constructed at various stages of analysis pipelines over time. Delta encoding is an effective and widely-used solution to compactly store a large number of datasets, that simultaneously exploits redundancies across them and keeps the average retrieval cost of reconstructing any dataset low. However, supporting any kind of rich retrieval or querying functionality, beyond single dataset checkout, is challenging in such storage engines. In this paper, we initiate a systematic study of this problem, and present DEX, a novel stand-alone delta-oriented execution engine, whose goal is to take advantage of the already computed deltas between the datasets for efficient query processing. In this work, we study how to execute checkout, intersection, union and t-threshold queries over record-based files; we show that processing of even these basic queries leads to many new and unexplored challenges and trade-offs. Starting from a query plan that confines query execution to a small set of deltas, we introduce new transformation rules based on the algebraic properties of the deltas, that allow us to explore the search space of alternative plans. For the case of checkout, we present a dynamic programming algorithm to efficiently select the optimal query plan under our cost model, while we design efficient heuristics to select effective plans that vastly outperform the base checkout-then-query approach. A key characteristic of our query execution methods is that the computational cost is primarily dependent on the size and the number of deltas in the expression (typically small), and not the input dataset versions (which can be very large). We have implemented DEX prototype on top of git, a widely used source control system. We present an extensive experimental evaluation on synthetic data with diverse characteristics, that shows that our methods perform exceedingly well compared to the baseline.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Amit Chavan"
      }, 
      {
        "name": "Amol Deshpande"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD693"
  }, 
  "SIGMOD623": {
    "title": "Data Canopy: Accelerating Statistical Analysis", 
    "abstract": "During exploratory statistical analysis, a core procedure in data science pipelines, data scientists repeatedly calculate statistics on the data set to infer knowledge. Modern data systems, software libraries, and domain-specific tools provide support to calculate statistics but lack a cohesive framework for storing, organizing, and reusing them. This creates a significant problem for exploratory statistical analysis as data grows: Despite existing overlap in exploratory workloads (which are repetitive in nature), different statistics are always computed from scratch. This leads to repeated data movement and computation, hindering interactive data exploration.\n\nWe address this challenge in Data Canopy, where descriptive and dependence statistics are synthesized from a library of basic aggregates. These basic aggregates are stored within an in-memory data structure, and are reused across range granularities and statistic types. What this means for exploratory statistical analysis is that repeated requests to compute different statistics do not trigger a full pass over the data. We discuss in detail the basic design elements in Data Canopy, which address multiple challenges: (1) How do we decompose statistics into basic aggregates for maximal reuse? (2) How do we represent, store, and access these basic aggregates? (3) Under different scenarios, which basic aggregates do we maintain?\n\nUsing both synthetic and real world data, we demonstrate experimentally that even under uniform workload Data Canopy results in an average speed-up of up to 10.2_ after just 100 exploratory queries when compared with other state-of-the-art systems currently used for exploratory statistical analysis.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Abdul Wasay"
      }, 
      {
        "name": "Stratos Idreos"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD623"
  }, 
  "SIGMOD378": {
    "title": "Database Learning: Toward a Database that Becomes Smarter Every Time", 
    "abstract": "In today\u00cds databases, previous query answers rarely benefit answering future queries. For the first time, to the best of our knowledge, we show we can change this paradigm in an approximate query processing (AQP) context. We make the following observation: the answer to each query reveals some degree of knowledge about the answer to another query because their answers stem from the same underlying distribution that has produced the entire dataset. Exploiting and refining this knowledge should allow us to answer queries more analytically, rather than by reading enormous amounts of raw data. Also, processing more queries should continuously enhance our knowledge of the underlying distribution, leading to faster processing of future queries.\n\nWe call this novel idea\u0084learning from past query answers---Database Learning. We exploit the principle of maximum entropy to produce answers, which are in expectation guaranteed to be more accurate than existing sample-based approximations. Empowered by this idea, we build a query engine on top of Spark SQL, called Verdict. We conduct extensive experiments on real-world query traces from a large customer of a major database vendor. Our results demonstrate that database learning supports 73.7% of these queries, speeding them up by up to 23.0_ for the same accuracy level compared to existing AQP systems.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Yongjoo Park"
      }, 
      {
        "name": "Ahmad Shahab Tajik"
      }, 
      {
        "name": "Michael Cafarella"
      }, 
      {
        "name": "Barzan Mozafari"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD378"
  }, 
  "SIGMOD29": {
    "title": "Debunking the Myths of Influence Maximization: An In-Depth Benchmarking Study", 
    "abstract": "Influence maximization (IM) on social networks is one of the most active areas of research in computer science. While the various IM techniques proposed over the last decade has definitely enriched the field, unfortunately, experimental reports on existing techniques fall short in validity and integrity since many comparisons are not based on a common platform or merely discussed in theory. In this paper, we perform an in-depth benchmarking study of IM techniques on social networks. Specifically, we design a benchmarking platform, which enables us to evaluate and compare the existing techniques systematically and thoroughly under identical experimental conditions. Our benchmarking results analyze and diagnose the inherent deficiencies of the existing approaches and surface the open challenges in IM even after a decade of research. More fundamentally, we unearth and debunk a series of incorrect claims made by highly cited papers in the field of IM. Overall, this study establishes that there is no single state-of-the-art technique in IM. At best, a technique is the state of the art on only a specific aspect.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Akhil Arora"
      }, 
      {
        "name": "Sainyam Galhotra"
      }, 
      {
        "name": "Sayan Ranu"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD29"
  }, 
  "PODS101": {
    "title": "Dependencies for Graphs", 
    "abstract": "This paper proposes a class of dependencies for graphs, referred\nto as graph entity dependencies (GEDs). A GED is defined as a \ncombination of a graph pattern and an attribute dependency.\nIn a uniform format, GEDs can express graph functional dependencies \nwith constant literals to catch inconsistencies, and keys carrying\nid literals to identify entities (vertices) in a graph.\n\nWe revise the chase for GEDs and prove its Church-Rosser property.\nWe provide characterizations for GED satisfiability and implication.\nWe establish the complexity of these problems and the validation \nproblem, in the presence and absence of id literals and constant \nliterals. We also develop a sound and complete finite axiom system \nfor finite implication of GEDs. In addition, we extend GEDs with\nbuilt-in predicates or disjunctions, to strike a balance between \nthe expressive power and complexity. We settle the complexity of \nthe satisfiability, implication and validation problems for these \nextensions.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Wenfei Fan"
      }, 
      {
        "name": "Ping Lu"
      }
    ], 
    "type": "paper", 
    "id": "PODS101"
  }, 
  "SIGMOD572": {
    "title": "Determining the Impact Regions of Competing Options in Preference Space", 
    "abstract": "In rank-aware processing, user preferences are typically represented by a numeric weight per data attribute, collectively forming a weight vector. The score of an option (data record) is defined as the weighted sum of its individual attributes. The highest scoring options across a set of alternatives (dataset) are shortlisted for the user as the recommended ones. In that setting, the user input is a vector (equivalently, a point) in a d-dimensional preference space, where d is the number of data attributes. In this paper, we study the problem of determining in which regions of the preference space the weight vector should lie so that a given option (focal record) is among the top-k score-wise. In effect, these regions capture all possible user profiles for which the focal record is highly preferable, and are therefore essential in market impact analysis, potential customer identification, profile-based marketing, targeted advertising, etc. We refer to our problem as -Shortlist Preference Region identification (kSPR), and exploit its computational geometric nature to develop a framework for its efficient (and exact) processing. Using real and synthetic benchmarks, we show that our most optimized algorithm outperforms by three orders of magnitude a competitor we constructed from existing methods for different problems.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Bo Tang"
      }, 
      {
        "name": "Kyriakos Mouratidis"
      }, 
      {
        "name": "Man Lung Yiu"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD572"
  }, 
  "PODS74": {
    "title": "Dichotomies in Ontology-Mediated Querying with the Guarded Fragment", 
    "abstract": "We study the complexity of ontology-mediated querying when ontologies are formulated in the guarded fragment of first-order logic (GF). Our general aim is to classify the data complexity on the level of ontologies where query evaluation w.r.t. ontology T is considered to be in PTime if all (unions of conjunctive) queries\ncan be evaluated in PTime w.r.t. T and coNP-hard if at least one query is coNP-hard w.r.t. T. We identify several large and relevant fragments of GF that enjoy a dichotomy between PTime and coNP, some of them additionally admitting a form of\ncounting. In fact, almost all ontologies in the Bioportal repository fall into these fragments or can easily be rewritten to do so. We then extend Ladner's Theorem on the existence of NP-intermediate problems to uniform membership and use this result\nto show that for other fragments there is provably no such dichotomy. Again for other fragments (such as full GF), establishing a dichotomy would imply the Feder-Vardi conjecture on the complexity of constraint satisfaction problems. We also link\nthese results to Datalog-rewritability and study the decidability of whether a given ontology enjoys PTime query evaluation, presenting both positive and negative\nresults.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Andr_ Hernich"
      }, 
      {
        "name": "Carsten Lutz"
      }, 
      {
        "name": "Fabio Papacchini"
      }, 
      {
        "name": "Frank Wolter"
      }
    ], 
    "type": "paper", 
    "id": "PODS74"
  }, 
  "SIGMOD604": {
    "title": "Differentially Private Stochastic Gradient Descent for in-RDBMS Analytics", 
    "abstract": "This paper studies differential privacy for stochastic gradient descent (SGD) in an in-RDBMS system. While significant progress has been made separately on in-RDBMS SGD and private SGD, none of the major in-RDBMS machine learning frameworks have incorporated differentially private SGD. There are two inter-related issues for this disconnect between research and practice: (1) low model accuracy due to added noise to guarantee privacy, and (2) high development and runtime overhead of the private algorithms. This paper takes a first step to remedy this disconnect and proposes a private SGD algorithm to address both issues in an integrated manner. In contrast to the white-box approach adopted by previous algorithms, we revisit and use the classical technique of output perturbation. While using output perturbation trivially addresses (2), it gives rise to challenges in addressing (1). We address this challenge by providing a novel analysis of the $L_2$-sensitivity of SGD, which allows, under the same privacy guarantees, better convergence of SGD when only a constant number of passes can be made over the data. We then integrate this algorithm, along with the state-of-the-art differentially private SGD, into Bismarck, an in-RDBMS analytics system. Extensive experiments demonstrate that our algorithm can be easily integrated, incurs virtually no overhead, scales well, and most importantly, for a number of datasets yields substantially better (up to 4X) test accuracy than the state-of-the-art algorithms.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Xi Wu"
      }, 
      {
        "name": "Fengan Li"
      }, 
      {
        "name": "Arun Kumar"
      }, 
      {
        "name": "Kamalika Chaudhuri"
      }, 
      {
        "name": "Somesh Jha"
      }, 
      {
        "name": "Jeffrey Naughton"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD604"
  }, 
  "SIGMOD201": {
    "title": "Discovering Your Selling Points: Personalized Social Influential Tag Exploration", 
    "abstract": "Social influence has attracted significant attention owing to the prevalence of social networks (SNs). In this paper, we study a new social influence problem, called personalized social influential tags exploration (PITEX), to help any user in the SN explore how she influences the network. Given a target user, it finds a size-k tag set that maximizes this user's social influence. We prove the problem is NP-hard to be approximated within any constant ratio. To solve it, we introduce a sampling-based framework, which has an approximation ratio of $\\frac{1-\\epsilon}{1+\\epsilon}$ with high probabilistic guarantee. To speedup the computation, we devise more efficient sampling techniques and propose best-effort exploration to quickly prune tag sets with small influence. To further enable instant exploration, we devise a novel index structure and develop effective pruning and materialization techniques. Experimental results on real large-scale datasets validate our theoretical findings and show high performances of our proposed methods.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Yuchen Li"
      }, 
      {
        "name": "Ju Fan"
      }, 
      {
        "name": "Dongxiang Zhang"
      }, 
      {
        "name": "Kian-Lee Tan"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD201"
  }, 
  "SIGMOD529": {
    "title": "Distance Oracle on Terrain Surface", 
    "abstract": "Due to the advance of the geo-spatial positioning and the computer graphics technology, digital terrain data become more and more popular nowadays. Query processing on terrain data has attracted considerable attention from both the academic community and the industry community. One fundamental and important query is the shortest distance query and many other applications such as proximity queries (including nearest neighbor queries and range queries), 3D object feature vector construction and 3D object data mining are built based on the result of the shortest distance query. In this paper, we study the shortest distance query which is to find the shortest distance between a point-of-interest and another point-of-interest on the surface of the terrain due to a variety of applications. As observed by existing studies, computing the exact shortest distance is very expensive. Some existing studies proposed epsilon-approximate distance oracles where epsilon is a non-negative real number and is an error parameter. However, the best-known algorithm has a large oracle construction time, a large oracle size and a large distance query time. Motivated by this, we propose a novel epsilon-approximate distance oracle called the Space Efficient distance oracle (SE) which has a small oracle construction time, a small oracle size and a small distance query time due to its compactness storing concise information about pairwise distances between any two points-of-interest. Our experimental results show that the oracle construction time, the oracle size and the distance query time of SE are up to two orders of magnitude, up to 3 orders of magnitude and up to 5 orders of magnitude faster than the best-known algorithm.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Victor Junqiu Wei"
      }, 
      {
        "name": "Raymond Chi-Wing Wong"
      }, 
      {
        "name": "Cheng Long"
      }, 
      {
        "name": "David Mount"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD529"
  }, 
  "SIGMOD15": {
    "title": "Distributed Algorithms on Exact Personalized PageRank", 
    "abstract": "As one of the most well known graph computation problems, Personalized PageRank is an effective approach for computing the similarity score between two nodes, and it has been widely used in various applications, such as link prediction and recommendation. Due to the high computational cost and space cost of computing the\nexact Personalized PageRank Vector (PPV), most existing studies compute PPV approximately. In this paper, we propose novel and efficient distributed algorithms that compute PPV exactly based on graph partitioning on a general coordinator-based share-nothing distributed computing platform. Our algorithms takes three aspects into account: the load balance, the communication cost, and the computation cost of each machine. The proposed algorithms only require one time of communication between each machine and the coordinator at query time. The communication cost is bounded, and the work load on each machine is balanced. Comprehensive experiments conducted on five real datasets demonstrate the efficiency and the scalability of our proposed methods.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Tao Guo"
      }, 
      {
        "name": "Xin Cao"
      }, 
      {
        "name": "Gao Cong"
      }, 
      {
        "name": "Jiaheng Lu"
      }, 
      {
        "name": "Xuemin Lin"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD15"
  }, 
  "SIGMOD39": {
    "title": "Distributed Provenance Compression", 
    "abstract": "Network accountability, forensic analysis, and failure diag-\nnosis are becoming increasingly important for network man-\nagement and security. Such capabilities often utilize network\nprovenance _ the ability to issue queries over network meta-\ndata. For example, network provenance may be used to\ntrace the path a message traverses on the network as well as\nto determine how message data were derived and which par-\nties were involved in its derivation. However, a challenge in\na live network is the fact the large number of messages that\narrive may result in substantial storage overhead. In this\npaper, we explore techniques to dynamically compress dis-\ntributed provenance stored at scale. Our approach combines\nthe use of static analysis of NDLog programs used for capturing\nand maintaining provenance, to identify opportunities for sharing \nat runtime within a single set of NDLog rules. Our experimental \nresults demonstrate that our approach result in significant storage savings\nover alternative approaches, while increasing similar band-\nwidth and latency overheads.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Chen Chen"
      }, 
      {
        "name": "Harshal Tushar Lehri"
      }, 
      {
        "name": "Lay Kuan Loh"
      }, 
      {
        "name": "Limin Jia"
      }, 
      {
        "name": "Boon Loo"
      }, 
      {
        "name": "Wenchao Zhou"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD39"
  }, 
  "SIGMOD615": {
    "title": "Dynamic Density Based Clustering", 
    "abstract": "{\\em Dynamic clustering}---how to efficiently maintain data clusters along with updates in the underlying dataset---is a difficult topic. This is especially true for {\\em density-based clustering}, where objects are aggregated based on transitivity of proximity, under which deciding the cluster(s) of an object may require the inspection of numerous other objects. The phenomenon is unfortunate, given the popular usage of this clustering approach in many applications demanding data updates.\n    \n    \\vgap \n    \n    Motivated by the above, we investigate the algorithmic principles for dynamic clustering by DBSCAN, a successful representative of density-based clustering, and $\\rho$-approximate DBSCAN, proposed to bring down the computational hardness of the former on \\uline{\\em static} data. Surprisingly, we prove that the $\\rho$-approximate version {\\em suffers from the very same hardness when the dataset is \\uline{fully dynamic}}, namely, when both insertions and deletions are allowed. We also show that this issue goes away as soon as tiny further relaxation is applied, yet still ensuring the same quality---known as the ``sandwich guarantee''---of $\\rho$-approximate DBSCAN. Our algorithms guarantee near-constant update processing, and outperform existing approaches by a factor over two orders of magnitude.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Junhao Gan"
      }, 
      {
        "name": "Yufei Tao"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD615"
  }, 
  "SIGMOD51": {
    "title": "Efficient Ad-Hoc Graph Inference and Matching in Biological Databases", 
    "abstract": "In many real applications such as bioinformatics and biological network analysis, it has always been an important, yet challenging, topic to accurately infer/reconstruct gene regulatory networks (GRNs) from microarray data, and efficiently identify those matching GRNs with similar interaction structures for potential disease analysis and treatment tasks. Motivated by this, in this paper, we formalize the problem of ad-hoc inference and matching over gene regulatory networks (IM-GRN), which deciphers ad-hoc GRN graph structures online from gene feature databases (without full GRN materializations), and retrieves the inferred GRNs that are subgraph-isomorphic to a query GRN graph with high confidences. Specifically, we propose a novel probabilistic score to measure the possible interaction between any two genes\n(inferred from gene feature vectors), and thus model GRNs by probabilistic graphs, containing edge existence probabilities. In order to efficiently process IM-GRN queries, we propose effective reduction, pruning, and embedding strategies to significantly reduce the search space of GRN inference and matching, without\nmaterializing all GRNs. We also present an effective indexing mechanism and an efficient IM-GRN query processing algorithm by the index traversal. Finally, extensive experiments have been conducted to verify the efficiency and effectiveness of our proposed IM-GRN query answering approaches over real/synthetic GRN data sets.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Xiang Lian"
      }, 
      {
        "name": "Dongchul Kim"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD51"
  }, 
  "SIGMOD87": {
    "title": "Efficient Computation of Regret-ratio Minimizing Set: A Compact Maxima Representative", 
    "abstract": "Finding the maxima of a database based on a user preference, especially when the ranking function is a linear combination of the attributes, has been the subject of recent research. A critical observation is that the convex hull is the subset of tuples that can be used to find the maxima of any linear function. However, in real world applications, the convex hull can be a significant portion of the database, and thus its performance is greatly reduced. Thus, computing a subset limited to r tuples that minimizes the regret ratio (a measure of the user\u00cds dissatisfaction with the result from the limited set versus the one from the entire database) is of interest.\nIn this paper, we make several fundamental theoretical as well as practical advances in developing such a compact set. In the case of two dimensional databases, we develop an optimal linearithmic time algorithm by leveraging the ordering of skyline tuples. In the case of higher dimensions, the problem is known to be NPcomplete. As one of our main results of this paper, we develop an approximation algorithm that runs in linearithmic time and guarantees a regret ratio, within any arbitrarily small user-controllable distance from the optimal regret ratio. The comprehensive set of\nexperiments on both synthetic and publicly available real datasets confirm the efficiency, quality of output, and scalability of our proposed\nalgorithms.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Abolfazl Asudeh"
      }, 
      {
        "name": "Azade Nazi"
      }, 
      {
        "name": "Nan Zhang"
      }, 
      {
        "name": "Gautam Das"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD87"
  }, 
  "SIGMOD514": {
    "title": "Efficient Computation of Top-k Frequent Terms over Spatial-Temporal Ranges", 
    "abstract": "The wide availability of tracking devices has drastically increased the role of geolocation in social networks, resulting in new commercial applications; for example, marketers can identify current trending topics within a region of interest and focus their products accordingly. In this paper we study a basic analytics query on geotagged data, namely: given a spatiotemporal region, find the most frequent terms among the social posts in that region. While there has been prior work on keyword search on spatial data (find the objects nearest to the query point that contain the query keywords), and on group keyword search on spatial data (retrieving groups of objects), our problem is different in that it returns keywords and aggregated frequencies as output, instead of having the keyword as input. Moreover, we differ from works addressing the streamed version of this query in that we operate on large, disk resident data and we provide exact answers. We propose an index structure and algorithms to efficiently answer such top-k spatiotemporal range queries, which we refer as Top-k Frequent Spatiotemporal Terms (kFST) queries. Our index structure employs an R-tree augmented by top-k sorted term lists (STLs), where a key challenge is to balance the size of the index to achieve faster execution and smaller space requirements. We theoretically study and experimentally validate the ideal length of the stored term lists, and perform detailed experiments to evaluate the performance of the proposed methods compared to baselines on real datasets.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Pritom Ahmed"
      }, 
      {
        "name": "Mahbub Hasan"
      }, 
      {
        "name": "Abhijith Kashyap"
      }, 
      {
        "name": "Vagelis Hristidis"
      }, 
      {
        "name": "Vassilis J. Tsotras"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD514"
  }, 
  "PODS129": {
    "title": "Efficient Matrix Sketching over Distributed Data", 
    "abstract": "A sketch or synopsis of a large dataset captures vital properties of the original\ndata while typically occupying much less space.\nIn this paper, we consider the problem of computing a sketch of a massive data matrix $A\\in\\real^{n\\times d}$, which is distributed across a large number of $s$ servers.  \nOur goal is to output a matrix $B\\in\\real^{\\ell\\times d}$ which is significantly smaller than but still approximates $A$ well in terms of \\emph{covariance error}, i.e., $\\norm{A^TA-B^TB}_2$. Here, for a matrix $A$, $\\norm{A}_2$ is the spectral norm of $A$, which is defined as the largest singular value of $A$. Following previous works, we call $B$ a covariance sketch of $A$.\nWe are mainly focused on minimizing the communication cost, which is arguably the most valuable resource in distributed computations. We show a gap between deterministic and randomized communication complexity for computing a covariance sketch. More specifically, we first prove a tight deterministic lower bound, then show how to bypass this lower bound using randomization. \nIn \\emph{Principle Component Analysis} (PCA), the goal is to find a low-dimensional\nsubspace that captures as much of the variance of a dataset as possible. \nBased on a well-known connection between covariance sketch and PCA, we give a new algorithm for distributed PCA with improved communication cost. Moreover, in our algorithms, each server only needs to make one pass over the data with limited working space.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Zengfeng Huang"
      }, 
      {
        "name": "Xuemin Lin"
      }, 
      {
        "name": "Wenjie Zhang"
      }, 
      {
        "name": "Ying Zhang"
      }
    ], 
    "type": "paper", 
    "id": "PODS129"
  }, 
  "PODS40": {
    "title": "Efficient and Provable Multi-Query Optimization", 
    "abstract": "Complex queries for massive data analysis jobs have become increasingly commonplace. Many such queries contain common subexpressions, either within a single query or among multiple queries submitted as a batch. Conventional query optimizers do not exploit these subexpressions and produce sub-optimal plans. The problem of multi-query optimization (MQO) is to generate an optimal combined evaluation plan by computing common subexpressions once and reusing them. Exhaustive algorithms for MQO explore an O(n^n) search space. Thus, this problem has primarily been tackled using various heuristic algorithms, without providing any theoretical guarantees on the quality of their solution.\nIn this paper, instead of the conventional cost minimization problem, we treat the problem as maximizing a linear transformation of the cost function. We propose a greedy algorithm for this transformed formulation of the problem, which under weak, intuitive assumptions that seem to hold in practice, provides an approximation factor guarantee for this formulation. We go on to show that this factor is optimal, unless P = NP. We also propose optimizations which can be used to improve the efficiency of our algorithm. Another noteworthy point about our algorithm is that it can be easily incorporated into existing transformation-based optimizers. Our experiments on benchmark datasets demonstrate that our algorithm performs comparable or superior to a popular algorithm for the problem.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Tarun Kathuria"
      }, 
      {
        "name": "S Sudarshan"
      }
    ], 
    "type": "paper", 
    "id": "PODS40"
  }, 
  "PODS83": {
    "title": "Efficiently Enumerating Minimal Triangulations", 
    "abstract": "We present an algorithm that enumerates all the minimal triangulations of a graph in incremental polynomial time. Consequently, we get an algorithm for enumerating all the proper tree decompositions, in incremental polynomial time, where \"proper\" means that the tree decomposition cannot be improved by removing or splitting a bag. The algorithm can incorporate any method for (ordinary, single result) triangulation or tree decomposition, and can serve as an anytime algorithm to improve such a method. We describe an extensive experimental study of an implementation on real data from different fields. Our experiments show that the algorithm improves upon central quality measures over the underlying tree decompositions, and is able to produce a large number of high-quality decompositions.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Nofar Carmeli"
      }, 
      {
        "name": "Batya Kenig"
      }, 
      {
        "name": "Benny Kimelfeld"
      }
    ], 
    "type": "paper", 
    "id": "PODS83"
  }, 
  "SIGMOD106": {
    "title": "Enabling Signal Processing over Data Streams", 
    "abstract": "Internet of Things applications analyze the data coming from large networks of sensor devices using relational and signal processing operations and running the same query logic over groups of sensor signals. To support such increasingly important scenarios, many data management systems integrate with numerical frameworks like R. Such solutions, however, incur significant performance penalties as relational data processing engines and numerical tools operate on fundamentally different data models with expensive inter-communication mechanisms. In addition, none of these solutions supports efficient real-time analysis.\n\nIn this paper, we advocate a deep integration of signal processing operations and general-purpose query processors. We aim to reconcile the disparate data models and provide a common query language that allows users to seamlessly interleave tempo-relational and signal operations for both online and offline processing. Our approach is extensible and offers frameworks for quick and easy integration of user-defined operations while supporting incremental computation. Our system that deeply integrates relational and signal operations, called SystemX, achieves up to 192x better performance than popular loosely-coupled data management systems on grouped signal processing workflows.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Milos Nikolic"
      }, 
      {
        "name": "Badrish Chandramouli"
      }, 
      {
        "name": "Jonathan Goldstein"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD106"
  }, 
  "SIGMOD244": {
    "title": "Exploiting Common Patterns for Tree-Structured Data", 
    "abstract": "Tree-structured data formats, such as JSON and Protocol Buffers, are capable of expressing sophisticated data types, including nested, repeated, and missing values.  While such expressing power contributes to their popularity in real-world applications, it presents a significant challenge for systems supporting tree-structured data.  Existing systems have focused on general-purpose solutions either extending RDBMSs or designing native systems.  However, the general-purpose approach often results in sophisticated data structures and algorithms, which may not reflect and optimize for the actual structure patterns in the real world.\n\nIn this paper, we aim to better understand tree-structured data types in real uses and optimize for the common patterns.  We present an in-depth study of five types of real-world use cases of tree-structured data.  We find that a majority of the root-to-leaf paths in the tree structures are simple, containing up to one repeated node.  Given this insight, we design and implement Steed,  a native analytical database system for tree-structured data.  Steed implements the baseline general-purpose support for storing and querying data in both row and column layouts.  Then we enhance the baseline design with a set of optimizations to simplify and improve the processing of simple paths.  Experimental evaluation shows that our optimization improves the baseline by a factor of up to 1.74x.  Compared to three representative state-of-the-art systems (i.e.  PostgreSQL, MongoDB, and Hive+Parquet), Steed achieves orders of magnitude better performance in both cold cache and hot cache scenarios.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Zhiyi Wang"
      }, 
      {
        "name": "Shimin Chen"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD244"
  }, 
  "SIGMOD22": {
    "title": "Extracting Top-K Insights from Multi-dimensional Data", 
    "abstract": "OLAP tools have been extensively used by enterprises to make better and faster decisions.\nNevertheless, they require users to specify group-by attributes and know precisely what they are looking for.\nThis paper takes the first attempt towards automatically extracting top-$k$ {\\em insights} from multi-dimensional data.\nThis is useful not only for non-expert users, but also reduces the manual effort of data analysts.\nIn particular, we propose the concept of {\\em insight} which captures interesting observation derived from aggregation results in multiple steps (e.g., rank by a dimension, compute the percentage of measure by a dimension).\nAn example insight is: ``BMW's rank (across brands) falls along the year, in terms of the increase in sales''.\nOur problem is to compute the top-$k$ insights by a score function.\nIt poses challenges on (i) the effectiveness of the result and (ii) the efficiency of computation. \nWe propose a meaningful scoring function for insights to address (i).\nThen, we contribute a systematic computation framework for top-$k$ insights, together with a suite of optimization techniques (i.e., pruning, ordering, specialized cube, and computation sharing) to address (ii).\nOur experimental study on both real sales data and synthetic data verifies the effectiveness and efficiency of our proposed solution.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Bo Tang"
      }, 
      {
        "name": "Shi Han"
      }, 
      {
        "name": "Man Lung Yiu"
      }, 
      {
        "name": "Rui Ding"
      }, 
      {
        "name": "Dongmei Zhang"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD22"
  }, 
  "SIGMOD193": {
    "title": "Extracting and Analyzing Hidden Graphs from Relational Databases", 
    "abstract": "Analyzing interconnection structures among underlying entities or objects in a dataset through the use of graph analytics has been shown to provide tremendous value in many application domains. However, graphs are not the primary representation choice for storing most data today, and in order to have access to these analyses, users are forced to manually extract data from their data stores, construct the requisite graphs, and then load them into some graph engine in order to execute their graph analysis task. Moreover, in many cases (i.e. when the graphs are dense) these graphs can turn out to be significantly larger than the initial input stored in the database, making it infeasible to construct or analyze such graphs in-memory. In this paper we tackle both of the above issues.\nWe are building a system that enables users to declaratively specify graph extraction tasks over a relational database schema and then execute graph algorithms on the extracted graphs. We propose a declarative domain specific language for this purpose, and pair it up with a series of optimizations for efficiently extracting a condensed representation of these graphs in-memory. The key challenge here is ensuring correctness in the presence of duplicate paths between the nodes in the graph. We present a suite of in-memory representations that handle this duplication in different ways and allow trading off the in-memory storage required and the computational cost for executing different graph algorithms. We also introduce several novel de-duplication algorithms for removing this duplication in the graph, which are of independent interest to graph compression, and provide a comprehensive experimental evaluation over several real-world and synthetic datasets illustrating these trade-offs.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Konstantinos Xirogiannopoulos"
      }, 
      {
        "name": "Amol Deshpande"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD193"
  }, 
  "SIGMOD362": {
    "title": "FEXIPRO: Fast and Exact Inner Product Retrieval in Recommender Systems", 
    "abstract": "Recommender systems have many successful applications in e-commerce and social media, including Amazon, Netflix, and Yelp. Matrix Factorization (MF) is one of the most popular recommendation approaches; the original user-product rating matrix R with millions of rows and columns is decomposed into a user matrix Q and an item matrix P, such that the product Q^T P approximates R. Each column q (p) of Q (P) holds the latent factors of the corresponding user (item), and q^T p is a prediction of the rating to item p by user q. Recommender systems based on MF suggest to a user in q the items with the top-k scores in q^T P. For this problem, we propose a Fast and EXact Inner PROduct retrieval (FEXIPRO) framework, based on sequential scan, which includes three novel elements. First, FEXIPRO applies an SVD transformation to P, after which the first several dimensions capture a large percentage of the inner products. This enables us to prune item vectors by only computing their partial inner products with q. Second, we construct an integer approximation version of P, which can be used to compute fast upper bounds for the inner products of item vectors that can potentially prune them. Finally, we apply a lossless transformation to P, such that the resulting matrix has only positive values, allowing for the inner products to be monotonically increasing with dimensionality. Extensive experiments on real data demonstrate that our proposal outperforms the previous state-of-the-art typically by an order of magnitude.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Hui Li"
      }, 
      {
        "name": "Tsz Nam Chan"
      }, 
      {
        "name": "Man Lung Yiu"
      }, 
      {
        "name": "Nikos Mamoulis"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD362"
  }, 
  "SIGMOD156": {
    "title": "FPGA Based Data Partitioning", 
    "abstract": "Implementing parallel operators in multicore machines often involves\ncareful tuning to the underlying hardware. An important part of that\nprocess is a first data partitioning step that divides the data into\nblocks fitting within the caches and arranged so as to allow concurrent\nthreads to process the blocks in parallel without incurring\nsynchronization overhead. Data partitioning is nevertheless expensive,\nin some cases being 90\\% of the cost of, e.g., a parallel hash join.\n\nIn this paper we explore using specialized hardware (an FPGA) to\naccelerate and reduce the overhead of data partitioning. We do so in the\ncontext of new hybrid architectures where the FPGA is located as a\nco-processor residing on a socket and with coherent access to the same\nmemory as the CPU residing on the other socket. Such an architecture\nreduces data transfer overheads between the CPU and the FPGA, enabling\nhybrid operator execution where the partitioning happens on the FPGA and\nthe build and probe phases of a join happen on the CPU. Our experiments\ndemonstrate that FPGA based partitioning is significantly faster and\nmore robust than CPU based partitioning. The results open  interesting\noptions as FPGAs are gradually integrated tighter within the CPU,\npermitting both higher performance but also a reduction in power\nconsumption and footprint for the accelerator.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Kaan Kara"
      }, 
      {
        "name": "Jana Giceva"
      }, 
      {
        "name": "Gustavo Alonso"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD156"
  }, 
  "SIGMOD370": {
    "title": "Fast Failure Recovery for Main-Memory DBMSs on Multicores", 
    "abstract": "Main-memory database management systems (DBMS) can achieve excellent performance when processing massive volume of on-line transactions on modern multicore machines. But existing durability schemes, namely, tuple-level and transaction-level logging-and-recovery mechanisms, either degrade the performance of transaction processing or slow down the process of failure recovery. In this paper, we show that, by exploiting application semantics, it is possible to achieve speedy failure recovery without introducing any costly logging overhead to the execution of concurrent transactions. \nWe propose PACMAN, a parallel database recovery mechanism that is specifically designed for lightweight, coarse-grained transaction-level logging. PACMAN leverages a combination of static and dynamic analyses to parallelize the log recovery: at compile time, PACMAN decomposes stored procedures by carefully analyzing dependencies within and across programs; at recovery time, PACMAN exploits the availability of the runtime parameter values to attain an execution schedule with a high degree of parallelism. As such, recovery rate is remarkably increased. We evaluated PACMAN in a fully-fledged main-memory DBMS running on a 40-core machine. Compared to several state-of-the-art database recovery mechanisms, PACMAN can significantly reduce recovery time without compromising the efficiency of transaction processing.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Yingjun Wu"
      }, 
      {
        "name": "Wentian Guo"
      }, 
      {
        "name": "Chee-Yong Chan"
      }, 
      {
        "name": "Kian-Lee Tan"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD370"
  }, 
  "SIGMOD696": {
    "title": "Fast Searchable Encryption with Optimal Locality", 
    "abstract": "Searchable encryption (SE) allows a client to outsource a dataset to an untrusted server while enabling the server to answer keyword queries in a private manner. SE can be used as a building block to support more expressive private queries such as range/point and boolean queries, while providing formal security guarantees. To scale SE to big data using external memory, new schemes with small locality have been proposed, where locality is defined as the number of non-continuous reads that the server makes for each query. Previous space-efficient SE schemes achieve optimal locality by increasing the read efficiency -the number of additional memory locations (false positives) that the server reads per result item. In this work, we design, formally prove secure, and evaluate the most practical SE scheme with optimal locality and linear space known to date, outperforming existing approaches by up to 2.5 orders of magnitude in terms of read efficiency, for all practical database sizes. We demonstrate that SE with optimal locality allows us to design an SE scheme that works fast in memory as well, leading to search time savings of up to 1 order of magnitude when compared to the most practical in-memory SE schemes. Our scheme can be also tuned to achieve trade-offs between space, read efficiency, locality, parallelism and communication overhead.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Ioannis Demertzis"
      }, 
      {
        "name": "Charalampos Papamanthou"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD696"
  }, 
  "SIGMOD412": {
    "title": "Feedback-Aware Social Event-Participant Arrangement", 
    "abstract": "Online event-based social network (EBSN) platforms and studies on global event-participant arrangement strategies for EBSNs are becoming popular recently. Existing works measure satisfaction of an arrangement by a linear combination of few factors, weights of which are predefined and fixed, and do not allow users to provide feedbacks on whether accepting the arrangement or not. Besides, most of them only consider offline scenarios, where full information of users is known in advance. However, on real-world EBSN platforms, users can dynamically log in the platform and register for events on a first come, first served basis. In other words, online scenarios of event-participant arrangement strategies should be considered. In this work, we study a new event-participant arrangement strategy for online scenarios, the Feedback-Aware Social Event-participant Arrangement (FASEA) problem, where satisfaction scores of an arrangement are learned adaptively and users can choose to accept or reject the arranged events. Particularly, we model the problem as a contextual combinatorial bandit setting and present efficient and effective algorithms to solve the problem. The effectiveness and efficiency of the solutions are evaluated with extensive experimental studies and our findings indicate that the state-of-art framework Thompson Sampling that is reported to work well under basic multi-armed bandit does not perform well under FASEA.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Lei Chen"
      }, 
      {
        "name": "Jieying She"
      }, 
      {
        "name": "Yongxin Tong"
      }, 
      {
        "name": "Tianshu Song"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD412"
  }, 
  "SIGMOD111": {
    "title": "Flexible and Feasible Support Measures for Mining Frequent Patterns in Large Labeled Graphs", 
    "abstract": "In recent years, graph databases such as Twitter and Facebook social graph and citation maps have grown rapidly, therefore graph mining techniques are becoming more and more important. In frequent pattern mining in a single-graph setting, there are two main problems: support measure and search scheme. In this paper, we propose a novel framework for constructing support measures that brings together existing minimum-image-based and overlap-graph-based support measures. Our framework is built on the concept of occurrence / instance hypergraphs. Within this framework, we present two new support measures: minimum instance (MI) measure and minimum vertex cover (MVC) measure, that combine the advantages of existing measures. In particular, we show that the existing minimum-image-based support measure is an upper bound of the MI measure, which is also linear-time computable and results in counts that are close to number of instances of a pattern. Although the MVC measure is NP-hard, which means it is as hard as the existing overlap-graph-based measure, it can be approximated to a constant factor in polynomial time. We also provide polynomial-time computable relaxations for both  measures. Bounding theorems are given for all presented support measures in the new hypergraph setting. We further show that the hypergraph-based framework can unify all support measures studied in this paper. This framework is also flexible in that more variations of support measures can be defined and profiled in it.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Jinghan Meng"
      }, 
      {
        "name": "Yicheng Tu"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD111"
  }, 
  "SIGMOD521": {
    "title": "Foofah: Transforming Data By Example", 
    "abstract": "Data transformation is a critical first step in modern data analysis: Before any analysis can be done, data from a variety of sources must be wrangled into a uniform format that is amenable to the intended analysis and analytical software package. This data transformation task is tedious, time-consuming, and often requires programming skills beyond the expertise of data analyst. In this paper, we develop a technique to specify data transformation programs by example, reducing the data transformation burden by allowing the analyst to specify only a few examples of what the data should eventually look like, without having to be concerned with the transformation steps required to get there. We implemented our technique in a system, Foofah, which efficiently searches the space of possible data transformation operations to generate a program that will transform the sampled raw data into the output data from input-output pair provided by the user. We experimentally show that data transformation programs can be created quickly through Foofah for a wide variety of cases, with 60% less user effort than the well-known Wrangler system.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Zhongjun Jin"
      }, 
      {
        "name": "Michael R. Anderson"
      }, 
      {
        "name": "Michael Cafarella"
      }, 
      {
        "name": "H. V. Jagadish"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD521"
  }, 
  "SIGMOD263": {
    "title": "From In-Place Updates to In-Place Appends: Revisiting Out-of-Place Updates on Flash", 
    "abstract": "Under traditional update intensive workloads (Graphs, OLTP) small updates dominate the write behavior: more than 70% of all updates change less than 10 bytes across all TPC OLTP workloads. These are typically performed as in-place updates and result in random writes in page-granularity, causing major write-overhead on Flash storage, a write am- plification of several hundred times and lower device longevity.\n\nIn this paper we propose an approach that transforms those small in-place updates into small update deltas that are appended to the original page. We utilize the commonly ignored fact that modern Flash memories (SLC, MLC, 3D NAND) can handle appends to already programmed physi- cal pages by using various low-level techniques such as ISPP to avoid expensive erases and page migrations. Further- more, we extend the traditional NSM page-layout with a delta-record area that can absorb those small updates. We propose a scheme to control the write behavior as well as the space allocation and sizing of database pages.\n\nThe proposed approach has been implemented under Shore- MT and evaluated on real Flash hardware (OpenSSD) and a Flash emulator. Compared to In-Page Logging [18] it per- forms up to 62% less reads and writes and up to 74% less erases on a range of workloads. The experimental evaluation indicates: (i) significant reduction of erase operations result- ing in twice the longevity of Flash devices under update- intensive workloads; (ii) 15%-60% lower read/write I/O la- tencies; (iii) up to 45% higher transactional throughput; (iv) 2x to 3x overall write amplification reduction.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Sergey Hardock"
      }, 
      {
        "name": "Ilia Petrov"
      }, 
      {
        "name": "Robert Gottstein"
      }, 
      {
        "name": "Alejandro Buchmann"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD263"
  }, 
  "SIGMOD294": {
    "title": "Handling Environments in a Verified Query Compiler", 
    "abstract": "Combinator-based algebras, which avoid the use of environment\n  variables, have been proposed as an alternative intermediate\n  representation for query compilation and optimization. A key benefit\n  of combinators is that they avoid the need to handle variable\n  shadowing or accidental capture in algebraic rewrites, which\n  simplifies both the optimizer specification and its correctness\n  analysis. However this can come at the cost of more complex query\n  plans, as operators to reify environments as records are\n  introduced. \n\n  We propose NRAe, an extension of a combinator-based\n  nested relational algebra (NRA) with built-in support for\n  environments that simplify the encoding of variables bindings in\n  query plans. We show that NRA optimizations automatically apply to\n  NRAe. \n\n  Based on NRAe, we have specified an optimizing query compiler\n  using the Coq proof-assistant. Most of the compiler, including its\n  optimizer, is accompanied by a (machine-checked) correctness\n  proof. The compiler implementation is automatically extracted from\n  the specification, resulting in an optimizing query compiler with a\n  verified core.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Joshua Auerbach"
      }, 
      {
        "name": "Martin Hirzel"
      }, 
      {
        "name": "Louis Mandel"
      }, 
      {
        "name": "Avraham Shinnar"
      }, 
      {
        "name": "Jerome Simeon"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD294"
  }, 
  "SIGMOD91": {
    "title": "Heterogeneity-aware Distributed Parameter Servers", 
    "abstract": "We conduct a study of distributed machine learning in heterogeneous environments in this work. Our first contribution is a systematic study of existing systems running distributed stochastic gradient descent; we find that, although these systems work well in homogeneous environments, they can suffer performance degradation, sometimes up to 10_, in heterogeneous environments where stragglers are common because their synchronization protocols cannot fit a heterogeneous setting. Our second contribution is\na heterogeneity-aware algorithm that uses a constant learning rate schedule for updates before adding them to the global parameter. This allows us to suppress stragglers\u00cd harm on robust convergence. As a further improvement over this algorithm, our third contribu-\ntion is a more sophisticated learning rate schedule that takes into consideration the delayed information of each local update. We design a dynamic, yet efficient data structure for this schedule. We theoretically prove the valid convergence of both approaches and implement a prototype system in the production cluster of our industrial partner. We validate the performance of this prototype using a range of machine-learning workloads and find that our prototype is 2-12_ faster than other state-of-the-art systems and that our proposed heterogeneity-aware algorithm takes up to 6_ fewer\niterations to converge.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Jiawei Jiang"
      }, 
      {
        "name": "Bin Cui"
      }, 
      {
        "name": "Ce Zhang"
      }, 
      {
        "name": "Lele Yu"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD91"
  }, 
  "PODS50": {
    "title": "How Fast can a Distributed Transaction Commit?", 
    "abstract": "The atomic commit problem lies at the heart of distributed database systems. The problem consists for a set of processes to agree on whether to commit or abort a transaction (agreement property). The commit decision can only be taken if all processes are initially willing to commit, and this decision must be taken if all processes are willing to commit and there is no failure (validity property). An atomic commit protocol is said to be non-blocking if every correct process eventually reaches a decision (commit or abort) even if there are failures (termination property).\n\nSurprisingly, despite the importance of the atomic commit problem, little is known about its complexity. In this paper, we present for the first time a systematic study on the time and message complexity of the problem. We measure complexity in the executions that are considered the most frequent in practice, i.e., failure-free, with all processes willing to commit. In other words, we measure  \\emph{how fast a transaction can commit}. Not surprisingly, this complexity depends on robustness, i.e., on which property (agreement, termination, validity) we require  in which executions (e.g., executions where delays are unbounded and/or processes may crash). Through our systematic study, we close many open questions like the complexity of synchronous non-blocking atomic commit. We also present optimal protocols which may be of independent interest. In particular, we present an effective protocol which solves what we call indulgent atomic commit that tolerates empirical systems which are synchronous ``most of the time'' (i.e., tolerates inaccurate timeouts).", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Rachid Guerraoui"
      }, 
      {
        "name": "Jingjing Wang"
      }
    ], 
    "type": "paper", 
    "id": "PODS50"
  }, 
  "SIGMOD524": {
    "title": "IC2: Indexed Cutoffs for Kernel Density Classification", 
    "abstract": "Density estimation forms a critical component of many analytics tasks including outlier detection, spatial visualization, and bounding statistical quantities like the log-likelihood and hazard rate. In each of these, analysts seek to classify data into low and high-density regions of a probability distribution. Kernel Density Estimation (KDE) is a powerful technique for capturing these densities, offering excellent statistical accuracy but computationally intractable runtime at scale. In this paper, we introduce a simple technique for improving the performance of using a KDE to classify points by their density (density classification) by up to three orders of magnitude. This is done by combining a spatial index with predicate pushdown to aggressively prune unnecessary computation. Using the spatial index to group neighboring points, our proposed method, Indexed Cutoffs for Classification (IC2) is able to classify data points above or below a threshold early on during index traversal. We demonstrate substantial speedups over alternative algorithms, and show how density classification via IC2 is able to make traditionally expensive Kernel Density Estimators both accurate and performant for a variety of use cases.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Edward Gan"
      }, 
      {
        "name": "Peter Bailis"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD524"
  }, 
  "SIGMOD142": {
    "title": "Incremental Graph Computations: Doable and Undoable", 
    "abstract": "The incremental problem for a class \\Q of graph queries aims to compute, given a query Q in \\Q, graph G, output Q(G) and updates \\Delta G to G as input, changes \\Delta O to Q(G) such that Q(G \\oplus \\Delta G) = Q(G) \\oplus \\Delta O. It is called bounded if its cost can be expressed as a polynomial function in the sizes of Q, \\Delta G and \\Delta O. It is to reduce computations on possibly big G to small \\Delta G and \\Delta O. No matter how desirable, however, our first results are negative: for common graph queries such as graph traversal, connectivity, keyword search and pattern matching, their incremental problems are unbounded. \nIn light of the negative results, we propose two characterizations for the effectiveness of incremental computations: (a) localizable, if its cost is decided by small neighbors of nodes in \\Delta G instead of the entire G; and (b) bounded relative to a batch algorithm \\T, if the cost is determined by the sizes of \\Delta G and changes to the affected area that is necessarily checked by \\T. We show that the incremental computations above are either localizable or relatively bounded, by providing corresponding incremental algorithms. That is, we can either reduce the incremental computations on big graphs to small data, or incrementalize batch algorithms by minimizing unnecessary recomputation. Using real-life graphs, we experimentally verify the effectiveness of our algorithms.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Wenfei Fan"
      }, 
      {
        "name": "Chunming Hu"
      }, 
      {
        "name": "Chao Tian"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD142"
  }, 
  "SIGMOD549": {
    "title": "Incremental View Maintenance over Array Data", 
    "abstract": "Science applications are producing an ever-increasing volume of multi-dimensional data that are mainly processed with distributed array databases. These raw arrays are ``cooked'' into derived data products using complex pipelines that are time-consuming. As a result, derived data products are released infrequently and become stale soon thereafter. In this paper, we introduce materialized array views as a database construct for scientific data products. We model the ``cooking'' process as incremental view maintenance with batch updates and give a three-stage heuristic that finds effective update plans. Moreover, the heuristic repartitions the array and the view continuously based on a window of past updates as a side-effect of view maintenance without overhead. We design an analytical cost model for integrating materialized array views in queries. A thorough experimental evaluation confirms that the proposed techniques are able to incrementally maintain a real astronomical data product in a production environment.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Weijie Zhao"
      }, 
      {
        "name": "Florin Rusu"
      }, 
      {
        "name": "Bin Dong"
      }, 
      {
        "name": "Kesheng Wu"
      }, 
      {
        "name": "Peter Nugent"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD549"
  }, 
  "SIGMOD473": {
    "title": "Interactive Mapping Specification with Exemplar Tuples", 
    "abstract": "While schema mapping specification is a cumbersome task for data curation specialists, it becomes unfeasible for non-expert users, who are unacquainted with the semantics and languages of the involved transformations. \n  \nIn this paper, we present an interactive framework for schema mapping specification suited for non-expert users. The underlying key intuition is to leverage a few exemplar tuples to learn the underlying mappings and iterate the learning process via simple user interactions under the form of boolean queries on the validity of the initial exemplar tuples. The approaches available so far are mainly assuming pairs of complete universal data examples, which can be solely provided by data curation experts, or are limited to poorly expressive mappings.\n\nWe present several two-step exploration strategies of the space of all possible mappings that satisfy arbitrary user exemplar tuples. Along the exploration, we challenge the user to retain the mappings that fit her requirements at best and to dynamically prune the exploration space, thus minimizing the number of user interactions. We prove that at each step the obtained mappings are correct. We present an extensive experimental analysis devoted to measure the feasibility of our interactive mapping strategies and the inherent quality of the obtained mappings.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Angela Bonifati"
      }, 
      {
        "name": "Ugo Comignani"
      }, 
      {
        "name": "Emmanuel Coquery"
      }, 
      {
        "name": "Romuald Thion"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD473"
  }, 
  "PODS64": {
    "title": "J-Logic: Logical foundations for JSON querying", 
    "abstract": "We propose a logical framework, based on Datalog, to study the\nfoundations of querying JSON data.  The main feature of our\napproach, which we call J-Logic, is the emphasis on paths.  Paths\nare sequences of keys and are used to access the tree structure\nof nested JSON objects.  J-Logic also features \"packing'' as a\nmeans to generate a new key from a path or subpath.  J-Logic with\nrecursion is computationally complete, but many queries can be\nexpressed without recursion.  We give a necessary condition for\nqueries to be expressible without recursion.  Most of our results\nfocus on the deterministic nature of JSON objects as partial\nfunctions from keys to values.  Predicates defined by J-Logic\nprograms may not properly describe objects, however.\nNevertheless we show that every object-to-object transformation\nin J-Logic can be defined using only proper object descriptions\nin intermediate results.  Moreover we show that it is decidable\nwhether a positive, nonrecursive J-Logic program always returns a\nproper object description when given proper object descriptions\nas inputs.  Regarding packing, we show that packing is\nunnecessary if the output does not require new keys.  Finally, we\nshow the decidability of query containment for positive,\nnonrecursive J-Logic programs.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Jan Hidders"
      }, 
      {
        "name": "Jan Paredaens"
      }, 
      {
        "name": "Jan Van den Bussche"
      }
    ], 
    "type": "paper", 
    "id": "PODS64"
  }, 
  "PODS130": {
    "title": "JSON: data model, query languages and schema specification", 
    "abstract": "Despite the fact that JSON is currently one of the most popular formats for exchanging data on the Web, there are very few studies on this topic and no agreed upon theoretical framework for dealing with JSON. Therefore in this paper we propose a formal data model for JSON documents and, based on the common features present in available systems using JSON, define a lightweight query language allowing us to navigate through JSON documents. We also introduce a logic capturing the schema proposal for JSON and study the complexity of basic computational tasks associated with these two formalisms.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Pierre Bourhis"
      }, 
      {
        "name": "Juan L. Reutter"
      }, 
      {
        "name": "Fernando Suarez"
      }, 
      {
        "name": "Domagoj Vrgoc"
      }
    ], 
    "type": "paper", 
    "id": "PODS130"
  }, 
  "SIGMOD221": {
    "title": "Landmark indexing for scalable evaluation of label-constrained reachability queries", 
    "abstract": "Consider a directed edge-labeled graph, such as a social network or a citation network.  A fundamental query on such data is to determine if there is a path in the graph from a given source vertex to a given target vertex, using only edges with labels in a restricted subset of the edge labels in the graph.  Such label-constrained reachability (LCR) queries play an important role in graph analytics, for example, as a core fragment of the so-called regular path queries which are supported in practical graph query languages such as the W3C's SPARQL 1.1, Neo4j's Cypher, and Oracle's PGQL.  Current solutions for LCR evaluation, however, do not scale to large graphs which are increasingly common in a broad range of application domains.  In this paper we present the first scalable solution for efficient LCR evaluation, leveraging landmark-based indexes for large graphs.    We show through extensive experiments that our indexes are significantly smaller than state-of-the-art LCR indexing techniques, while supporting orders of magnitude faster query evaluation times.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Lucien Valstar"
      }, 
      {
        "name": "George H. L. Fletcher"
      }, 
      {
        "name": "Yuichi Yoshida"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD221"
  }, 
  "SIGMOD536": {
    "title": "Leveraging Re-costing for Online Optimization of Parameterized Queries with Guarantees", 
    "abstract": "Parametric query optimization (PQO) deals with the problem of finding and reusing a relatively small number of plans that can achieve good plan quality across multiple instances of a parameterized query. An ideal solution to PQO would process query instances online and ensure (a) bounded cost sub-optimality for each instance (b) low optimization overheads and (c) require a small number of plans to be cached. Existing solutions to online PQO however, fall short on the above metrics. They can either guarantee a bound on sub-optimality but suffer from high optimizer overheads, or they achieve small optimizer overheads but cannot guarantee a bound on sub-optimality. Furthermore, all previous online techniques require caching a large number of plans. We propose a plan re-costing based approach that enables us to perform well on all three metrics. By leveraging re-costing we are able to reduce optimizer overheads and the number of plans required since re-costing increases plan reusability. Our technique can also guarantee sub-optimality using only a mild assumption on the cost function. We empirically show the effectiveness of our technique on industry benchmark and real-world query workloads in a commercial database engine optimizer.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Anshuman Dutt"
      }, 
      {
        "name": "Vivek Narasayya"
      }, 
      {
        "name": "Surajit Chaudhuri"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD536"
  }, 
  "SIGMOD580": {
    "title": "Living in Parallel Realities -- Co-Existing Schema Versions with a Bidirectional Database Evolution Language", 
    "abstract": "We introduce end-to-end support of co-existing schema versions within one database. While it is state of the art to run multiple versions of a continuously developed application concurrently, the same is hard for databases. In order to keep multiple co-existing schema versions alive--all accessing the same data set--developers usually employ handwritten delta code (e.g. views and triggers in SQL). This delta code is hard to write and hard to maintain: if a database administrator decides to adapt the physical table schema, all handwritten delta code needs to be adapted as well, which is expensive and error-prone in practice. We present the tool CoXdb, where developers use the simple bidirectional database evolution language ReDEL, which carries enough information to generate all the delta code automatically. Without additional effort, new schema versions become immediately accessible and data changes in any version are visible in all schema versions at the same time. CoXdb also allows for easily changing the physical table design without affecting the availability of co-existing schema versions. This greatly increases robustness (orders of magnitude less lines of code) and allows for significant performance optimization. A main contribution is the formal evaluation that each schema version acts like a common full-fledged database schema independently of the chosen physical table design.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Kai Herrmann"
      }, 
      {
        "name": "Hannes Voigt"
      }, 
      {
        "name": "Andreas Behrend"
      }, 
      {
        "name": "Jonas Rausch"
      }, 
      {
        "name": "Wolfgang Lehner"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD580"
  }, 
  "SIGMOD606": {
    "title": "Massively Parallel Processing of Whole Genome Sequence Data: An In-Depth Performance Study", 
    "abstract": "This paper presents a joint effort between a group of computer scientists and bioinformaticians to take an important step towards a general big data platform for genome analysis pipelines. The key goals of this study are to develop a thorough understanding of the strengths and limitations of big data technology for genomic data analysis, and to identify the key questions that the research community could address to realize the vision of personalized genomic medicine. Our platform, called System G, is based on the new \"Wrapper Technology\" that supports existing genomic data analysis programs in their native forms, without having to rewrite them. To do so, our system provides several layers of software, including a new Genome Data Parallel Toolkit (GDPT), which can be used to \"wrap\" existing data analysis programs. This platform offers a concrete context for evaluating big data technology for genomics: In particular, we report on super-linear speedup, sublinear speedup, and limited resource efficiency for various tasks, as well as the reasons why a data parallel program produces different results from those of a serial program. These results lead to key research questions that require a synergy between genomics scientists and computer scientists to find solutions.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Abhishek Roy"
      }, 
      {
        "name": "Yanlei Diao"
      }, 
      {
        "name": "Toby Bloom"
      }, 
      {
        "name": "Uday Evani"
      }, 
      {
        "name": "Clinton Howarth"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD606"
  }, 
  "SIGMOD653": {
    "title": "Monkey: Optimal Navigable Key-Value Store", 
    "abstract": "The backbone of modern, persistent key-value stores is an LSM-tree. Modern applications require both efficient lookups and updates, and LSM-trees provide a good balance by buffering updates in main memory, flushing the buffer to persistent storage as a sorted run when it is full, and bounding lookup cost by (1) merging runs in the background and (2) using main memory-resident Bloom filters to probabilistically skip runs that do not contain a target key. In this paper, we show that key-value stores backed by an LSM-tree exhibit an intrinsic, trade-off between lookup cost, update cost, and main memory utilization, yet all existing designs enable a subopti- mal and difficult to tune trade-off among these metrics for all combinations of tunings and workloads. We pinpoint the problem to the fact that all modern key-value stores suboptimally co-tune the merge policy, the buffer size, and the different Bloom filters\u00cd false positive rates.\n\nWe present Monkey, an LSM-based key-value store that strikes the best possible balance between the costs of updates and lookups with any given main memory budget. The insight is that worst-case lookup cost is proportional to the sum of the false positive rates of the Bloom filters across all levels of the LSM-tree. Contrary to state-of-the-art key-value stores that assign a fixed number of bits-per-element to all Bloom filters, Monkey allocates memory to filters across different levels so as to minimize this sum. We implemented Monkey on top of LevelDB and we demonstrate that it reduces lookup cost by 50% _ 90% without losing anything. Furthermore, we map the LSM-tree design space onto a quick to compute closed-form model that enables co-tuning the merge policy, the buffer size and the filters\u00cd false positive rates to trade the gain in lookup cost for update cost and/or main memory utilization, depending on the workload (proportion of lookups and updates), the dataset (number and size of entries), and the underlying hardware (main memory available, disk vs. flash). We show how to use this model to answer what-if design questions about how changes in environmental parameters would impact performance and how to adapt the various LSM-tree design elements accordingly.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Niv Dayan"
      }, 
      {
        "name": "Manos Athanassoulis"
      }, 
      {
        "name": "Stratos Idreos"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD653"
  }, 
  "SIGMOD435": {
    "title": "OctopusFS: A Distributed File System with Tiered Storage Management", 
    "abstract": "The ever-growing data storage and I/O demands of modern large-scale data analytics are challenging the current distributed storage systems. A promising trend is to exploit the recent improvements in memory, storage media, and networks for sustaining high performance and low cost. While past work explores using memory or SSDs as local storage or combine local with network-attached storage in cluster computing, this work focuses on managing multiple storage tiers in a distributed setting. We present OctopusFS, a novel distributed file system that is aware of heterogeneous storage media (e.g., memory, SSDs, HDDs, NAS) with different capacities and performance characteristics. The system offers a variety of pluggable policies for automating data management across the storage tiers and cluster nodes. The policies employ multi-objective optimization techniques for making intelligent data management decisions based on the requirements of fault tolerance, data and load balancing, and throughput maximization. At the same time, the storage media are explicitly exposed to users and applications, allowing them to choose the distribution and placement of replicas in the cluster based on their own performance and fault tolerance requirements. Our extensive evaluation shows the immediate benefits of using OctopusFS with data-intensive processing systems such as Hadoop and Spark, both in terms of increased performance and better cluster utilization.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Elena Kakoulli"
      }, 
      {
        "name": "Herodotos Herodotou"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD435"
  }, 
  "PODS38": {
    "title": "On Asymptotic Cost of Triangle Listing in Random Graphs", 
    "abstract": "Triangle listing has been a long-standing problem, with many heuristics,\nbounds, and experimental results, but not much\nasymptotically accurate complexity analysis. To address this issue, we\nintroduce a novel stochastic framework, based on Glivenko-Cantelli\nresults for functions of order statistics, that allows modeling cost of\nin-memory triangle enumeration in families of random graphs. Unlike prior\nwork that studies the O(.) notation, we derive the exact limits of CPU\ncomplexity of all vertex/edge iterators under arbitrary acyclic\norientations as graph size n\\to\\infty. These results are obtained in\nsimple closed form as functions of the degree distribution. This allows\nus to establish optimal orientations for all studied algorithms, compare\nthem to each other, and discover the best technique within each class.\n", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Di Xiao"
      }, 
      {
        "name": "Yi Cui"
      }, 
      {
        "name": "Daren Cline"
      }, 
      {
        "name": "Dmitri Loguinov"
      }
    ], 
    "type": "paper", 
    "id": "PODS38"
  }, 
  "SIGMOD124": {
    "title": "Online Deduplication for Databases", 
    "abstract": "SystemX is a similarity-based deduplication scheme for online\ndatabase management systems (DBMSs). Beyond block-level\ncompression of individual database pages or operation log (oplog)\nmessages, as used in today\u00cds DBMSs, SystemX uses byte-level\ndelta encoding of individual records within the database to achieve\ngreater savings. SystemX's online encoding method can be\nintegrated into the storage and logging components of a DBMS\nto provide two benefits: (1) reduced size of data stored on disk\nbeyond what traditional compression schemes provide, and (2) reduced\namount of data transmitted over the network for replication\nservices. To evaluate our work, we implemented SystemX in a distributed\nNoSQL DBMS and analyzed its properties using four real\ndatasets. Our results show that SystemX achieves up to 37 reduction\nin the storage size and replication traffic of the database on its\nown and up to 61 reduction when paired with the DBMS\u00cds block-level\ncompression. SystemX provides both benefits with negligible\neffect on DBMS throughput or client latency (average and tail).", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Lianghong Xu"
      }, 
      {
        "name": "Andrew Pavlo"
      }, 
      {
        "name": "Sudipta Sengupta"
      }, 
      {
        "name": "Gregory Ganger"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD124"
  }, 
  "SIGMOD430": {
    "title": "Optimization of Disjunctive Predicates for Main Memory Column Stores", 
    "abstract": "Optimization of disjunctive predicates is a very challenging task which has been vastly neglected by the research community, and commercial databases. In this work, we focus on the complex problem of optimizing disjunctive predicates by means of the bypass processing technique.\nIn bypass processing, selection operators split the input tuple stream into two disjoint output streams. The true-stream with tuples that satisfy the selection predicate and the false-stream with tuples that do not. Bypass processing is crucial in avoiding expensive predicates whenever the fate of the query can be determined by evaluating the less expensive ones. \n\nIn main memory databases, CPU architectural characteristics such as the branch misprediction penalty becomes a prominent cost factor which cannot be ignored. Our algorithm takes into account the branch misprediction penalty, and in addition, it eliminates common subexpressions.\n\nCurrent literature relies on two assumptions: (1) predicate costs are assumed to be constant, (2) predicate selectivities are assumed to be independent. Since both assumptions do not hold, our approach is not based on any of these assumptions.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Fisnik Kastrati"
      }, 
      {
        "name": "Guido Moerkotte"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD430"
  }, 
  "SIGMOD638": {
    "title": "Optimizing Iceberg Queries with Complex Joins", 
    "abstract": "Iceberg queries, commonly used for decision support, find groups whose\naggregate values are above or below a threshold.  In practice, iceberg\nqueries are often posed over complex joins that are expensive to\nevaluate.  This paper proposes a framework for combining a number of\ntechniques---a-priori, memoization, and pruning---to optimize iceberg\nqueries with complex joins.  A-priori pushes partial GROUP\nBY and HAVING condition before a join to reduce its\ninput size.  Memoization caches and reuses join computation results.\nPruning uses cached results to infer that certain tuples cannot\ncontribute to the final query result, and short-circuits join\ncomputation.  We formally derive conditions for correctly applying\nthese techniques.  Our practical rewrite algorithm produces highly\nefficient SQL that can exploit combinations of optimization\nopportunities in ways previously not possible.  We evaluate our\nPostgreSQL-based implementation experimentally and show that it\noutperforms both baseline PostgreSQL and a commercial database system.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Brett Walenz"
      }, 
      {
        "name": "Sudeepa Roy"
      }, 
      {
        "name": "Jun Yang"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD638"
  }, 
  "PODS85": {
    "title": "Output-optimal Parallel Algorithms for Similarity Joins", 
    "abstract": "Parallel join algorithms have received much attention in the recent years, due to the rapid development of massively parallel systems such as MapReduce and Spark.  In the database theory community, most efforts have been focused on studying worst-optimal algorithms.  However, the worst-case optimality of these join algorithms relies on the hard instances having very large output sizes.  In the case of a two-relation join, the hard instance is just a Cartesian product, with an output size that is quadratic in the input size.\n\n  In practice, however, the output size is usually much smaller.  One recent parallel join algorithm by Beame et al. has achieved {\\em output-optimality}, i.e., its cost is optimal in terms of both the input size and the output size, but their algorithm only works for a 2-relation equi-join, and has some imperfections. In this paper, we first improve their algorithm to true optimality.  Then we design output-optimal algorithms for a large class of similarity joins.  Finally, we present a lower bound, which essentially eliminates the possibility of having output-optimal algorithms for any join on more than two relations.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Xiao Hu"
      }, 
      {
        "name": "Yufei Tao"
      }, 
      {
        "name": "Ke Yi"
      }
    ], 
    "type": "paper", 
    "id": "PODS85"
  }, 
  "SIGMOD138": {
    "title": "Parallelizing Sequential Graph Computations", 
    "abstract": "This paper presents GRAPE, a parallel system for graph computations. GRAPE differs from previous systems in its ability to automatically parallelize existing sequential graph algorithms, without the need for recasting the algorithms into a new model. Underlying GRAPE are a simple programming model and a principled approach, based on partial evaluation and incremental computation. We show that sequential graph algorithms can be ``plugged into'' GRAPE and get parallelized.  As long as the sequential algorithms are correct, their GRAPE parallelization guarantees to terminate with correct answers under a monotonic condition.  Moreover, we show that algorithms in MapReduce, BSP and PRAM can be optimally simulated on GRAPE. In addition to the ease of programming, we experimentally verify that GRAPE achieves comparable performance to the state-of-the-art graph systems, using real-life and synthetic graphs.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Wenfei Fan"
      }, 
      {
        "name": "Jingbo Xu"
      }, 
      {
        "name": "Yinghui Wu"
      }, 
      {
        "name": "Jiaxin Jiang"
      }, 
      {
        "name": "Zeyu Zheng"
      }, 
      {
        "name": "Bohan Zhang"
      }, 
      {
        "name": "Yang Cao"
      }, 
      {
        "name": "Chao Tian"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD138"
  }, 
  "PODS42": {
    "title": "Private Incremental Regression", 
    "abstract": "Data is continuously generated by modern data sources, and a recent challenge in machine learning has been to develop techniques that perform well in an incremental (streaming) setting. A variety of offline machine learning tasks are known to be feasible under differential privacy, where generic construction exist that, given a large enough input sample, perform tasks such as PAC learning, Empirical Risk Minimization (ERM), regression, etc. In this paper, we investigate the problem of private machine learning, where as often in practice, the data is not given at once, but rather arrives incrementally over time.\n\nWe introduce the problems of \u00f1private incremental ERM\u00ee and \u00f1private incremental regression\u00ee where the general goal is to always maintain a good empirical risk minimizer for the history observed. Our first contribution is a generic transformation of private batch ERM mechanisms into private incremental ERM mechanisms, based on a simple idea of invoking the private batch ERM procedure at some regular time intervals. We take this construction as a baseline for comparison. We then provide two mechanisms for the private incremental regression problem. Our first mechanism is based on privately constructing a noisy incremental gradient function, which is then used in a modified projected gradient procedure at every timestep. This mechanism has an excess empirical risk of d^{1/2}, where d is the dimensionality of the data. While from the results of Bassily et al. [FOCS 2014] this bound is tight in the worst-case, we show that certain geometric properties of the input and constraint set can be used to derive significantly better results for certain interesting regression problems. Our second mechanism which achieves this is based on the idea of projecting the data to a lower dimensional space using random projections, and then adding privacy noise in this low dimensional space. The mechanism overcomes the issues of adaptivity inherent with the use of random projections in online streams, and uses recent developments in high-dimensional estimation to achieve an excess risk bound of T^{1/3} W^{2/3}, where T is the length of the stream and W is the sum of the Gaussian widths of the input domain and the constraint set that we optimize over.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Shiva Kasiviswanathan"
      }, 
      {
        "name": "Kobbi Nissim"
      }, 
      {
        "name": "Hongxia Jin"
      }
    ], 
    "type": "paper", 
    "id": "PODS42"
  }, 
  "SIGMOD452": {
    "title": "Pufferfish Privacy Mechanisms for Correlated Data", 
    "abstract": "Many modern database applications, such as those in healthcare and power management, include personal and sensitive correlated data, such as private information on users connected together in a social network, and measurements of physical activity of single subjects across time. However, the current gold standard of data privacy, differential privacy, does not adequately address privacy issues in this kind of data.\n\nThis work looks at a recent generalization of differential privacy, called Pufferfish, that can be used to address privacy in such data. The main challenge in applying Pufferfish is a lack of suitable mechanisms. In this paper, we provide the first general mechanism -- the Wasserstein Mechanism -- which applies to any Pufferfish framework. Since this mechanism may be computationally inefficient, we provide an additional mechanism that applies to some practical cases such as physical activity measurements across time, and is computationally efficient. Our experimental evaluations indicate that this mechanism provides privacy and utility for synthetic as well as real data in two different domains.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Shuang Song"
      }, 
      {
        "name": "Yizhen Wang"
      }, 
      {
        "name": "Kamalika Chaudhuri"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD452"
  }, 
  "SIGMOD155": {
    "title": "Pythia: Data Dependent Differentially Private Algorithm Selection", 
    "abstract": "Differential privacy has emerged as a preferred standard for ensuring privacy in analysis tasks on sensitive datasets.  Recent algorithms have allowed for significantly lower error by adapting to properties of the input data.  These so-called data-dependent algorithms have different error rates for different inputs.  There is now a complex and growing landscape of algorithms, without a clear winner that can offer low error over all datasets.  As a result, the best possible error rates are not attainable in practice, because the data curator cannot know which algorithm to select prior to actually running the algorithm.\n\nWe address this challenge by proposing a novel meta-algorithm designed to relieve the data curator of the burden of algorithm selection.  It works by learning (from non-sensitive data) the association between dataset properties and the best-performing algorithm.  The meta-algorithm is deployed by first testing the input for low-sensitivity properties and then using the results to select a good algorithm.  The result is an end-to-end differentially private system: Pythia, which we show that offers improvements over using any single algorithm alone.  We empirically demonstrate the benefit of Pythia for the tasks of  releasing histograms, answering 1- and 2-dimensional range queries, as well as for constructing private Naive Bayes classifiers.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Ios Kotsogiannis"
      }, 
      {
        "name": "Ashwin Machanavajjhala"
      }, 
      {
        "name": "Gerome Miklau"
      }, 
      {
        "name": "Michael Hay"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD155"
  }, 
  "SIGMOD32": {
    "title": "QFix: Diagnosing errors through query histories", 
    "abstract": "Data-driven applications rely on the correctness of their data to function properly and effectively. Errors in data can be incredibly costly and disruptive, leading to loss of revenue, incorrect conclusions, and misguided policy decisions. While data cleaning tools can purge datasets of many errors before the data is used, applications and users interacting with the data can introduce new errors. Subsequent valid updates can obscure these errors and propagate them through the dataset causing more discrepancies. Even when some of these discrepancies are discovered, they are often corrected superficially, on a case-by-case basis, further obscuring the true underlying cause, and making detection of the remaining errors harder.\nIn this paper, we propose QFix, a framework that derives explanations and repairs for discrepancies in relational data, by analyzing the effect of queries that operated on the data and identifying potential mistakes in those queries. QFix is flexible, handling scenarios where only a subset of the true discrepancies is known, and robust to different types of update workloads. We make four important contributions:  (a) we formalize the problem of diagnosing the causes of data errors based on the queries that operated on and introduced errors to a dataset; (b) we develop exact methods for deriving diagnoses and fixes for identified errors using state-of-the-art tools; (c) we present several optimization techniques that improve our basic approach without compromising accuracy, and (d) we leverage a tradeoff between accuracy and performance to scale diagnosis to large datasets and query logs, while achieving near-optimal results. We demonstrate the effectiveness of QFix through extensive evaluation over benchmark and synthetic data.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Eugene Wu"
      }, 
      {
        "name": "Xiaolan Wang"
      }, 
      {
        "name": "Alexandra Meliou"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD32"
  }, 
  "SIGMOD387": {
    "title": "QIRANA: a Framework for Scalable Query Pricing", 
    "abstract": "Users are increasingly engaging in the buying and selling of data over the web. Facilitated by proliferation of online marketplaces that bring such users together, data brokers need to serve requests where they provide results for user queries over the underlying  datasets, and price them fairly according to the information disclosed by the query. \n\nIn this work, we present a novel pricing system, called Qirana, that performs query-based data pricing for a large class of SQL queries (including aggregations and join) in real time. Qirana provides prices with formal guarantees: for example, it avoids prices that will create arbitrage opportunities. Our framework also allows flexible pricing, by allowing the data seller to choose from a variety of pricing functions, as well as specify relation and attribute-level parameters that control the price of queries and assign different value to different portions of the data. \n\nWe test Qirana on a variety of real-world datasets and query workloads. Our experiments show that our pricing system can efficiently compute the prices for queries large-scale data.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Shaleen Deep"
      }, 
      {
        "name": "Paraschos Koutris"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD387"
  }, 
  "SIGMOD99": {
    "title": "QUILTS: Multidimensional Data Partitioning Based on Query Aware, Skew Tolerant Space Filling Curves", 
    "abstract": "Recent days, it is getting popular to collect massive amount of data\nand analyze them. In decade, MapReduce has been a common ap-\nproach for such data analysis. However, since typical data analysis\nrequires only a part of collected data, MapReduce incurrs unnec-\nessary I/O overhead due to its parallelized full scan approach. To\nreduce the I/O cost, data management methods such as data skip-\nping and horizontal data partitioning are considered.\nA space-filling curve based data partitioning is known as a method\nfor multidimensional data. It is widely applied to a range-partitioned\nkey-value store such as HBase, a distributed data infrastructure.\nThe method constructs a key from a multidimensional data, and\ndistributes the data according to the key range. The query perfor-\nmance highly depends on the key design. Therefore, it is critical to\ndesign an appropriate key to maximize the query performance by\nfitting a query pattern and a data distribution pattern.\nWe abstract the problem as a curve selection to minimize page\naccess. We design a model to identify the characteristics of space-\nfilling curve on a query pattern and a data density. The model re-\nveals that the composite index (the C-curve) and the Z-curve have\nsymmetric characteristics on a query pattern and a data density. We\ndevise a method to synthesize a new curve that inherits a hybrid\ncharacteristics from the C-curve and the Z-curve in order to fit a\nquery pattern and to tolerate a skewed data distribution.\nFinally, we propose QUILTS, a framework of multidimensional\ndata partitioning based on the proposed curve. We implemented\nthe framework on top of HBase, and conducted experiments with\na skewed dataset. We confirmed that QUILTS reduced the amount\nof page access by quarter for a DWH application and a GIS application with real-world data.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Shoji Nishimura"
      }, 
      {
        "name": "Haruo Yokota"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD99"
  }, 
  "SIGMOD380": {
    "title": "QuadrillionG: A Quadrillion-scale Synthetic Graph Generator using a Recursive Vector Model", 
    "abstract": "As many applications encounter exponential growth in graph sizes, the scalability of graph processing methods has become more important than ever before. However, sharing large-scale real-world graphs is very limited since they are proprietary data, or practically impossible to collect. Thus, the methods for generating realistic graphs are critical for developing better graph algorithms or systems. Although there have been proposed a number of methods to generate synthetic graphs, they are not very efficient in terms of space and time complexities, and so, cannot generate even trillion-scale graphs using a moderate size cluster of commodity machines. Here, we propose an efficient and scalable disk-based graph generator, QuadrillionG that can generate massive graphs in a short time only using a small amount of memory. It can generate a graph of trillion edges following RMAT or Kronecker models within two hours only using 10 PCs, and a graph of quadrillion edges in about 200 hours using 100 PCs in theory. We first generalize existing graph generation models to the scope-based generation model, where RMAT and Kronecker correspond to two extremes. Then, we propose a new graph generation model called the recursive vector model, which compromises two extremes, and so, solves the space and time complexity problems existing in RMAT and Kronecker. We also present the QuadrillionG system that follows the recursive vector model and generates more ``realistic'' graphs by adding random noise. Through extensive experiments, we have demonstrated that QuadrillionG outperforms the state-of-the-art graph generators by orders of magnitude.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Himchan Park"
      }, 
      {
        "name": "Min-Soo Kim"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD380"
  }, 
  "SIGMOD630": {
    "title": "Query Centric Partitioning and Allocation for Partially Replicated Database Systems", 
    "abstract": "A key feature of database systems is to provide transparent access to stored data. In distributed database systems, this includes data allocation and fragmentation. Transparent access introduces data dependencies and increases system complexity and inter-process communication. Therefore, many developers and deployments are exchanging this transparency for better scalability using sharding and similar techniques. However, explicitly managing data distribution and data flow requires a deep understanding of the distributed system and the data access and reduces the possibilities for optimizations. \n\nTo address this problem, we present an approach for efficient data allocation that features good scalability while keeping the data distribution transparent. We propose a workload-aware, query-centric, heterogeneity-aware analytical model. We formalize our approach and present an efficient allocation algorithm. The algorithm optimizes the partitioning and data layout for local query execution and balances the workload on homogeneous and heterogeneous systems according to the query history. In our evaluation, we demonstrate that our approach scales well in performance for OLTP and OLAP style workloads and reduces storage requirements significantly over replicated systems while guaranteeing configurable availability.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Tilmann Rabl"
      }, 
      {
        "name": "Hans-Arno Jacobsen"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD630"
  }, 
  "PODS94": {
    "title": "Querying Probabilistic Preferences in Databases", 
    "abstract": "We propose a novel framework wherein probabilistic preferences can be naturally represented and analyzed in a probabilistic relational database. The framework augments the relational schema with a special type of a relation symbol -- a preference symbol. A deterministic instance of this symbol holds a collection of binary relations. Abstractly, the probabilistic variant is a probability space over databases of the augmented form (i.e., probabilistic database). Effectively, each instance of a preference symbol can be represented as a collection of parametric preference distributions such as Mallows. We establish positive and negative complexity results for evaluating Conjunctive Queries (CQs) over databases where preferences are represented in the Repeated Insertion Model (RIM), Mallows being a special case. We show how CQ evaluation reduces to a novel inference problem (of independent interest) over RIM, and devise a solver with polynomial data complexity.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Batya Kenig"
      }, 
      {
        "name": "Benny Kimelfeld"
      }, 
      {
        "name": "Haoyue Ping"
      }, 
      {
        "name": "Julia Stoyanovich"
      }
    ], 
    "type": "paper", 
    "id": "PODS94"
  }, 
  "SIGMOD407": {
    "title": "ROBUS: Fair Cache Allocation for Data-parallel Workloads", 
    "abstract": "Systems for processing big data---e.g., Hadoop, Spark, and massively parallel databases---need to run workloads on behalf of multiple tenants simultaneously. The abundant disk-based storage in these systems is usually complemented by a smaller, but much faster, {\\em cache}. Cache is a precious resource: Tenants who get to use the cache can see two orders of magnitude performance improvement. Cache is also a limited and hence shared resource: Unlike a resource like a CPU core which can be used by only one tenant at a time, a cached data item can be accessed by multiple tenants at the same time. Cache, therefore, has to be shared by a multi-tenancy-aware policy across tenants, each having a unique set of priorities and workload characteristics. \n\nIn this paper, we develop cache allocation strategies that speed up the overall workload while being {\\em fair} to each tenant. We build a novel fairness model targeted at the shared resource setting that incorporates not only the more standard concepts of Pareto-efficiency and sharing incentive, but we also define envy freeness via the notion of {\\em core} from cooperative game theory. Our cache management platform, ROBUS, uses randomization over small time batches, and we develop a proportionally fair allocation mechanism that satisfies the core property in expectation. We show that this algorithm and related fair algorithms can be approximated to arbitrary precision in polynomial time. We evaluate these algorithms on a ROBUS prototype implemented on Spark with RDD store used as cache. Our evaluation on an industry-standard workload shows that our algorithms score high on both performance and fairness metrics across a wide variety of practical multi-tenant setups.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Mayuresh Kunjir"
      }, 
      {
        "name": "Brandon Fain"
      }, 
      {
        "name": "Kamesh Munagala"
      }, 
      {
        "name": "Shivnath Babu"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD407"
  }, 
  "SIGMOD6": {
    "title": "Repairing Transaction Conflicts in Optimistic Multi-Version Concurrency Control", 
    "abstract": "The optimistic variants of Multi-Version Concurrency Control (MVCC) avoid blocking concurrent transactions at the cost of having a validation phase. Upon failure in the validation phase, the transaction is usually aborted and restarted from scratch. The \"abort and restart\" approach becomes a performance bottleneck for use cases with high contention objects or long running transactions. In addition, restarting from scratch creates a negative feedback loop in the system, because the system incurs additional overhead that may create even more conflicts.\n\nIn this paper, we propose a novel approach for conflict resolution in MVCC for in-memory databases. This low overhead approach summarizes the transaction programs in the form of a dependency graph. The dependency graph also contains the constructs used in the validation phase of the MVCC algorithm. Then, when encountering conflicts among transactions, our mechanism quickly detects the conflict locations in the program and partially re-executes the conflicting transactions. This approach maximizes the reuse of the computations done in the initial execution round, and increases the transaction processing throughput.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Mohammad Dashti"
      }, 
      {
        "name": "Sachin Basil John"
      }, 
      {
        "name": "Amir Shaikhha"
      }, 
      {
        "name": "Christoph Koch"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD6"
  }, 
  "PODS96": {
    "title": "Reverse Engineering SPJ-Queries from Examples", 
    "abstract": "This paper investigates the problem of reverse engineering, i.e., learning, select-project-join  (SPJ) queries  from a user-provided example set, containing positive and negative tuples. The goal is then to determine whether there exists  a query returning all the positive tuples, but none of the negative tuples, and furthermore, to find such a query, if it exists. These are called the satisfiability and learning problems, respectively. The ability to solve these problems is an important step in simplifying the querying process for non-expert users. \n\nThis paper thoroughly investigates the satisfiability and learning problems in a variety of settings. In particular, we consider several classes of queries, which allow different combinations of the operators select, project and join. In addition, we compare the complexity of satisfiability and learning, when the query is, or is not, of bounded size. We note that bounded-size queries are of particular interest, as they can be used to avoid over-fitting. \n\nIn order to fully understand the underlying factors which make satisfiability and learning (in)tractable, we consider different components of the problem, namely, the size of a query to be learned, the size of the schema and the number of examples. We study the complexity of our problems, when considering these  as part of the input,  as constants or as parameters (i.e., as in parameterized complexity analysis). Depending on the setting, the complexity of satisfiability and learning can vary significantly. Among other results, our analysis also provides new problems that are complete for W[3], for which few natural problems are known. Finally, by considering a variety of settings, we derive insight on how the different facets of our problem interplay with the size of the database, thereby providing the theoretical foundations necessary for a future implementation of query learning from examples.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Yaacov Y. Weiss"
      }, 
      {
        "name": "Sara Cohen"
      }
    ], 
    "type": "paper", 
    "id": "PODS96"
  }, 
  "SIGMOD245": {
    "title": "Revisiting Reuse in Main Memory Database Systems", 
    "abstract": "Reusing intermediates in databases to speed-up analytical query processing has been studied in the past.\nExisting solutions typically require intermediate results of individual operators be materialized into temporary tables to be considered for reuse in subsequent queries. However, these approaches are fundamentally ill-suited for use in modern main memory databases. The reason is that modern main memory DBMSs are typically limited by the bandwidth of the memory bus and query execution is thus heavily optimized to keep tuples in the CPU caches and registers. To that end, adding additional materialization operations into a query plan not only add additional traffic to the memory bus but more importantly prevent the important cache- and register-locality, which results in high performance penalties.\n\nIn this paper we study a novel reuse model for intermediates, which cached internal physical data structures materialized during query processing (due to pipeline breakers) and externalizes them so that they become reusable. We focus on hash tables, the most commonly used internal data structures in main memory databases to perform join and aggregation operations. As queries arrive, our reuse-aware optimizer reasons about the reuse opportunities for hash tables, employing cost models that take into account hash table statistics as well as the CPU and data movement costs within the cache hierarchy. Experimental results, based on our HashStash prototype demonstrate performance gains of 2x for typical analytical workloads with no additional overhead for materializing intermediates.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Kayhan Dursun"
      }, 
      {
        "name": "Carsten Binnig"
      }, 
      {
        "name": "Ugur Cetintemel"
      }, 
      {
        "name": "Tim Kraska"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD245"
  }, 
  "SIGMOD199": {
    "title": "SLiMFast: Guaranteed Results for Data Fusion and Source Reliability", 
    "abstract": "We focus on data fusion, i.e., the problem of unifying conflicting data from data sources into a single representation by estimating the source accuracies. We propose SLiMFast, a framework that expresses data fusion as a statistical learning problem over discriminative probabilistic models, which in many cases correspond to logistic regression. In contrast to previous approaches that use complex generative models, discriminative models make fewer distributional assumptions over data sources and allow us to obtain rigorous theoretical guarantees. Furthermore, we show how SLiMFast enables incorporating domain knowledge into data fusion, yielding accuracy improvements of up to 50% over state-of-the-art baselines. Building upon our theoretical results, we design an optimizer that obviates the need for users to manually select an algorithm for learning SLiMFast's parameters. We validate our optimizer on multiple real-world datasets and show that it can accurately predict the learning algorithm that yields the best data fusion results.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Theodoros Rekatsinas"
      }, 
      {
        "name": "Manas Joglekar"
      }, 
      {
        "name": "Hector Garcia-Molina"
      }, 
      {
        "name": "Aditya Parameswaran"
      }, 
      {
        "name": "Christopher Re"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD199"
  }, 
  "SIGMOD414": {
    "title": "Scaling Locally Linear Embedding", 
    "abstract": "Locally Linear Embedding (LLE) is a popular approach to dimensionality reduction as it can effectively represent nonlinear structures of high-dimensional data.  \nFor dimensionality reduction, it computes a nearest neighbor graph from a given dataset where edge weights are obtained by applying the Lagrange multiplier method, and it then computes eigenvectors of the LLE kernel where the edge weights are used to obtain the kernel. \nAlthough LLE is used in many applications, its computation cost is significantly high. \nThis is because, in obtaining edge weights, its computation cost is cubic in the number of edges to each data point. \nIn addition, the computation cost in obtaining the eigenvectors of the LLE kernel is cubic in the number of data points. \nOur approach, Ripple, is based on two ideas: \n(1) it incrementally updates the edge weights by exploiting the Woodbury formula and \n(2) it efficiently computes eigenvectors of the LLE kernel by exploiting the LU decomposition-based inverse power method. \nExperiments show that Ripple is significantly faster than the original approach of LLE by guaranteeing the same results of dimensionality reduction.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Yasuhiro Fujiwara"
      }, 
      {
        "name": "Naoki Marumo"
      }, 
      {
        "name": "Mathieu Blondel"
      }, 
      {
        "name": "Koh Takeuchi"
      }, 
      {
        "name": "Hideaki Kim"
      }, 
      {
        "name": "Iwata Tomoharu"
      }, 
      {
        "name": "Naonori Ueda"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD414"
  }, 
  "SIGMOD284": {
    "title": "Scaling Up Hands-Off Crowdsourced Entity Matching: An RDBMS Approach", 
    "abstract": "Many works have applied crowdsourcing to entity matching (EM). While promising, these approaches are limited in that they often require a developer to be in the loop. As such, it is difficult for an organization to deploy multiple crowdsourced EM solutions, because there are simply not enough developers. To address this problem, a recent work has proposed Corleone, a solution that crowdsources the entire EM workflow, requiring no developers. While promising, Corleone is severely limited in that it does not scale to large tables. We propose Falcon, a solution that scales up the hands-off crowdsourced EM approach of Corleone, using RDBMS-style query execution and optimization over a Hadoop cluster. Specifically, we define a set of operators and develop efficient implementations. We translate a hands-off crowdsourced EM workflow into a plan consisting of these operators, optimize, then execute the plan. These plans involve both machine and crowd activities, giving rise to novel optimization techniques such as using crowd time to mask machine time. Extensive experiments show that Falcon can scale up to tables of millions of tuples, thus providing a practical solution for hands-off crowdsourced EM.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Sanjib Das"
      }, 
      {
        "name": "Paul Suganthan G. C."
      }, 
      {
        "name": "Anhai Doan"
      }, 
      {
        "name": "Jeffrey Naughton"
      }, 
      {
        "name": "Ganesh Krishnan"
      }, 
      {
        "name": "Rohit Deep"
      }, 
      {
        "name": "Esteban Arcaute"
      }, 
      {
        "name": "Vijay Raghavendra"
      }, 
      {
        "name": "Youngchoon Park"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD284"
  }, 
  "SIGMOD28": {
    "title": "Schema Independent Relational Learning", 
    "abstract": "Learning novel relations from relational databases is an important problem with many applications in database systems and machine learning. Relational learning algorithms leverage the properties of the database schema to find the definition of the target relation in terms of the existing relations in the database. Nevertheless, the same data set may be represented under different schemas for various reasons, such as efficiency, data quality, and usability. Unfortunately, the output of current relational learning algorithms tends to vary quite substantially over the choice of schema, both in terms of learning accuracy and efficiency. This variation complicates their off-the-shelf application. In this paper, we introduce and formalize the property of schema independence of relational learning algorithms, and study both the theoretical and empirical dependence of existing algorithms on the common class of (de)composition schema transformations. We prove that current relational learning algorithms are generally not schema independent. We present Castor, a relational learning algorithm that achieves schema independence by leveraging data dependencies. We support the theoretical results with an empirical study that demonstrates the schema dependence/independence of several algorithms on existing benchmark and real-world data sets under natural (de)compositions.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Jose Picado"
      }, 
      {
        "name": "Arash Termehchy"
      }, 
      {
        "name": "Alan Fern"
      }, 
      {
        "name": "Parisa Ataei"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD28"
  }, 
  "PODS98": {
    "title": "Schema Mappings for Data Graphs", 
    "abstract": "Schema mappings are a fundamental concept in data integration and\nexchange, and they have been thoroughly studied in different data\nmodels. For graph data, however, mappings have been studied in a very\nrestricted context that, unlike real-life graph databases, completely\ndisregards the data they store.  Our main goal is to understand query\nanswering under graph schema mappings - in particular, in exchange and\nintegration of graph data - for graph databases that mix graph\nstructure with data. We show that adding data querying alters the\npicture in a very significant way.\n\nAs the model, we use data graphs: a theoretical abstraction of\nproperty graphs employed by graph database implementations. We start\nby showing a very strong negative result: using the simplest form of\nnontrivial navigation in mappings makes answering even simple queries\nthat mix navigation and data undecidable. This result suggests that\nfor the purposes of integration and exchange, schema mappings ought to\nexclude recursively defined navigation over target data.  For such\nmappings and analogs of regular path queries that take data into\naccount, query answering becomes decidable, although intractable. To\nrestore tractability without imposing further restrictions on queries,\nwe propose a new approach based on the use of null values that\nresemble usual nulls of relational DBMSs, as opposed to marked nulls\none typically uses in integration and exchange tasks.  If one moves\naway from path queries and considers more complex patterns, query\nanswering becomes undecidable again, even for the simplest possible\nmappings.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Nadime Francis"
      }, 
      {
        "name": "Leonid Libkin"
      }
    ], 
    "type": "paper", 
    "id": "PODS98"
  }, 
  "SIGMOD534": {
    "title": "Solving the Join Ordering Problem via Mixed Integer Linear Programming", 
    "abstract": "We transform join ordering into a mixed integer linear program (MILP). This allows to address query optimization by mature MILP solver implementations that have evolved over decades and steadily improved their performance. They offer features such as anytime optimization and parallel search that are highly relevant for query optimization. \n\nWe present a MILP formulation for searching left-deep query plans. We use sets of binary variables to represent join operands and intermediate results, operator implementation choices or the presence of interesting orders. Linear constraints restrict value assignments to the ones representing valid query plans. We approximate the cost of scan and join operations via linear functions, allowing to increase approximation precision up to arbitrary degrees. We integrated a prototypical implementation of our approach into the Postgres optimizer and compare against the original optimizer and several variants. Our experimental results are encouraging: we are able to optimize queries joining 40 tables within less than one minute of optimization time. Such query sizes are far beyond the capabilities of traditional query optimization algorithms with worst case guarantees on plan quality. Furthermore, as we use an existing solver, our optimizer implementation is small and can be integrated with low overhead.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Immanuel Trummer"
      }, 
      {
        "name": "Christoph Koch"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD534"
  }, 
  "PODS41": {
    "title": "Stable Model Semantics for Tuple-Generating Dependencies Revisited", 
    "abstract": "Normal tuple-generating dependencies (NTGDs) are TGDs enriched with default negation, a.k.a. negation as failure. Query answering under NTGDs, where negation is interpreted according to the stable model semantics, is an intriguing new problem that gave rise to flourishing research activity in the database and KR communities. So far, all the existing works that investigate this problem, except for one recent paper by Baget et al., follow the so-called logic programming (LP) approach, that is, the existentially quantified variables are first eliminated via Skolemization, which leads to a normal logic program, and then the standard stable model semantics for normal logic programs is applied. However, as we thoroughly discuss in the paper, Skolemization is not appropriate in the presence of default negation since it fails to capture the intended meaning of NTGDs. This reveals the need to adopt an alternative approach to stable model semantics that is directly applicable to NTGDs with existentially quantified variables. As we explain in the paper, the semantics by Baget et al. inherits the limitations of the LP approach, and thus, it fails to capture the intended meaning of the existentially quantified variables.\n\nWe propose a new approach to stable model semantics, based on a recent characterization of stable models in terms of second-order logic, which overcomes the limitations of the LP approach. In particular, the stable models of a database and a set of NTGDs are defined as the classical models of a second-order formula, which encodes the vital properties of stable model semantics. We then perform an in-depth analysis of the complexity of conjunctive query answering under prominent classes of NTGDs based on the main decidability paradigms for TGDs, namely (weak-)acyclicity, guardedness and stickiness. For (weakly-)acyclic NTGDs we show that the problem remains decidable, and we establish complexity results ranging from coNP via \\Pi_{2}^{P} to coN2EXPTIME with an NP oracle, pointing out several interesting effects. In the case of guardedness and stickiness the problem becomes undecidable, which indicates that in the presence of default negation more refined notions are needed. Although for sticky NTGDs this was expected, for guarded NTGDs this is a surprising outcome. Finally, we show that weakly-acyclic NTGDs give rise to highly expressive query languages that express exactly the queries with \\Pi_{2}^{P} (resp., \\Sigma_{2}^{P}) data complexity when the cautious (resp., brave) semantics is adopted. Notably, these languages allow us to solve in a declarative way central problems that lie at the second level of the polynomial hierarchy, e.g., consistent query answering under weakly-acyclic TGDs w.r.t. set-based repairs.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Mario Alviano"
      }, 
      {
        "name": "Michael Morak"
      }, 
      {
        "name": "Andreas Pieris"
      }
    ], 
    "type": "paper", 
    "id": "PODS41"
  }, 
  "SIGMOD133": {
    "title": "Staging User Feedback toward Rapid Conflict Resolution in Data Fusion", 
    "abstract": "In domains such as the Web, sensor networks and social media, sources often provide conflicting information for the same data item. Several data fusion techniques have been proposed recently to resolve conflicts and identify correct data. The performance of these fusion systems, while quite accurate, is far from perfect. In this paper, we propose to leverage user feedback for validating data conflicts and rapidly improving the performance of fusion. To present the most beneficial data items for the user to validate, we take advantage of the level of consensus among sources, and the output of fusion to generate an effective ordering of items. We first evaluate data items individually, and then define a novel decision-theoretic framework based on the concept of value of perfect information (VPI) to order items by their ability to boost the performance of fusion. We further derive approximate formulae to scale up the decision-theoretic framework to large-scale data. We empirically evaluate our algorithms on two real-world datasets with different characteristics, and show that the accuracy of fusion can be significantly improved even while requesting feedback on a few data items. We also show that the performance of the proposed methods depends on the structure of the dataset, and assess the trade-off between the amount of feedback acquired, and the effectiveness and efficiency of the methods.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Romila Pradhan"
      }, 
      {
        "name": "Siarhei Bykau"
      }, 
      {
        "name": "Sunil Prabhakar"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD133"
  }, 
  "PODS126": {
    "title": "Streaming Algorithms for Measuring H-Impact", 
    "abstract": "We consider publication settings with positive user feedback, such as, users publishing tweets and other users retweeting them, friends posting photos and others liking them or even authors publishing research papers and others citing these publications. A well-accepted notion of \u00f1impact\u00ee for users in these settings is the H-Index, which is the largest k such that at least k publications have k or more (positive) feedback.\nWe study how to calculate H-index on large streams of user publications and feedback. If all the items can be stored, H-index of a user can be computed by sorting. We focus on the streaming setting where as is typical, we do not have space to store all the items.\nWe present the first known streaming algorithm for computing the H-index of a user in the cash register streaming model using space poly(1/_, log(1/_), log n); this algorithm provides an additive _ approximation. For the aggregated model where feedback for a publication is collated, we present streaming algorithms that use much less space, either only dependent on _ and even a small constant. We also address the problem of finding \u00f1heavy hitters\u00ee users in H-index without estimating everyones\u00cd H-index. We present randomized streaming algorithms for finding 1 + _ approximation to heavy hitters that uses space poly(1/_, log(1/_), log n) and succeeds with probability at least 1 _ _. Again, this is the first sublinear space algorithm for this problem, despite extensive research on heavy hitters in general. Our work initiates study of streaming algorithms for problems that estimate impact or identify impactful users.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Priya Govindan"
      }, 
      {
        "name": "Morteza Monemizadeh"
      }, 
      {
        "name": "S Muthukrishnan"
      }
    ], 
    "type": "paper", 
    "id": "PODS126"
  }, 
  "SIGMOD363": {
    "title": "Synthesizing Mapping Relationship Using Table Corpus", 
    "abstract": "Mapping relationships, such as (country, country-code) or (company, stock-ticker), are valuable data assets for an array of applications in data cleaning and data integration such as auto-correction and auto-join. However, today there is no good repository of mapping tables that can enable these intelligent applications.\n\nGiven a corpus of tables such as web tables or spreadsheet tables, we observe that values of such mapping often exist in two columns in same tables. Motivated by the broad applicability of mapping relationships, we study the problem of producing such relationships by synthesizing tables in a large corpus. Our synthesis process leverages compatibility based on co-occurrence statistics, as well as constraints such as functional dependency that are required for the resulting mapping relationships to ensure high quality. Experiment results using a corpus of web tables and a corpus of enterprise spreadsheets suggest that the proposed approach can synthesize high-quality mapping relationships.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Yue Wang"
      }, 
      {
        "name": "Yeye He"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD363"
  }, 
  "SIGMOD306": {
    "title": "Template Skycube Algorithms for Heterogeneous Parallelism on Multicore and GPU Architectures", 
    "abstract": "The skyline of a dataset is a subset containing only the points that competitively balance attribute values, greatly aiding multi-criteria decision making. The skycube materialises an exponential number of skylines, providing fast skyline response times, but it is intensive to compute. This paper proposes parallel algorithms for building skycubes on multicore CPUs, GPUs, and heterogeneous systems containing both. To tackle the challenges of heterogeneous parallelism, we present three template algorithms that are portable, yet architecture-aware. Two algorithms leverage insights from previous skycube research, whereas the third utilises a novel point-based paradigm to expose more data parallelism. An experimental study shows that our algorithms provide an order of magnitude improvement over the previous (sequential) state-of-the-art on either architecture and proportionately improve as more GPUs are added.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Kenneth S. B\u00c0gh"
      }, 
      {
        "name": "Sean Chester"
      }, 
      {
        "name": "Darius _idlauskas"
      }, 
      {
        "name": "Ira Assent"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD306"
  }, 
  "SIGMOD116": {
    "title": "The BUDS Language for Distributed Machine Learning", 
    "abstract": "We describe BUDS, a declarative language for succinctly and simply specifying the implementation of large-scale machine learning algorithms on a distributed computing platform. The types supported in BUDS_vectors, arrays, etc._are simply logical abstractions useful for programming, and do not correspond to the actual implementation. In fact, BUDS automatically chooses the physical realization of these abstractions in a distributed system, by taking into account the characteristics of the data. Likewise, there are many available implementations of the abstract operations offered by BUDS (matrix multiplies, transposes, Hadamard products, etc.). These are tightly coupled with the physical representation. In BUDS, these implementations are co-optimized along with the representation. All of this allows for the BUDS compiler to automatically perform deep optimizations of the user\u00cds program, and automatically generate efficient implementations.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Luis Perez"
      }, 
      {
        "name": "Zekai Gao"
      }, 
      {
        "name": "Shangyu Luo"
      }, 
      {
        "name": "Chris Jermaine"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD116"
  }, 
  "PODS39": {
    "title": "The Complexity of Ontology-Based Data Access with OWL2QL and Bounded Treewidth Queries", 
    "abstract": "Our concern is the overhead of answering OWL 2 QL ontology-mediated queries (OMQs) in ontology-based data access compared to evaluating their underlying tree-shaped and bounded treewidth conjunctive queries (CQs). We show that OMQs with bounded-depth ontologies have nonrecursive datalog (NDL) rewritings that can be constructed and evaluated in LOGCFL for combined complexity, even in NL if their CQs are tree-shaped with a bounded number of leaves, and so incur no overhead in complexity-theoretic terms. For OMQs with arbitrary ontologies and bounded-leaf CQs, NDL-rewritings are constructed and evaluated in LOGCFL. We show experimentally feasibility and scalability of our rewritings compared to standard NDL-rewritings. On the negative side, we prove that answering OMQs with tree-shaped CQs is not _xed-parameter tractable if the ontology depth or the number of leaves in the CQs is regarded as the parameter, and that answering OMQs with a _xed ontology (of in_nite depth) is NP-complete for tree-shaped and LOGCFL for bounded-leaf CQs. ", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Meghyn Bienvenu"
      }, 
      {
        "name": "Stanislav Kikot"
      }, 
      {
        "name": "Roman Kontchakov"
      }, 
      {
        "name": "Vladimir Podolskii"
      }, 
      {
        "name": "Vladislav Ryzhikov"
      }, 
      {
        "name": "Michael Zakharyaschev"
      }
    ], 
    "type": "paper", 
    "id": "PODS39"
  }, 
  "SIGMOD471": {
    "title": "The Dynamic Yannakakis Algorithm: Compact and Efficient Query Processing Under Updates", 
    "abstract": "Modern computing tasks such as real-time analytics require constant refreshing of query results under high update rates. Incremental View Maintenance (IVM) approaches this problem by materializing results in order to avoid re-computations. IVM naturally induces a trade-off between the space needed to maintain the materialized results and the time used to process updates.\n\n\tIn this paper we show that the full materialization of results is a barrier for more general optimization strategies. In particular, we present a new approach for evaluating queries under updates by dropping the materialization of results. Instead, we require a data structure that features both efficient maintenance under updates and constant-delay enumeration of the output, while using only linear space in the size of the database. We call such a structure a Dynamic Constant-delay Linear Representation (DCLR). We show that DYN, a dynamic version of the Yannakakis algorithm, yields DCLRs for the class of free-connex acyclic aggregate join queries, and prove that it naturally generalizes known worst-case optimal static algorithms. In addition, we identify a sub-class of queries for which DYN features constant-time update per tuple. We introduce a cost model for DCLRs and describe an associated compilation algorithm for the more general class of acyclic aggregate join queries.\n\n\tFinally, using the industry-standard benchmarks TPC-H and TPC-DS, we present an experimental comparison between DYN and a higher-order IVM (HIVM) engine. HIVM aggressively maintains intermediate results, optimizing speed as much as possible at the expense of high memory consumption. The experiments show that our approach is not only more efficient in terms of memory consumption (as expected), but also up to one order of magnitude faster in processing updates.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Muhammad Idris"
      }, 
      {
        "name": "Stijn Vansummeren"
      }, 
      {
        "name": "Martin Ugarte"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD471"
  }, 
  "PODS105": {
    "title": "Tight Space-Approximation Tradeoff for the Multi-Pass Streaming Set Cover Problem", 
    "abstract": "We study the classic set cover problem in the streaming model: the sets that comprise the instance are revealed one by one in a stream and the goal is to solve the problem by making one or few passes over the stream while maintaining a sublinear space o(mn) in the input size; here m denotes the number of the sets and n is the universe size. Notice that in this model we are mainly concerned with the space requirement of the algorithms and do not restrict their computation time.\n\t\nOur main result is a resolution of the space-approximation tradeoff for the streaming set cover problem: we show that any \\alpha-approximation algorithm for the set cover problem requires \\Omega(mn^{1/\\alpha}) space, even if it is allowed \\polylog{(n)} passes over the stream, and even if the sets are arriving in a random order in the stream.  This space-approximation tradeoff matches the best known bounds achieved by the recent algorithm of Har-Peled etal (PODS 2016) that requires only O(\\alpha) passes over the stream in an adversarial order, hence settling the space complexity of approximating the set cover problem in data streams in a quite robust manner.  Additionally, our approach yields tight lower bounds for the space complexity of (1-\\eps)-approximating the streaming maximum coverage problem studied in several recent works.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Sepehr Assadi"
      }
    ], 
    "type": "paper", 
    "id": "PODS105"
  }, 
  "SIGMOD20": {
    "title": "Two-Level Sampling for Join Size Estimation", 
    "abstract": "Join size estimation is a critical step in query optimization, and has been extensively studied in the literature. Among the many techniques, sampling based approaches are particularly appealing, due to their ability to handle arbitrary selection predicates. In this paper, we propose a new sampling algorithm for join size estimation, called two-level sampling, which combines the advantages of three previous sampling methods while making further improvements. Both analytical and empirical comparisons show that the new algorithm outperforms all the previous algorithms on a variety of joins, including primary key-foreign key joins, many-to-\nmany joins, and multi-table joins. The new sampling algorithm is also very easy to implement, requiring just one pass over the data. It only relies on some basic statistical information about the data, such as the `k-norms and the heavy hitters.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Yu Chen"
      }, 
      {
        "name": "Ke Yi"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD20"
  }, 
  "SIGMOD438": {
    "title": "UGuide - User-Guided Discovery of FD-Detectable Errors", 
    "abstract": "Error detection is to identify problematic data cells that are different from their ground truth. Functional dependencies (FDs) have been widely studied to detect data errors. Oftentimes, it is assumed that FDs are given by experts. Unfortunately, it is usually hard and expensive for the experts to define such FDs. In addition, automatic data profiling of dirty data in order to find correct FDs is known to be a hard problem. In this paper, we propose an end-to-end solution to detect FD-detectable errors from dirty data. The broad intuition is that given a dirty dataset, it is feasible to automatically find approximate FDs, as well as data that are possibly erroneous. Arguably, at this point, only experts can confirm true FDs or true errors. However, in practice, experts never have enough budget or knowledge to find all errors. Hence, our problem is, given a limited budget of expert\u00cds time, which question we should ask, either FDs or tuples, such that we can find as many data errors as possible. We present efficient algorithms to interact with the user. Extensive experiments demonstrate that our proposed framework is effective in detecting errors from dirty data.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Saravanan Thirumuruganathan"
      }, 
      {
        "name": "Laure Berti-Equille"
      }, 
      {
        "name": "Mourad Ouzzani"
      }, 
      {
        "name": "Jorge-Arnulfo Quiane-Ruiz"
      }, 
      {
        "name": "Nan Tang"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD438"
  }, 
  "SIGMOD129": {
    "title": "Utility Cost of Formal Privacy for Releasing National Employer-Employee Statistics", 
    "abstract": "National statistical agencies around the world publish tabular summaries based on combined employer-employee data. The privacy of both individuals and business establishments that feature in these data are protected by law in most countries. These data are currently released using a variety of statistical disclosure limitation (SDL) techniques that  do not reveal the exact characteristics of particular employers and employees, but fail to provide any provable privacy guarantees limiting inferential disclosures.\n\nIn this work, we present novel algorithms for releasing tabular summaries of linked employer-employee data with formal, provable guarantees of privacy. We show that state of the art differentially private algorithms add too much noise for the output to be useful. Instead, we identify the privacy requirements mandated by current interpretations of the relevant laws, and formalize them using the Pufferfish framework. We then develop new privacy definitions that are customized to employer-employee data and satisfy the statutory privacy requirements.  Our algorithms are currently being implemented on data gathered by a national statistical agency. An empirical evaluation of utility for these data shows that for reasonable values of the privacy-loss parameter epsilon at least 1, the additive error introduced by our provably private algorithms is comparable, and in some cases better, than the error introduced by existing SDL techniques that have no provable privacy guarantees. For some complex queries currently published, however, our algorithms do not have utility comparable to the existing traditional SDL algorithms. Those queries are fodder for future research.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Samuel Haney"
      }, 
      {
        "name": "Ashwin Machanavajjhala"
      }, 
      {
        "name": "John Abowd"
      }, 
      {
        "name": "Matthew Graham"
      }, 
      {
        "name": "Mark Kutzbach"
      }, 
      {
        "name": "Lars Vilhuber"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD129"
  }, 
  "SIGMOD356": {
    "title": "Utility-Aware Ridesharing on Road Networks", 
    "abstract": "Ridesharing enable drivers to share the empty seats to others such that the efficiency of the transportation can be improved and it can benefit both drivers and riders in earning and saving money respectively. Different from existing works on ridesharing, we consider that the satisfactions of riders (the utility values) is more important nowadays. Thus, we formulate a utility-aware ridesharing problem on road networks (URR), which finds an optimal vehicles arrangement and riders scheduling to maximize the summation of the utility values of riders while the pickup and drop-off deadlines of riders and capacity constraints of vehicles are met. To handle the case of scheduling a new rider request to a vehicle without reordering operations on existing sequences, we propose an exact algorithm with minimum traveling cost. We prove the URR problem is NP-hard by reducing from Knapsack problem. Therefore, we propose three efficient approximate algorithms, including a bilateral arrangement algorithm, an efficient greedy algorithm and a grouping-based scheduling algorithm, to arrange riders to suitable vehicles with high utility value increase and low traveling cost increase. Through extensive experiments, we demonstrate the efficiency and effectiveness of our URR processing approaches on both real and synthetic data sets.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Peng Cheng"
      }, 
      {
        "name": "Hao Xin"
      }, 
      {
        "name": "Lei Chen"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD356"
  }, 
  "SIGMOD67": {
    "title": "Waldo: An Adaptive Human Interface for Crowd Entity Resolution", 
    "abstract": "In Entity Resolution, the objective is to find which records of a dataset refer to the same real-world entity. Crowd Entity Resolution uses humans, in addition to machine algorithms, to improve the quality of the outcome. We study a hybrid approach that combines two common interfaces for human tasks in Crowd Entity Resolution, taking into account key observations about the advantages and disadvantages of the two interfaces. We give a formal definition to the problem of human task selection and we derive algorithms with strong optimality guarantees. Our experiments with four real-world datasets show that our hybrid approach gives an improvement of 50% to 300% in the crowd cost to resolve a dataset, compared to using a single interface.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Vasilis Verroios"
      }, 
      {
        "name": "Hector Garcia-Molina"
      }, 
      {
        "name": "Yannis Papakonstantinou"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD67"
  }, 
  "PODS55": {
    "title": "What do Shannon-type inequalities, submodular width, and disjunctive datalog have to do with one another?", 
    "abstract": "Recent works on bounding the output size of a conjunctive query with\nfunctional dependencies and degree bounds have shown a deep connection between\nfundamental questions in information theory and database theory.\nWe prove analogous output bounds for disjunctive datalog queries, and\nanswer several open questions regarding the tightness and looseness of these\nbounds along the way.\nThe bounds are intimately related to Shannon-type information inequalities.\nWe devise the notion of a \"proof sequence\" of a specific class of\nShannon-type information inequalities called \"Shannon flow inequalities\". \nWe then show how the proof sequence can be used as symbolic instructions \nto guide an algorithm called \"PANDA\", which answers disjunctive datalog queries \nwithin the size bound predicted.\nWe show that PANDA can be used as a black-box to devise algorithms matching\nprecisely the fractional hypertree width and the submodular width runtimes\nfor queries with functional dependencies and degree bounds.\n\nOur results improve upon known results in three ways. First, our bounds and\nalgorithms are for the much more general class of disjunctive datalog queries, \nof which conjunctive queries are a special case. \nSecond, the runtime of PANDA matches precisely the submodular width bound, \nwhile the previous algorithm by Marx has a runtime that is polynomial in this bound.\nThird, our bounds and algorithms work for queries with input cardinality bounds, \nfunctional dependencies, and degree bounds.\n\nOverall, our results showed a deep connection between three seemingly\nunrelated lines of research; and, our results on proof sequences for Shannon\nflow inequalities might be of independent interest.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Mahmoud Abo Khamis"
      }, 
      {
        "name": "Hung Ngo"
      }, 
      {
        "name": "Dan Suciu"
      }
    ], 
    "type": "paper", 
    "id": "PODS55"
  }, 
  "PODS118": {
    "title": "Write-Optimized Skip Lists", 
    "abstract": "The skip list is an elegant dictionary data structure that is commonly deployed\nin RAM-based systems.  A skip list with $N$ elements supports searches,\ninserts, and deletes in $O(\\log N)$ operations with high probability (w.h.p.)\nand range queries returning $K$ elements in $O(\\log N + K)$ operations w.h.p.\n\n\nA seemingly natural way generalize the skip list to external memory is to\n``promote'' with probability $1/B$ ($B$ is the block size), rather than $1/2$.\nHowever, there are practical and theoretical obstacles to getting the skip list\nto retain its high-probability guarantees, space bounds, and efficient\nperformance.  This paper explains how the theoretical approaches for retaining\nhigh-probability bounds in external memory can have additional performance\nbenefits.\n\n\nIn this paper, we give an external-memory skip list that achieves\nwrite-optimized bounds. That is, for $0 < \\varepsilon < 1$, range\nqueries take $O(\\log_{B^\\varepsilon} N + K/B)$ I/Os w.h.p. and insertions and\ndeletions take $O((\\log_{B^\\varepsilon} N) / B^{1-\\varepsilon})$ amortized I/Os\nw.h.p.\n\n\nOur write-optimized skip list inherits the virtue of simplicity from RAM skip\nlists. Moreover, it matches or beats the asymptotic bounds of prior write-optimized data\nstructures such as the $B^\\varepsilon$ tree or LSM trees, which have been successfully deployed in high-performance databases and file systems.\n \n\nThe structure has high probability search bounds  and high-probability\namortized insert/delete bounds.  To prove the insert/delete bounds, we\nestablish a strong lower bound on the number of root-to-leaf paths that are\ndisjoint outside of the cached region near the root.  The bound is general,\nholding as long as the cache is large enough to hold $\\Omega(1)$ levels of the\ndata structure.  We  use an  extremal-graph-coloring argument to show that\nthere is always a good partitioning of the paths into uncorrelated sets,\nregardless of the insertion and deletion pattern.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Michael Bender"
      }, 
      {
        "name": "Martin Farach-Colton"
      }, 
      {
        "name": "Rob Johnson"
      }, 
      {
        "name": "Simon Mauras"
      }, 
      {
        "name": "Tyler Mayer"
      }, 
      {
        "name": "Cynthia Phillips"
      }, 
      {
        "name": "Helen Xu"
      }
    ], 
    "type": "paper", 
    "id": "PODS118"
  }, 
  "SIGMOD373": {
    "title": "ZipG: A Memory-efficient Graph Store for Interactive Queries", 
    "abstract": "We present ZipG, a distributed graph store for serving interactive queries on graphs. ZipG builds upon two key ideas. First, it uses a simple and intuitive layout for storing the input graph as a flat (unstructured) file. This layout admits efficient graph compression, and yet enables efficient query execution directly on compressed graph data. ZipG thus executes a significantly larger fraction of queries in main memory than existing graph stores, achieving performance gains. Second, ZipG uses the idea of fanned updates to efficiently overcome the challenges of updating compressed flat files; a set of update pointers allow ZipG to handle high write rates using log-structured storage while touching partitions necessary for query execution.\n\nWe evaluate the performance of ZipG against several popular open-source graph stores for Facebook TAO, LinkBench and graph search workloads. On a single server with 244GB memory, ZipG executes tens of thousands of queries from all these workloads for raw graph data over half a TB. This leads to an order of magnitude (sometimes as much as 23_) higher throughput than Neo4j and Titan. We get similar gains in distributed settings compared to Titan.", 
    "subtype": "paper", 
    "authors": [
      {
        "name": "Anurag Khandelwal"
      }, 
      {
        "name": "Zongheng Yang"
      }, 
      {
        "name": "Evan Ye"
      }, 
      {
        "name": "Rachit Agarwal"
      }, 
      {
        "name": "Ion Stoica"
      }
    ], 
    "type": "paper", 
    "id": "SIGMOD373"
  }
}