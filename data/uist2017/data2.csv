Paper Number,Day,S #,S Name,Session Start Time,S End,Makeup,T Start,T End,Length,Expected length,Previous Paper,Order in session,Total Ordering,Award,Concern,ID,Decision,Title,Contact given name,Contact family name,Contact Email,Document,Page length,Page size,Non-embedded fonts,Incomplete,Author list,Author emails,Author ID 1,Author given name or first name 1,Middle initial or name 1,Author last name or family name 1,Valid email address 1,Primary Affiliation (no labs or dept names in this field) 1 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 1 - Institution,Primary Affiliation (no labs or dept names in this field) 1 - City,Primary Affiliation (no labs or dept names in this field) 1 - State or Province,Primary Affiliation (no labs or dept names in this field) 1 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 1 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 1 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 1 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 1 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 1 - Country,Author ID 2,Author given name or first name 2,Middle initial or name 2,Author last name or family name 2,Valid email address 2,Primary Affiliation (no labs or dept names in this field) 2 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 2 - Institution,Primary Affiliation (no labs or dept names in this field) 2 - City,Primary Affiliation (no labs or dept names in this field) 2 - State or Province,Primary Affiliation (no labs or dept names in this field) 2 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 2 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 2 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 2 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 2 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 2 - Country,Author ID 3,Author given name or first name 3,Middle initial or name 3,Author last name or family name 3,Valid email address 3,Primary Affiliation (no labs or dept names in this field) 3 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 3 - Institution,Primary Affiliation (no labs or dept names in this field) 3 - City,Primary Affiliation (no labs or dept names in this field) 3 - State or Province,Primary Affiliation (no labs or dept names in this field) 3 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 3 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 3 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 3 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 3 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 3 - Country,Author ID 4,Author given name or first name 4,Middle initial or name 4,Author last name or family name 4,Valid email address 4,Primary Affiliation (no labs or dept names in this field) 4 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 4 - Institution,Primary Affiliation (no labs or dept names in this field) 4 - City,Primary Affiliation (no labs or dept names in this field) 4 - State or Province,Primary Affiliation (no labs or dept names in this field) 4 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 4 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 4 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 4 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 4 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 4 - Country,Author ID 5,Author given name or first name 5,Middle initial or name 5,Author last name or family name 5,Valid email address 5,Primary Affiliation (no labs or dept names in this field) 5 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 5 - Institution,Primary Affiliation (no labs or dept names in this field) 5 - City,Primary Affiliation (no labs or dept names in this field) 5 - State or Province,Primary Affiliation (no labs or dept names in this field) 5 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 5 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 5 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 5 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 5 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 5 - Country,Author ID 6,Author given name or first name 6,Middle initial or name 6,Author last name or family name 6,Valid email address 6,Primary Affiliation (no labs or dept names in this field) 6 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 6 - Institution,Primary Affiliation (no labs or dept names in this field) 6 - City,Primary Affiliation (no labs or dept names in this field) 6 - State or Province,Primary Affiliation (no labs or dept names in this field) 6 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 6 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 6 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 6 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 6 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 6 - Country,Author ID 7,Author given name or first name 7,Middle initial or name 7,Author last name or family name 7,Valid email address 7,Primary Affiliation (no labs or dept names in this field) 7 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 7 - Institution,Primary Affiliation (no labs or dept names in this field) 7 - City,Primary Affiliation (no labs or dept names in this field) 7 - State or Province,Primary Affiliation (no labs or dept names in this field) 7 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 7 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 7 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 7 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 7 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 7 - Country,Author ID 8,Author given name or first name 8,Middle initial or name 8,Author last name or family name 8,Valid email address 8,Primary Affiliation (no labs or dept names in this field) 8 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 8 - Institution,Primary Affiliation (no labs or dept names in this field) 8 - City,Primary Affiliation (no labs or dept names in this field) 8 - State or Province,Primary Affiliation (no labs or dept names in this field) 8 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 8 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 8 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 8 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 8 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 8 - Country,Author ID 9,Author given name or first name 9,Middle initial or name 9,Author last name or family name 9,Valid email address 9,Primary Affiliation (no labs or dept names in this field) 9 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 9 - Institution,Primary Affiliation (no labs or dept names in this field) 9 - City,Primary Affiliation (no labs or dept names in this field) 9 - State or Province,Primary Affiliation (no labs or dept names in this field) 9 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 9 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 9 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 9 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 9 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 9 - Country,Author ID 10,Author given name or first name 10,Middle initial or name 10,Author last name or family name 10,Valid email address 10,Primary Affiliation (no labs or dept names in this field) 10 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 10 - Institution,Primary Affiliation (no labs or dept names in this field) 10 - City,Primary Affiliation (no labs or dept names in this field) 10 - State or Province,Primary Affiliation (no labs or dept names in this field) 10 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 10 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 10 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 10 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 10 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 10 - Country,Author ID 11,Author given name or first name 11,Middle initial or name 11,Author last name or family name 11,Valid email address 11,Primary Affiliation (no labs or dept names in this field) 11 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 11 - Institution,Primary Affiliation (no labs or dept names in this field) 11 - City,Primary Affiliation (no labs or dept names in this field) 11 - State or Province,Primary Affiliation (no labs or dept names in this field) 11 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 11 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 11 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 11 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 11 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 11 - Country,Author ID 12,Author given name or first name 12,Middle initial or name 12,Author last name or family name 12,Valid email address 12,Primary Affiliation (no labs or dept names in this field) 12 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 12 - Institution,Primary Affiliation (no labs or dept names in this field) 12 - City,Primary Affiliation (no labs or dept names in this field) 12 - State or Province,Primary Affiliation (no labs or dept names in this field) 12 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 12 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 12 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 12 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 12 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 12 - Country,Author ID 13,Author given name or first name 13,Middle initial or name 13,Author last name or family name 13,Valid email address 13,Primary Affiliation (no labs or dept names in this field) 13 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 13 - Institution,Primary Affiliation (no labs or dept names in this field) 13 - City,Primary Affiliation (no labs or dept names in this field) 13 - State or Province,Primary Affiliation (no labs or dept names in this field) 13 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 13 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 13 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 13 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 13 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 13 - Country,Author ID 14,Author given name or first name 14,Middle initial or name 14,Author last name or family name 14,Valid email address 14,Primary Affiliation (no labs or dept names in this field) 14 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 14 - Institution,Primary Affiliation (no labs or dept names in this field) 14 - City,Primary Affiliation (no labs or dept names in this field) 14 - State or Province,Primary Affiliation (no labs or dept names in this field) 14 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 14 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 14 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 14 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 14 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 14 - Country,Author ID 15,Author given name or first name 15,Middle initial or name 15,Author last name or family name 15,Valid email address 15,Primary Affiliation (no labs or dept names in this field) 15 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 15 - Institution,Primary Affiliation (no labs or dept names in this field) 15 - City,Primary Affiliation (no labs or dept names in this field) 15 - State or Province,Primary Affiliation (no labs or dept names in this field) 15 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 15 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 15 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 15 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 15 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 15 - Country,Author ID 16,Author given name or first name 16,Middle initial or name 16,Author last name or family name 16,Valid email address 16,Primary Affiliation (no labs or dept names in this field) 16 - Department/School/Lab,Primary Affiliation (no labs or dept names in this field) 16 - Institution,Primary Affiliation (no labs or dept names in this field) 16 - City,Primary Affiliation (no labs or dept names in this field) 16 - State or Province,Primary Affiliation (no labs or dept names in this field) 16 - Country,Secondary Affiliation (optional) (no labs or dept names in this field) 16 - Department/School/Lab,Secondary Affiliation (optional) (no labs or dept names in this field) 16 - Institution,Secondary Affiliation (optional) (no labs or dept names in this field) 16 - City,Secondary Affiliation (optional) (no labs or dept names in this field) 16 - State or Province,Secondary Affiliation (optional) (no labs or dept names in this field) 16 - Country,Abstract,Contact Author Email Address,References,Author Keywords,ACM Classifications,Document Source,Thumbnail Image,Video Figure,Auxiliary Material,Thumbnail Image Caption,Auxiliary Material Descriptive (Read me text) Information,Contribution & Benefit Statement (aka Program Description -- Mandatory Field),Summary of Changes,Presenting Author,Backup Presenting Author,Format Complete (Publications chair use only),Grant Number 1,Grant Agency 1,Grant Agency 2,Grant Number 2,Grant Agency 3,Grant Number 3,Last Update,Notes
uistf2935,10/23,1,Touch,11:40:00 AM,1:00:00 PM,3+2,11:40:00 AM,12:00:00 PM,long,long,none,1,101,,,uistf2935,A,Dwell+: Multi-Level Mode Selection Using Vibrotactile Cues,Yi-Chi,Liao,haptical.man@gmail.com,uistf2935-paper.pdf,12,letter,,,"Yi-Chi Liao, Yen-Chiu Chen, Liwei Chan, Bing-Yu Chen","imetliao@gmail.com, kerkerball@gmail.com, liwei.name@gmail.com, robin@ntu.edu.tw",47176,Yi-Chi,,Liao,imetliao@gmail.com,,National Taiwan University,Taipei,,Taiwan,,,,,,71834,Yen-Chiu,,Chen,kerkerball@gmail.com,,National Taiwan University,Taipei,,Taiwan,,,,,,19308,Liwei,,Chan,liwei.name@gmail.com,Computer Science,National Chiao Tung University,Hsinchu,,Taiwan,,,,,,11664,Bing-Yu,,Chen,robin@ntu.edu.tw,,National Taiwan University,Taipei,,Taiwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present Dwell+, a method that boosts the effectiveness of typical dwell selection by augmenting the passive dwell duration with active haptic ticks which promptly drives rapid switches of modes forward through the user's skin sensations. In this way, Dwell+ enables multi-level dwell selection using rapid haptic ticks. To select a mode from a button, users dwell-touch the button until the mode of selection is haptically prompted. \  \ Our haptic stimulation design consists of a short 10ms vibrotacile feedback that indicates a mode arriving and a break that separates consecutive modes. We first tested the effectiveness of 170ms, 150ms, 130ms, and 110ms intervals between modes for a 10-level selection. The results reveal that 3-beats-per-chunk rhythm design, e.g., displaying longer 25ms vibrations initially for all three modes, could potentially achieve higher accuracy. The second study reveals significant improvement wherein a 94.5% accuracy was achieved for a 10-level Dwell+ selection using the 170ms interval with 3-beats-per-chunk design, and a 93.82% rate of accuracy using the more frequent 150ms interval with similar chunks for 5-level selection. The performance of conducting touch and receiving vibration from disparate hands was investigated for our final study to provide a wider range of usage. Our applications demonstrated implementing Dwell+ across interfaces, such as text input on a smartwatch, enhancing touch space for HMDs, boosting modalities of stylus-based tool selection, and extending the input vocabulary of physical interfaces.",imetliao@gmail.com,"1. Appert, C., Chapuis, O., and Pietriga, E. Dwell-and-spring: Undo for direct manipulation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’12, ACM (New York, NY, USA, 2012), 1957–1966. \ 2. Beukelman, D. R., and P/, M. Augmentative and Alternative Communication. Brooks Publishing, 2005. \ 3. Bianchi, A., Oakley, I., and Kwon, D. S. Counting clicks and beeps: Exploring numerosity based haptic and audio pin entry. Interact. Comput. 24, 5 (Sept. 2012), 409–422. \ 4. Bonnet, D., Appert, C., and Beaudouin-Lafon, M. Extending the vocabulary of touch events with thumbrock. In Proceedings of Graphics Interface 2013, GI ’13, Canadian Information Processing Society (Toronto, Ont., Canada, Canada, 2013), 221–228. \ 5. Boring, S., Ledo, D., Chen, X. A., Marquardt, N., Tang, A., and Greenberg, S. The fat thumb: Using the thumb’s contact size for single-handed mobile interaction. In Proceedings of the 14th International Conference on Human-computer Interaction with Mobile Devices and Services, MobileHCI ’12, ACM (New York, NY, USA, 2012), 39–48. \ 6. Brewster, S., and Brown, L. M. Tactons: Structured tactile messages for non-visual information display. In Proceedings of the Fifth Conference on Australasian User Interface - Volume 28, AUIC ’04, Australian Computer Society, Inc. (Darlinghurst, Australia, Australia, 2004), 15–23. \ 7. Chen, X. A., Grossman, T., and Fitzmaurice, G. Swipeboard: A text entry technique for ultra-small interfaces that supports novice to expert transitions. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology, UIST ’14, ACM (New York, NY, USA, 2014), 615–620. \ 8. Chen, X. A., Schwarz, J., Harrison, C., Mankoff, J., and Hudson, S. E. Air+touch: Interweaving touch & in-air gestures. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology, UIST ’14, ACM (New York, NY, USA, 2014), 519–525. \ 9. Cody, F. W., Garside, R. A., Lloyd, D., and Poliakoff, E. Tactile spatial acuity varies with site and axis in the human upper limb. Neuroscience Letters 433, 2 (2008), 103 – 108. \ 10. Craig, J. C., and Lyle, K. B. A comparison of tactile spatial sensitivity on the palm and ﬁngerpad. Perception & Psychophysics 63, 2 (Feb 2001), 337–347. \ 11. Harrison, C., and Hudson, S. Using shear as a supplemental two-dimensional input channel for rich touchscreen interaction. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’12, ACM (New York, NY, USA, 2012), 3149–3152. \ 12. Harrison, C., Schwarz, J., and Hudson, S. E. Tapsense: Enhancing ﬁnger interaction on touch surfaces. In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology, UIST ’11, ACM (New York, NY, USA, 2011), 627–636. \ 13. Heo, S., and Lee, G. Forcetap: Extending the input vocabulary of mobile touch screens by adding tap gestures. In Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services, MobileHCI ’11, ACM (New York, NY, USA, 2011), 113–122. \ 14. Hinckley, K., and Song, H. Sensor synaesthesia: Touch in motion, and motion in touch. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’11, ACM (New York, NY, USA, 2011), 801–810. \ 15. Holz, C., and Baudisch, P. Fiberio: A touchscreen that senses ﬁngerprints. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology, UIST ’13, ACM (New York, NY, USA, 2013), 41–50. \ 16. Hoshiyama, M., Kakigi, R., and Tamura, Y. Temporal discrimination threshold on various parts of the body. Muscle & nerve 29, 2 (2004), 243–247. \ 17. Huang, D.-Y., Tsai, M.-C., Tung, Y.-C., Tsai, M.-L., Yeh, Y.-T., Chan, L., Hung, Y.-P., and Chen, M. Y. Touchsense: Expanding touchscreen input vocabulary using different areas of users’ ﬁnger pads. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’14, ACM (New York, NY, USA, 2014), 189–192. \ 18. Jacob, R., and Karn, K. S. Eye tracking in human-computer interaction and usability research: Ready to deliver the promises. Mind 2, 3 (2003), 4. \ 19. Jacob, R. J. K. What you look at is what you get: Eye movement-based interaction techniques. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’90, ACM (New York, NY, USA, 1990), 11–18. \ 20. Kulik, A., Dittrich, J., and Froehlich, B. The hold-and-move gesture for multi-touch interfaces. In Proceedings of the 14th International Conference on Human-computer Interaction with Mobile Devices and Services, MobileHCI ’12, ACM (New York, NY, USA, 2012), 49–58. \ 21. Kuribara, T., Shizuki, B., and Tanaka, J. Vibrainput: Two-step pin entry system based on vibration and visual information. In CHI ’14 Extended Abstracts on Human Factors in Computing Systems, CHI EA ’14, ACM (New York, NY, USA, 2014), 2473–2478. \ 22. Kurtenbach, G., and Buxton, W. User learning and performance with marking menus. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’94, ACM (New York, NY, USA, 1994), 258–264. \ 23. Lechelt, E. C. Temporal numerosity discrimination: Intermodal comparisons revisited. British Journal of Psychology 66, 1 (1975), 101–108. \ 24. Lee, S. C., and Starner, T. Buzzwear: Alert perception in wearable tactile displays on the wrist. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’10, ACM (New York, NY, USA, 2010), 433–442. \ 25. Li, Y., Hinckley, K., Guan, Z., and Landay, J. A. Experimental analysis of mode switching techniques in pen-based user interfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’05, ACM (New York, NY, USA, 2005), 461–470. \ 26. Liao, Y.-C., Chen, Y.-L., Lo, J.-Y., Liang, R.-H., Chan, L., and Chen, B.-Y. Edgevib: Effective alphanumeric character output using a wrist-worn tactile display. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, UIST ’16, ACM (New York, NY, USA, 2016), 595–601. \ 27. Miller, G. A. The magical number seven, plus or minus two: Some limits on our capacity for processing information. The Psychological Review 63, 2 (1956). \ 28. Pasquero, J., Stobbe, S. J., and Stonehouse, N. A haptic wristwatch for eyes-free interactions. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’11, ACM (New York, NY, USA, 2011), 3257–3266. \ 29. Pook, S., Lecolinet, E., Vaysseix, G., and Barillot, E. Control menus: Excecution and control in a single interactor. In CHI ’00 Extended Abstracts on Human Factors in Computing Systems, CHI EA ’00, ACM (New York, NY, USA, 2000), 263–264. \ 30. Roudaut, A., Lecolinet, E., and Guiard, Y. Microrolls: Expanding touch-screen input vocabulary by distinguishing rolls vs. slides of the thumb. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’09, ACM (New York, NY, USA, 2009), 927–936. \ 31. Saund, E., and Lank, E. Stylus input and editing without prior selection of mode. In Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology, UIST ’03, ACM (New York, NY, USA, 2003), 213–216. \ 32. Simpson, R. C., and Koester, H. H. Adaptive one-switch row-column scanning. IEEE Transactions on Rehabilitation Engineering 7, 4 (Dec 1999), 464–473. \ 33. Sugiura, A., and Koseki, Y. A user interface using ﬁngerprint recognition: Holding commands and data objects on ﬁngers. In Proceedings of the 11th Annual ACM Symposium on User Interface Software and Technology, UIST ’98, ACM (New York, NY, USA, 1998), 71–79. \ 34. Ternes, D., and MacLean, K. E. Designing Large Sets of Haptic Icons with Rhythm. Springer Berlin Heidelberg, Berlin, Heidelberg, 2008, 199–208. \ 35. Ware, C., and Mikaelian, H. H. An evaluation of an eye tracker as a device for computer input2. In Proceedings of the SIGCHI/GI Conference on Human Factors in Computing Systems and Graphics Interface, CHI ’87, ACM (New York, NY, USA, 1987), 183–188. \ 36. Wilson, G., Stewart, C., and Brewster, S. A. Pressure-based menu selection for mobile devices. In Proceedings of the 12th International Conference on Human Computer Interaction with Mobile Devices and Services, MobileHCI ’10, ACM (New York, NY, USA, 2010), 181–190. \ ",Dwell; Touch; Vibrotactile Feedback; Numerosity Perception; Haptically-augmented Input; Touchscreen; Input Modality; Finger; Smartwatch; Stylus; Experiment,H.5.2,uistf2935-file1.zip,uistf2935-file2.jpg,uistf2935-file3.mp4,,Dwell+ boosts the effectiveness of typical long-press by augmenting the passive dwell duration with active haptic ticks which promptly drives rapid switches of modes forward through the user's skin sensation.,,Dwell+ boosts the effectiveness of typical long-press by augmenting the passive dwell duration with active haptic ticks which promptly drives rapid switches of modes forward through the user's skin sensation.,"Followed by the reviewers’ comments and our claim in rebuttal, the camera-ready version includes these changes: \  \ [Title]  \ The title of the work has changed into “Dwell: Haptic-Augmented Multi-Level Touch”. \  \ [Comparison of Dwell++ & other methods] \ We emphasized the differences between our work and other methods techniques for enhancing input modalities at the “Dwell++” subsection in the Introduction. \  \ [Fix of Related Works] \ “Numerosity Perception for Switch Access Scanning” was added to fix the missing references. \  \ [User Study 1] \ The pilot test which initially derived 4 candidate intervals was added in the beginning of User Study 1. We also highlight the motivation and purpose of this study. \  \ [Length of shorter gap for chunking design] \ The pilot study that derived the duration of longer 25ms vibration was added in the “Using 3-beat-per-chunk Design for Enhancing the Effective Level of Dwell++” section. And we have clarified the duration of the shorter gaps for all the intervals. \  \ [User Study 2] \ The results analysis of higher-level selection was revised and with more descriptions. \  \ [User Study 3] \ Some misreported DOFs were corrected and we have highlight the limitation of this study, the results can’t be applied to different parts of body directly. Such limitation was also mentioned and highlighted in the “Entering Texts on Small Screens” application and the “Exploring Effective Dwell++ Design across Body” subsection of the LIMITATIONS. \  \ [Discussion & Limitations] \ We separated the Discussion section into 2 sections, Discussion and Limitations. In the Limitations section, we added a subsection about “Evaluation of applications”. \  \ [Normality & homogeneity of variance] \ “Tests for normality and homogeneity of variance” were added to every User Study. \  \  \ Lastly, this paper has gone through a thorough proof-reading with a professional native editor, and has fixed the grammar problems.",Yi-Chi Liao,Liwei Chan,FormatComplete,,,,,,,Aug 7 14:14,
uistf3598,10/23,1,Touch,11:40:00 AM,1:00:00 PM,3+2,12:00:00 PM,12:20:00 PM,long,long,uistf2935,2,102,,,uistf3598,A,CommandBoard: Creating a General-Purpose Command Gesture Input Space for Soft Keyboard,Jessalyn,Alvina,jessalyn.alvina@live.com,uistf3598-paper.pdf,12,letter,,,"Jessalyn Alvina, Carla Florencia Griggio, Xiaojun Bi, Wendy E. Mackay","jessalyn.alvina@live.com, carla.griggio@gmail.com, xjunbi@gmail.com, mackay@lri.fr",37099,Jessalyn,,Alvina,jessalyn.alvina@live.com,"LRI, Univ. Paris-Sud, CNRS","Inria, Université Paris-Saclay",Orsay,,France,,,,,,41183,Carla,Florencia,Griggio,carla.griggio@gmail.com,LRI,"Univ. Paris-Sud, CNRS, Inria, Université Paris-Saclay",Orsay,,France,,,,,,9137,Xiaojun,,Bi,xjunbi@gmail.com,Department of Computer Science,Stony Brook University,Stony Brook,NY,United States,,,,,,1688,Wendy,E.,Mackay,mackay@lri.fr,LRI,"Univ. Paris-Sud, CNRS, Inria, Université Paris-Saclay",Orsay,,France,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"CommandBoard offers a simple, efficient and incrementally learnable technique for issuing gesture commands from a soft keyboard. We transform the area above the keyboard into a command-gesture input space that lets users draw unique command gestures or type command names followed by execute. Novices who pause see an in-context dynamic guide, whereas experts simply draw. Our studies show that CommandBoard’s inline gesture shortcuts are significantly faster (almost double) than markdown symbols and significantly preferred by users. We demonstrate additional techniques for more complex commands, and discuss trade-offs with respect to the user’s knowledge and motor skills, as well as the size and structure of the command space.",jessalyn.alvina@gmail.com,"1. Jessalyn Alvina, Joseph Malloch, and Wendy E. Mackay. 2016. Expressive Keyboards: Enriching Gesture-Typing on Mobile Devices. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 583–593. DOI: http://dx.doi.org/10.1145/2984511.2984560 \ 2. Caroline Appert and Shumin Zhai. 2009. Using Strokes As Command Shortcuts: Cognitive Beneﬁts and Toolkit Support. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’09). ACM, New York, NY, USA, 2289–2298. DOI: http://dx.doi.org/10.1145/1518701.1519052 \ 3. Olivier Bau and Wendy E. Mackay. 2008. OctoPocus: A Dynamic Guide for Learning Gesture-based Command Sets. In Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology (UIST ’08). ACM, New York, NY, USA, 37–46. DOI: http://dx.doi.org/10.1145/1449715.1449724 \ 4. Barry Brown, Moira McGregor, and Donald McMillan. 2014. 100 Days of iPhone Use: Understanding the Details of Mobile Device Use. In Proceedings of the 16th International Conference on Human-computer Interaction with Mobile Devices & Services (MobileHCI ’14). ACM, New York, NY, USA, 223–232. DOI:http://dx.doi.org/10.1145/2628363.2628377 \ 5. Daniel Buschek, Alexander De Luca, and Florian Alt. 2015. There is More to Typing Than Speed: Expressive Mobile Touch Keyboards via Dynamic Font Personalisation. In Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI ’15). ACM, New York, NY, USA, 125–130. DOI: http://dx.doi.org/10.1145/2785830.2785844 \ 6. Vittorio Fuccella, Poika Isokoski, and Benoit Martin. 2013. Gestures and Widgets: Performance in Text Editing on Multi-touch Capable Mobile Devices. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’13). ACM, New York, NY, USA, 2785–2794. DOI: http://dx.doi.org/10.1145/2470654.2481385 \ 7. Emilien Ghomi, Stéphane Huot, Olivier Bau, Michel Beaudouin-Lafon, and Wendy E. Mackay. 2013. ArpèGe: Learning Multitouch Chord Gestures Vocabularies. In Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces (ITS ’13). ACM, New York, NY, USA, 209–218. DOI: http://dx.doi.org/10.1145/2512349.2512795 \ 8. François Guimbretiére and Terry Winograd. 2000. FlowMenu: Combining Command, Text, and Data Entry. In Proceedings of the 13th Annual ACM Symposium on User Interface Software and Technology (UIST ’00). ACM, New York, NY, USA, 213–216. DOI: http://dx.doi.org/10.1145/354401.354778 \ 9. Sunjun Kim and Geehyuk Lee. 2016. TapBoard 2: Simple and Effective Touchpad-like Interaction on a Multi-Touch Surface Keyboard. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 5163–5168. DOI: http://dx.doi.org/10.1145/2858036.2858452 \ 10. Per-Ola Kristensson and Shumin Zhai. 2004. SHARK2: A Large Vocabulary Shorthand Writing System for Pen-based Computers. In Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology (UIST ’04). ACM, New York, NY, USA, 43–52. DOI:http://dx.doi.org/10.1145/1029632.1029640 \ 11. Per Ola Kristensson and Shumin Zhai. 2007. Command Strokes with and Without Preview: Using Pen Gestures on Keyboard for Command Selection. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’07). ACM, New York, NY, USA, 1137–1146. DOI: http://dx.doi.org/10.1145/1240624.1240797 \ 12. Gordon Kurtenbach and William Buxton. 1991. Issues in Combining Marking and Direct Manipulation Techniques. In Proceedings of the 4th Annual ACM Symposium on User Interface Software and Technology (UIST ’91). ACM, New York, NY, USA, 137–144. DOI: http://dx.doi.org/10.1145/120782.120797 \ 13. Gordon Kurtenbach and William Buxton. 1993. The Limits of Expert Performance Using Hierarchic Marking Menus. In Proceedings of the INTERACT ’93 and CHI ’93 Conference on Human Factors in Computing Systems (CHI ’93). ACM, New York, NY, USA, 482–487. DOI: http://dx.doi.org/10.1145/169059.169426 \ 14. David M. Lane, H. Albert Napier, S. Camille Peres, and Aniko Sandor. 2005. Hidden Costs of Graphical User Interfaces: Failure to Make the Transition from Menus and Icon Toolbars to Keyboard Shortcuts. Int. J. Hum. Comput. Interaction 18, 2 (2005), 133–144. http://dblp. uni-trier.de/db/journals/ijhci/ijhci18.html#LaneNPS05 \ 15. Yang Li. 2010. Gesture Search: A Tool for Fast Mobile Data Access. In Proceedings of the 23Nd Annual ACM Symposium on User Interface Software and Technology (UIST ’10). ACM, New York, NY, USA, 87–96. DOI: http://dx.doi.org/10.1145/1866029.1866044 \ 16. Wendy E. Mackay. 2002. Which Interaction Technique Works when?: Floating Palettes, Marking Menus and Toolglasses Support Different Task Strategies. In Proceedings of the Working Conference on Advanced Visual Interfaces (AVI ’02). ACM, New York, NY, USA, 203–208. DOI: http://dx.doi.org/10.1145/1556262.1556294 \ 17. Joseph Malloch, Carla F. Griggio, Joanna McGrenere, and Wendy E. Mackay. 2017. Fieldward and Pathward: Dynamic Guides for Deﬁning Your Own Gestures. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17). ACM, New York, NY, USA, 4266–4277. DOI: http://dx.doi.org/10.1145/3025453.3025764 \ 18. Miguel A. Nacenta, Yemliha Kamber, Yizhou Qiang, and Per Ola Kristensson. 2013. Memorability of Pre-designed and User-deﬁned Gesture Sets. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’13). ACM, New York, NY, USA, 1099–1108. DOI: http://dx.doi.org/10.1145/2470654.2466142 \ 19. Petra Neumann, Annie Tat, Torre Zuk, and Sheelagh Carpendale. 2007. KeyStrokes: Personalizing Typed Text with Visualization. In Proceedings of the 9th Joint Eurographics / IEEE VGTC Conference on Visualization (EUROVIS’07). Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, 43–50. DOI: http://dx.doi.org/10.2312/VisSym/EuroVis07/043-050 \ 20. Richard C. Omanson, Craig S. Miller, Elizabeth Young, and David Schwantes. 2010. Comparison of Mouse and Keyboard Efﬁciency. Proceedings of the Human Factors and Ergonomics Society Annual Meeting 54, 6 (2010), 600–604. DOI: http://dx.doi.org/10.1177/154193121005400612 \ 21. Robert Pastel. 2006. Measuring the Difﬁculty of Steering Through Corners. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’06). ACM, New York, NY, USA, 1087–1096. DOI: http://dx.doi.org/10.1145/1124772.1124934 \ 22. Dario D. Salvucci, Niels A. Taatgen, and Jelmer P. Borst. 2009. Toward a Uniﬁed Theory of the Multitasking Continuum: From Concurrent Performance to Task Switching, Interruption, and Resumption. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’09). ACM, New York, NY, USA, 1819–1828. DOI: http://dx.doi.org/10.1145/1518701.1518981 \ 23. Christopher D Wickens, Justin G Hollands, Simon Banbury, and Raja Parasuraman. 2015. Engineering psychology & human performance. Psychology Press. 197–244 pages. \ 24. Shumin Zhai and Per Ola Kristensson. 2012. The Word-gesture Keyboard: Reimagining Keyboard Interaction. Commun. ACM 55, 9 (Sept. 2012), 91–101. DOI:http://dx.doi.org/10.1145/2330667.2330689 \ ",Gesture Shortcuts; Gesture Typing; Mobile; Soft keyboards,H.5.2,uistf3598-file1.zip,,uistf3598-file3.mp4,,,,CommandBoard offers efficient and incrementally learnable techniques for issuing gesture commands from soft keyboards. Users draw unique command gestures or type command names followed by execute gesture as they type.,"All the changes we made are based on the rebuttal and the final review, as well as fixing minor error. \  \ Clarification: \ 1. Study design: Why markdown as the baseline? It is obvious that CommandBoard would outperform. Why not menu or MarkingMenu? (all) \  \ 2. Error-rate does not represent ""expert level"" (R2) \  \ 3. Discussion of the technique and limitation, error-rate, and memorability (all) \  \ 4. Why is the ""cancel"" gesture simpler than the ""execute"" gesture? (R1) \  \ Minor changes: \ 1. Page 6, first paragraph, we clarified the last sentence that is difficult to understand (R1) \ 2. We fixed the way ANOVA is reported (R1) \ 3. We considered not to use abbreviation in Results (R1) \ 4. We changed the tenses in Description of Experiment into past tense (R1) \ 5. We fixed the Suggestion Bar display in Figure 2, Figure 3 \ 6. We changed Figure 1 and Figure 4 to make it clearer (R1, R2) \ 7. We fixed the term in Figure 7. \ 8. We made sure that all terms are using the same format. \ 9. We discussed about how Search-and-Execute technique requires text selection (AC2) \ 10. We clarified what an ""input"" meant in the Measure (AC2) \ 11. We changed the name of the techniques into ""Type-and-Execute"" and ""Inline Gesture Shortcuts""",Jessalyn Alvina,Wendy Mackay,FormatComplete,ERC Grant NO 321135 CREATIV,European Research Council (ERC),,,,,Aug 9 17:29,
uistf1706,10/23,1,Touch,11:40:00 AM,1:00:00 PM,3+2,12:20:00 PM,12:40:00 PM,long,long,uistf3598,3,103,,,uistf1706,A,Characterizing Latency in Touch and Button-Equipped Interactive Systems,Géry,Casiez,gery.casiez@univ-lille1.fr,uistf1706-paper.pdf,11,letter,,,"Géry Casiez, Thomas Pietrzak, Damien Marchal, Sébastien Poulmane, Matthieu Falce, Nicolas Roussel","gery.casiez@univ-lille1.fr, thomas.pietrzak@univ-lille1.fr, damien.marchal@univ-lille1.fr, sebastien.poulmane@inria.fr, falce.matthieu@gmail.com, nicolas.roussel@inria.fr",3033,Géry,,Casiez,gery.casiez@univ-lille1.fr,,Université de Lille,Lille,,France,,,,,,14172,Thomas,,Pietrzak,thomas.pietrzak@univ-lille1.fr,,Univ. Lille,Lille,,France,,,,,,41935,Damien,,Marchal,damien.marchal@univ-lille1.fr,,Université de Lille,Lille,,France,,,,,,71673,Sébastien,,Poulmane,sebastien.poulmane@inria.fr,,Inria,Lille,,France,,,,,,71675,Matthieu,,Falce,falce.matthieu@gmail.com,,Mathieu Falce Consulting,Lille,,France,,,,,,1169,Nicolas,,Roussel,nicolas.roussel@inria.fr,,Inria,Lille,,France,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,We present a low cost method to measure and characterize the end-to-end latency when using a touch system (tap latency) or an input device equipped with a physical button. Our method relies on a vibration sensor attached to a finger and a photo-diode to detect the screen response. Both are connected to a micro-controller connected to a host computer using a low-latency USB communication protocol in order to combine software and hardware probes to help determine where the latency comes from. We present the operating principle of our method before investigating the main sources of latency in several systems. We show that most of the latency originates from the display side. Our method can help application designers characterize and troubleshoot latency on a wide range of interactive systems.,gery.casiez@univ-lille1.fr,"1. Michelle Annett, Albert Ng, Paul Dietz, Walter F. Bischof, and Anoop Gupta. 2014. How Low Should We Go? Understanding the Perception of Latency While Inking. In Proceedings of Graphics Interface 2014 (GI ’14). Canadian Information Processing Society, Toronto, Ont., Canada, Canada, 167–174. http://dl.acm.org/citation.cfm?id=2619648.2619677 \ 2. François Bérard and Renaud Blanch. 2013. Two Touch System Latency Estimators: High Accuracy and Low Overhead. In Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces (ITS ’13). ACM, New York, NY, USA, 241–250. DOI: http://dx.doi.org/10.1145/2512349.2512796 \ 3. Géry Casiez, Stéphane Conversy, Matthieu Falce, Stéphane Huot, and Nicolas Roussel. 2015. Looking Through the Eye of the Mouse: A Simple Method for Measuring End-to-end Latency Using an Optical Mouse. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST ’15). ACM, New York, NY, USA, 629–636. DOI: http://dx.doi.org/10.1145/2807442.2807454 \ 4. Géry Casiez and Nicolas Roussel. 2011. No More Bricolage! Methods and Tools to Characterize, Replicate and Compare Pointing Transfer Functions. In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (UIST ’11). ACM, New York, NY, USA, 603–614. DOI: http://dx.doi.org/10.1145/2047196.2047276 \ 5. Elie Cattan, Amélie Rochet-Capellan, and François Bérard. 2015a. A Predictive Approach for an End-to-End Touch-Latency Measurement. In Proceedings of the 2015 International Conference on Interactive Tabletops & Surfaces (ITS ’15). ACM, New York, NY, USA, 215–218. DOI: http://dx.doi.org/10.1145/2817721.2817747 \ 6. Elie Cattan, Amélie Rochet-Capellan, Pascal Perrier, and François Bérard. 2015b. Reducing Latency with a Continuous Prediction: Effects on Users’ Performance in Direct-Touch Target Acquisitions. In Proceedings of the 2015 International Conference on Interactive Tabletops & Surfaces (ITS ’15). ACM, New York, NY, USA, 205–214. DOI: http://dx.doi.org/10.1145/2817721.2817736 \ 7. Jonathan Deber, Bruno Araujo, Ricardo Jota, Clifton Forlines, Darren Leigh, Steven Sanders, and Daniel Wigdor. 2016. Hammer Time! A Low-Cost, High Precision, High Accuracy Tool to Measure the Latency of Touchscreen Devices. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 2857–2868. DOI:http://dx.doi.org/10.1145/2858036.2858394 \ 8. Jonathan Deber, Ricardo Jota, Clifton Forlines, and Daniel Wigdor. 2015. How Much Faster is Fast Enough? User Perception of Latency & Latency Improvements in Direct and Indirect Touch. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15). ACM, New York, NY, USA, 1827–1836. DOI: http://dx.doi.org/10.1145/2702123.2702300 \ 9. Stephen R Ellis, François Bréant, Brian M Menges, Richard H Jacoby, and Bernard D Adelstein. 1997. Operator interaction with virtual objects: effect of system latency. In Proceedings of HCI International ’97. Elsevier, 973–976. \ 10. Ricardo Jota, Albert Ng, Paul Dietz, and Daniel Wigdor. 2013. How Fast is Fast Enough? A Study of the Effects of Latency in Direct-touch Pointing Tasks. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’13). ACM, New York, NY, USA, 2291–2300. DOI: http://dx.doi.org/10.1145/2470654.2481317 \ 11. Jiandong Liang, Chris Shaw, and Mark Green. 1991. On Temporal-spatial Realism in the Virtual Reality Environment. In Proceedings of the 4th Annual ACM Symposium on User Interface Software and Technology (UIST ’91). ACM, New York, NY, USA, 19–25. DOI: http://dx.doi.org/10.1145/120782.120784 \ 12. I. Scott MacKenzie and Colin Ware. 1993. Lag As a Determinant of Human Performance in Interactive Systems. In Proceedings of the INTERACT ’93 and CHI ’93 Conference on Human Factors in Computing Systems (CHI ’93). ACM, New York, NY, USA, 488–493. DOI:http://dx.doi.org/10.1145/169059.169431 \ 13. Albert Ng, Michelle Annett, Paul Dietz, Anoop Gupta, and Walter F. Bischof. 2014. In the Blink of an Eye: Investigating Latency Perception During Stylus Interaction. In Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI ’14). ACM, New York, NY, USA, 1103–1112. DOI:http://dx.doi.org/10.1145/2556288.2557037 \ 14. Albert Ng, Julian Lepinski, Daniel Wigdor, Steven Sanders, and Paul Dietz. 2012. Designing for Low-latency Direct-touch Input. In Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology (UIST ’12). ACM, New York, NY, USA, 453–464. DOI: http://dx.doi.org/10.1145/2380116.2380174 \ 15. Andriy Pavlovych and Wolfgang Stuerzlinger. 2009. The Tradeoff Between Spatial Jitter and Latency in Pointing Tasks. In Proceedings of the 1st ACM SIGCHI Symposium on Engineering Interactive Computing Systems (EICS ’09). ACM, New York, NY, USA, 187–196. DOI: http://dx.doi.org/10.1145/1570433.1570469 \ 16. Andriy Pavlovych and Wolfgang Stuerzlinger. 2011. Target Following Performance in the Presence of Latency, Jitter, and Signal Dropouts. In Proceedings of Graphics Interface 2011 (GI ’11). Canadian Human-Computer Communications Society, School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada, 33–40. http://dl.acm.org/citation.cfm?id=1992917.1992924 \ 17. Daniel R. Saunders and Russell L. Woods. 2014. Direct measurement of the system latency of gaze-contingent displays. Behav Res Methods 46, 2 (June 2014), 439–447. http://dx.doi.org/10.3758/s13428-013-0375-5 \ 18. Anthony Steed. 2008. A Simple Method for Estimating the Latency of Interactive, Real-time Graphics Simulations. In Proceedings of the 2008 ACM Symposium on Virtual Reality Software and Technology (VRST ’08). ACM, New York, NY, USA, 123–129. DOI: http://dx.doi.org/10.1145/1450579.1450606 \ 19. Colin Swindells, John C. Dill, and Kellogg S. Booth. 2000. System Lag Tests for Augmented and Virtual Environments. In Proceedings of the 13th Annual ACM Symposium on User Interface Software and Technology (UIST ’00). ACM, New York, NY, USA, 161–170. DOI: http://dx.doi.org/10.1145/354401.354444 \ 20. R. J. Teather, A. Pavlovych, W. Stuerzlinger, and I. S. MacKenzie. 2009. Effects of tracking technology, latency, and spatial jitter on object movement. In 2009 IEEE Symposium on 3D User Interfaces. 43–50. DOI: http://dx.doi.org/10.1109/3DUI.2009.4811204 \ 21. Colin Ware and Ravin Balakrishnan. 1994. Reaching for Objects in VR Displays: Lag and Frame Rate. ACM Trans. Comput.-Hum. Interact. 1, 4 (Dec. 1994), 331–356. DOI:http://dx.doi.org/10.1145/198425.198426 \ ",latency; lag; measure; toolkits; measurement tools,H.5.2,uistf1706-file1.zip,uistf1706-file2.jpg,uistf1706-file3.mp4,,"Our method comprises a vibration sensor mounted on the finger to measure when a surface is touched, and a photo-diode to determine when the screen is updated.",,We present a low cost method to measure and characterize the end-to-end latency when using a touch system or an input device equipped with a physical button.,"- We clarified tap vs drag latency in the abstract, intro and related work to be more explicit our method only measures tap latency \  \ - Referenced WALT and MSFT in the related work section and explained the differences with our method. Also referenced Saunders et al. \  \ - Clarified that a tap was considered ""moderate"" when the experimenter tapped as she would normally do on a touch surface and taps were ""stronger"" above. Also that taps were done with the hand naturally posed on the desk and using finger pad. \  \ - In the description of the method, we provided a discussion of the initial mechanical steps and we provided additional measures as initiated in the rebuttal. \  \ - Clarify in the discussion the benefits of doing ""ecological"" measures and the interest of measuring on non-capacitive surfaces \  \ - We moved tables 4 and 8 with the associated texts to an Appendix and added a screen capture on the piezo response. \  \ In addition we integrated the different minor modifications as suggested by R3. \  \ Changes are highlighted in this pdf: https://ocm.univ-lille1.fr/index.php/s/v9AxczB2EGRIl3J \ Added text is highlighted in green \ Removed text is highlighted in light gray \ Changed text is highlighted in orange \  \ The materials promised in the paper will be soon available here: http://mjolnir.lille.inria.fr/turbotouch/lagmeter/interactivelagmeter.php",Géry Casiez,Thomas Pietrzak,FormatComplete,"TurboTouch, ANR-14-CE24-0009",ANR,,,,,Aug 7 7:42,
uistf2440,10/23,1,Touch,11:40:00 AM,1:00:00 PM,3+2,12:40:00 PM,12:50:00 PM,short,short,uistf1706,4,104,,,uistf2440,A,WhichFingers: Identifying Fingers on Touch Surfaces and Keyboards Using Vibration Sensors,Sylvain,Malacria,sylvain.malacria@inria.fr,uistf2440-paper.pdf,8,letter,,,"Damien Masson, Alix Goguey, Sylvain Malacria, Géry Casiez","damien.masson.a@gmail.com, alix.goguey@gmail.com, sylvain.malacria@inria.fr, gery.casiez@univ-lille1.fr",63056,Damien,,Masson,damien.masson.a@gmail.com,,Université de Lille,Lille,,France,,Inria,Lille,,France,36948,Alix,,Goguey,alix.goguey@gmail.com,Department of Computer Science,University of Saskatchewan,Saskatoon,Saskatchewan,Canada,,Inria,Lille,,France,11388,Sylvain,,Malacria,sylvain.malacria@inria.fr,,Inria,Lille,,France,,,,,,3033,Géry,,Casiez,gery.casiez@univ-lille1.fr,,Université de Lille,Lille,,France,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"HCI researchers lack low latency and robust systems to support the design and development of interaction techniques using finger identification. We developed a low cost prototype using piezo based vibration sensors attached to each finger. By combining the events from an input device with the information from the vibration sensors we demonstrate how to achieve low latency and robust finger identification. Our prototype was evaluated in a controlled experiment, using two keyboards and a touchpad, showing recognition rates of 98.2% for the keyboard and, for the touchpad, 99.7% for single touch and 94.7% for two simultaneous touches. These results were confirmed in an additional laboratory style experiment with ecologically valid tasks. Last we present new interactions techniques made possible using this technology.",sylvain.malacria@inria.fr,"1. Oscar Kin-Chung Au and Chiew-Lan Tai. 2010. Multitouch Finger Registration and Its Applications. In Proceedings of the 22nd Conference of the Computer-Human Interaction Special Interest Group of Australia on Computer-Human Interaction (OZCHI ’10). ACM, New York, NY, USA, 41–48. DOI: http://dx.doi.org/10.1145/1952222.1952233 \ 2. Hrvoje Benko, T. Scott Saponas, Dan Morris, and Desney Tan. 2009. Enhancing Input on and Above the Interactive Surface with Muscle Sensing. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces (ITS ’09). ACM, New York, NY, USA, 93–100. DOI: http://dx.doi.org/10.1145/1731903.1731924 \ 3. Géry Casiez, Thomas Pietrzak, Damien Marchal, Sébastien Poulmane, Mathieu Falce, and Nicolas Roussel. 2017. Characterizing Latency in Touch and Button-Equipped Interactive Systems. In Proceedings of UIST’17, the 30th ACM Symposium on User Interface Software and Technology (UIST ’17). ACM, New York, NY, USA, 9. DOI: http://dx.doi.org/10.1145/3126594.3126606 \ 4. Ashley Colley and Jonna Häkkilä. 2014. Exploring Finger Speciﬁc Touch Screen Interaction for Mobile Phone User Interfaces. In Proceedings of the 26th Australian Computer-Human Interaction Conference on Designing Futures: The Future of Design (OzCHI ’14). ACM, New York, NY, USA, 539–548. DOI: http://dx.doi.org/10.1145/2686612.2686699 \ 5. Philipp Ewerling, Alexander Kulik, and Bernd Froehlich. 2012. Finger and Hand Detection for Multi-touch Interfaces Based on Maximally Stable Extremal Regions. In Proceedings of the 2012 ACM International Conference on Interactive Tabletops and Surfaces (ITS ’12). ACM, New York, NY, USA, 173–182. DOI: http://dx.doi.org/10.1145/2396636.2396663 \ 6. A. Fadell, A. Hodge, S. Schell, R. Caballero, J.L. Dorogusker, S. Zadesky, and E. Sanford. 2014. Embedded authentication systems in an electronic device. (July 15 2014). https://www.google.com/patents/US8782775 US Patent 8,782,775. \ 7. Masaaki Fukumoto and Yasuhito Suenaga. 1994. “FingeRing”: A Full-time Wearable Interface. In Conference Companion on Human Factors in Computing Systems (CHI ’94). ACM, New York, NY, USA, 81–82. DOI: http://dx.doi.org/10.1145/259963.260056 \ 8. Masaaki Fukumoto and Yoshinobu Tonomura. 1997. “Body Coupled FingerRing”: Wireless Wearable Keyboard. In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI ’97). ACM, New York, NY, USA, 147–154. DOI: http://dx.doi.org/10.1145/258549.258636 \ 9. Emilien Ghomi, Stéphane Huot, Olivier Bau, Michel Beaudouin-Lafon, and Wendy E. Mackay. 2013. Arpège: Learning Multitouch Chord Gestures Vocabularies. In Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces (ITS ’13). ACM, New York, NY, USA, 209–218. DOI: http://dx.doi.org/10.1145/2512349.2512795 \ 10. Alix Goguey, Géry Casiez, Thomas Pietrzak, Daniel Vogel, and Nicolas Roussel. 2014a. Adoiraccourcix: Multi-touch Command Selection Using Finger Identiﬁcation. In Proceedings of the 26th Conference on L’Interaction Homme-Machine (IHM ’14). ACM, New York, NY, USA, 28–37. DOI: http://dx.doi.org/10.1145/2670444.2670446 \ 11. Alix Goguey, Géry Casiez, Daniel Vogel, Fanny Chevalier, Thomas Pietrzak, and Nicolas Roussel. 2014b. A Three-step Interaction Pattern for Improving Discoverability in Finger Identiﬁcation Techniques. In Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST’14 Adjunct). ACM, New York, NY, USA, 33–34. DOI: http://dx.doi.org/10.1145/2658779.2659100 \ 12. Alix Goguey, Mathieu Nancel, Géry Casiez, and Daniel Vogel. 2016. The Performance and Preference of Different Fingers and Chords for Pointing, Dragging, and Object Transformation. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 4250–4261. DOI: http://dx.doi.org/10.1145/2858036.2858194 \ 13. Alix Goguey, Daniel Vogel, Fanny Chevalier, Thomas Pietrzak, Nicolas Roussel, and Géry Casiez. 2017. Leveraging ﬁnger identiﬁcation to integrate multi-touch command selection and parameter manipulation. In IJHCS Journal. Elsevier, 21–36. DOI: http://dx.doi.org/10.1016/j.ijhcs.2016.11.002 \ 14. Aakar Gupta, Muhammed Anwar, and Ravin Balakrishnan. 2016. Porous Interfaces for Small Screen Multitasking Using Finger Identiﬁcation. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 145–156. DOI: http://dx.doi.org/10.1145/2984511.2984557 \ 15. Aakar Gupta and Ravin Balakrishnan. 2016. DualKey: Miniature Screen Text Entry via Finger Identiﬁcation. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 59–70. DOI: http://dx.doi.org/10.1145/2858036.2858052 \ 16. Christian Holz and Patrick Baudisch. 2013. Fiberio: A Touchscreen That Senses Fingerprints. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST ’13). ACM, New York, NY, USA, 41–50. DOI: http://dx.doi.org/10.1145/2501988.2502021 \ 17. Noor A Ibraheem. 2016. Finger Identiﬁcation and Gesture Recognition Using Gaussian Classiﬁer Model. In IJAER journal. Research India Publications, 6924–6931. http://www.ripublication.com/ ijaer16/ijaerv11n10_08.pdf \ 18. G. Julian Lepinski, Tovi Grossman, and George Fitzmaurice. 2010. The Design and Evaluation of Multitouch Marking Menus. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’10). ACM, New York, NY, USA, 2233–2242. DOI: http://dx.doi.org/10.1145/1753326.1753663 \ 19. K. Ma, J. Li, J. Han, Y. Hu, and W. Xu. 2016. Enhancing touchscreen input via ﬁnger identiﬁcation. In Proc. CSTIC. IEEE, 1–3. DOI: http://dx.doi.org/10.1109/CSTIC.2016.7463927 \ 20. Sylvain Malacria, Alix Goguey, Gilles Bailly, and Géry Casiez. 2016. Multi-touch Trackpads in the Wild. In Actes De La 28e Conférence Francophone Sur L’Interaction Homme-Machine (IHM ’16). ACM, New York, NY, USA, 19–24. DOI: http://dx.doi.org/10.1145/3004107.3004113 \ 21. Shahzad Malik, Abhishek Ranjan, and Ravin Balakrishnan. 2005. Interacting with Large Displays from a Distance with Vision-tracked Multi-ﬁnger Gestural Input. In Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology (UIST ’05). ACM, New York, NY, USA, 43–52. DOI: http://dx.doi.org/10.1145/1095034.1095042 \ 22. Nicolai Marquardt, Johannes Kiemer, David Ledo, Sebastian Boring, and Saul Greenberg. 2011. Designing User-, Hand-, and Handpart-aware Tabletop Interactions with the TouchID Toolkit. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces (ITS ’11). ACM, New York, NY, USA, 21–30. DOI:http://dx.doi.org/10.1145/2076354.2076358 \ 23. David C. McCallum and Pourang Irani. 2009. ARC-Pad: Absolute+Relative Cursor Positioning for Large Displays with a Mobile Touchscreen. In Proceedings of the 22nd Annual ACM Symposium on User Interface Software and Technology (UIST ’09). ACM, New York, NY, USA, 153–156. DOI: http://dx.doi.org/10.1145/1622176.1622205 \ 24. Sundar Murugappan, Vinayak, Niklas Elmqvist, and Karthik Ramani. 2012. Extended Multitouch: Recovering Touch Posture and Differentiating Users Using a Depth Camera. In Proceedings of the ACM Symposium on User Interface Software and Technology (UIST ’12). ACM, New York, NY, USA, 487–496. DOI:http://dx.doi.org/10.1145/2380116.2380177 \ 25. R.M. Schmitt and R.A. Craig. 2014. Biometric sensing device for three dimensional imaging of subcutaneous structures embedded within ﬁnger tissue. (Aug. 7 2014). https://www.google.com/patents/US20140219521 US Patent App. 14/174,761. \ 26. Measurement Specialties. 2017. Minisense 100 Vibration sensors. (2017). http: //www.te.com/usa-en/product-CAT-PFS0011.html, visited July 10th, 2017. \ 27. Atsushi Sugiura and Yoshiyuki Koseki. 1998. A User Interface Using Fingerprint Recognition: Holding Commands and Data Objects on Fingers. In Proceedings of the 11th Annual ACM Symposium on User Interface Software and Technology (UIST ’98). ACM, 71–79. DOI:http://dx.doi.org/10.1145/288392.288575 \ 28. Katia Vega and Hugo Fuks. 2013. Beauty Tech Nails: Interactive Technology at Your Fingertips. In Proceedings of the 8th International Conference on Tangible, Embedded and Embodied Interaction (TEI ’14). ACM, New York, NY, USA, 61–64. DOI: http://dx.doi.org/10.1145/2540930.2540961 \ 29. Julie Wagner, Eric Lecolinet, and Ted Selker. 2014. Multi-ﬁnger Chords for Hand-held Tablets: Recognizable and Memorable. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’14). ACM, New York, NY, USA, 2883–2892. DOI: http://dx.doi.org/10.1145/2556288.2556958 \ 30. Jingtao Wang and John Canny. 2004. FingerSense: Augmenting Expressiveness to Physical Pushing Button by Fingertip Identiﬁcation. In CHI ’04 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’04). ACM, New York, NY, USA, 1267–1270. DOI:http://dx.doi.org/10.1145/985921.986040 \ 31. Wayne Westerman. 1999. Hand tracking, ﬁnger identiﬁcation, and chordic manipulation on a multi-touch surface. Ph.D. Dissertation. University of Delaware. http://www.eecis.udel.edu/~westerma/main.pdf \ 32. M. Yousefpor, J.M. Bussat, B.B. Lyon, G. Gozzini, S.P. Hotelling, and D. Setlak. 2015. Fingerprint Sensor in an Electronic Device. (2015). http://www.google.com/patents/US20150036065 US Patent App. 14/451,076. \ 33. Jingjie Zheng and Daniel Vogel. 2016. Finger-Aware Shortcuts. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 4274–4285. DOI: http://dx.doi.org/10.1145/2858036.2858355 \ ",finger identification; touch interaction; vibration sensor, H.5.2 Information interfaces (e.g. HCI): User interfaces,uistf2440-file1.zip,uistf2440-file2.jpg,uistf2440-file3.mp4,,WhichFingers uses vibration sensors attached to the fingers to support robust and low latency finger identification.,,WhichFingers combines user events from an input device with the vibration sensors attached to each of user's fingers to achieve low latency and robust finger identification on any input device.,"- added ref to FingeRing in RW \  \ - clarified in the related work section what chords are typically used in the literature and how they are constructed \  \ - removed the part on a futuristic version by simply referring to FingeRing (figure removed, caption and refs need to be edited depending on what replace the figure) \  \ - renamed our wireless version ""WichFingersWiFi"" and moved its description to the discussion section \  \ - updated the analysis \  \ - clarified that in the second experiment we also video recorded all trials and used the videos to label the ground truth. \  \ - clarified that our participants in both experiments were Computer Science university staff and students \  \ - added results for finger sets in exp1: right set of fingers and wrong contact-finger assignment. The results are the same as for Chords (right set of fingers and correct contact-finger assignment) \  \ - renamed ""chord"" and ""finger set"" in the xp2 results section and consistently use contacts, fingers, chords and sets as in exp1 \  \ - reshaped of the discussion section: creation of a ""Single contacts accuracy"", a ""Two or more contacts accuracy"", a ""Causes of recognition errors"", a ""Improving accuracy"", a ""How designers would use WhichFingers"" and a ""Wireless version"" subsections. \ 	 \  \  \  \ Not that a PDF version of the paper where revisions are displayed in pink and sections that have been removed are in light-grey has been uploaded as auxiliary material, and can be accessed at the following address : https://www.dropbox.com/s/05n4hq948vilfob/paperDraft.pdf?dl=0.",Alix Goguey,Sylvain Malacria,FormatComplete,,,,,,,Aug 4 12:27,
uistf2786,10/23,1,Touch,11:40:00 AM,1:00:00 PM,3+2,12:50:00 PM,1:00:00 PM,short,short,uistf2440,5,105,,,uistf2786,A,Carpacio: Repurposing Capacitive Sensors to Distinguish Driver and Passenger Touches on In-Vehicle Screens,Edward,Wang,ejaywang@uw.edu,uistf2786-paper.pdf,7,letter,,,"Edward Jay Wang, Jake Garrison, Eric Whitmire, Mayank Goel, Shwetak N Patel","ejaywang@uw.edu, omonoid@uw.edu, emwhit@cs.washington.edu, mayank@cs.cmu.edu, shwetak@cs.washington.edu",46521,Edward,Jay,Wang,ejaywang@uw.edu,Electrical Engineering,University of Washington,Seattle,Washington,United States,,,,,,71503,Jake,,Garrison,omonoid@uw.edu,Electrical Engineering,University of Washington,Seattle,Washington,United States,,,,,,54493,Eric,,Whitmire,emwhit@cs.washington.edu,Computer Science and Engineering,University of Washington,Seattle,Washington,United States,,,,,,22935,Mayank,,Goel,mayank@cs.cmu.edu,School of Computer Science,Carnegie Mellon University,Pittsburgh,Pennsylvania,United States,,,,,,4536,Shwetak,N,Patel,shwetak@cs.washington.edu,Computer Science and Engineering,University of Washington,Seattle,Washington,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Standard vehicle infotainment systems often include touch screens that allow the driver to control their mobile phone, navigation, audio, and vehicle configurations. For the driver’s safety, these interfaces are often disabled or simplified while the car is in motion. Although this reduced functionality aids in reducing distraction for the driver, it also disrupts the usability of infotainment systems for passengers. Current infotainment systems are unaware of the seating position of their user and hence, cannot adapt. We present Carpacio, a system that takes advantage of the capacitive coupling created between the touchscreen and the electrode present in the seat when the user touches the capacitive screen. Using this capacitive coupling phenomenon, a car infotainment system can intelligently distinguish who is interacting with the screen seamlessly, and adjust its user interface accordingly. Manufacturers can easily incorporate Carpacio into vehicles since the included seat occupancy detection sensor or seat heating coils can be used as the seat electrode. We evaluated Carpacio in eight different cars and five mobile devices and found that it correctly detected over 2600 touches with an accuracy of 99.4%.",ejaywang@uw.edu,"1. John Abbott, Otto Matheke, and Scott Yon. 2015. Passenger Front Air Bag Suppression. Retrieved from https://wwwodi.nhtsa.dot.gov/acms/cs/jaxrs/download/doc/UC M476210/INOA-PE15012-3268.PDF \ 2. Grant S. Anderson and Charles G. Sodini. 2013. Body coupled communication: The channel and implantable sensors. 2013 IEEE International Conference on Body Sensor Networks, BSN 2013, 3– 7. http://doi.org/10.1109/BSN.2013.6575490 \ 3. Bibin Babu. 2011. Connected Me : Hardware for high speed BCC. \ 4. Joonsung Bae, Hyunwoo Cho, Kiseok Song, Hyungwoo Lee, and Hoi Jun Yoo. 2012. The signal transmission mechanism on the surface of human body for body channel communication. IEEE Transactions on Microwave Theory and Techniques 60, 3 PART 1, 582–593. http://doi.org/10.1109/TMTT.2011.2178857 \ 5. Paul H Dietz, Bret Harsham, Clifton Forlines, et al. 2005. DT controls: adding identity to physical interfaces. Proceedings of the ACM Symposium on User Interface Software and Technology, 245–252. http://doi.org/10.1145/1095034.1095075 \ 6. Paul Dietz and Darren Leigh. 2001. DiamondTouch: A Multi-User Touch Technology. UIST ’01, 219– 226. http://doi.org/10.1145/502348.502389 \ 7. Vladimir Fillippov, Kristopher Desrochers, Vleri Stepanov, Otman Adam Basir, and Fakhreddine Karray. 2006. Occupant Presence Detection Device. 2. \ 8. Bernard O. Geaghan. 2003. Touch Screen With Selective Touch Sources. 1. http://doi.org/10.1016/j.(73) \ 9. Tobias Grosse-puppendahl, Christian Holz, Gabe Cohn, et al. 2017. Finding Common Ground : A Survey of Capacitive Sensing in Human-Computer Interaction. http://doi.org/10.1145/3025453.3025808 \ 10. M Hessar, V Iyer, and S Gollakota. 2016. Enabling on-body transmissions with commodity devices. UbiComp ’16 Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing, 452–453. http://doi.org/10.1145/2973750.2985273 \ 11. Christian Holz and Marius Knaust. 2015. Biometric Touch Sensing: Seamlessly Augmenting Each Touch with Continuous Authentication. Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology - UIST ’15, 303– 312. http://doi.org/10.1145/2807442.2807458 \ 12. Luben Hristov. 2011. Enabling capacitive touchscreens to determine touch objects. Automotive Compilation, 1–4. Retrieved from http://goo.gl/TjNSns \ 13. Behailu Kibret, Mir Hojjat Seyedi, Daniel T H Lai, and Micheal Faulkner. 2014. Investigation of galvanic-coupled intrabody communication using the human body circuit model. IEEE Journal of Biomedical and Health Informatics 18, 4, 1196– 1206. http://doi.org/10.1109/JBHI.2014.2301165 \ 14. Nobuyuki Matsushita, Shigeru Tajima, Yuji Ayatsuka, and Jun Rekimoto. 2000. Wearable key: device for personalizing nearby environment. Wearable Computers, 2000. The Fourth International Symposium on, 119–126. http://doi.org/10.1109/ISWC.2000.888473 \ 15. Department of Transportation. 2000. Federal Motor Vehicle Safety Standards; Occupant Crash Protection ; Final Rule. \ 16. David G. Wright. 2006. Method and Apparatus for Discriminating Between User Interactions. 1. http://doi.org/US 2013/0154292 A1 \ 17. Jie Yang, Simon Sidhom, Gayathri Chandrasekaran, et al. 2011. Detecting Driver Phone Use Leveraging Car Speakers. Challenges, 97. http://doi.org/10.1145/2030613.2030625 \ 18. Hoi Jun Yoo and Namjun Cho. 2008. Body channel communication for low energy BSN/BAN. IEEE Asia-Pacific Conference on Circuits and Systems, Proceedings, APCCAS, 7–11. http://doi.org/10.1109/APCCAS.2008.4745947 \ 19. Thomas G. Zimmerman, Joshua R. Smith, Joseph A. Paradiso, David Allport, and Neil Gershenfeld. 1995. Applying Electric Field Sensing to HumanComputer Interfaces. In Proc. CHI ’95. http://doi.org/10.1145/223904.223940 \ 20. Thomas Guthrie Zimmerman. 1995. Personal area networks (PAN): Near-field intra-body communication. http://doi.org/10.1147/sj.353.0609 \ ",Driver detection; Vehicle infotainment; Capacitive sensing; Touch; Driver assistance,H.5.2. [User Interfaces] – Input devices and strategies,uistf2786-file1.docx,,uistf2786-file3.mp4,,,,Carpacio enables driver and passenger differentiation by leveraging the electrical signals that couples to the user's body when touching the car's capacitive screen. ,Revision of related work includes a thorough discussion of DT Controls and discusses the differentiating factors.  \  \ Revision of related work to include works in capacitive body area network work from the HCI and BSN community.  \  \ Added device impedance detail \  \ Added touch duration information of the study \  \ Added discussion about 1.5s window \  \ Removed comment on firmware update \  \  \  \ ,Edward Wang,Jake Garrison,FormatComplete,,,,,,,Jul 26 13:12,
uistf4298,10/23,2,Creativity Support Tools,11:40:00 AM,1:00:00 PM,4,11:40:00 AM,12:00:00 PM,long,long,none,1,201,Honorable Mention,,uistf4298,A,Learning Visual Importance for Graphic Designs and Data Visualizations,Zoya,Bylinskii,zoya@mit.edu,uistf4298-paper.pdf,13,letter,,,"Zoya Bylinskii, Nam Wook Kim, Peter O'Donovan, Sami Alsheikh, Spandan Madan, Hanspeter Pfister, Fredo Durand, Bryan Russell, Aaron Hertzmann","zoya@mit.edu, namwkim@seas.harvard.edu, podonova@adobe.com, alsheikh@mit.edu, spandan_madan@g.harvard.edu, pfister@seas.harvard.edu, fredo@mit.edu, brussell@adobe.com, hertzman@dgp.toronto.edu",49970,Zoya,,Bylinskii,zoya@mit.edu,Computer Science and Artificial Intelligence Laboratory,MIT,Cambridge,Massachusetts,United States,,,,,,21690,Nam Wook,,Kim,namwkim@seas.harvard.edu,SEAS,Harvard University,Cambridge,Massachusetts,United States,,,,,,65376,Peter,,O'Donovan,podonova@adobe.com,,Adobe Systems,Seattle,Washington,United States,,,,,,71406,Sami,,Alsheikh,alsheikh@mit.edu,Computer Science and Artificial Intelligence Laboratory,MIT,Cambridge,Massachusetts,United States,,,,,,71407,Spandan,,Madan,spandan_madan@g.harvard.edu,SEAS,Harvard University,Cambridge,Massachusetts,United States,,,,,,18776,Hanspeter,,Pfister,pfister@seas.harvard.edu,School of Engineering and Applied Sciences,Harvard University,Cambridge,Massachusetts,United States,,,,,,60513,Fredo,,Durand,fredo@mit.edu,MIT,CSAIL,Cambridge,Massachusetts,United States,,,,,,65377,Bryan,,Russell,brussell@adobe.com,,Adobe Research,San Francisco,California,United States,,,,,,9020,Aaron,,Hertzmann,hertzman@dgp.toronto.edu,Adobe Research,Adobe Research,San Francisco,CA,USA,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process. ",zoya@mit.edu,"1. Shai Avidan and Ariel Shamir. 2007. Seam Carving for Content-aware Image Resizing. ACM Trans. Graph. 26, 3, Article 10 (July 2007). DOI: http://dx.doi.org/10.1145/1276377.1276390 \ 2. Michelle A. Borkin, Zoya Bylinskii, Nam Wook Kim, Constance May Bainbridge, Chelsea S. Yeh, Daniel Borkin, Hanspeter Pﬁster, and Aude Oliva. 2016. Beyond Memorability: Visualization Recognition and Recall. IEEE Transactions on Visualization and Computer Graphics 22, 1 (Jan 2016), 519–528. DOI: http://dx.doi.org/10.1109/TVCG.2015.2467732 \ 3. Michelle A. Borkin, Azalea A Vo, Zoya Bylinskii, Phillip Isola, Shashank Sunkavalli, Aude Oliva, and Hanspeter Pﬁster. 2013. What Makes a Visualization Memorable? IEEE Transactions on Visualization and Computer Graphics 19, 12 (Dec 2013), 2306–2315. DOI: http://dx.doi.org/10.1109/TVCG.2013.234 \ 4. Georg Buscher, Edward Cutrell, and Meredith Ringel Morris. 2009. What Do You See when You’Re Surﬁng?: Using Eye Tracking to Predict Salient Regions of Web Pages. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’09). ACM, New York, NY, USA, 21–30. DOI: http://dx.doi.org/10.1145/1518701.1518705 \ 5. Zoya Bylinskii, Tilke Judd, Ali Borji, Laurent Itti, Frédo Durand, Aude Oliva, and Antonio Torralba. 2012. MIT Saliency Benchmark. (2012). \ 6. Zoya Bylinskii, Tilke Judd, Aude Oliva, Antonio Torralba, and Frédo Durand. 2016a. What do different evaluation metrics tell us about saliency models? CoRR abs/1604.03605 (2016). http://arxiv.org/abs/1604.03605 \ 7. Zoya Bylinskii, Adrià Recasens, Ali Borji, Aude Oliva, Antonio Torralba, and Frédo Durand. 2016b. Where Should Saliency Models Look Next? Springer International Publishing (ECCV), Cham, 809–824. DOI: http://dx.doi.org/10.1007/978-3-319-46454-1_49 \ 8. Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, and Rita Cucchiara. 2016. A deep multi-level network for saliency prediction. In 2016 23rd International Conference on Pattern Recognition (ICPR). 3488–3493. DOI:http://dx.doi.org/10.1109/ICPR.2016.7900174 \ 9. Andrew T Duchowski. 2007. Eye tracking methodology. Theory and practice 328 (2007). \ 10. Stas Goferman, Lihi Zelnik-Manor, and Ayellet Tal. 2012. Context-Aware Saliency Detection. IEEE Transactions on Pattern Analysis and Machine Intelligence 34, 10 (Oct 2012), 1915–1926. DOI: http://dx.doi.org/10.1109/TPAMI.2011.272 \ 11. Michael J. Haass, Andrew T. Wilson, Laura E. Matzen, and Kristin M. Divis. 2016. Modeling Human Comprehension of Data Visualizations. Springer International Publishing (VAMR), Cham, 125–134. DOI: http://dx.doi.org/10.1007/978-3-319-39907-2_12 \ 12. Lane Harrison, Katharina Reinecke, and Remco Chang. 2015. Infographic Aesthetics: Designing for the First Impression. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15). ACM, New York, NY, USA, 1187–1190. DOI: http://dx.doi.org/10.1145/2702123.2702545 \ 13. Xiaodi Hou and Liqing Zhang. 2007. Saliency Detection: A Spectral Residual Approach. In 2007 IEEE Conference on Computer Vision and Pattern Recognition. 1–8. DOI: http://dx.doi.org/10.1109/CVPR.2007.383267 \ 14. Xun Huang, Chengyao Shen, Xavier Boix, and Qi Zhao. 2015. SALICON: Reducing the Semantic Gap in Saliency Prediction by Adapting Deep Neural Networks. In 2015 IEEE International Conference on Computer Vision (ICCV). 262–270. DOI: http://dx.doi.org/10.1109/ICCV.2015.38 \ 15. Laurent Itti, Christof Koch, and Ernst Niebur. 1998. A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence 20, 11 (Nov 1998), 1254–1259. DOI:http://dx.doi.org/10.1109/34.730558 \ 16. Robert Jacob and Keith S Karn. 2003. Eye tracking in human-computer interaction and usability research: Ready to deliver the promises. Mind 2, 3 (2003), 4. \ 17. Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross B. Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional Architecture for Fast Feature Embedding. CoRR abs/1408.5093 (2014). http://arxiv.org/abs/1408.5093 \ 18. Ming Jiang, Shengsheng Huang, Juanyong Duan, and Qi Zhao. 2015. SALICON: Saliency in Context. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 1072–1080. DOI: http://dx.doi.org/10.1109/CVPR.2015.7298710 \ 19. Binxing Jiao, Linjun Yang, Jizheng Xu, and Feng Wu. 2010. Visual Summarization of Web Pages. In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’10). ACM, New York, NY, USA, 499–506. DOI: http://dx.doi.org/10.1145/1835449.1835533 \ 20. Tilke Judd, Frédo Durand, and Antonio Torralba. 2012. A Benchmark of Computational Models of Saliency to Predict Human Fixations. In MIT Technical Report. \ 21. Tilke Judd, Krista Ehinger, Frédo Durand, and Antonio Torralba. 2009. Learning to predict where humans look. In 2009 IEEE 12th International Conference on Computer Vision. 2106–2113. DOI: http://dx.doi.org/10.1109/ICCV.2009.5459462 \ 22. Nam Wook Kim, Zoya Bylinskii, Michelle A. Borkin, Krzysztof Z. Gajos, Aude Oliva, Frédo Durand, and Hanspeter Pﬁster. 2017. BubbleView: an interface for crowdsourcing image importance maps and tracking visual attention. TOCHI (2017). DOI: http://dx.doi.org/10.1145/3131275 \ 23. Nam Wook Kim, Zoya Bylinskii, Michelle A. Borkin, Aude Oliva, Krzysztof Z. Gajos, and Hanspeter Pﬁster. 2015. A Crowdsourced Alternative to Eye-tracking for Visualization Understanding. In Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA ’15). ACM, New York, NY, USA, 1349–1354. DOI: http://dx.doi.org/10.1145/2702613.2732934 \ 24. Kyle Krafka, Aditya Khosla, Petr Kellnhofer, Harini Kannan, Suchendra Bhandarkar, Wojciech Matusik, and Antonio Torralba. 2016. Eye Tracking for Everyone. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2176–2184. DOI: http://dx.doi.org/10.1109/CVPR.2016.239 \ 25. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet Classiﬁcation with Deep Convolutional Neural Networks. In Proceedings of the 25th International Conference on Neural Information Processing Systems (NIPS’12). Curran Associates Inc., USA, 1097–1105. http://dl.acm.org/citation.cfm?id=2999134.2999257 \ 26. Srinivas S. Kruthiventi, Kumar Ayush, and R. Venkatesh Babu. 2015. DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations. CoRR abs/1510.02927 (2015). http://arxiv.org/abs/1510.02927 \ 27. Ranjitha Kumar, Arvind Satyanarayan, Cesar Torres, Maxine Lim, Salman Ahmad, Scott R. Klemmer, and Jerry O. Talton. 2013. Webzeitgeist: Design Mining the Web. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’13). ACM, New York, NY, USA, 3083–3092. DOI: http://dx.doi.org/10.1145/2470654.2466420 \ 28. Ranjitha Kumar, Jerry O. Talton, Salman Ahmad, and Scott R. Klemmer. 2011. Bricolage: Example-based Retargeting for Web Design. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’11). ACM, New York, NY, USA, 2197–2206. DOI: http://dx.doi.org/10.1145/1978942.1979262 \ 29. Matthias Kümmerer, Lucas Theis, and Matthias Bethge. 2014. Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet. CoRR abs/1411.1045 (2014). http://arxiv.org/abs/1411.1045 \ 30. Matthias Kümmerer, Thomas S. A. Wallis, and Matthias Bethge. 2017. Saliency Benchmarking: Separating Models, Maps and Metrics. CoRR abs/1704.08615 (2017). http://arxiv.org/abs/1704.08615 \ 31. Sharon Lin and Pat Hanrahan. 2013. Modeling How People Extract Color Themes from Images. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’13). ACM, New York, NY, USA, 3101–3110. DOI: http://dx.doi.org/10.1145/2470654.2466424 \ 32. Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2017. Fully Convolutional Networks for Semantic Segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence 39, 4 (April 2017), 640–651. DOI:http://dx.doi.org/10.1109/TPAMI.2016.2572683 \ 33. Peter O’Donovan, Aseem Agarwala, and Aaron Hertzmann. 2014. Learning Layouts for Single-PageGraphic Designs. IEEE Transactions on Visualization and Computer Graphics 20, 8 (Aug 2014), 1200–1213. DOI: http://dx.doi.org/10.1109/TVCG.2014.48 \ 34. Junting Pan, Kevin McGuinness, Elisa Sayrol, Noel E. O’Connor, and Xavier Giró i Nieto. 2016a. Shallow and Deep Convolutional Networks for Saliency Prediction. CoRR abs/1603.00845 (2016). http://arxiv.org/abs/1603.00845 \ 35. Junting Pan, Elisa Sayrol, Xavier Giro-i Nieto, Kevin McGuinness, and Noel E O’Connor. 2016b. Shallow and deep convolutional networks for saliency prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 598–606. \ 36. Xufang Pang, Ying Cao, Rynson W. H. Lau, and Antoni B. Chan. 2016. Directing User Attention via Visual Flow on Web Designs. ACM Trans. Graph. 35, 6, Article 240 (Nov. 2016), 11 pages. DOI: http://dx.doi.org/10.1145/2980179.2982422 \ 37. Alexandra Papoutsaki, Patsorn Sangkloy, James Laskey, Nediyana Daskalova, Jeff Huang, and James Hays. 2016. Webgazer: Scalable Webcam Eye Tracking Using User Interactions. In Proceedings of the Twenty-Fifth International Joint Conference on Artiﬁcial Intelligence (IJCAI’16). AAAI Press, 3839–3845. http://dl.acm.org/citation.cfm?id=3061053.3061156 \ 38. Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. 2014. CNN Features Off-the-Shelf: An Astounding Baseline for Recognition. In Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW ’14). IEEE Computer Society, Washington, DC, USA, 512–519. DOI: http://dx.doi.org/10.1109/CVPRW.2014.131 \ 39. Ronald A Rensink. 2011. The management of visual attention in graphic displays. Cambridge University Press, Cambridge, England. \ 40. Ruth Rosenholtz, Amal Dorai, and Rosalind Freeman. 2011. Do Predictions of Visual Perception Aid Design? ACM Trans. Appl. Percept. 8, 2, Article 12 (Feb. 2011), 20 pages. DOI: http://dx.doi.org/10.1145/1870076.1870080 \ 41. Michael Rubinstein, Diego Gutierrez, Olga Sorkine, and Ariel Shamir. 2010. A Comparative Study of Image Retargeting. ACM Trans. Graph. 29, 6, Article 160 (Dec. 2010), 10 pages. DOI: http://dx.doi.org/10.1145/1882261.1866186 \ 42. Manolis Savva, Nicholas Kong, Arti Chhajta, Li Fei-Fei, Maneesh Agrawala, and Jeffrey Heer. 2011. ReVision: Automated Classiﬁcation, Analysis and Redesign of Chart Images. In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (UIST ’11). ACM, New York, NY, USA, 393–402. DOI: http://dx.doi.org/10.1145/2047196.2047247 \ 43. Chengyao Shen and Qi Zhao. 2014. Webpage Saliency. Springer International Publishing (ECCV), Cham, 33–46. DOI:http://dx.doi.org/10.1007/978-3-319-10584-0_3 \ 44. Karen Simonyan and Andrew Zisserman. 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. CoRR abs/1409.1556 (2014). http://arxiv.org/abs/1409.1556 \ 45. Jeremiah D Still and Christopher M Masciocchi. 2010. A saliency model predicts ﬁxations in web interfaces. In 5 th International Workshop on Model Driven Development of Advanced User Interfaces (MDDAUI 2010). Citeseer, 25. \ 46. Jaime Teevan, Edward Cutrell, Danyel Fisher, Steven M. Drucker, Gonzalo Ramos, Paul André, and Chang Hu. 2009. Visual Snippets: Summarizing Web Pages for Search and Revisitation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’09). ACM, New York, NY, USA, 2023–2032. DOI: http://dx.doi.org/10.1145/1518701.1519008 \ 47. Allison Woodruff, Andrew Faulring, Ruth Rosenholtz, Julie Morrsion, and Peter Pirolli. 2001. Using Thumbnails to Search the Web. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’01). ACM, New York, NY, USA, 198–205. DOI:http://dx.doi.org/10.1145/365024.365098 \ 48. Pingmei Xu, Krista A. Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R. Kulkarni, and Jianxiong Xiao. 2015. TurkerGaze: Crowdsourcing Saliency with Webcam based Eye Tracking. CoRR abs/1504.06755 (2015). http://arxiv.org/abs/1504.06755 \ 49. Rui Zhao, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang. 2015. Saliency detection by multi-context deep learning. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 1265–1274. DOI: http://dx.doi.org/10.1109/CVPR.2015.7298731 \ ",Saliency; Computer Vision; Machine Learning; Eye Tracking; Visualization; Graphic Design; Deep Learning; Retargeting,H.5.1,uistf4298-file1.zip,uistf4298-file2.jpg,uistf4298-file3.mp4,uistf4298-file4.zip,Our automatic model predicts the importance of graphic design elements and can offer immediate feedback in an interactive design tool. ,"We include Supplemental Materials in a PDF document, with additional experimental details and figures to accompany the paper.","Our model automatically predicts the importance of image elements in data visualizations and graphic designs. We use it for retargeting, for thumbnailing, and for interactive feedback in a design tool.","We updated our submission and marked all major changes in red font. The marked paper can be found in the attached Auxiliary Materials. Below we indicate how we modified the paper in response to reviewer suggestions/comments. \  \ *** General *** \  \ * Added a paragraph at the end of the Introduction to clarify the contributions of this work, including models and datasets. \ * Added a “Limitations” section at the end of the paper to account for some of the concerns raised by reviewers. \ * Added a link to our project page http://visimportance.csail.mit.edu/ which currently has working demos, and to which we will upload code, trained models, and data.  \ * Changed the tables format to 'booktabs' to reduce clutter.  \  \ *** Additional time-complexity analysis (R2) *** \  \ * Added a small subsection on “Timing” under the “Interactive applications” section. Added Table 4. \  \ *** Experimental details and analysis (R1,R2) *** \  \ * Moved some of the experimental details of the user studies from the Supplemental Material to the main paper under the “Evaluation” subsections of the “Applications” section. Added requested statistics. \  \ *** Generalization to data visualizations (R3,R4) *** \  \ * Added a statement to the “Data Collection” section to indicate the diversity of the data visualizations used.  \ * Added some discussion to the “Limitations” section.  \  \ *** Comparison to OD model (R1) *** \  \ * Added an extra paragraph under “Prediction performance on graphic designs” to explain the speed trade-off of the OD model, as well as the provide more details about how Ours+OD was computed. \  \ *** Interpretation of CC and KL scores (R1,R2) *** \  \ * Added extra text to the evaluation section to discuss the metrics used and their properties. \  \ *** Minor requests (R2) *** \  \ * Updated the caption in Fig. 6 for clarity. \ * Added citations to [9] and [38] as requested. We also added [30], a recent paper on saliency metrics that is relevant to this paper. \ * Ensured consistent use of the terms “MTurk” and “neural network” throughout the paper. \ ",Zoya Bylinskii,Peter O'Donovan,FormatComplete,PGS-D,Natural Sciences and Engineering Research Council of Canada ,,,,,Aug 8 17:37,
uistf2024,10/23,2,Creativity Support Tools,11:40:00 AM,1:00:00 PM,4,12:00:00 PM,12:20:00 PM,long,long,uistf4298,2,202,,,uistf2024,A,ImAxes: Immersive Axes as Embodied Affordances for Interactive Multivariate Data Visualisation,Maxime,Cordeil,maxime.cordeil@gmail.com,uistf2024-paper.pdf,13,letter,,,"Maxime Cordeil, Andrew Cunningham, Tim Dwyer, Bruce H Thomas, Kim Marriott","max.cordeil@monash.edu, andrewcunningham@mac.com, tgdwyer@gmail.com, bruce.thomas@unisa.edu.au, kim.marriott@monash.edu",67375,Maxime,,Cordeil,max.cordeil@monash.edu,Information Technology,Monash University,Melbourne,Victoria,Australia,,,,,,9881,Andrew,,Cunningham,andrewcunningham@mac.com,School of Information Technology & Mathematical Scineces,University of South Australia,Adelaide,South Australia,Australia,,,,,,13327,Tim,,Dwyer,tgdwyer@gmail.com,Faculty of IT,Monash University,Melbourne,VIC,Australia,,,,,,4652,Bruce,H,Thomas,bruce.thomas@unisa.edu.au,School of Computer and Information Science,The University of South Australia,Mawson Lakes,South Australia,Australia,,,,,,38606,Kim,,Marriott,kim.marriott@monash.edu,Faculty of Information Technology,Monash University,Melbourne,Victoria,Australia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We introduce ImAxes immersive system for exploring multivariate data using fluid, modeless interaction. \ The basic interface element is an embodied data axis.  The user can manipulate these axes like physical objects in the immersive environment and combine them into sophisticated visualisations.  The type of visualisation that appears depends on the proximity and relative orientation of the axes with respect to one another, which we describe with a formal grammar.  This straight-forward composability leads to a number of emergent visualisations and interactions, which we review, and then demonstrate with a detailed multivariate data analysis use case.",maxime.cordeil@gmail.com,"1. Bach, B., Dachselt, R., Carpendale, S., Dwyer, T., Collins, C., and Lee, B. Immersive analytics: Exploring future interaction and visualization technologies for data analytics. In Proceedings of the 2016 ACM on Interactive Surfaces and Spaces, ISS ’16, ACM (New York, NY, USA, 2016), 529–533. \ 2. Bach, B., Sicat, R., Beyer, J., Cordeil, M., and Pﬁster, H. The hologram in my hand: How effective is interactive exploration of 3d visualizations in immersive tangible augmented reality? IEEE Transactions on Visualization and Computer Graphics (2018). \ 3. Bartram, L., and Ware, C. Filtering and brushing with motion. Information Visualization 1, 1 (2002), 66–79. \ 4. Bennett, R., Zielinski, D. J., and Kopper, R. Comparison of Interactive Environments for the Archaeological Exploration of 3D Landscape Data. In IEEE VIS International Workshop on 3DVis (2014). \ 5. Brandes, U., Dwyer, T., and Schreiber, F. Visual understanding of metabolic pathways across organisms using layout in two and a half dimensions. Journal of Integrative Bioinformatics (JIB) 1, 1 (2004), 11–26. \ 6. Bryson, S. Virtual reality in scientiﬁc visualization. Communications of the ACM 39, 5 (1996), 62–71. \ 7. Buja, A., McDonald, J. A., Michalak, J., and Stuetzle, W. Interactive data visualization using focusing and linking. In Proceedings of the 2Nd Conference on Visualization ’91, VIS ’91, IEEE Computer Society Press (Los Alamitos, CA, USA, 1991), 156–163. \ 8. Buja, A., Swayne, D. F., Littman, M. L., Dean, N., Hofmann, H., and Chen, L. Data visualization with multidimensional scaling. Journal of Computational and Graphical Statistics 17, 2 (2008), 444–472. \ 9. Chandler, T., Cordeil, M., Czauderna, T., Dwyer, T., Glowacki, J., Goncu, C., Klapperstueck, M., Klein, K., Marriott, K., Schreiber, F., et al. Immersive analytics. In Big Data Visual Analytics (BDVA), 2015, IEEE (2015), 1–8. \ 10. Claessen, J. H., and Van Wijk, J. J. Flexible linked axes for multivariate data visualization. IEEE Transactions on Visualization and Computer Graphics 17, 12 (2011), 2310–2316. \ 11. Collins, C., and Carpendale, S. Vislink: Revealing relationships amongst visualizations. IEEE Transactions on Visualization and Computer Graphics 13, 6 (2007), 1192–1199. \ 12. Cordeil, M., Bach, B., Li, Y., Wilson, E., and Dwyer, T. A design space for spatio-data coordination: Tangible interaction devices for immersive information visualisation. In Proceedings of the 10th IEEE Paciﬁc Visualization Symposium (PaciﬁcVis) (2017). \ 13. Cortez, P., Cerdeira, A., Almeida, F., Matos, T., and Reis, J. Modeling wine preferences by data mining from physicochemical properties. Decision Support Systems 47, 4 (2009), 547–553. \ 14. Dourish, P. Where the action is: the foundations of embodied interaction. MIT press, 2004. \ 15. Elmqvist, N., Dragicevic, P., and Fekete, J.-D. Rolling the dice: Multidimensional visual exploration using scatterplot matrix navigation. IEEE transactions on Visualization and Computer Graphics 14, 6 (2008), 1539–1148. \ 16. Elmqvist, N., Moere, A. V., Jetter, H.-C., Cernea, D., Reiterer, H., and Jankun-Kelly, T. Fluid interaction for information visualization. Information Visualization 10, 4 (2011), 327–340. \ 17. Falconer, C. J., Slater, M., Rovira, A., King, J. A., Gilbert, P., Antley, A., and Brewin, C. R. Embodying compassion: A virtual reality paradigm for overcoming excessive self-criticism. PLOS ONE 9, 11 (11 2014), 1–7. \ 18. Gaver, W. W. Technology affordances. In Proceedings of the SIGCHI conference on Human factors in computing systems, ACM (1991), 79–84. \ 19. Gracia, A., Gonz´alez, S., Robles, V., Menasalvas, E., and von Landesberger, T. New insights into the suitability of the third dimension for visualizing multivariate/multidimensional data: A study based on loss of quality quantiﬁcation. Information Visualization (2014), 1473871614556393. \ 20. Gratzl, S., Gehlenborg, N., Lex, A., Pﬁster, H., and Streit, M. Domino: Extracting, comparing, and manipulating subsets across multiple tabular datasets. IEEE transactions on visualization and computer graphics 20, 12 (2014), 2023–2032. \ 21. Greffard, N., Picarougne, F., and Kuntz, P. Visual community detection: An evaluation of 2d, 3d perspective and 3d stereoscopic displays. In Graph Drawing, Springer (2011), 215–225. \ 22. Helbig, C., Bauer, H.-S., Rink, K., Wulfmeyer, V., Frank, M., and Kolditz, O. Concept and workﬂow for 3d visualization of atmospheric data in a virtual reality environment for analytical approaches. Environmental Earth Sciences 72, 10 (2014), 3767–3780. \ 23. Hohmann, B., Krispel, U., Havemann, S., and Fellner, D. Cityﬁt-high-quality urban reconstructions by ﬁtting shape grammars to images and derived textured point clouds. In Proceedings of the 3rd ISPRS International Workshop 3D-ARCH, vol. 2009 (2009), 3D. \ 24. Hsieh, T.-J., Chang, Y.-L., and Huang, B. Visual Analytics of Terrestrial Lidar Data for Cliff Erosion Assessment on Large Displays. In Proceedings SPIE Satellite Data Compression, Communications, and Processing VII, vol. 8157, SPIE (2011), 81570D.1–17. \ 25. Inselberg, A. Multidimensional detective. In Information Visualization, 1997. Proceedings., IEEE Symposium on, IEEE (1997), 100–107. \ 26. Jacob, R. J., Girouard, A., Hirshﬁeld, L. M., Horn, M. S., Shaer, O., Solovey, E. T., and Zigelbaum, J. Reality-based interaction: a framework for post-wimp interfaces. In Proceedings of the SIGCHI conference on Human factors in computing systems, ACM (2008), 201–210. \ 27. Jansen, Y., and Dragicevic, P. An interaction model for visualizations beyond the desktop. IEEE Transactions on Visualization and Computer Graphics 19, 12 (2013), 2396–2405. \ 28. Jansen, Y., Dragicevic, P., and Fekete, J.-D. Tangible remote controllers for wall-size displays. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM (2012), 2865–2874. \ 29. Jansen, Y., Dragicevic, P., and Fekete, J.-D. Evaluating the efﬁciency of physical visualizations. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM (2013), 2593–2602. \ 30. Kageyama, A., Tamura, Y., and Sato, T. Visualization of Vector Field by Virtual Reality. Progress of Theoretical Physics Supplement 138 (2000), 665–673. \ 31. Kilten, K., Groten, R., and Slater, M. The sense of embodiment in virtual reality. Presence: Teleoperators & Virtual Environments 21, 4 (2012), 373 – 387. \ 32. Kosara, R. Indirect multi-touch interaction for brushing in parallel coordinates. In IS&T/SPIE Electronic Imaging, International Society for Optics and Photonics (2011), 786809–786809. \ 33. Kurillo, G., and Forte, M. Telearch – Integrated visual simulation environment for collaborative virtual archaeology. Mediterranean Archaeology and Archaeometry 12, 1 (2012), 11–20. \ 34. Lee, B., Isenberg, P., Riche, N. H., and Carpendale, S. Beyond mouse and keyboard: Expanding design considerations for information visualization interactions. IEEE Transactions on Visualization and Computer Graphics 18, 12 (2012), 2689–2698. \ 35. Lee, J. M., MacLachlan, J., and Wallace, W. A. The effects of 3d imagery on managerial data interpretation. MIS Quarterly (1986), 257–269. \ 36. Loorak, M. H., Perin, C., Collins, C., and Carpendale, S. Exploring the possibilities of embedding heterogeneous data attributes in familiar visualizations. IEEE Transactions on Visualization and Computer Graphics 23, 1 (2017), 581–590. \ 37. Mackinlay, J. Automating the design of graphical presentations of relational information. Acm Transactions On Graphics (Tog) 5, 2 (1986), 110–141. \ 38. Marriott, K., Meyer, B., and Wittenburg, K. B. A survey of visual language speciﬁcation and recognition. In Visual language theory. Springer, 1998, 5–85. \ 39. Mine, M. R., Brooks Jr, F. P., and Sequin, C. H. Moving objects in space: exploiting proprioception in virtual-environment interaction. In Proceedings of the 24th annual conference on Computer graphics and interactive techniques, ACM Press/Addison-Wesley Publishing Co. (1997), 19–26. \ 40. M¨uller, P., Wonka, P., Haegler, S., Ulmer, A., and Van Gool, L. Procedural modeling of buildings. In ACM Transactions On Graphics (Tog), vol. 25, ACM (2006), 614–623. \ 41. Munzner, T. Visualization analysis and design. CRC Press, 2014. \ 42. Nelson, L., Cook, D., and Cruz-Neira, C. Xgobi vs the c2: Results of an experiment comparing data visualization in a 3-d immersive virtual reality environment with a 2-d workstation display. Computational Statistics 14, 1 (1999), 39–52. \ 43. Nolle, S., and Klinker, G. Augmented reality as a comparison tool in automotive industry. In Mixed and Augmented Reality, 2006. ISMAR 2006. IEEE/ACM International Symposium on, IEEE (2006), 249–250. \ 44. Oestreich, T. W. Magic quadrant for business intelligence and analytics platforms. Analyst (s) 501 (2016), G00275847. \ 45. Ribarsky, W., Bolter, J., den Bosch, A. O., and van Teylingen, R. Visualization and analysis using virtual reality. IEEE Computer Graphics and Applications 14, 1 (Jan 1994), 10–12. \ 46. Sadana, R., and Stasko, J. Expanding selection for information visualization systems on tablet devices. In Proceedings of the 2016 ACM on Interactive Surfaces and Spaces, ACM (2016), 149–158. \ 47. Satyanarayan, A., and Heer, J. Lyra: An interactive visualization design environment. In Computer Graphics Forum, vol. 33, Wiley Online Library (2014), 351–360. \ 48. Schaefer, G., Budnik, M., and Krawczyk, B. Immersive browsing in an image sphere. In Proceedings of the 11th International Conference on Ubiquitous Information Management and Communication, IMCOM ’17, ACM (New York, NY, USA, 2017), 26:1–26:4. \ 49. Sedlmair, M., Munzner, T., and Tory, M. Empirical guidance on scatterplot and dimension reduction technique choices. IEEE transactions on visualization and computer graphics 19, 12 (2013), 2634–2643. \ 50. Slater, M. A note on presence terminology. Presence connect 3, 3 (2003), 1–5. \ 51. Slater, M., and Wilbur, S. A framework for immersive virtual environments (ﬁve): Speculations on the role of presence in virtual environments. Presence: Teleoperators and virtual environments 6, 6 (1997), 603–616. \ 52. Smith, N. G., Knabb, K., DeFanti, C., Weber, P., Schulze, J., Prudhomme, A., Kuester, F., Levy, T. E., and DeFanti, T. A. ArtifactVis2: Managing real-time archaeological data in immersive 3D environments. In Proceedings Digital Heritage International Congress, vol. 1, IEEE (2013), 363–370. \ 53. St. John, M., Cowen, M. B., Smallman, H. S., and Oonk, H. M. The use of 2d and 3d displays for shape-understanding versus relative-position tasks. Human Factors: The Journal of the Human Factors and Ergonomics Society 43, 1 (2001), 79–98. \ 54. St. John, M., Smallman, H. S., Bank, T. E., and Cowen, M. B. Tactical routing using two-dimensional and three-dimensional views of terrain. In Proceedings of the Human Factors and Ergonomics Society Annual Meeting, vol. 45, SAGE Publications (2001), 1409–1413. \ 55. Stahnke, J., D¨ork, M., M¨uller, B., and Thom, A. Probing projections: Interaction techniques for interpreting arrangements and errors of dimensionality reductions. IEEE transactions on visualization and computer graphics 22, 1 (2016), 629–638. \ 56. Thomas, B., Piekarski, W., and Gunther, B. Using augmented reality to visualise architecture designs in an outdoor environment. International Journal of Design Computing Special Issue on Design Computing on the Net (DCNet) 1, 4.2 (1999). \ 57. Tory, M., Kirkpatrick, A. E., Atkins, M. S., and M¨oller, T. Visualization task performance with 2d, 3d, and combination displays. Visualization and Computer Graphics, IEEE Transactions on 12, 1 (2006), 2–13. \ 58. Tukey, J. W., and Tukey, P. A. Computer graphics and exploratory data analysis: An introduction. The Collected Works of John W. Tukey: Graphics: 1965-1985 5 (1988), 419. \ 59. Ware, C., and Franck, G. Viewing a graph in a virtual reality display is three times as good as a 2d diagram. In Visual Languages, 1994. Proceedings., IEEE Symposium on (Oct 1994), 182–183. \ 60. Ware, C., and Mitchell, P. Reevaluating stereo and motion cues for visualizing graphs in three dimensions. In Proceedings of the 2nd symposium on Applied perception in graphics and visualization, ACM (2005), 51–58. \ 61. Westerman, S. J., and Cribbin, T. Mapping semantic information in virtual space: dimensions, variance and individual differences. International Journal of Human-Computer Studies 53, 5 (2000), 765–787. \ 62. Wickens, C. D., Merwin, D. H., and Lin, E. L. Implications of graphics enhancements for the visualization of scientiﬁc data: Dimensional integrality, stereopsis, motion, and mesh. Human Factors 36, 1 (1994), 44–61. \ 63. Wilkinson, L. The grammar of graphics. Springer Science & Business Media, 2006. \ 64. Zhang, S., Demiralp, C., Keefe, D., DaSilva, M., Laidlaw, D., Greenberg, B., Basser, P., Pierpaoli, C., Chiocca, E., and Deisboeck, T. An immersive virtual environment for DT-MRI volume visualization applications: a case study. In Proceedings Visualization 2001, IEEE (2001), 437–440. \ ",Information Visualization; Virtual Reality; Multidimensional Data Visualization; Immersion; Immersive Visualization; Immersive Analytics,H.5.2,uistf2024-file1.zip,,uistf2024-file3.mp4,uistf2024-file4.zip,,The auxiliary files contain the letter to the AC and the author version of the paper with highlighted changes from the 1st submission.,"We introduce a VR system for exploring data using fluid, modeless interaction. The user manipulates embodied data dimensions, which are recognised with a formal spatial grammar. This leads to emergent visualisations and interactions.","Submission # 2024 \ ImAxes: Immersive Axes as Embodied Affordances for Interactive Multivariate Data \ Visualisation \  \ We would like to thank the reviewers for their time and their constructive feedback. In this \ letter we list the changes made since our initial submission, which cover the changes \ promised in our rebuttal but also taking into account the general enquiries and concerns of \ the reviewers since the rebuttal, especially as highlighted by 1AC and 2AC. This submission \ includes two PDF files for the latest version of our paper, one with changes highlighted in \ yellow, and one without. \  \ 1AC \ --Is the lack of automated generation tools and high-level visualization constructs really a \ problem in a research testbed intended to address VR interaction techniques for \ visualization? \ We added a discussion paragraph regarding the automated generation tools and analytics in \ the Limitation section. We emphasise the need for a user evaluation in the next paragraph. \ --Organizational issues pointed out by R2 \ We reworked our related work, more details under the Literature Review, 3D Infovis \ section of this letter. We included more references in the discussion of 3D infovis and \ reworked this section to emphasise that our focus is more on beyond the desktop MDV \ interaction and visualisation. \  \ 2AC \ --Limitation discussion at the end of the paper \ We added a limitation section in our paper that discusses scalability, interaction vs \ automation and evaluation. We included Jansens’ et al. physical visualisation \ --Deeper dive into the literature could also be included in the related work section \ We added references to recent InfoVis work that investigates abstract visualisation using \ Virtual Reality. \ The following changes were made based upon our rebuttal list: \ Major changes \ We reworked our literature review in order to: \ - Reduce and simplify the MDV section (especially the references to MDS-style \ techniques) \ - Merged Virtual Reality Visualisation and 3D infovis, and added a substantial number \ of references regarding 2D vs 3D infovis, and 3D infovis in general. \  \ The ImAxes Grammar section has been reworked: \ - We added illustrative diagrams for the grammar rule \ - We redefined the operators to recognise the symbols in the grammar \ - We added an explanation in the caption of figure 4 to help the reader understand \ how ImAxes deals with ambiguities \  \ We added a limitation section before our conclusion. In this section we discuss the \ automated generation issue, the need for future usability testing and investigation, and the \ actual limitations of our current system (number of axes, data size, available visualisations). \  \ Automated Generation \ We discuss the need of automated analytics tools and presentations. \  \ Literature Review, 3D InfoVis \ We merged the Multidimensional Data Visualisation (MDV) and interactive MDV into a single \ Interactive Multidimensional Data Visualisation section. We also simplified the related work \ and number of references to MDV techniques (such as MDS) to set more the focus on \ interactive techniques. \ We merged Visualisation in Virtual and 3D information visualisation into the same section. \ We reworked this section to include the Wickens and the Sedlmair & Munzner references to \ the discussion on 3D infovis. \ We also added related work that shows that the InfoVis community currently investigates \ Mixed Reality and Virtual Reality for visualisation / visual analytics purpose. \  \ Grammar section - clarify our contribution \ We added Mackinlay’s Automating the Design of Graphical Presentations of Relational \ Information reference, acknowledging his previous formalisation of the primitive languages \ and operators involved in the description and the generation of visualisations. We clarify our \ work by positioning it as a formal grammar for constructing visualisations in VREs using the \ axes’ spatial placement. \  \ Grammar section - insufficient explanation \ We clarified our grammar section by reducing the number of rules and added diagrams that \ visually support the ImAxes symbol creation rules. We reworked the formalism to define the \ different SPLOMs (we renamed splom1,2,3 to S1,2,3), and clarified the conditions to create \ such SPLOMs (that is the parallel, orthogonal and connected conditions). We added red \ visual symbols to illustrate the conditions and guide the reader. \ We also clarified how the grammar works with ambiguities (Figure 4) by breaking down the \ steps of the symbol recognition in ImAxes. \  \ Technical implementation \ We provide the following diagram to help the reader understanding how ImAxes works, as \ suggested by the reviewers. \  \ Usability testing \ We discuss usability testing in the Limitations section before our Conclusion. \ Additional modifications \ We added a Fluid Visualisation item in our Emergent Interactions list, page 7. \ We reduced and simplify our scenario section. In particular we deleted redondants user \ interactions, such as the consecutive creation of several similar 2D and 3D scatterplots. We \ now provide a more concise scenario with few redundancy and a reduced number of steps, \ making it easier to follow. \  \ Global presentation \ We: \ - reduced the size of the images to produce a lighter PDF file \ - Proofread the paper and corrected minor edits and typos \ - Changed the formatting of the subsections in IMAXES VISUALISATIONS and \ IMAXES INTERACTIONS AND METAPHORS to make the hierarchy and structure \ of the text more visible \ - Reworked the visual presentation of the USE CASE scenario section \ - Reworked the visual presentation of the grammar rules \ - Merged Figure 8 and 9 into Figure 8, that show the interactive handles to filter and \ normalise data on the Axes",Maxime Cordeil,Andrew Cunningham,FormatComplete,,,,,,,Aug 8 21:07,
uistf1143,10/23,2,Creativity Support Tools,11:40:00 AM,1:00:00 PM,4,12:20:00 PM,12:40:00 PM,long,long,uistf2024,3,203,Best Paper,,uistf1143,A,Triggering Artwork Swaps for Live Animation,Nora,Willett,noraw@princeton.edu,uistf1143-paper.pdf,11,letter,,,"Nora S Willett, Wilmot Li, Jovan Popovic, Adam Finkelstein","noraw@princeton.edu, wilmotli@adobe.com, jovan@adobe.com, af@cs.princeton.edu",60198,Nora,S,Willett,noraw@princeton.edu,Computer Science,Princeton University,Princeton,New Jersey,United States,,,,,,25445,Wilmot,,Li,wilmotli@adobe.com,,Adobe Systems,Seattle,Washington,United States,,,,,,14615,Jovan,,Popovic,jovan@adobe.com,,Adobe Systems,Seattle,Washington,United States,,,,,,21296,Adam,,Finkelstein,af@cs.princeton.edu,,Princeton Univ.,Princeton,New Jersey,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Live animation of 2D characters is a new form of storytelling that has started to appear on streaming platforms and broadcast TV. Unlike traditional animation, human performers control characters in real time so that they can respond and improvise to live events. Current live animation systems provide a range of animation controls, such as camera input to drive head movements, audio for lip sync, and keyboard shortcuts to trigger discrete pose changes via artwork swaps. However, managing all of these controls during a live performance is challenging. In this work, we present a new interactive system that specifically addresses the problem of triggering artwork swaps in live settings. Our key contributions are the design of a multi-touch triggering interface that overlays visual triggers around a live preview of the character, and a predictive triggering model that leverages practice performances to suggest pose transitions during live performances. We evaluate our system with quantitative experiments, a user study with novice participants, and interviews with professional animators.",noraw@princeton.edu,"1. Adobe. Character Animator. https://helpx.adobe.com/ after-effects/character-animator.html, 2016. Accessed: 2016-04-10. \ 2. Barnes, C., Jacobs, D. E., Sanders, J., Goldman, D. B., Rusinkiewicz, S., Finkelstein, A., and Agrawala, M. Video Puppetry: A performative interface for cutout animation. ACM Transactions on Graphics (Proc. SIGGRAPH ASIA) 27, 5 (Dec. 2008). \ 3. Bell, B., Feiner, S., and H¨ollerer, T. View management for virtual and augmented reality. In Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology, UIST ’01, ACM (New York, NY, USA, 2001), 101–110. \ 4. Christensen, J., Marks, J., and Shieber, S. An empirical study of algorithms for point-feature label placement. ACM Trans. Graph. 14, 3 (1995), 203–232. \ 5. Davis, R. C., Colwell, B., and Landay, J. A. K-sketch: A ’kinetic’ sketch pad for novice animators. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’08, ACM (New York, NY, USA, 2008), 413–422. \ 6. Davison, B. D., and Hirsh, H. Predicting sequences of user actions. In Notes of the AAAI/ICML 1998 Workshop on Predicting the Future: AI Approaches to Time-Series Analysis (1998), 5–12. \ 7. Dontcheva, M., Yngve, G., and Popovi´c, Z. Layered acting for character animation. In ACM Transactions on Graphics (TOG), vol. 22, ACM (2003), 409–416. \ 8. Failes, I. How the simpsons used adobe character animator to create a live episode. Cartoon Brew (may 2016). \ 9. Findlater, L., and McGrenere, J. A comparison of static, adaptive, and adaptable menus. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’04, ACM (New York, NY, USA, 2004), 89–96. \ 10. Findlater, L., and Wobbrock, J. Personalized input: Improving ten-ﬁnger touchscreen typing through automatic adaptation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’12, ACM (New York, NY, USA, 2012), 815–824. \ 11. Gajos, K. Z., Everitt, K., Tan, D. S., Czerwinski, M., and Weld, D. S. Predictability and accuracy in adaptive user interfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM (2008), 1271–1274. \ 12. Gallina, M. Cartoon donald trump delights audiences on the late show with stephen colbert. Adobe Creative Cloud (sept 2016). \ 13. Guay, M., Ronfard, R., Gleicher, M., and Cani, M.-P. Space-time sketching of character animation. ACM Trans. Graph. 34, 4 (July 2015), 118:1–118:10. \ 14. Hook, J. D. Interaction Design for Live Performance. PhD thesis, Newcastle University, 2013. \ 15. Igarashi, T., Moscovich, T., and Hughes, J. F. As-rigid-as-possible shape manipulation. In ACM SIGGRAPH 2005 Papers, SIGGRAPH ’05, ACM (New York, NY, USA, 2005), 1134–1141. \ 16. Jacobson, A., Panozzo, D., Glauser, O., Pradalier, C., Hilliges, O., and Sorkine-Hornung, O. Tangible and modular input device for character articulation. ACM Trans. Graph. 33, 4 (2014), 82:1–82:12. \ 17. Kazi, R. H., Chevalier, F., Grossman, T., Zhao, S., and Fitzmaurice, G. Draco: Bringing life to illustrations with kinetic textures. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’14, ACM (New York, NY, USA, 2014), 351–360. \ 18. Kurtenbach, G., and Buxton, W. User learning and performance with marking menus. In Proceedings of the SIGCHI conference on Human factors in computing systems, ACM (1994), 258–264. \ 19. Laursen, L. F., Goto, M., and Igarashi, T. A multi-touch dj interface with remote audience feedback. In Proceedings of the 22Nd ACM International Conference on Multimedia, MM ’14, ACM (New York, NY, USA, 2014), 1225–1228. \ 20. Lepinski, G. J., Grossman, T., and Fitzmaurice, G. The design and evaluation of multitouch marking menus. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM (2010), 2233–2242. \ 21. Marin, G., Dominio, F., and Zanuttigh, P. Hand gesture recognition with leap motion and kinect devices. In Image Processing (ICIP), 2014 IEEE International Conference on, IEEE (2014), 1565–1569. \ 22. Matejka, J., Li, W., Grossman, T., and Fitzmaurice, G. Communitycommands: command recommendations for software applications. In Proceedings of the 22nd annual ACM symposium on User interface software and technology, ACM (2009), 193–202. \ 23. Messmer, S., Fleischmann, S., and Sorkine-Hornung, O. Animato: 2D shape deformation and animation on mobile devices. In Proceedings of ACM SIGGRAPH ASIA Symposium on Mobile Graphics and Interactive Applications (December 2016). \ 24. Molchanov, P., Gupta, S., Kim, K., and Kautz, J. Hand gesture recognition with 3d convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops (2015), 1–7. \ 25. Momeni, A., and Rispoli, Z. Dranimate: Rapid real-time gestural rigging and control of animation. In Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology, ACM (2015), 61–62. \ 26. Motion, L. Leap motion controller. URl: https://www. leapmotion. com (2015). \ 27. Munkres, J. Algorithms for the assignment and transportation problems. Journal of the society for industrial and applied mathematics 5, 1 (1957), 32–38. \ 28. Nitsche, M., and Nayak, S. Cell phone puppets: turning mobile phones into performing objects. In International Conference on Entertainment Computing, Springer (2012), 363–372. \ 29. Oberweger, M., Wohlhart, P., and Lepetit, V. Hands deep in deep learning for hand pose estimation. arXiv preprint arXiv:1502.06807 (2015). \ 30. Ponsard, A., Ardekani, K., Zhang, K., Ren, F., Negulescu, M., and McGrenere, J. Twist and pulse: Ephemeral adaptation to improve icon selection on smartphones. In Proceedings of the 41st Graphics Interface Conference, GI ’15, Canadian Information Processing Society (Toronto, Ont., Canada, Canada, 2015), 219–222. \ 31. Reallusion. Crazy talk animator. https://www.reallusion.com/crazytalk-animator/, 2016. Accessed: 2016-10-10. \ 32. Samurai, O. Red monster live! https://www.youtube.com/ watch?v=qBUOKI22hzI&feature=youtu.be, 2017. Accessed: 2017-03-20. \ 33. SANDERS, J. W. Disney xd drops animated livestreams for star vs. the forces of evil. http://promaxbda.org/brief/content/ disney-xd-drops-animated-livestreams-for-star-vs. -the-forces-of-evil, 2017. Accessed: 2017-03-20. \ 34. Scribbleh. Scribbleh. https://www.twitch.tv/scribbleh, 2016. \ 35. Snap. Snapchat lenses, 2016. \ 36. Sturman, D. J. Computer puppetry. IEEE Comput. Graph. Appl. 18, 1 (Jan. 1998), 38–45. \ 37. Supancic, J. S., Rogez, G., Yang, Y., Shotton, J., and Ramanan, D. Depth-based hand pose estimation: data, methods, and challenges. In Proceedings of the IEEE international conference on computer vision (2015), 1868–1876. \ 38. ToonBoom. Harmony: Animation and storyboarding software. https://www.toonboom.com/, 2016. Accessed: 2016-04-10. \ 39. Vollick, I., Vogel, D., Agrawala, M., and Hertzmann, A. Specifying label layout style by example. In Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology, UIST ’07, ACM (New York, NY, USA, 2007), 221–230. \ 40. Wei, S.-E., Ramakrishna, V., Kanade, T., and Sheikh, Y. Convolutional pose machines. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2016), 4724–4732. \ 41. Williams, E. Predictive, adaptive mobile user interfaces: State of the art and open problems. In Proceedings of the 2014 ACM Southeast Regional Conference, ACM SE ’14, ACM (New York, NY, USA, 2014), 35:1–35:3. \ 42. Williams, L. Performance-driven facial animation. In ACM SIGGRAPH Computer Graphics, vol. 24, ACM (1990), 235–242. \ 43. Yu, X., Yang, J., Luo, L., Li, W., Brandt, J., and Metaxas, D. Customized expression recognition for performance-driven cutout character animation. In Proceedings of Winter Conference on Applications of Computer Vision (WACV), IEEE (2016). \ 44. Zhai, S., Hunter, M., and Smith, B. A. Performance optimization of virtual keyboards. Human-Computer Interaction 17, 2–3 (2002).  \ ",2D animation; live performance,H.5.2,uistf1143-file1.zip,uistf1143-file2.jpg,uistf1143-file3.mp4,uistf1143-file4.zip,A character is animated during a live performance with our triggering interface.,To view our supplemental materials please open index.html in a web browser.  From that page you can explore the different parts of our system.   \ ,"We present an automatic layout design for triggers representing artwork changes, and a predictive triggering model trained on practice performances to suggest next poses during a live performance. ","Thank you for the helpful comments and suggestions in the reviews! In the updated draft, differences from the submission are highlighted in yellow. Here is a summary of the most significant changes: \   -  In the introduction, we clarify how the mouth shapes are calculated.  We also explicitly state that our user study has mainly novices in it. \   -  After our submission, we noticed a minor bug in our model validation algorithm to compute the errors in Figures 9 and 10.  We fixed the bug and have updated the figures accordingly. As you can see, the changes to the graphs are small. \   -  We understand the potential concerns with an unreviewed study being included in the paper. That said, we feel that our improved study offers a fairer comparison between the keyboard and touch interface conditions. Moreover, the findings from the new study are quite similar to the original study. Therefore, we have included two versions of the paper in the submitted pdf.  The first contains the original user study but explicitly outlines the limitations of the study (as proposed in the summary review). The second version contains our new user study, with differences highlighted in orange. You can verify that the changes with the new user study design address a concern in the reviews without making a substantial change to other aspects of the reviewed design. We hope the reviewers will find the latter version acceptable, but defer to the PC as to which version should appear in the finalized paper. \   -  We changed the title of the Results section to Examples.  We also state in this section that our characters were created independently of our project. \ ",Nora Willett,Wilmot Li,FormatComplete,,,,,,,Aug 9 17:44,
uistf3763,10/23,2,Creativity Support Tools,11:40:00 AM,1:00:00 PM,4,12:40:00 PM,1:00:00 PM,long,long,uistf1143,4,204,,,uistf3763,A,Secondary Motion for Performed 2D Animation,Nora,Willett,noraw@princeton.edu,uistf3763-paper.pdf,12,letter,,,"Nora S Willett, Wilmot Li, Jovan Popovic, Floraine Berthouzoz, Adam Finkelstein","noraw@princeton.edu, wilmotli@adobe.com, jovan@adobe.com, floraine@adobe.com, af@cs.princeton.edu",60198,Nora,S,Willett,noraw@princeton.edu,Computer Science,Princeton University,Princeton,New Jersey,United States,,,,,,25445,Wilmot,,Li,wilmotli@adobe.com,,Adobe Systems,Seattle,Washington,United States,,,,,,14615,Jovan,,Popovic,jovan@adobe.com,,Adobe Systems,Seattle,Washington,United States,,,,,,43098,Floraine,,Berthouzoz,floraine@adobe.com,CTL,Adobe Systems,San Francisco,CA,USA,,,,,,21296,Adam,,Finkelstein,af@cs.princeton.edu,,Princeton Univ.,Princeton,New Jersey,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"When bringing animated characters to life, artists often augment the primary motion of a figure by adding secondary animation -- subtle movement of parts like hair, foliage or cloth that complements and emphasizes the primary motion. Traditionally, artists add secondary motion to animated illustrations only through arduous manual effort, and often eschew it entirely. Emerging ``live'' performance applications allow both novices and experts to perform the primary motion of a character, but only a virtuoso performer could manage the degrees of freedom needed to specify both primary and secondary motion together. This paper introduces physically-inspired rigs that propagate the primary motion of layered, illustrated characters to produce plausible secondary motion. These composable elements are rigged and controlled via a small number of parameters to produce an expressive range of effects. Our approach supports a variety of the most common secondary effects, which we demonstrate with an assortment of characters of varying complexity.",noraw@princeton.edu,"1. Adobe. After effects. http://www.adobe.com/products/aftereffects.html, 2016. Accessed: 2016-04-10. \ 2. Adobe. Character animator. https://helpx.adobe.com/ after-effects/character-animator.html, 2016. Accessed: 2016-04-10. \ 3. Adobe. Flash. http://www.adobe.com/products/flash.html, 2016. Accessed: 2016-04-10. \ 4. Alexa, M., Cohen-Or, D., and Levin, D. As-rigid-as-possible shape interpolation. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques, ACM Press/Addison-Wesley Publishing Co. (2000), 157–164. \ 5. Autodesk. Maya. https://www.autodesk.com/products/maya/overview, 2016. Accessed: 2016-04-10. \ 6. Baecker, R. M. Picture-driven animation. In Proceedings of the May 14-16, 1969, Spring Joint Computer Conference, AFIPS ’69 (Spring), ACM (New York, NY, USA, 1969), 273–288. \ 7. Bai, Y., Kaufman, D. M., Liu, C. K., and Popovi´c, J. Artist-directed dynamics for 2d animation. ACM Trans. Graph. 35, 4 (Jul 2016), 145:1–145:10. \ 8. Baraff, D., Witkin, A., and Kass, M. Untangling cloth. ACM Trans. Graph. 22, 3 (July 2003), 862–870. \ 9. Barnes, C., Jacobs, D. E., Sanders, J., Goldman, D. B., Rusinkiewicz, S., Finkelstein, A., and Agrawala, M. Video Puppetry: A performative interface for cutout animation. ACM Transactions on Graphics (Proc. SIGGRAPH ASIA) 27, 5 (Dec. 2008). \ 10. Barzel, R. Faking dynamics of ropes and springs. IEEE Computer Graphics and Applications, 3 (1997), 31–39. \ 11. Barzel, R., and Barr, A. H. A modeling system based on dynamic constraints. In ACM SIGGRAPH Computer Graphics, vol. 22, ACM (1988), 179–188. \ 12. Barzel, R., Hughes, J. R., and Wood, D. N. Plausible motion simulation for computer graphics animation. In Computer Animation and Simulation96. Springer, 1996, 183–197. \ 13. Bergou, M., Mathur, S., Wardetzky, M., and Grinspun, E. Tracks: Toward directable thin shells. ACM Trans. Graph. 26, 3 (July 2007). \ 14. Caliri, J. Dragon. https://www.youtube.com/watch?v=7ptwjJFgemQ, 2011. Accessed: 2017-01-13. \ 15. Catto, E. Box2d: A 2d physics engine for games. http://box2d.org/, 2016. \ 16. ChuuStar. Dancing rabbits. https://www.youtube.com/watch?v=D306PFWwTjE, 2013. Accessed: 2017-01-13. \ 17. Davis, R. C., Colwell, B., and Landay, J. A. K-sketch: A ’kinetic’ sketch pad for novice animators. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’08, ACM (New York, NY, USA, 2008), 413–422. \ 18. Disney. Live with starfan13. https://www.youtube.com/watch?v=xxYsrCt5Z9c, 2016. Accessed: 2016-12-10. \ 19. DuckDuckMoose. Princess fairy tale maker. http://www.duckduckmoose.com/ educational-iphone-itouch-apps-for-kids/ princess-fairy-tale-maker/, 2016. Accessed: 2016-12-10. \ 20. Gallina, M. Cartoon Donald Trump delights audiences on The Late Show with Stephen Colbert. Adobe Creative Cloud (Sept 2016). \ 21. Guay, M., Ronfard, R., Gleicher, M., and Cani, M.-P. Adding dynamics to sketch-based character animations. In Proceedings of the Workshop on Sketch-Based Interfaces and Modeling, SBIM ’15, Eurographics Association (Aire-la-Ville, Switzerland, Switzerland, 2015), 27–34. \ 22. Guay, M., Ronfard, R., Gleicher, M., and Cani, M.-P. Space-time sketching of character animation. ACM Trans. Graph. 34, 4 (July 2015), 118:1–118:10. \ 23. Hahn, F., Martin, S., Thomaszewski, B., Sumner, R., Coros, S., and Gross, M. Rig-space physics. ACM transactions on graphics (TOG) 31, 4 (2012), 72. \ 24. Hahn, F., Thomaszewski, B., Coros, S., Sumner, R. W., and Gross, M. Efﬁcient simulation of secondary motion in rig-space. In Proceedings of the 12th ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA ’13, ACM (New York, NY, USA, 2013), 165–171. \ 25. Igarashi, T., Moscovich, T., and Hughes, J. F. As-rigid-as-possible shape manipulation. ACM Trans. Graph. 24, 3 (July 2005), 1134–1141. \ 26. Iwamoto, N., Shum, H. P., Yang, L., and Morishima, S. Multi-layer lattice model for real-time dynamic character deformation. In Computer Graphics Forum, vol. 34, Wiley Online Library (2015), 99–109. \ 27. Jacobson, A., Baran, I., Popovic, J., and Sorkine, O. Bounded biharmonic weights for real-time deformation. ACM Trans. Graph. 30, 4 (2011), 78. \ 28. Jain, E., Sheikh, Y., Mahler, M., and Hodgins, J. Three-dimensional proxies for hand-drawn characters. ACM Trans. Graph. 31, 1 (Feb. 2012), 8:1–8:16. \ 29. Jakobsen, T. Advanced character physics. In Game Developers Conference (2001), 383–401. \ 30. Jones, B., Popovic, J., McCann, J., Li, W., and Bargteil, A. Dynamic sprites. In Proceedings of Motion on Games, MIG ’13, ACM (New York, NY, USA, 2013), 17:39–17:46. \ 31. Kazi, R. H., Chevalier, F., Grossman, T., and Fitzmaurice, G. Kitty: sketching dynamic and interactive illustrations. In Proceedings of the 27th annual ACM symposium on User interface software and technology, ACM (2014), 395–405. \ 32. Kazi, R. H., Chevalier, F., Grossman, T., Zhao, S., and Fitzmaurice, G. Draco: bringing life to illustrations with kinetic textures. In Proceedings of the 32nd annual ACM conference on Human factors in computing systems, ACM (2014), 351–360. \ 33. Kazi, R. H., Grossman, T., Umetani, N., and Fitzmaurice, G. Sketching stylized animated drawings with motion ampliﬁers. In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems, CHI EA ’16, ACM (New York, NY, USA, 2016), 6–6. \ 34. Kondo, R., Kanai, T., and Anjyo, K.-i. Directable animation of elastic objects. In Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA ’05, ACM (New York, NY, USA, 2005), 127–134. \ 35. Kupferer, T. The girl and the fox. http://www.girlandthefox.com/, 2011. Accessed: 2017-01-13. \ 36. LaunchpadToys. Toontastic. https://itunes.apple.com/us/ app/toontastic/id404693282?mt=8, 2016. Accessed: 2016-12-10. \ 37. Machinima, I. Machinima. https://www.machinima.com/, 2016. Accessed: 2017-01-13. \ 38. Messmer, S., Fleischmann, S., and Sorkine-Hornung, O. Animato: 2d shape deformation and animation on mobile devices. In SIGGRAPH ASIA 2016 Mobile Graphics and Interactive Applications, SA ’16, ACM (New York, NY, USA, 2016), 6:1–6:4. \ 39. Moore, M., and Wilhelms, J. Collision detection and response for computer animation. SIGGRAPH Comput. Graph. 22, 4 (June 1988), 289–298. \ 40. M¨uller, M., Heidelberger, B., Teschner, M., and Gross, M. Meshless deformations based on shape matching. ACM Trans. Graph. 24, 3 (July 2005), 471–478. \ 41. M¨uller, M., Kim, T.-Y., and Chentanez, N. Fast simulation of inextensible hair and fur. VRIPHYS 12 (2012), 39–44. \ 42. Norstein, Y. The fox and the hare. https://www.youtube.com/watch?v=fGVQu32dHb8, 1973. Accessed: 2017-01-13. \ 43. Norstein, Y. The heron and the crane. https://www.youtube.com/watch?v=H57Z8PB-N60, 1974. Accessed: 2017-01-13. \ 44. Norstein, Y. Hedgehog in the fog. https://www.youtube.com/watch?v=oW0jvJC2rvM, 1975. Accessed: 2017-01-13. \ 45. Reallusion. Crazy talk animator. https://www.reallusion.com/crazytalk-animator/, 2016. Accessed: 2016-10-10. \ 46. Scott, J., and Davis, R. Physink: Sketching physical behavior. In Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology, UIST ’13 Adjunct, ACM (New York, NY, USA, 2013), 9–10. \ 47. Selle, A., Lentine, M., and Fedkiw, R. A mass spring model for hair simulation. ACM Transactions on Graphics (TOG) 27, 3 (2008), 64. \ 48. Shewchuk, J. R. Triangle: Engineering a 2D Quality Mesh Generator and Delaunay Triangulator. In Applied Computational Geometry: Towards Geometric Engineering, M. C. Lin and D. Manocha, Eds., vol. 1148 of Lecture Notes in Computer Science. Springer-Verlag, May 1996, 203–222. From the First ACM Workshop on Applied Computational Geometry. \ 49. Stam, J. Nucleus: Towards a uniﬁed dynamics solver for computer graphics. In Computer-Aided Design and Computer Graphics, 2009. CAD/Graphics’ 09. 11th IEEE International Conference on, IEEE (2009), 1–11. \ 50. Stumpf, J. F. Motion capture system, May 27 2010. US Patent App. 12/802,016. \ 51. Sumner, R. W., and Popovi´c, J. Deformation transfer for triangle meshes. ACM Transactions on Graphics (TOG) 23, 3 (2004), 399–405. \ 52. Sumner, R. W., Zwicker, M., Gotsman, C., and Popovi´c, J. Mesh-based inverse kinematics. ACM transactions on graphics (TOG) 24, 3 (2005), 488–495. \ 53. ToonBoom. Harmony: Animation and storyboarding software. https://www.toonboom.com/, 2016. Accessed: 2016-04-10. \ 54. Umetani, N., Schmidt, R., and Stam, J. Position-based elastic rods. In Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA ’14, Eurographics Association (Aire-la-Ville, Switzerland, Switzerland, 2014), 21–30. \ 55. Williams, L. Performance-driven facial animation. In Proceedings of the 17th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’90, ACM (New York, NY, USA, 1990), 235–242. \ 56. Witkin, A. An introduction to physically based modeling: Constrained dynamics. Robotics Institute (1997). \ 57. Xing, J., Kazi, R. H., Grossman, T., Wei, L.-Y., Stam, J., and Fitzmaurice, G. Energy-brushes: Interactive tools for illustrating stylized elemental dynamics. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, UIST ’16, ACM (New York, NY, USA, 2016), 755–766. \ 58. Yeh, C.-K., Jayaraman, P. K., Liu, X., Fu, C.-W., and Lee, T.-Y. 2.5 d cartoon hair modeling and manipulation. IEEE transactions on visualization and computer graphics 21, 3 (2015), 304–314.  \ ",secondary motion; live performance; 2D animation; plausible physics; constrained dynamics,H.5.2,uistf3763-file1.zip,uistf3763-file2.jpg,uistf3763-file3.mp4,uistf3763-file4.zip,"We provide a follow, rest pose and collision rig to add secondary animation to this frog’s legs and lily pads colliding with her head as she swims through the water.",To view our supplemental materials please open index.html in a web browser.  From that page you can explore the secondary animation on each character.   \  \ The exploration of secondary animation categories can be found in the excel spreadsheet.  \ ,"We introduce composable, physically-inspired rigs that propagate the primary motion of layered, illustrated characters to produce plausible secondary motion during live performances.  ","Thank you for the helpful comments and suggestions in the reviews! In the updated draft, differences from the submission are highlighted in yellow. Here is a summary of the most significant changes: \   -  We changed all references to 2.5D animation to 2D animation. \   -  In the related work, we added a more thorough discussion comparing our work to Draco, Motion Amplifiers, and commercial tools. \   -  When describing our set of secondary animation categories, we describe the motivation for curating our list and acknowledge that our list is perhaps not comprehensive.  We also updated the numbers to reflect the expanded analysis of additional animations that we described in the rebuttal. \   -  We note that our system was developed as a plug-in to a full-featured commercial animation system to show how it enables live animation workflows. \   -  We clarify in our user study why we did not let users create their own characters. \   -  We have also added some of the user created videos and our expanded curated set of analyzed animations to our supplemental materials. \ ",Nora Willett,Wilmot Li,FormatComplete,,,,,,,Aug 9 18:14,
uistf1201,10/23,3,Haptics,2:30:00 PM,3:50:00 PM,4,2:30:00 PM,2:40:00 PM,short,short,none,1,301,,"listed in our initial schedule as long, but only 7 pages of text; shouldn't have first paper in a session short

I'd be in favor of keeping it as long (we initially set cut off at 6 pages + refs) -KZG",uistf1201,A,HapticClench: Investigating Squeeze Sensations using Memory Alloys,Aakar,Gupta,aakar@cs.toronto.edu,uistf1201-paper.pdf,9,letter,,,"Aakar Gupta, Antony Albert Raj Irudayaraj, Ravin Balakrishnan","aakar@cs.toronto.edu, antonyalbertraj.irudayaraj@mail.utoronto.ca, ravin@dgp.toronto.edu",22718,Aakar,,Gupta,aakar@cs.toronto.edu,,University of Toronto,Toronto,Ontario,Canada,,,,,,60442,Antony Albert Raj,,Irudayaraj,antonyalbertraj.irudayaraj@mail.utoronto.ca,,University of Toronto,Toronto,Ontario,Canada,,,,,,1019,Ravin,,Balakrishnan,ravin@dgp.toronto.edu,,University of Toronto,Toronto,Ontario,Canada,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Squeezing sensations are one of the most common and intimate forms of human contact. In this paper, we investigate HapticClench, a device that generates squeezing sensations using shape memory alloys. We define squeezing feedback in terms of it perceptual properties and conduct a psychophysical evaluation of HapticClench. HapticClench is capable of generating up to four levels of distinguishable load and works well in distracted scenarios. HapticClench has a high spatial acuity and can generate spatial patterns on the wrist that the user can accurately recognize. We also demonstrate the use of HapticClench for communicating gradual progress of an activity, and for generating squeezing sensations using rings and loose bracelets.",aakar@dgp.toronto.edu,"1. Christian Antfolk, Christian Balkenius, Göran Lundborg, Birgitta Rosén, and Fredrik Sebelius. 2010. Design and technical construction of a tactile display for sensory feedback in a hand prosthesis system. BioMedical Engineering OnLine 9, 1: 50. http://doi.org/10.1186/1475-925X-9-50 \ 2. Matthew A. Baumann, Karon E. MacLean, Thomas W. Hazelton, and Ashley McKay. 2010. Emulating human attention-getting practices with wearable haptics. 2010 IEEE Haptics Symposium, IEEE, 149– 156. http://doi.org/10.1109/HAPTIC.2010.5444662 \ 3. Francesco Chinello, Mirko Aurilio, Claudio Pacchierotti, and Domenico Prattichizzo. 2014. The HapBand: A Cutaneous Device for Remote Tactile Interaction. . Springer, Berlin, Heidelberg, 284–291. http://doi.org/10.1007/978-3-662-44193-0_36 \ 4. Yoon Gi Chung, Sang Woo Han, Hyung-Sik Kim, et al. 2015. Adaptation of cortical activity to sustained pressure stimulation on the fingertip. BMC neuroscience 16: 71. http://doi.org/10.1186/s12868015-0207-x \ 5. T. W. Darling, F. Chu, A. Migliori, et al. 2001. Elastic and thermodynamic properties of the shapememory alloy AuZn. http://doi.org/10.1080/13642810110109535 \ 6. E. Faran and D. Shilo. 2015. Ferromagnetic Shape Memory Alloys-Challenges, Applications, and Experimental Characterization. Experimental Techniques: n/a-n/a. http://doi.org/10.1111/ext.12153 \ 7. RF Fletcher. 1962. The measurement of total body fat with skinfold calipers. Clinical science. Retrieved July 17, 2017 from https://www.cabdirect.org/cabdirect/abstract/19631 \ 8. Aakar Gupta, Thomas Pietrzak, Nicolas Roussel, and Ravin Balakrishnan. 2016. Direct Manipulation in Tactile Displays. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems - CHI ’16, ACM Press, 3683–3693. http://doi.org/10.1145/2858036.2858161 \ 9. Ken Hinckley and Hyunyoung Song. 2011. Sensor synaesthesia. Proceedings of the 2011 annual conference on Human factors in computing systems - CHI ’11, ACM Press, 801. http://doi.org/10.1145/1978942.1979059 \ 10. Joseph Santos, Matthew Graves, Kwangtaek Kim, Hong Z. Tan Hsiang-yu Chen. Tactor Localization at the Wrist. Retrieved February 19, 2015 from http://citeseerx.ist.psu.edu/viewdoc/summary?doi= 10.1.1.151.7152&rank=1 \ 11. R. Kikuuwe and T. Yoshikawa. Haptic display device with fingertip presser for motion/force teaching to human. Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164), IEEE, 868–873. http://doi.org/10.1109/ROBOT.2001.932659 \ 12. S. J. Lederman and R. L. Klatzky. 2009. Haptic perception: A tutorial. Attention, Perception & Psychophysics 71, 7: 1439–1459. http://doi.org/10.3758/APP.71.7.1439 \ 13. Michael Matscheko, Alois Ferscha, Andreas Riener, and Manuel Lehner. 2010. Tactor placement in wrist worn wearables. International Symposium on Wearable Computers (ISWC) 2010, IEEE, 1–8. http://doi.org/10.1109/ISWC.2010.5665867 \ 14. Francis McGlone and David Reilly. 2010. The cutaneous sensory system. Neuroscience & Biobehavioral Reviews 34, 2: 148–159. http://doi.org/10.1016/j.neubiorev.2009.08.004 \ 15. Takashi Mitsuda. 2013. Pseudo Force Display that Applies Pressure to the Forearms. Presence: Teleoperators and Virtual Environments 22, 3: 191– 201. http://doi.org/10.1162/PRES_a_00150 \ 16. Vernon B. Mountcastle. 2005. The sensory hand : neural mechanisms of somatic sensation. Harvard University Press. Retrieved March 30, 2017 from http://www.hup.harvard.edu/catalog.php?isbn=9780 674019744&content=reviews \ 17. S. Paneels, M. Anastassova, S. Strachan, S. P. Van, S. Sivacoumarane, and C. Bolzmacher. 2013. What’s around me? Multi-actuator haptic feedback on the wrist. 2013 World Haptics Conference (WHC), IEEE, 407–412. http://doi.org/10.1109/WHC.2013.6548443 \ 18. P E Patterson and J A Katz. 1992. Design and evaluation of a sensory feedback system that provides grasping pressure in a myoelectric hand. Journal of rehabilitation research and development 29, 1: 1–8. Retrieved March 24, 2017 from http://www.ncbi.nlm.nih.gov/pubmed/1740774 \ 19. Henning Pohl, Peter Brandes, Quang HN, and Michael Rohs. 2017. Squeezeback: Pneumatic Compression for Notifications. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI’17. \ 20. Sunghyun Song, Geeyoung Noh, Junwoo Yoo, Ian Oakley, Jundong Cho, and Andrea Bianchi. 2015. Hot & tight: exploring thermo and squeeze cues recognition on wrist wearables. Proceedings of the 2015 ACM International Symposium on Wearable Computers - ISWC ’15, ACM Press, 39–42. http://doi.org/10.1145/2802083.2802092 \ 21. Andrew A. Stanley and Katherine J. Kuchenbecker. 2011. Design of body-grounded tactile actuators for playback of human physical contact. 2011 IEEE World Haptics Conference, IEEE, 563–568. http://doi.org/10.1109/WHC.2011.5945547 \ 22. Katja Suhonen, Kaisa Väänänen-Vainio-Mattila, and Kalle Mäkelä. 2012. User experiences and expectations of vibrotactile, thermal and squeeze feedback in interpersonal communication. Proceedings of the 26th Annual BCS Interaction Specialist Group Conference on People and Computers, 205–214. Retrieved March 24, 2017 from http://dl.acm.org/citation.cfm?id=2377939 \ 23. Camilo Tejeiro, Cara E. Stepp, Mark Malhotra, Eric Rombokas, and Yoky Matsuoka. 2012. Comparison of remote pressure and vibrotactile feedback for prosthetic hand control. 2012 4th IEEE RAS & EMBS International Conference on Biomedical Robotics and Biomechatronics (BioRob), IEEE, 521–525. http://doi.org/10.1109/BioRob.2012.6290268 \ 24. Rongrong Wang, Francis Quek, Deborah Tatar, Keng Soon Teh, and Adrian Cheok. 2012. Keep in touch. Proceedings of the 2012 ACM annual conference on Human Factors in Computing Systems - CHI ’12, ACM Press, 139. http://doi.org/10.1145/2207676.2207697 \ 25. J C K Wells and M S Fewtrell. 2006. Measuring body composition. Archives of disease in childhood 91, 7: 612–7. http://doi.org/10.1136/adc.2005.085522 \ 26. Ying Zheng, E. Su, and J. B. Morrell. 2013. Design and evaluation of pactors for managing attention capture. 2013 World Haptics Conference (WHC), IEEE, 497–502. http://doi.org/10.1109/WHC.2013.6548458 \ 27. Ying Zheng and John B. Morrell. 2012. Haptic actuator design parameters that influence affect and attention. 2012 IEEE Haptics Symposium (HAPTICS), IEEE, 463–470. http://doi.org/10.1109/HAPTIC.2012.6183832 \ 28. CandyCrush. Retrieved from https://king.com/game/candycrush \ 29. Helicopter. Retrieved from http://www.helicoptergame.net/ \ ",Haptics; Squeezing; Wrist; Wearable; Compression,H.5.2,uistf1201-file1.docx,uistf1201-file2.jpg,uistf1201-file3.mp4,uistf1201-file4.zip,Squeezing sensations on the wrist using shape memory alloy springs.,Highlighted Changes document,"HapticClench generates squeezing sensations on the wrist using shape memory alloy springs. We investigate its perceptual properties and capabilities including spatial patterns, gradual squeezing sensations, squeezing rings and bracelets.",The camera-ready deanonymized version.,Aakar Gupta,Antony Albert Raj Irudayaraj,FormatComplete,,,,,,,Aug 5 15:09,
uistf1335,10/23,3,Haptics,2:30:00 PM,3:50:00 PM,4,2:40:00 PM,3:00:00 PM,long,long,uistf1201,2,302,Best Paper,,uistf1335,A,Grabity: A Wearable Haptic Interface for Simulating Weight and Grasping in Virtual Reality,Inrak,Choi,irchoi@stanford.edu,uistf1335-paper.pdf,12,letter,,,"Inrak Choi, Heather Culbertson, Mark Roman Miller, Alex Olwal, Sean Follmer","irchoi@stanford.edu, hculbert@stanford.edu, mrmillr@stanford.edu, olwal@google.com, sfollmer@stanford.edu",60335,Inrak,,Choi,irchoi@stanford.edu,Mechanical Engineering / Stanford University,Stanford University,Stanford,CA,USA,,,,,,63209,Heather,,Culbertson,hculbert@stanford.edu,Mechanical Engineering,Stanford University,Stanford,California,United States,,,,,,71891,Mark,Roman,Miller,mrmillr@stanford.edu,Computer Science,Stanford University,Stanford,California,United States,,,,,,4018,Alex,,Olwal,olwal@google.com,,"Google, Inc.",Mountain View,California,United States,,,,,,15323,Sean,,Follmer,sfollmer@stanford.edu,Mechanical Engineering,Stanford University,Palo Alto,California,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Ungrounded haptic devices for virtual reality (VR) applications lack the ability to convincingly render the sensations of a grasped virtual object's rigidity and weight. We present Grabity, a wearable haptic device designed to simulate kinesthetic pad opposition grip forces and weight for grasping virtual objects in VR. The device is mounted on the index finger and thumb and enables precision grasps with a wide range of motion. A unidirectional brake creates rigid grasping force feedback. Two voice coil actuators create virtual force tangential to each finger pad through asymmetric skin deformation. These forces can be perceived as gravitational and inertial forces of virtual objects. The rotational orientation of the voice coil actuators is passively aligned with the real direction of gravity through a revolute joint, causing the virtual forces to always point downward. This paper evaluates the performance of Grabity through two user studies, finding promising ability to simulate different levels of weight with convincing object rigidity. The first user study shows that Grabity can convey various magnitudes of weight and force sensations to users by manipulating the amplitude of the asymmetric vibration. The second user study shows that users can differentiate different weights in a virtual environment using Grabity.",irchoi@stanford.edu,"1. CyberGrasp, CyberGlove Systems Inc. http://www.cyberglovesystems.com/cybergrasp/. (????). Accessed: 2017-04-03. \ 2. Tomohiro Amemiya and Hiroaki Gomi. 2014. Distinct pseudo-attraction force sensation by a thumb-sized vibrator that oscillates asymmetrically. In International Conference on Human Haptic Sensing and Touch Enabled Computer Applications. Springer, 88–95. \ 3. Tomohiro Amemiya and Taro Maeda. 2008. Asymmetric oscillation distorts the perceived heaviness of handheld objects. IEEE Transactions on Haptics 1, 1 (2008), 9–18. \ 4. AA Amis. 1987. Variation of ﬁnger forces in maximal isometric grasp tests on a range of cylinder diameters. Journal of biomedical engineering 9, 4 (1987), 313–320. \ 5. Federico Barbagli, Kenneth Salisbury, and Roman Devengenzo. 2004. Toward virtual manipulation: from one point of contact to four. Sensor Review 24, 1 (2004), 51–59. \ 6. Karlin Bark, Jason Wheeler, Gayle Lee, Joan Savall, and Mark Cutkosky. 2009. A wearable skin stretch device for haptic feedback. In EuroHaptics conference, 2009 and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems. World Haptics 2009. Third Joint. IEEE, 464–469. \ 7. Hrvoje Benko, Christian Holz, Mike Sinclair, and Eyal Ofek. 2016. NormalTouch and TextureTouch: High-ﬁdelity 3D Haptic Shape Rendering on Handheld Virtual Reality Controllers. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology. ACM, 717–728. \ 8. Mourad Bouzit, Grigore Burdea, George Popescu, and Rares Boian. 2002. The Rutgers Master II-new design force-feedback glove. IEEE/ASME Transactions on mechatronics 7, 2 (2002), 256–263. \ 9. Francesco Chinello, Monica Malvezzi, Claudio Pacchierotti, and Domenico Prattichizzo. 2015. Design and development of a 3RRS wearable ﬁngertip cutaneous device. In Advanced Intelligent Mechatronics (AIM), 2015 IEEE International Conference on. IEEE, 293–298. \ 10. Inrak Choi, Elliot W Hawkes, David L Christensen, Christopher J Ploch, and Sean Follmer. 2016. Wolverine: A wearable haptic interface for grasping in virtual reality. In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on. IEEE, 986–993. \ 11. Tom N Cornsweet. 1962. The staircase-method in psychophysics. The American journal of psychology 75, 3 (1962), 485–491. \ 12. Heather Culbertson, Julie M Walker, and Allison M Okamura. 2016. Modeling and design of asymmetric vibrations to induce ungrounded pulling sensation through asymmetric skin displacement. In Haptics Symposium (HAPTICS), 2016 IEEE. IEEE, 27–33. \ 13. Heather Culbertson, Julie M Walker, Michael Raitor, and Allison M Okamura. 2017. WAVES: A Wearable Asymmetric Vibration Excitation System for Presenting Three-Dimensional Translation and Rotation Cues. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 4972–4982. \ 14. Takahiro Endo, Haruhisa Kawasaki, Tetsuya Mouri, Yasuhiko Ishigure, Hisayuki Shimomura, Masato Matsumura, and Kazumi Koketsu. 2011. Five-ﬁngered haptic interface robot: HIRO III. IEEE Transactions on Haptics 4, 1 (2011), 14–27. \ 15. Xiaochi Gu, Yifei Zhang, Weize Sun, Yuanzhe Bian, Dao Zhou, and Per Ola Kristensson. 2016. Dexmo: An Inexpensive and Lightweight Mechanical Exoskeleton for Motion Capture and Force Feedback in VR. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 1991–1995. \ 16. Sidhant Gupta, Tim Campbell, Jeffrey R Hightower, and Shwetak N Patel. 2010. SqueezeBlock: using virtual springs in mobile devices for eyes-free interaction. In Proceedings of the 23nd annual ACM symposium on User interface software and technology. ACM, 101–104. \ 17. Vincent Hayward and M Cruz-Hernandez. 2000. Tactile display device using distributed lateral skin stretch. In Proceedings of the haptic interfaces for virtual environment and teleoperator systems symposium, Vol. 69. ASME, 1309–1314. \ 18. James Houk and William Simon. 1967. Responses of Golgi tendon organs to forces applied to muscle tendon. Journal of neurophysiology 30, 6 (1967), 1466–1481. \ 19. Alexandra Ion, Edward Jay Wang, and Patrick Baudisch. 2015. Skin drag displays: Dragging a physical tactor across the user’s skin produces a stronger tactile stimulus than vibrotactile. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 2501–2504. \ 20. RS Johansson and G Westling. 1984. Roles of glabrous skin receptors and sensorimotor memory in automatic control of precision grip when lifting rougher or more slippery objects. Experimental brain research 56, 3 (1984), 550–564. \ 21. Kenneth O Johnson. 2001. The roles and functions of cutaneous mechanoreceptors. Current opinion in neurobiology 11, 4 (2001), 455–461. \ 22. Lynette A Jones. 1986. Perception of force and weight: Theory and research. Psychological bulletin 100, 1 (1986), 29. \ 23. Rebecca Khurshid, Naomi Fitter, Elizabeth Fedalei, and Katherine Kuchenbecker. 2016. Effects of Grip-Force, Contact, and Acceleration Feedback on a Teleoperated Pick-and-Place Task. IEEE transactions on haptics (2016). \ 24. Katherine J Kuchenbecker, Jamie Gewirtz, William McMahan, Dorsey Standish, Paul Martin, Jonathan Bohren, Pierre J Mendoza, and David I Lee. 2010. VerroTouch: High-frequency acceleration feedback for telerobotic surgery. In International Conference on Human Haptic Sensing and Touch Enabled Computer Applications. Springer, 189–196. \ 25. Susan J Lederman and Roberta L Klatzky. 1987. Hand movements: A window into haptic object recognition. Cognitive psychology 19, 3 (1987), 342–368. \ 26. Christine L MacKenzie and Thea Iberall. 1994. The grasping hand. Vol. 104. Elsevier. \ 27. Thomas H Massie, J Kenneth Salisbury, and others. 1994. The phantom haptic interface: A device for probing virtual objects. In Proceedings of the ASME winter annual meeting, symposium on haptic interfaces for virtual environment and teleoperator systems, Vol. 55. Citeseer, 295–300. \ 28. William McMahan and Katherine J Kuchenbecker. 2014. Dynamic modeling and control of voice-coil actuators for high-ﬁdelity display of haptic vibrations. In Haptics Symposium (HAPTICS), 2014 IEEE. IEEE, 115–122. \ 29. Kouta Minamizawa, Souichiro Fukamachi, Hiroyuki Kajimoto, Naoki Kawakami, and Susumu Tachi. 2007a. Gravity grabber: wearable haptic display to present virtual mass sensation. In ACM SIGGRAPH 2007 emerging technologies. ACM, 8. \ 30. Kouta Minamizawa, Hiroyuki Kajimoto, Naoki Kawakami, and Susumu Tachi. 2007b. A wearable haptic display to present the gravity sensation-preliminary observations and device design. In EuroHaptics Conference, 2007 and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems. World Haptics 2007. Second Joint. IEEE, 133–138. \ 31. Jun Murayama, Laroussi Bougrila, YanLin Luo, Katsuhito Akahane, Shoichi Hasegawa, Béat Hirsbrunner, and Makoto Sato. 2004. SPIDAR G&G: a two-handed haptic interface for bimanual VR interaction. In Proceedings of EuroHaptics, Vol. 2004. 138–146. \ 32. Zoran Najdovski and Saeid Nahavandi. 2008. Extending haptic device capability for 3D virtual grasping. Haptics: perception, devices and scenarios (2008), 494–503. \ 33. Ryuma Niiyama, Lining Yao, and Hiroshi Ishii. 2014. Weight and volume changing device with liquid metal transfer. In Proceedings of the 8th International Conference on Tangible, Embedded and Embodied Interaction. ACM, 49–52. \ 34. Claudio Pacchierotti, Gionata Salvietti, Irfan Hussain, Leonardo Meli, and Domenico Prattichizzo. 2016. The hRing: A wearable haptic device to avoid occlusions in hand tracking. In Haptics Symposium (HAPTICS), 2016 IEEE. IEEE, 134–139. \ 35. Jérôme Pasquero and Vincent Hayward. 2003. STReSS: A practical tactile display system with one millimeter spatial resolution and 700 Hz refresh rate. In Proc. Eurohaptics, Vol. 2003. 94–110. \ 36. Christopher J Ploch, Jung Hwa Bae, Wendy Ju, and Mark Cutkosky. 2016. Haptic skin stretch on a steering wheel for displaying preview information in autonomous cars. In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on. IEEE, 60–65. \ 37. William Provancher. 2014. Creating greater VR immersion by emulating force feedback with ungrounded tactile feedback. IQT Q 6, 2 (2014), 18–21. \ 38. William R Provancher and Nicholas D Sylvester. 2009. Fingerpad skin stretch increases the perception of virtual friction. IEEE Transactions on Haptics 2, 4 (2009), 212–223. \ 39. Zhan Fan Quek, Samuel B Schorr, Ilana Nisky, William R Provancher, and Allison M Okamura. 2015. Sensory substitution and augmentation using 3-degree-of-freedom skin deformation feedback. IEEE transactions on haptics 8, 2 (2015), 209–221. \ 40. Jun Rekimoto. 2014. Traxion: a tactile interaction device with virtual force sensation. In ACM SIGGRAPH 2014 Emerging Technologies. ACM, 25. \ 41. Samuel B Schorr and Allison M Okamura. 2017. Fingertip Tactile Devices for Virtual Object Manipulation and Exploration. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 3115–3119. \ 42. Hong Z Tan, Xiao Dong Pang, Nathaniel I Durlach, and others. 1992. Manual resolution of length, force, and compliance. Advances in Robotics 42 (1992), 13–18. \ 43. Nikolaos G Tsagarakis, T Horne, and Darwin G Caldwell. 2005. Slip aestheasis: A portable 2d slip/skin stretch display for the ﬁngertip. In Eurohaptics Conference, 2005 and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems, 2005. World Haptics 2005. First Joint. IEEE, 214–219. \ 44. Ernst Heinrich Weber, Helen Elizabeth Ross, and David J Murray. 1996. EH Weber on the tactile senses. Psychology Press. \ 45. Vibol Yem, Ryuta Okazaki, and Hiroyuki Kajimoto. 2016a. FinGAR: combination of electrical and mechanical stimulation for high-ﬁdelity tactile presentation. In ACM SIGGRAPH 2016 Emerging Technologies. ACM, 7. \ 46. Vibol Yem, Ryuta Okazaki, and Hiroyuki Kajimoto. 2016b. Vibrotactile and pseudo force presentation using motor rotational acceleration. In Haptics Symposium (HAPTICS), 2016 IEEE. IEEE, 47–51. \ 47. André Zenner and Antonio Krüger. 2017. Shifty: A Weight-Shifting Dynamic Passive Haptic Proxy to Enhance Object Perception in Virtual Reality. IEEE Transactions on Visualization and Computer Graphics 23, 4 (2017), 1285–1294. \ ",Haptics; Virtual Reality; Mass Perception; Weight Force; Grasp,H.5.1; H.5.2,uistf1335-file1.zip,,uistf1335-file3.mp4,,,,"We present Grabity, a wearable haptic device designed to simulate kinesthetic pad opposition grip forces and weight for grasping virtual objects in VR.","1. Added actuation mechanism and details in Figure 3 \  \ 2. Added the circuitry design (Figure 5) \  \ 3. Clarified about the inertia rendering direction (1), interference between vibrations (2), and vibration for contact (3) \  \ 4. Corrected the explanation of Figure 9 \  \ 5. Added additional papers to related work \  \ 6. Fixed minor grammar problems \  \ 7. Added authors and aknowledgements \ ",Inrak Choi,Sean Follmer,FormatComplete,,Google Faculty Research Award,,,,,Aug 8 13:40,
uistf2681,10/23,3,Haptics,2:30:00 PM,3:50:00 PM,4,3:00:00 PM,3:20:00 PM,long,long,uistf1335,3,303,,,uistf2681,A,Frictio: Passive Kinesthetic Force Feedback for Smart Ring Output,Teng,Han,hanteng1021@gmail.com,uistf2681-paper.pdf,12,letter,,,"Teng Han, Qian Han, Michelle Annett, Fraser Anderson, Da-Yuan Huang, Xing-Dong Yang","hanteng1021@gmail.com, Qian.Han.GR@dartmouth.edu, mkannett@gmail.com, fraser.anderson@autodesk.com, dayuansmile@gmail.com, xing-dong.yang@dartmouth.edu",22861,Teng,,Han,hanteng1021@gmail.com,Department of Computer Science,University of Manitoba,Winnipeg,Manitoba,Canada,,,,,,71752,Qian,,Han,Qian.Han.GR@dartmouth.edu,Dartmouth College,Department of Computer Science,Hanover,New Hampshire,United States,,,,,,11551,Michelle,,Annett,mkannett@gmail.com,"DGP, University of Toronto, Toronto, Ontario, Canada",,,,,,,,,,25365,Fraser,,Anderson,fraser.anderson@autodesk.com,,Autodesk Research,Toronto,Ontario,Canada,,,,,,27289,Da-Yuan,,Huang,dayuansmile@gmail.com,,Dartmouth College,Hanover,New Hampshire,United States,,,,,,10966,Xing-Dong,,Yang,xing-dong.yang@dartmouth.edu,Department of Computer Science,Dartmouth College,Hanover,New Hampshire,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Smart rings have a unique form factor suitable for many applications, however, they offer little opportunity to provide the user with natural output. We propose passive kinesthetic force feedback as a novel output method for rotational input on smart rings. With this new output channel, friction force profiles can be designed, programmed, and felt by a user when they rotate the ring. This modality enables new interactions for ring form factors. We demonstrate the potential of this new haptic output method though Frictio, a prototype smart ring. In a controlled experiment, we determined the recognizability of six force profiles, including Hard Stop, Ramp-Up, Ramp-Down, Resistant Force, Bump, and No Force. The results showed that participants could distinguish between the force profiles with 94% accuracy. We conclude by presenting a set of novel interaction techniques that Frictio enables, and discuss insights and directions for future research.",hanteng1021@gmail.com,"1. Global Smart Rings Market 2017-2021, http://www.researchandmarkets.com/research/xqhhnz/gl obal_smart; Accessed March 2017. \ 2. Daniel Ashbrook, Patrick Baudisch, and Sean White. 2011. Nenya: subtle and eyes-free mobile input with a magnetically-tracked finger ring. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '11). ACM, New York, NY, USA, 20432046. https://doiorg.uml.idm.oclc.org/10.1145/1978942.1979238 \ 3. Mircea Badescu, Charles Wampler and Constantinos Mavroidis. 2002. Rotary haptic knob for vehicular instrument controls. In Proceedings 10th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems (HAPTICS 2002). IEEE, 342343. \ 4. Andrew Bragdon, Eugene Nelson, Yang Li, and Ken Hinckley. 2011. Experimental analysis of touch-screen gesture designs in mobile environments. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '11). ACM, New York, NY, USA, 403-412. https://doiorg.uml.idm.oclc.org/10.1145/1978942.1979000 \ 5. Liwei Chan, Yi-Ling Chen, Chi-Hao Hsieh, Rong-Hao Liang, and Bing-Yu Chen. 2015. CyclopsRing: Enabling Whole-Hand and Context-Aware Interactions Through a Fisheye Ring. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST '15). ACM, New York, NY, USA, 549-556. https://doiorg.uml.idm.oclc.org/10.1145/2807442.2807450 \ 6. Ke-Yu Chen, Kent Lyons, Sean White, and Shwetak Patel. 2013. uTrack: 3D input using two magnetic sensors. In Proceedings of the 26th annual ACM symposium on User interface software and technology (UIST '13). ACM, New York, NY, USA, 237-244. http://dx.doi.org.uml.idm.oclc.org/10.1145/2501988.250 \ 7. Yongsoon Choi, Jordan Tewell, Yukihiro Morisawa, Gilang A. Pradana, and Adrian David Cheok. 2014. Ring*U: a wearable system for intimate communication using tactile lighting expressions. In Proceedings of the 11th Conference on Advances in Computer Entertainment Technology (ACE '14). ACM, New York, NY, USA, Article 63, 4 pages. https://doiorg.uml.idm.oclc.org/10.1145/2663806.2663814 \ 8. Euan Freeman, Stephen Brewster, and Vuokko Lantz. 2014. Tactile Feedback for Above-Device Gesture Interfaces: Adding Touch to Touchless Interactions. In Proceedings of the 16th International Conference on Multimodal Interaction (ICMI '14). ACM, New York, NY, USA, 419-426. https://doiorg.uml.idm.oclc.org/10.1145/2663204.2663280 \ 9. Joseph Greenspun and Kristofer SJ Pister. 2013. Ring GINA: a wearable computer interaction device. In International Conference on Mobile Computing, Applications, and Services, Springer, 98-103. \ 10. Xiaochi Gu, Yifei Zhang, Weize Sun, Yuanzhe Bian, Dao Zhou, and Per Ola Kristensson. 2016. Dexmo: An Inexpensive and Lightweight Mechanical Exoskeleton for Motion Capture and Force Feedback in VR. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16). ACM, New York, NY, USA, 1991-1995. https://doiorg.uml.idm.oclc.org/10.1145/2858036.2858487 \ 11. Jeremy Gummeson, Bodhi Priyantha, and Jie Liu. 2014. An energy harvesting wearable ring platform for gestureinput on surfaces. In Proceedings of the 12th annual international conference on Mobile systems, applications, and services (MobiSys '14). ACM, New York, NY, USA, 162-175. http://doi.acm.org.uml.idm.oclc.org/10.1145/2594368.2 \ 12. Sungdo Ha, Laehyun Kim, Sehyung Park, Cha-soo Jun and H Rho. 2009. Virtual prototyping enhanced by a haptic interface. CIRP annals-manufacturing technology, 58 (1). 135-138. \ 13. Pilwon Heo and Jung Kim. 2012. Finger flexion force sensor based on volar displacement of flexor tendon. In Robotics and Automation (ICRA), 2012 IEEE International Conference on, IEEE, 1392-1397. \ 14. Seongkook Heo, Michelle Annett, Ben Lafreniere, Tovi Grossman and George Fitzmaurice. 2017. No Need to Stop What You’re Doing: Exploring No-Handed Smartwatch Interaction. In Proceedings of Graphics Interface (GI'17). 107-116. \ 15. Apple Inc. 2015. Devices and Methods for a Ring Computing Device USA. \ 16. Takayuki Iwamoto and Hiroyuki Shinoda. 2007. Finger ring device for tactile sensing and human machine interface. In SICE, 2007 Annual Conference, IEEE, 2132-2136. \ 17. Seungwoo Je, Brendan Rooney, Liwei Chan, and Andrea Bianchi. 2017. tactoRing: A Skin-Drag Discrete Display. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17). ACM, New York, NY, USA, 3106-3114. https://doiorg.uml.idm.oclc.org/10.1145/3025453.3025703 \ 18. Lei Jing, Yinghui Zhou, Zixue Cheng and Tongjun Huang. 2012. Magic ring: A finger-worn device for multiple appliances control using static finger gestures. Sensors, 12 (5). 5775-5790. \ 19. Hamed Ketabdar, Peyman Moghadam and Mehran Roshandel. 2012. Pingu: A new miniature wearable device for ubiquitous computing environments. in Complex, Intelligent and Software Intensive Systems (CISIS), 2012 Sixth International Conference on, IEEE, 502-506. \ 20. Hamed Ketabdar, Mehran Roshandel, and Kamer Ali Yüksel. 2010. MagiWrite: towards touchless digit entry using 3D space around mobile devices. In Proceedings of the 12th international conference on Human computer interaction with mobile devices and services (MobileHCI '10). ACM, New York, NY, USA, 443446. https://doiorg.uml.idm.oclc.org/10.1145/1851600.1851701 \ 21. Hamed Ketabdar, Kamer Ali Yüksel, and Mehran Roshandel. 2010. MagiTact: interaction with mobile devices based on compass (magnetic) sensor. In Proceedings of the 15th international conference on Intelligent user interfaces (IUI '10). ACM, New York, NY, USA, 413-414. https://doiorg.uml.idm.oclc.org/10.1145/1719970.1720048 \ 22. Wolf Kienzle and Ken Hinckley. 2014. LightRing: always-available 2D input on any surface. In Proceedings of the 27th annual ACM symposium on User interface software and technology (UIST '14). ACM, New York, NY, USA, 157-160. https://doiorg.uml.idm.oclc.org/10.1145/2642918.2647376 \ 23. Alan HF Lam, Wen J Li, Yunhui Liu and Ning Xi. 2002. MIDS: micro input devices system using MEMS sensors. in Intelligent Robots and Systems, 2002. IEEE/RSJ International Conference on, IEEE, 11841189. \ 24. Stefan Marti and Chris Schmandt. 2005. Giving the caller the finger: collaborative responsibility for cellphone interruptions. In CHI '05 Extended Abstracts on Human Factors in Computing Systems (CHI EA '05). ACM, New York, NY, USA, 1633-1636. http://dx.doi.org.uml.idm.oclc.org/10.1145/1056808.105 \ 25. Cameron S. Miner, Denise M. Chan, and Christopher Campbell. 2001. Digital jewelry: wearable technology for everyday life. In CHI '01 Extended Abstracts on Human Factors in Computing Systems (CHI EA '01). ACM, New York, NY, USA, 45-46. http://dx.doi.org.uml.idm.oclc.org/10.1145/634067.6340 \ 26. Suranga Nanayakkara, Roy Shilkrot, Kian Peen Yeo, and Pattie Maes. 2013. EyeRing: a finger-worn input device for seamless interactions with our surroundings. In Proceedings of the 4th Augmented Human International Conference (AH '13). ACM, New York, NY, USA, 13-20. http://dx.doi.org.uml.idm.oclc.org/10.1145/2459236.245 \ 27. Shahriar Nirjon, Jeremy Gummeson, Dan Gelb, and Kyu-Han Kim. 2015. TypingRing: A Wearable Ring Platform for Text Input. In Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys '15). ACM, New York, NY, USA, 227-239. http://dx.doi.org.uml.idm.oclc.org/10.1145/2742647.274 \ 28. Masa Ogata, Yuta Sugiura, Hirotaka Osawa, and Michita Imai. 2012. iRing: intelligent ring using infrared reflection. In Proceedings of the 25th annual ACM symposium on User interface software and technology (UIST '12). ACM, New York, NY, USA, 131-136. https://doiorg.uml.idm.oclc.org/10.1145/2380116.2380135 \ 29. Masayasu Ogata, Yuta Sugiura, Hirotaka Osawa, and Michita Imai. 2012. Pygmy: a ring-like anthropomorphic device that animates the human hand. In CHI '12 Extended Abstracts on Human Factors in Computing Systems (CHI EA '12). ACM, New York, NY, USA, 1003-1006. http://dx.doi.org.uml.idm.oclc.org/10.1145/2212776.221 \ 30. Jerome Pasquero, Scott J. Stobbe, and Noel Stonehouse. 2011. A haptic wristwatch for eyes-free interactions. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '11). ACM, New York, NY, USA, 3257-3266. https://doiorg.uml.idm.oclc.org/10.1145/1978942.1979425 \ 31. Gilang Andi Pradana, Adrian David Cheok, Masahiko Inami, Jordan Tewell, and Yongsoon Choi. 2014. Emotional priming of mobile text messages with ringshaped wearable device using color lighting and tactile expressions. In Proceedings of the 5th Augmented Human International Conference (AH '14). ACM, New York, NY, USA, , Article 14 , 8 pages. http://dx.doi.org.uml.idm.oclc.org/10.1145/2582051.258 \ 32. Sokwoo Rhee, Boo-Ho Yang, Kuowei Chang and Hamhiko H Asada. 1998. The ring sensor: a new ambulatory wearable sensor for twenty-four hour patient monitoring. in Engineering in Medicine and Biology Society, 1998. Proceedings of the 20th Annual International Conference of the IEEE, IEEE, 1906-1909. \ 33. Louis Rosenberg and Scott Brave. 1996. Using force feedback to enhance human performance in graphical user interfaces. In Conference Companion on Human Factors in Computing Systems (CHI '96), Michael J. Tauber (Ed.). ACM, New York, NY, USA, 291-292. http://dx.doi.org.uml.idm.oclc.org/10.1145/257089.2573 \ 34. Mehran Roshandel, Aarti Munjal, Peyman Moghadam, Shahin Tajik and Hamed Ketabdar. 2014. Multi-sensor finger ring for authentication based on 3D signatures. In International Conference on Human-Computer Interaction, Springer, 131-138. \ 35. Thijs Roumen, Simon T. Perrault, and Shengdong Zhao. 2015. NotiRing: A Comparative Study of Notification Channels for Wearable Interactive Rings. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15). ACM, New York, NY, USA, 2497-2500. https://doiorg.uml.idm.oclc.org/10.1145/2702123.2702350 \ 36. Roy Shilkrot, Jochen Huber, Jürgen Steimle, Suranga Nanayakkara and Pattie Maes. 2015. Digital Digits: A Comprehensive Survey of Finger Augmentation Devices. ACM Computing Surveys (CSUR), 48 (2). 30. \ 37. Scott S. Snibbe, Karon E. MacLean, Rob Shaw, Jayne Roderick, William L. Verplank, and Mark Scheeff. 2001. Haptic techniques for media control. In Proceedings of the 14th annual ACM symposium on User interface software and technology (UIST '01). ACM, New York, NY, USA, 199-208. http://dx.doi.org.uml.idm.oclc.org/10.1145/502348.5023 \ 38. J Ridley Stroop. 1935. Studies of interference in serial verbal reactions. Journal of experimental psychology, 18 (6). 643. \ 39. Colin Edward Swindells. 2007. Incorporating affect into the design of 1-D rotary physical controls, University of British Columbia. \ 40. Julia Werner, Reto Wettach, and Eva Hornecker. 2008. United-pulse: feeling your partner's pulse. In Proceedings of the 10th international conference on Human computer interaction with mobile devices and services (MobileHCI '08). ACM, New York, NY, USA, 535-538. http://dx.doi.org.uml.idm.oclc.org/10.1145/1409240.140 \ 41. Katrin Wolf and Jonas Willaredt. 2015. PickRing: seamless interaction through pick-up detection. In Proceedings of the 6th Augmented Human International Conference (AH '15). ACM, New York, NY, USA, 1320. http://dx.doi.org.uml.idm.oclc.org/10.1145/2735711.273 \ 42. Yu-Chi Wu, Wei-Hong Hsu, Chao-Shu Chang, WenChing Yu, Wen-Liang Huang and Meng-Jen Chen. 2010. A smart-phone-based health management system using a wearable ring-type pulse sensor. in International Conference on Mobile and Ubiquitous Systems: Computing, Networking, and Services, Springer, 409416. \ 43. Renqiang Xie, Xia Sun, Xiang Xia and Juncheng Cao. 2015. Similarity matching-based extensible hand gesture recognition. IEEE sensors journal, 15 (6). 34753483. \ 44. Boo-Ho Yang, Sokwoo Rhee and Haruhiko H Asada. 1998. A twenty-four hour tele-nursing system using a ring sensor. in Robotics and Automation, 1998. Proceedings. 1998 IEEE International Conference on, IEEE, 387-392. \ 45. Kiwon Yeom, Jounghuem Kwon, JooHyun Maeng and Bum-Jae You. 2015. [POSTER] Haptic Ring Interface Enabling Air-Writing in Virtual Reality Environment. in Mixed and Augmented Reality (ISMAR), 2015 IEEE International Symposium on, IEEE, 124-127. \ 46. Sang Ho Yoon, Yunbo Zhang, Ke Huo, and Karthik Ramani. 2016. TRing: Instant and Customizable Interactions with Objects Using an Embedded Magnet and a Finger-Worn Device. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). ACM, New York, NY, USA, 169-181. https://doiorg.uml.idm.oclc.org/10.1145/2984511.2984529 \ 47. Boning Zhang, Yiqiang Chen, Yueliang Qian, and Xiangdong Wang. 2011. A ring-shaped interactive device for large remote display and mobile device control. In Proceedings of the 13th international conference on Ubiquitous computing (UbiComp '11). ACM, New York, NY, USA, 473-474. https://doiorg.uml.idm.oclc.org/10.1145/2030112.2030177 \ ",Haptics; Ring; Wearable; Passive Kinesthetic Force Feedback,H.5.2,uistf2681-file1.docx,uistf2681-file2.jpg,uistf2681-file3.mp4,,A user feels a friction force when they rotate the Frictio ring.,,Contributes passive kinesthetic force feedback as a novel output method for rotational input on smart rings.,"We thank again the AC and reviewers for the feedback and suggestions. Below is the list of changes for the AC to follow up when reviewing the paper. \  \ - page 1. first paragraph \  \ 	deleted the incorrect claims ""... 2014"" and ""... disruptive"". \  \ - page 1, second paragraph \  \ 	added a few sentences to clarify that such output is unique and complements existing modalities. \  \ - page 2, smart ring output \  \ 	added the suggested references \  \ - page 3, design parameters \  \ 	made the defination more concise: kinesthetic force feedback relates to the feeling sensed from muscles, tendons, and joints. \  \ - page 3, design paramters \  \ 	removed the claim ""... a fusion of kinesthetic and passive feedback"" \  \ - page 4, prototype \  \ 	clarified the force profiles were rendered on rotation angle. \  \ 	added the implementation details stated in the rebuttal. \  \ 	modified the force profile figure for ""Hard Stop"". \  \ - page 6, dependent measures \  \ 	clarfied the response time for bump and hard stop profiles. \  \ 	corrected the labels for the matrixes in Figure 6. \  \ - page 9, future work \  \ 	added a paragraph to indicate that potentially, more complex force profiles are possible and worth to explore. \  \ 	added a limitation of current prototype: size \  \ - Typos were fixed through the paper. \  \  \ ",Teng Han,Xing-Dong Yang,FormatComplete,,,,,,,Aug 6 23:07,
uistf1814,10/23,3,Haptics,2:30:00 PM,3:50:00 PM,4,3:20:00 PM,3:40:00 PM,long,long,uistf2681,4,304,Honorable Mention,,uistf1814,A,MagTics: Flexible and Thin Form Factor Magnetic Actuators for Dynamic and Wearable Haptic Feedback,Fabrizio,Pece,fabrizio.pece@inf.ethz.ch,uistf1814-paper.pdf,12,letter,Times-Roman,,"Fabrizio Pece, Juan Jose Zarate, Velko Vechev, Nadine Besse, Olexandr Gudozhnik, Herbert Shea, Otmar Hilliges","fabrizio.pece@inf.ethz.ch, juan.zarate@epfl.ch, velko@student.chalmers.se, nadine.besse@epfl.ch, olexandr.gudozhnik@epfl.ch, herbert.shea@epfl.ch, otmar.hilliges@inf.ethz.ch",43359,Fabrizio,,Pece,fabrizio.pece@inf.ethz.ch,,ETH Zurich,Zurich,,Switzerland,,,,,,68289,Juan,Jose,Zarate,juan.zarate@epfl.ch,,École Polytechnique Fédérale de Lausanne,Neuchâtel,,Switzerland,,,,,,50851,Velko,,Vechev,velko@student.chalmers.se,,Chalmers University of Technology,Gothenburg,,Sweden,,,,,,71780,Nadine,,Besse,nadine.besse@epfl.ch,,École Polytechnique Fédérale de Lausanne,Neuchâtel,,Switzerland,,,,,,68323,Olexandr,,Gudozhnik,olexandr.gudozhnik@epfl.ch,,École Polytechnique Fédérale de Lausanne,Neuchâtel,,Switzerland,,,,,,68325,Herbert,,Shea,herbert.shea@epfl.ch,,École Polytechnique Fédérale de Lausanne,Neuchâtel,,Switzerland,,,,,,6754,Otmar,,Hilliges,otmar.hilliges@inf.ethz.ch,,ETH Zurich,Zurich,,Switzerland,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present MagTics, a novel flexible and wearable haptic interface based on magnetically actuated bidirectional tactile pixels (taxels). \ MagTics' thin form factor and flexibility allows for rich haptic feedback in mobile settings. \ We propose a novel actuation mechanism based on bistable electromagnetic latching that combines high frame rate and holding force with low energy consumption and a soft and flexible form factor. \ We overcome limitations of traditional soft actuators by placing several hard actuation cells, driven by flexible printed electronics, in a soft 3D printed case. A novel EM-shielding prevents magnet-magnet interactions and allows for high actuator densities. \ A prototypical implementation comprising of 4 actuated pins on a 1.7 cm pitch, with 2 mm travel, and generating 160 mN to 200 mN  of latching force is used to implement a number of compelling application scenarios including adding haptic and tactile display capabilities to wearable devices, to existing input devices and to provide localized haptic feedback in virtual reality. \ Finally, we report results of a psychophysical study, conducted to inform future developments and to identify possible application domains.",fabrizio.pece@inf.ethz.ch,"1. Alvina, J. et al. OmniVibe: Towards Cross-body Spatiotemporal Vibrotactile Notiﬁcations for Mobile Phones. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, CHI ’15, ACM, 2015, 2487–2496. \ 2. Araujo, B. et al. Snake Charmer: Physically Enabling Virtual Objects. In Proceedings of the TEI ’16: Tenth International Conference on Tangible, Embedded, and Embodied Interaction, TEI ’16, ACM, 2016, 218–226. \ 3. Bau, O. et al. TeslaTouch: Electrovibration for Touch Surfaces. In Proceedings of the 23Nd Annual ACM Symposium on User Interface Software and Technology, UIST ’10, ACM, 2010, 283–292. \ 4. Benali-Khoudja, M., M. Hafez, and A. Kheddar. VITAL: An electromagnetic integrated tactile display. Displays 28, 3 (2007), 133–144. \ 5. Benko, H. et al. NormalTouch and TextureTouch: High-ﬁdelity 3D Haptic Shape Rendering on Handheld Virtual Reality Controllers. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, UIST ’16, ACM, 2016, 717–728. \ 6. Bouzit, M. et al. The Rutgers Master II-ND Force Feedback Glove. In Proceedings of the 10th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems, HAPTICS ’02, IEEE Computer Society, 2002, 145. \ 7. Brewster, S., and L. M. Brown. Tactons: Structured Tactile Messages for Non-visual Information Display. In Proceedings of the Fifth Conference on Australasian User Interface - Volume 28, AUIC ’04, Australian Computer Society, Inc., 2004, 15–23. \ 8. Brown, L. M., S. A. Brewster, and H. C. Purchase. Multidimensional Tactons for Non-visual Information Presentation in Mobile Devices. In Proceedings of the 8th Conference on Human-computer Interaction with Mobile Devices and Services, MobileHCI ’06, ACM, 2006, 231–238. \ 9. Carcedo, M. G. et al. HaptiColor: Interpolating Color Information As Haptic Feedback to Assist the Colorblind. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI ’16, ACM, 2016, 3572–3583. \ 10. Cauchard, J. R. et al. ActiVibe: Design and Evaluation of Vibrations for Progress Monitoring. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI ’16, ACM, 2016, 3261–3271. \ 11. Chancey, E. T. et al. Vibrotactile Stimuli Parameters on Detection Reaction Times. Proceedings of the Human Factors and Ergonomics Society Annual Meeting 58, 1 (2014), 1701–1705. \ 12. Choi, I., and S. Follmer. Wolverine: A Wearable Haptic Interface for Grasping in VR. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, UIST ’16 Adjunct, ACM, 2016, 117–119. \ 13. CyberGlove Systems Inc. CyberGrasp Glove. http://www.cyberglovesystems.com/cybergrasp. Last accessed: 12.03.2017. \ 14. CyberGlove Systems Inc. CyberTouch Glove. http://www.cyberglovesystems.com/cybertouch. Last accessed: 18.03.2017. \ 15. Dobbelstein, D., P. Henzler, and E. Rukzio. Unconstrained Pedestrian Navigation Based on Vibro-tactile Feedback Around the Wristband of a Smartwatch. In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems, CHI EA ’16, ACM, 2016, 2439–2445. \ 16. Dosher, J., and B. Hannaford. Human Interaction with Small Haptic Effects. Presence: Teleoper. Virtual Environ. 14, 3 (June 2005), 329–344. \ 17. Follmer, S. et al. inFORM: Dynamic Physical Affordances and Constraints Through Shape and Object Actuation. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology, UIST ’13, ACM, 2013, 417–426. \ 18. Gu, X. et al. Dexmo: An Inexpensive and Lightweight Mechanical Exoskeleton for Motion Capture and Force Feedback in VR. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI ’16, ACM, 2016, 1991–1995. \ 19. Hayward, V. et al. Haptic Interfaces and Devices. Sensor Review 24, 1 (2004), 16–29. \ 20. Hirsh, I. J., and C. E. S. Jr. Perceived order in different sense modalities. Journal of Experimental Psychology 62 (1961), 423–432. \ 21. Hoffman, H. G. Physically Touching Virtual Objects Using Tactile Augmentation Enhances the Realism of Virtual Environments. In Proceedings of the Virtual Reality Annual International Symposium, VRAIS ’98, IEEE Computer Society, 1998, 59. \ 22. Hoshi, T. et al. Noncontact Tactile Display Based on Radiation Pressure of Airborne Ultrasound. EEE Trans. Haptics 3, 3 (July 2010), 155–165. \ 23. Iwata, H. et al. Project FEELEX: Adding Haptic Surface to Graphics. In Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’01, ACM, 2001, 469–476. \ 24. Jang, S. et al. Haptic Edge Display for Mobile Tactile Interaction. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI ’16, ACM, 2016, 3706–3716. \ 25. Karuei, I. et al. Detecting Vibrations Across the Body in Mobile Contexts. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’11, ACM, 2011, 3267–3276. \ 26. Khurelbaatar, S. et al. Tactile Presentation to the Back of a Smartphone with Simultaneous Screen Operation. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI ’16, ACM, 2016, 3717–3721. \ 27. Kim, H., M. Kim, and W. Lee. HapThimble: A Wearable Haptic Device Towards Usable Virtual Touch Screen. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI ’16, ACM, 2016, 3694–3705. \ 28. King, H. H., R. Donlin, and B. Hannaford. Perceptual Thresholds for Single vs. Multi-Finger Haptic Interaction. In Proceedings of the 2010 IEEE Haptics Symposium, HAPTIC ’10, IEEE Computer Society, 2010, 95–99. \ 29. Koo, I. M. et al. Development of Soft-Actuator-Based Wearable Tactile Display. Trans. Rob. 24, 3 (June 2008), 549–558. \ 30. Kyung, K.-U., and J.-Y. Lee. Ubi-Pen: A Haptic Interface with Texture and Vibrotactile Display. IEEE Comput. Graph. Appl. 29, 1 (Jan. 2009), 56–64. \ 31. Lee, J., J. Han, and G. Lee. Investigating the Information Transfer Efﬁciency of a 3x3 Watch-back Tactile Display. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, CHI ’15, ACM, 2015, 1229–1232. \ 32. Lee, J. C. et al. Haptic Pen: A Tactile Feedback Stylus for Touch Screens. In Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology, UIST ’04, ACM, 2004, 291–294. \ 33. Lee, S. C., and T. Starner. BuzzWear: Alert Perception in Wearable Tactile Displays on the Wrist. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’10, ACM, 2010, 433–442. \ 34. Liao, Y.-C. et al. EdgeVib: Effective Alphanumeric Character Output Using a Wrist-Worn Tactile Display. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, UIST ’16, ACM, 2016, 595–601. \ 35. Louw, S., A. M. L. Kappers, and J. J. Koenderink. Active Haptic Detection and Discrimination of Shape. Perception & Psychophysics 64, 7 (2002), 1108–1119. \ 36. Luk, J. et al. A Role for Haptics in Mobile Interaction: Initial Design Using a Handheld Tactile Display Prototype. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’06, ACM, 2006, 171–180. \ 37. Machida, T., N. K. Dim, and X. Ren. Suitable Body Parts for Vibration Feedback in Walking Navigation Systems. In Proceedings of the Third International Symposium of Chinese CHI, Chinese CHI ’15, ACM, 2015, 32–36. \ 38. Massie, T. H., and J. K. Salisbury. The PHANToM Haptic Interface: A Device for Probing Virtual Objects. In Proceedings of the ASME Winter Annual Metting ’94, Dynamics and Control 1994, 1994, 295–301. \ 39. NeuroDigital Technologies. Gloveone Glove. https://www.neurodigital.es/gloveone/. Last accessed: 29.03.2017. \ 40. Niu, X. et al. Bistable Large-Strain Actuation of Interpenetrating Polymer Networks. Advanced Materials 24, 48 (dec 2012), 6513–6519. \ 41. Pasquero, J., S. J. Stobbe, and N. Stonehouse. A Haptic Wristwatch for Eyes-free Interactions. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’11, ACM, 2011, 3257–3266. \ 42. Poupyrev, I. et al. Project Jacquard: Interactive Digital Textiles at Scale. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI ’16, ACM, 2016, 4216–4227. \ 43. Poupyrev, I., and S. Maruyama. Tactile Interfaces for Small Touch Screens. In Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology, UIST ’03, ACM, 2003, 217–220. \ 44. Poupyrev, I., S. Maruyama, and J. Rekimoto. Ambient Touch: Designing Tactile Interfaces for Handheld Devices. In Proceedings of the 15th Annual ACM Symposium on User Interface Software and Technology, UIST ’02, ACM, 2002, 51–60. \ 45. Poupyrev, I. et al. Lumen: Interactive Visual and Shape Display for Calm Computing. In ACM SIGGRAPH 2004 Emerging Technologies, SIGGRAPH ’04, ACM, 2004, 17. \ 46. Prattichizzo, D. et al. Towards Wearability in Fingertip Haptics: a 3-DoF Wearable Device for Cutaneous Force Feedback. EEE Trans. Haptics 6, 4 (Oct 2013), 506–516. \ 47. Prescher, D., G. Weber, and M. Spindler. A Tactile Windowing System for Blind Users. In Proceedings of the 12th International ACM SIGACCESS Conference on Computers and Accessibility, ASSETS ’10, ACM, 2010, 91–98. \ 48. Saket, B. et al. Designing an Effective Vibration-based Notiﬁcation Interface for Mobile Phones. In Proceedings of the 2013 Conference on Computer Supported Cooperative Work, CSCW ’13, ACM, 2013, 149–1504. \ 49. Schneider, O. S. et al. HapTurk: Crowdsourcing Affective Ratings of Vibrotactile Icons. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI ’16, ACM, 2016, 3248–3260. \ 50. Stamper, R. E. A Three Degree Of Freedom Parallel Manipulator With Only Translational Degrees Of Freedom. PhD thesis, University of Maryland at College Park, 1997. \ 51. Strasnick, E., and S. Follmer. Applications of Switchable Permanent Magnetic Actuators in Shape Change and Tactile Display. In n Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16 Adjunct), UIST ’16, 2016. \ 52. Summers, I. R. et al. Results from a Tactile Array on the Fingertip. In Proceedings of Proceedings of Eurohaptics 2001, 2001. \ 53. Szabo, Z., and E. T. Enikov. Electromagnetic Microactuator-Array Based Virtual Tactile Display. In Computers Helping People with Special Needs: 15th International Conference, ICCHP 2016, Springer International Publishing, 2016, 53–60. \ 54. Tsukada, K., and M. Yasumura. ActiveBelt: Belt-Type Wearable Tactile Display for Directional Navigation. In Proceedings of UbiComp 2004: Ubiquitous Computing: 6th International Conference, N. Davies, E. D. Mynatt, and I. Siio, Eds., Springer Berlin Heidelberg, 2004, 384–399. \ 55. Van Der Linde, R. Q. et al. The HapticMaster, a new highperformance haptic interface. In Proceedings of Eurohaptics 2002, 2002. \ 56. Wang, Q., and V. Hayward. Biomechanically Optimized Distributed Tactile Transducer Based on Lateral Skin Deformation. The International Journal of Robotics Research 29, 4 (2010), 323–335. \ 57. Wang, Q., X. Ren, and X. Sun. EV-pen: an Electrovibration Haptic Feedback Pen for Touchscreens. In SIGGRAPH ASIA 2016 Emerging Technologies, SA ’16, ACM, 2016, 8:1–8:2. \ 58. Weinstein, S. Intensive and Extensive Aspects of Tactile Sensitivity as a Function of Body Part, Sex and Laterality. Kenshalo. The Skin Senses (1968), 195–222. \ 59. Wellman, P., and R. D. Howe. Towards Realistic Vibrotactile Display in Virtual Environments. In Proceedings of the ASME Dynamic Systems and Control Division, 1995, 713–718. \ 60. Zárate, J. J., and H. Shea. Using Pot-Magnets to Enable Stable and Scalable Electromagnetic Tactile Displays. IEEE Transactions on Haptics 10, 1 (Jan 2017), 106–112. \ 61. Zárate, J. J. et al. Optimization of the Force and Power Consumption of a Microfabricated Magnetic Actuator. Sensors and Actuators A: Physical 234 (2015), 57 – 64. \ ",Mobile Haptics; Tactile Displays; Wearable Computing,H.5.2,uistf1814-file1.zip,uistf1814-file2.jpg,uistf1814-file3.mp4,,"MagTics is a wearable haptic interface that allows for localized haptic and tactile feedback. Our fabrication technique integrates hard electromagnetic actuators in a soft holder, resulting in a fast, but yet power efficient actuation technique.",,"MagTics is a wearable haptic interface that allows for haptic and tactile feedback. We integrate hard electromagnetic actuators in a soft holder, obtaining a fast, but power efficient actuation technique, which we demonstrate with applications. \ ","We thank the reviewers for their insightful comments before and after the rebuttal period. Following their suggestions and what discussed in our rebuttal, we have now prepared a revised paper that addresses all the mains points for a strengthened version of our work. \  \ Specifically, we have added details on the current hardware implementation (i.e., circuit driving voltage, overheating issues and connection length impact -- Sec. Implementation), and explained how we plan to mitigate some of these issues in future version of our device (Sec. Future Work). Similarly, we have clarified aspects related to the lightweight magnetic shielding (Sec. Principle of Operations), and discussed the scalability property of the magnetic actuators (Sec. Implementation). We have also motivated the rationale for using 3D printed flexible material rather than fabric (Sec. Introduction), and discussed the strength of our generated forces wrt. previous work (Sec. Implementation). Further, we have commented on the durability of the device, proposing possible solutions to this issue to explore in future work (Sec. Future Work), and discussed how the actuation modes supported by our device can be combined to form an expanded actuation set (Sec. Application Scenarios). \ We have then clarified aspects related to the experimental studies, as outlined in our rebuttal (Table 1 and Sec. Exploratory Study), and amended the questionnaire reporting to include median and interquartile ranges (Table 1 and Sec. Qualitative Results). Similarly, we have modified the plot figures as suggested by the reviewers, and rephrased some of the comparison with previous studies as outlined in our rebuttal (Sec. Quantitative Results). Finally, we have added and discussed the references suggested by the reviewers in the Related Work section. ",Fabrizio Pece,Juan Zarate,FormatComplete,200021_162958,Swiss National Science Foundation (SNF),,,,,Aug 5 10:19,
uistf3285,10/23,4,Eye Tracking,2:30:00 PM,1:00:00 PM,4,2:30:00 PM,2:50:00 PM,long,long,none,1,401,,,uistf3285,A,EyeScout: Active Eye Tracking for Position and Movement Independent Gaze Interaction with Large Public Displays,Mohamed,Khamis,mkhamis89@gmail.com,uistf3285-paper.pdf,12,letter,,,"Mohamed Khamis, Axel Hoesl, Alexander Klimczak, Martin Reiss, Florian Alt, Andreas Bulling","mohamed.khamis@ifi.lmu.de, axel.hoesl@ifi.lmu.de, alexander.klimczack@campus.lmu.de, martin.reiss@campus.lmu.de, florian.alt@ifi.lmu.de, bulling@mpi-inf.mpg.de",38635,Mohamed,,Khamis,mohamed.khamis@ifi.lmu.de,,LMU Munich,Munich,,Germany,,,,,,40430,Axel,,Hoesl,axel.hoesl@ifi.lmu.de,,LMU Munich,Munich,,Germany,,,,,,63197,Alexander,,Klimczak,alexander.klimczack@campus.lmu.de,,LMU Munich,Munich,,Germany,,,,,,63198,Martin,,Reiss,martin.reiss@campus.lmu.de,,LMU Munich,Munich,,Germany,,,,,,13952,Florian,,Alt,florian.alt@ifi.lmu.de,,LMU Munich,Munich,,Germany,,,,,,11540,Andreas,,Bulling,bulling@mpi-inf.mpg.de,Max Planck Institute for Informatics,Saarland Informatics Campus,Saarbrücken,,Germany,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"While gaze holds a lot of promise for hands-free interaction with public displays, remote eye trackers with their confined tracking box restrict users to a single stationary position in front of the display. We present EyeScout, an active eye tracking system that combines an eye tracker mounted on a rail system with a computational method to automatically detect and align the tracker with the user’s lateral movement. EyeScout addresses key limitations of current gaze-enabled large public displays by offering two novel gaze-interaction modes for a single user: In ""Walk then Interact"" the user can walk up to an arbitrary position in front of the display and interact, while in ""Walk and Interact"" the user can interact even while on the move. We report on a user study that shows that EyeScout is well perceived by users, extends a public display's sweet spot into a sweet line, and reduces gaze interaction kick-off time to 3.5 seconds – a 62% improvement over state of the art solutions. We discuss sample applications that demonstrate how EyeScout can enable position and movement-independent gaze interaction with large public displays.",mohamed.khamis@ifi.lmu.de,"1. Florian Alt, Andreas Bulling, Gino Gravanis, and Daniel Buschek. 2015. GravitySpot: Guiding Users in Front of Public Displays Using On-Screen Visual Cues. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST ’15). ACM, New York, NY, USA, 47–56. DOI: http://dx.doi.org/10.1145/2807442.2807490 \ 2. Gilles Bailly, Sidharth Sahdev, Sylvain Malacria, and Thomas Pietrzak. 2016. LivingDesktop: Augmenting Desktop Workstation with Actuated Devices. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 5298–5310. DOI: http://dx.doi.org/10.1145/2858036.2858208 \ 3. Hrvoje Benko, Andrew D. Wilson, and Ravin Balakrishnan. 2008. Sphere: Multi-touch Interactions on a Spherical Display. In Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology (UIST ’08). ACM, New York, NY, USA, 77–86. DOI: http://dx.doi.org/10.1145/1449715.1449729 \ 4. Gilbert Beyer, Florian Alt, J¨org M¨uller, Albrecht Schmidt, Karsten Isakovic, Stefan Klose, Manuel Schiewe, and Ivo Haulsen. 2011. Audience Behavior Around Large Interactive Cylindrical Screens. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’11). ACM, New York, NY, USA, 1021–1030. DOI: http://dx.doi.org/10.1145/1978942.1979095 \ 5. Gilbert Beyer, Florian K¨ottner, Manuel Schiewe, Ivo Haulsen, and Andreas Butz. 2013. Squaring the Circle: How Framing Inﬂuences User Behavior Around a Seamless Cylindrical Display. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’13). ACM, New York, NY, USA, 1729–1738. DOI: http://dx.doi.org/10.1145/2470654.2466228 \ 6. David Beymer and Myron Flickner. 2003. Eye gaze tracking using an active stereo head. In Computer Vision and Pattern Recognition, 2003. Proceedings. 2003 IEEE Computer Society Conference on, Vol. 2. II–451–8 vol.2. DOI: http://dx.doi.org/10.1109/CVPR.2003.1211502 \ 7. John Bolton, Peng Wang, Kibum Kim, and Roel Vertegaal. 2012. BodiPod: Interacting with 3D Human Anatomy via a 360◦ Cylindrical Display. In CHI ’12 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’12). ACM, New York, NY, USA, 1039–1042. DOI: http://dx.doi.org/10.1145/2212776.2212380 \ 8. Harry Brignull and Yvonne Rogers. 2003. Enticing people to interact with large public displays in public spaces. In In Proceedings of the IFIP International Conference on Human-Computer Interaction (INTERACT 2003. 17–24. \ 9. Chao-Ning Chan, Shunichiro Oe, and Chern-Sheng Lin. 2007a. Active Eye-tracking System by Using Quad PTZ Cameras. In Industrial Electronics Society, 2007. IECON 2007. 33rd Annual Conference of the IEEE. 2389–2394. DOI: http://dx.doi.org/10.1109/IECON.2007.4460366 \ 10. Chao Ning Chan, Shunichiro Oe, and Chern-Sheng Lin. 2007b. Development of an active gaze tracking system in unrestricted posture. In International Conference on Control, Automation and Systems, 2007. ICCAS ’07. 1348–1353. DOI: http://dx.doi.org/10.1109/ICCAS.2007.4406547 \ 11. Dong-Chan Cho and Whoi-Yul Kim. 2013. Long-Range Gaze Tracking System for Large Movements. IEEE Transactions on Biomedical Engineering 60, 12 (Dec 2013), 3432–3440. DOI: http://dx.doi.org/10.1109/TBME.2013.2266413 \ 12. Nicholas S. Dalton, Emily Collins, and Paul Marshall. 2015. Display Blindness?: Looking Again at the Visibility of Situated Displays Using Eye-tracking. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15). ACM, New York, NY, USA, 3889–3898. DOI: http://dx.doi.org/10.1145/2702123.2702150 \ 13. Nigel Davies, Sarah Clinch, and Florian Alt. 2014. Pervasive Displays: Understanding the Future of Digital Signage (1st ed.). Morgan & Claypool Publishers. \ 14. Heiko Drewes and Albrecht Schmidt. 2007. Interacting with the Computer Using Gaze Gestures. Springer Berlin Heidelberg, Berlin, Heidelberg, 475–488. DOI: http://dx.doi.org/10.1007/978-3-540-74800-7_43 \ 15. Marc Eaddy, Gabor Blasko, Jason Babcock, and Steven Feiner. 2004. My own private kiosk: privacy-preserving public displays. In Eighth International Symposium on Wearable Computers, Vol. 1. 132–135. DOI: http://dx.doi.org/10.1109/ISWC.2004.32 \ 16. Augusto Esteves, Eduardo Velloso, Andreas Bulling, and Hans Gellersen. 2015. Orbits: Gaze Interaction for Smart Watches Using Smooth Pursuit Eye Movements. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST ’15). ACM, New York, NY, USA, 457–466. DOI: http://dx.doi.org/10.1145/2807442.2807499 \ 17. Craig Hennessey and Jacob Fiset. 2012. Long Range Eye Tracking: Bringing Eye Tracking into the Living Room. In Proceedings of the Symposium on Eye Tracking Research and Applications (ETRA ’12). ACM, New York, NY, USA, 249–252. DOI: http://dx.doi.org/10.1145/2168556.2168608 \ 18. Axel Hoesl, Julie Wagner, and Andreas Butz. 2015. Delegation Impossible?: Towards Novel Interfaces for Camera Motion. In Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA ’15). ACM, New York, NY, USA, 1729–1734. DOI: http://dx.doi.org/10.1145/2702613.2732904 \ 19. Shahram Jalaliniya and Diako Mardanbegi. 2016. EyeGrip: Detecting Targets in a Series of Uni-directional Moving Objects Using Optokinetic Nystagmus Eye Movements. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 5801–5811. DOI: http://dx.doi.org/10.1145/2858036.2858584 \ 20. Mohamed Khamis, Florian Alt, and Andreas Bulling. 2015. A Field Study on Spontaneous Gaze-based Interaction with a Public Display Using Pursuits. In Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers (UbiComp/ISWC’15 Adjunct). ACM, New York, NY, USA, 863–872. DOI: http://dx.doi.org/10.1145/2800835.2804335 \ 21. Mohamed Khamis, Florian Alt, and Andreas Bulling. 2016a. Challenges and Design Space of Gaze-enabled Public Displays. In Ajunct Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp ’16). ACM, New York, NY, USA, 10. DOI: http://dx.doi.org/10.1145/2968219.2968342 \ 22. Mohamed Khamis, Ozan Saltuk, Alina Hang, Katharina Stolz, Andreas Bulling, and Florian Alt. 2016b. TextPursuits: Using Text for Pursuits-Based Interaction and Calibration on Public Displays. In Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp ’16). ACM, New York, NY, USA, 12. DOI: http://dx.doi.org/10.1145/2971648.2971679 \ 23. Mohamed Khamis, Ludwig Trotter, Markus Tessmann, Christina Dannhart, Andreas Bulling, and Florian Alt. 2016c. EyeVote in the Wild: Do Users Bother Correcting System Errors on Public Displays?. In Proceedings of the 15th International Conference on Mobile and Ubiquitous Multimedia (MUM ’16). ACM, New York, NY, USA, 57–62. DOI: http://dx.doi.org/10.1145/3012709.3012743 \ 24. Christian Lander, Sven Gehring, Antonio Kr¨uger, Sebastian Boring, and Andreas Bulling. 2015. GazeProjector: Accurate Gaze Estimation and Seamless Gaze Interaction Across Multiple Displays. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST ’15). ACM, New York, NY, USA, 395–404. DOI: http://dx.doi.org/10.1145/2807442.2807479 \ 25. Michael Mrochen, Mostafa Salah Eldine, Maik Kaemmerer, Theo Seiler, and Werner Htz. 2001. Improvement in photorefractive corneal laser surgery results using an active eye-tracking system. Journal of Cataract & Refractive Surgery 27, 7 (2001), 1000 – 1006. DOI: http://dx.doi.org/10.1016/S0886-3350(00)00884-1 \ 26. J¨org M¨uller, Florian Alt, Daniel Michelis, and Albrecht Schmidt. 2010. Requirements and Design Space for Interactive Public Displays. In Proceedings of the 18th ACM International Conference on Multimedia (MM ’10). ACM, New York, NY, USA, 1285–1294. DOI: http://dx.doi.org/10.1145/1873951.1874203 \ 27. J¨org M¨uller, Robert Walter, Gilles Bailly, Michael Nischt, and Florian Alt. 2012. Looking Glass: A Field Study on Noticing Interactivity of a Shop Window. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’12). ACM, New York, NY, USA, 297–306. DOI: http://dx.doi.org/10.1145/2207676.2207718 \ 28. Borna Noureddin, Peter D. Lawrence, and C.F. Man. 2005. A non-contact device for tracking gaze in a human computer interface. Computer Vision and Image Understanding 98, 1 (2005), 52 – 82. DOI: http://dx.doi.org/10.1016/j.cviu.2004.07.005 Special Issue on Eye Detection and Tracking. \ 29. Takehiko Ohno and Naoki Mukawa. 2004. A Free-head, Simple Calibration, Gaze Tracking System That Enables Gaze-based Interaction. In Proceedings of the 2004 Symposium on Eye Tracking Research & Applications (ETRA ’04). ACM, New York, NY, USA, 115–122. DOI: http://dx.doi.org/10.1145/968363.968387 \ 30. Constantin Schmidt, J¨org M¨uller, and Gilles Bailly. 2013. Screenﬁnity: Extending the Perception Area of Content on Very Large Public Displays. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’13). ACM, New York, NY, USA, 1719–1728. DOI: http://dx.doi.org/10.1145/2470654.2466227 \ 31. Julian Seifert, Sebastian Boring, Christian Winkler, Florian Schaub, Fabian Schwab, Steffen Herrdum, Fabian Maier, Daniel Mayer, and Enrico Rukzio. 2014. Hover Pad: Interacting with Autonomous and Self-actuated Displays in Space. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST ’14). ACM, New York, NY, USA, 139–147. DOI: http://dx.doi.org/10.1145/2642918.2647385 \ 32. Yusuke Sugano, Xucong Zhang, and Andreas Bulling. 2016. AggreGaze: Collective Estimation of Audience Attention on Public Displays. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST ’16). ACM, New York, NY, USA. DOI:http://dx.doi.org/10.1145/2984511.2984536 \ 33. Eduardo Velloso, Marcus Carter, Joshua Newn, Augusto Esteves, Christopher Clarke, and Hans Gellersen. 2017. Motion Correlation: Selecting Objects by Matching Their Movement. ACM Trans. Comput.-Hum. Interact. 24, 3, Article 22 (April 2017), 35 pages. DOI: http://dx.doi.org/10.1145/3064937 \ 34. M´elodie Vidal, Andreas Bulling, and Hans Gellersen. 2013. Pursuits: Spontaneous Interaction with Displays Based on Smooth Pursuit Eye Movement and Moving Targets. In Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp ’13). ACM, New York, NY, USA, 439–448. DOI:http://dx.doi.org/10.1145/2493432.2493477 \ 35. Robert Walter, Andreas Bulling, David Lindlbauer, Martin Schuessler, and J¨org M¨uller. 2015. Analyzing Visual Attention During Whole Body Interaction with Public Displays. In Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp ’15). ACM, New York, NY, USA, 1263–1267. DOI: http://dx.doi.org/10.1145/2750858.2804255 \ 36. Miaosen Wang, Sebastian Boring, and Saul Greenberg. 2012. Proxemic Peddler: A Public Advertising Display That Captures and Preserves the Attention of a Passerby. In Proceedings of the 2012 International Symposium on Pervasive Displays (PerDis ’12). ACM, New York, NY, USA, Article 3, 6 pages. DOI: http://dx.doi.org/10.1145/2307798.2307801 \ 37. Julie R. Williamson, Daniel Sund´en, and Jay Bradley. 2015a. GlobalFestival: Evaluating Real World Interaction on a Spherical Display. In Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp ’15). ACM, New York, NY, USA, 1251–1261. DOI: http://dx.doi.org/10.1145/2750858.2807518 \ 38. Julie R. Williamson, John Williamson, Daniel Sund´en, and Jay Bradley. 2015b. Multi-Player Gaming on Spherical Displays. In Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA ’15). ACM, New York, NY, USA, 355–358. DOI: http://dx.doi.org/10.1145/2702613.2725447 \ 39. Lawrence H Yu and Moshe Eizenman. 2004. A new methodology for determining point-of-gaze in head-mounted eye tracking systems. IEEE Transactions on Biomedical Engineering 51, 10 (2004), 1765–1773. \ 40. Yanxia Zhang, Andreas Bulling, and Hans Gellersen. 2014. Pupil-canthi-ratio: A Calibration-free Method for Tracking Horizontal Gaze Direction. In Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces (AVI ’14). ACM, New York, NY, USA, 129–132. DOI: http://dx.doi.org/10.1145/2598153.2598186 \ 41. Yanxia Zhang, MingKi Chong, J¨org M¨uller, Andreas Bulling, and Hans Gellersen. 2015. Eye tracking for public displays in the wild. Personal and Ubiquitous Computing 19, 5-6 (2015), 967–981. \ ",Gaze Estimation; Body Tracking; Gaze-enabled Displays,H.5.m,uistf3285-file1.zip,uistf3285-file2.jpg,uistf3285-file3.mp4,uistf3285-file4.zip,"EyeScout is an active eye tracking system that enables gaze interaction with large public displays when users  (1) interact from random positions, or (2) interact while walking. ","By unzipping this file, you will find: \ (1) a changelog with the detailed changes to the paper (the changes are already copied to the changelog) \ (2) a version of the paper in which the changes are highlighted in yellow","Instead of asking users to stand in front of the eye tracker, EyeScout moves the eye tracker to the user. EyeScout allows gaze-based interaction while walking and from random positions. ","The text below has been copied from the changelog: \  \ 1.1) Scalability: supporting a single user only (AC,R3) \ We modified the abstract, introduction, and system description as follows to clarify that  EyeScout supports a single user.  \  \ 1.2) Scalability: How EyeScout behaves with multiple users (AC,R2,R3) \ - Acknowledged the lack of multi-user support at the end of the limitations section.  \ - Moved the a sentence about multi-user support from the Upgrading EyeScout section in the discussion to the Limitations section. \ - Added a paragraph to the Limitations to describe how EyeScout currently behaves with multiple users.  \  \ 2.1) Distinguishing 3 targets (AC,R1) \ As recommended by R1, we added a subsection Mean Correlation Coefficient in page 6. \  \ 2.2)  Selection/Correlation time (AC,R3) \ - Added a sentence in Eye Tracking Module to compare the window size we used to previous work.  \ - Added a subsection about selection times in the results as requested by R3.  \ - Added a table for selection times (Table 2) \  \ 3.1) “Moving speed” in walk and interact experiment (AC)  \ Replaced all occurrences of ""moving speed"" by ""carriage speed"" \  \ 3.2) Maximum speed (R1,R3) \ - Added a paragraph in the Limitations section about EyeScout's performance when walking fast.  \  \ 3.3) Focus on an element while passing by (R1) \ - Added a sentence in the procedure description of experiment 2 as requested by R1 to clarify that content moved with users as typically done in previous work.  \  \ 4) Improvement and kick-off time (AC,R2) \ - Added a definition of kick-off time in the last paragraph of the introduction. \ - Added a sentence to the Body tracking module subsection to explain when the Kinect detects the user. \  \  \ 5) Other issues: \ - Fixed typos \ - Corrected the p-value in page 6 from 0.09 to 0.009 \ - Added a paragraph to ""62% – 87% Faster in Kickstarting Gaze-Interaction"" as recommended by R2 to clarify why this improvement can increase conversion rate.  \ - We referenced the related work suggested by R2 in page 7 and discussed it.  \ - Added in ""Study design and procedure"" that participants had to select 1 of 3 targets in each selection.",Mohamed Khamis,Florian Alt,FormatComplete,,,,,,,Aug 7 5:41,
uistf2241,10/23,4,Eye Tracking,2:30:00 PM,1:00:00 PM,4,2:50:00 PM,3:10:00 PM,long,long,uistf3285,2,402,,,uistf2241,A,SmoothMoves: Smooth Pursuits Head Movements for Augmented Reality,Augusto,Esteves,augustoeae@gmail.com,uistf2241-paper.pdf,12,letter,,,"Augusto Esteves, David Verweij, Liza Jahan Suraiya, MD. Rasel Islam, Youryang Lee, Ian Oakley","a.esteves@napier.ac.uk, davidverweij@gmail.com, suraiya_liza@yahoo.com, islam@unist.ac.kr, youryang.lee@whereveriam.org, ian.r.oakley@gmail.com",15473,Augusto,,Esteves,a.esteves@napier.ac.uk,Centre for Interaction Design,Edinburgh Napier University,Edinburgh,,United Kingdom,,,,,,64165,David,,Verweij,davidverweij@gmail.com,Department of Industrial Design,Eindhoven University of Technology,Eindhoven,,The Netherlands,Centre for Interaction Design,Edinburgh Napier University,Edinburgh,,United Kingdom,49405,Liza,Jahan,Suraiya,suraiya_liza@yahoo.com,Human and System Engineering,Ulsan National Institute of Science and Technology,Ulsan,Ulsan,Republic of South Korea,,,,,,45639,MD. Rasel,,Islam,islam@unist.ac.kr,Human and Systems Engineering,Ulsan National Institute of Science and Technology,Ulsan,Korea,Republic of,,,,,,63779,Youryang,,Lee,youryang.lee@whereveriam.org,Department of Human Factors Engineering,UNIST,Ulsan,,"Korea, Republic of",,,,,,1336,Ian,,Oakley,ian.r.oakley@gmail.com,Department of Human and Systems Engineering,Ulsan National Institute of Science and Technology,Ulsan,Korea,Republic of,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"SmoothMoves is an interaction technique for augmented reality (AR) based on smooth pursuits head movements. It works by computing correlations between the movements of on-screen targets and the user’s head while tracking those targets. The paper presents three studies. The first suggests that head based input can act as an easier and more affordable surrogate for eye-based input in many smooth pursuits interface designs. A follow-up study grounds the technique in the domain of augmented reality, and captures the error rates and acquisition times on different types of AR devices: head-mounted (2.6%, 1965ms) and hand-held (4.9%, 2089ms). Finally, the paper presents an interactive lighting system prototype that demonstrates the benefits of using smooth pursuits head movements in interaction with AR interfaces. A final qualitative study reports on positive feedback regarding the technique’s suitability for this scenario. Together, these results indicate show SmoothMoves is viable, efficient and immediately available for a wide range of wearable devices that feature embedded motion sensing.",augustoeae@gmail.com,"1. Emilio Bizzi. 2011. Eye-Head Coordination. In Comprehensive Physiology, Ronald Terjung (ed.). John Wiley & Sons, Inc., Hoboken, NJ, USA. Retrieved April 5, 2016 from http://doi.wiley.com/10.1002/cphy.cp010229 \ 2. Derya Ozcelik Buskermolen and Jacques Terken. 2012. Co-constructing Stories: A Participatory Design Technique to Elicit In-depth User Feedback and Suggestions About Design Concepts. In Proceedings of the 12th Participatory Design Conference: Exploratory Papers, Workshop Descriptions, Industry Cases Volume 2 (PDC ’12), 33–36. https://doi.org/10.1145/2348144.2348156 \ 3. Christopher Clarke, Alessio Bellino, Augusto Esteves, Eduardo Velloso, and Hans Gellersen. 2016. TraceMatch: a computer vision technique for user input by tracing of animated controls. In UbiComp ’16: Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing, 298–303. https://doi.org/10.1145/2971648.2971714 \ 4. Nicholas Cooper, Aaron Keatley, Maria Dahlquist, Simon Mann, Hannah Slay, Joanne Zucco, Ross Smith, and Bruce H. Thomas. 2004. Augmented Reality Chinese Checkers. In Proceedings of the 2004 ACM SIGCHI International Conference on Advances in Computer Entertainment Technology (ACE ’04), 117– 126. https://doi.org/10.1145/1067343.1067357 \ 5. Andrew Crossan, Mark McGill, Stephen Brewster, and Roderick Murray-Smith. 2009. Head Tilting for Interaction in Mobile Contexts. In Proceedings of the 11th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI ’09), 6:1–6:10. https://doi.org/10.1145/1613858.1613866 \ 6. Dietlind Helene Cymek, Antje Christine Venjakob, Stefan Ruff, Otto Hans-Martin Lutz, Simon Hofmann, and Matthias Roetting. Entering PIN codes by smooth pursuit eye movements. Journal of Eye Movement Research 7(4), 1: 1–11. \ 7. Murtaza Dhuliawala, Juyoung Lee, Junichi Shimizu, Andreas Bulling, Kai Kunze, Thad Starner, and Woontack Woo. 2016. Smooth Eye Movement Interaction Using EOG Glasses. In Proceedings of the 18th ACM International Conference on Multimodal Interaction (ICMI 2016), 307–311. https://doi.org/10.1145/2993148.2993181 \ 8. David Dobbelstein, Philipp Hock, and Enrico Rukzio. 2015. Belt: An Unobtrusive Touch Input Device for Head-worn Displays. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15), 2135–2138. https://doi.org/10.1145/2702123.2702450 \ 9. Augusto Esteves, Eduardo Velloso, Andreas Bulling, and Hans Gellersen. 2015. Orbits: Gaze Interaction for Smart Watches using Smooth Pursuit Eye Movements. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology, 457–466. https://doi.org/10.1145/2807442.2807499 \ 10. Karen M. Evans, Robert A. Jacobs, John A. Tarduno, and Jeff B. Pelz. 2012. Collecting and Analyzing EyeTracking Data in Outdoor Environments. Journal of Eye Movement Research 5, 2. https://doi.org/10.16910/jemr.5.2.6 \ 11. David G. Fleming, Gehard W. Vossius, George Bowman, and Ensign L. Johnson. 1969. Adaptive Properties Of The Eye-tracking System As Revealed By Moving-head And Open-loop Studies. Annals of the New York Academy of Sciences 156, 2 Rein Control,: 825–850. https://doi.org/10.1111/j.17496632.1969.tb14017.x \ 12. Yi-Ta Hsieh, Antti Jylhä, Valeria Orso, Luciano Gamberini, and Giulio Jacucci. 2016. Designing a Willing-to-Use-in-Public Hand Gestural Interaction Technique for Smart Glasses. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16), 4203–4215. https://doi.org/10.1145/2858036.2858436 \ 13. Aulikki Hyrskykari, Howell Istance, and Stephen Vickers. 2012. Gaze Gestures or Dwell-based Interaction? In Proceedings of the Symposium on Eye Tracking Research and Applications (ETRA ’12), 229– 232. https://doi.org/10.1145/2168556.2168602 \ 14. Jari Kangas, Oleg Špakov, Poika Isokoski, Deepak Akkil, Jussi Rantala, and Roope Raisamo. 2016. Feedback for Smooth Pursuit Gaze Tracking Based Control. In Proceedings of the 7th Augmented Human International Conference 2016 (AH ’16), 6:1–6:8. https://doi.org/10.1145/2875194.2875209 \ 15. Moritz Kassner, William Patera, and Andreas Bulling. 2014. Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile Gaze-based Interaction. In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication (UbiComp ’14 Adjunct), 1151– 1160. https://doi.org/10.1145/2638728.2641695 \ 16. Mohamed Khamis, Ozan Saltuk, Alina Hang, Katharina Stolz, Andreas Bulling, and Florian Alt. 2016. TextPursuits: Using Text for Pursuits-based Interaction and Calibration on Public Displays. In Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp ’16), 274–285. https://doi.org/10.1145/2971648.2971679 \ 17. Tiiu Koskela and Kaisa Väänänen-Vainio-Mattila. 2004. Evolution towards smart home environments: empirical evaluation of three user interfaces. Personal and Ubiquitous Computing 8, 3–4: 234–240. https://doi.org/10.1007/s00779-004-0283-x \ 18. Jeremy Lanman, Emilio Bizzi, and John Allum. 1978. The coordination of eye and head movement during smooth pursuit. Brain Research 153, 1: 39–53. https://doi.org/10.1016/0006-8993(78)91127-7 \ 19. R. John Leigh and David S. Zee M.D. 2015. The Neurology of Eye Movements. Oxford University Press. \ 20. Otto Hans-Martin Lutz, Antje Christine Venjakob, and Stefan Ruff. 2015. SMOOVS: Towards calibration-free text entry by gaze using smooth pursuit movements. Journal of Eye Movement Research 8(1), 2: 1–11. \ 21. Robert Mahony, Tarek Hamel, and Jean-Michel Pflimlin. 2008. Nonlinear Complementary Filters on the Special Orthogonal Group. IEEE Transactions on Automatic Control 53, 5: 1203–1218. https://doi.org/10.1109/TAC.2008.923738 \ 22. Microsoft. Microsoft HoloLens. Microsoft HoloLens. Retrieved September 20, 2016 from https://www.microsoft.com/microsoft-hololens/en-us \ 23. Pranav Mistry and Pattie Maes. 2009. SixthSense: A Wearable Gestural Interface. In ACM SIGGRAPH ASIA 2009 Sketches (SIGGRAPH ASIA ’09), 11:1– 11:1. https://doi.org/10.1145/1667146.1667160 \ 24. Louis-Philippe Morency and Trevor Darrell. 2006. Head Gesture Recognition in Intelligent Interfaces: The Role of Context in Improving Recognition. In Proceedings of the 11th International Conference on Intelligent User Interfaces (IUI ’06), 32–38. https://doi.org/10.1145/1111449.1111464 \ 25. Kasım Özacar, Juan David Hincapié-Ramos, Kazuki Takashima, and Yoshifumi Kitamura. 2017. 3D Selection Techniques for Mobile Augmented Reality Head-Mounted Displays. Interacting with Computers 29, 4: 579–591. https://doi.org/10.1093/iwc/iww035 \ 26. Ken Pfeuffer, Melodie Vidal, Jayson Turner, Andreas Bulling, and Hans Gellersen. 2013. Pursuit Calibration: Making Gaze Calibration Less Tedious and More Flexible. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST ’13), 261–270. https://doi.org/10.1145/2501988.2501998 \ 27. Christoph Rasche and Karl R. Gegenfurtner. 2009. Precision of speed discrimination and smooth pursuit eye movements. Vision Research 49, 5: 514–523. https://doi.org/10.1016/j.visres.2008.12.003 \ 28. Julie Rico and Stephen Brewster. 2010. Usable Gestures for Mobile Interfaces: Evaluating Social Acceptability. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’10), 887–896. https://doi.org/10.1145/1753326.1753458 \ 29. D A Robinson. 1965. The mechanics of human smooth pursuit eye movement. The Journal of Physiology 180, 3: 569–591. \ 30. Boris Smus and Christopher Riederer. 2015. Magnetic Input for Mobile Virtual Reality. In Proceedings of the 2015 ACM International Symposium on Wearable Computers (ISWC ’15), 43–44. https://doi.org/10.1145/2802083.2808395 \ 31. Sophie Stellmach and Raimund Dachselt. 2012. Look & Touch: Gaze-supported Target Acquisition. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’12), 2981–2990. https://doi.org/10.1145/2207676.2208709 \ 32. Noboru Sugie and Makoto Wakakuwa. 1970. Visual Target Tracking with Active Head Rotation. IEEE Transactions on Systems Science and Cybernetics 6, 2: 103–109. https://doi.org/10.1109/TSSC.1970.300283 \ 33. Gabriel Takacs, Vijay Chandrasekhar, Natasha Gelfand, Yingen Xiong, Wei-Chao Chen, Thanos Bismpigiannis, Radek Grzeszczuk, Kari Pulli, and Bernd Girod. 2008. Outdoors Augmented Reality on Mobile Phone Using Loxel-based Visual Feature Organization. In Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval (MIR ’08), 427–434. https://doi.org/10.1145/1460096.1460165 \ 34. Eduardo Velloso, Markus Wirth, Christian Weichel, Augusto Esteves, and Hans Gellersen. 2016. AmbiGaze: Direct Control of Ambient Devices by Gaze. In Proceedings of the 2016 ACM Conference on Designing Interactive Systems, 812–817. https://doi.org/10.1145/2901790.2901867 \ 35. Mélodie Vidal, Andreas Bulling, and Hans Gellersen. 2013. Pursuits: Spontaneous Interaction with Displays Based on Smooth Pursuit Eye Movement and Moving Targets. In Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp ’13), 439–448. https://doi.org/10.1145/2493432.2493477 \ 36. Chun Yu, Ke Sun, Mingyuan Zhong, Xincheng Li, Peijun Zhao, and Yuanchun Shi. 2016. OneDimensional Handwriting: Inputting Letters and Words on Smart Glasses. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16), 71–82. https://doi.org/10.1145/2858036.2858542 \ 37. ES | PRODUCTS. JINS MEME. Retrieved July 5, 2017 from https://jins-meme.com/en/products/es/ \ 38. Epson Moverio Next Generation Smart Eyewear Epson America, Inc. Retrieved September 20, 2016 from http://www.epson.com/cgibin/Store/jsp/Landing/moverio-augmented-realitysmart-eyewear-technology.do \ 39. Philips Hue. Retrieved September 21, 2016 from http://www2.meethue.com/en-gb/ \ 40. Unity - Game Engine. Unity. Retrieved April 4, 2017 from https://unity3d.com \ 41. Vuforia | Augmented Reality. Retrieved April 4, 2017 from https://www.vuforia.com/ \ ",Wearable computing; eye tracking; augmented reality; AR; input technique; smooth pursuits; motion matching; HMD,H.5.m,uistf2241-file1.docx,uistf2241-file2.jpg,,,An interactive lighting system prototype that uses AR for displaying moving controls in space. Users make selections by tracking these movements with their heads.,,SmoothMoves is an interaction technique for Augmented-Reality that relies on moving targets. Users select these by tracking their movements with their heads. Our talk describes three studies and a demo.,"We have addressed all the changes discussed in the rebuttal, including: \  \ - Integrating Dhuliawala et al. and Shimizu et al.'s articles (R2). \  \ - Running a 2-way RM ANOVA on each study variable (R1). \  \ - Updating Hincapié-Ramos et al. to Özacar et al. (R1). \  \ - Including more comparative data from prior work (AC, R3). \  \ All these changes are highlighted in yellow in the source document.",Augusto Esteves,David Verweij,FormatComplete,2017R1D1A1B03031364,"Ministry of Science, ICT and Future Planning",MSIP/IITP,R0190-15-2054,,,Aug 3 6:40,
uistf2925,10/23,4,Eye Tracking,2:30:00 PM,1:00:00 PM,4,3:10:00 PM,3:30:00 PM,long,long,uistf2241,3,403,,,uistf2925,A,MatchPoint: Spontaneous Spatial Coupling of Body Movement for Touchless Pointing,Christopher,Clarke,c.clarke1@lancaster.ac.uk,uistf2925-paper.pdf,14,letter,,,"Christopher Clarke, Hans Gellersen","c.clarke1@lancaster.ac.uk, hwg@comp.lancs.ac.uk",62347,Christopher,,Clarke,c.clarke1@lancaster.ac.uk,School of Computing and Communications,Lancaster University,Lancaster,Lancashire,United Kingdom,,,,,,5955,Hans,,Gellersen,hwg@comp.lancs.ac.uk,,Lancaster University,Lancaster,,United Kingdom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Pointing is a fundamental interaction technique where user movement is translated to spatial input on a display. Conventionally, this is based on a rigid configuration of a display coupled with a pointing device that determines the types of movement that can be sensed, and the specific ways users can affect pointer input. Spontaneous spatial coupling is a novel input technique that instead allows any body movement, or movement of tangible objects, to be appropriated for touchless pointing on an ad hoc basis. Pointer acquisition is facilitated by the display presenting graphical objects in motion, to which users can synchronise to define a temporary spatial coupling with the body part or tangible object they used in the process. The technique can be deployed using minimal hardware, as demonstrated by MatchPoint, a generic computer vision-based implementation of the technique that requires only a webcam. We explore the design space of spontaneous spatial coupling, demonstrate the versatility of the technique with application examples, and evaluate MatchPoint performance using a multi-directional pointing task.",c.clarke1@lancaster.ac.uk,"1. Johnny Accot and Shumin Zhai. 2002. More Than Dotting the I’s — Foundations for Crossing-based Interfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’02). ACM, New York, NY, USA, 73–80. DOI: http://dx.doi.org/10.1145/503376.503390 \ 2. Daniel Avrahami, Jacob O. Wobbrock, and Shahram Izadi. 2011. Portico: Tangible Interaction on and Around a Tablet. In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (UIST ’11). ACM, New York, NY, USA, 347–356. DOI: http://dx.doi.org/10.1145/2047196.2047241 \ 3. Boris Babenko, Ming-Hsuan Yang, and Serge Belongie. 2009. Visual tracking with online multiple instance learning. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 983–990. \ 4. Rafael Ballagas, Michael Rohs, and Jennifer G. Sheridan. 2005. Sweep and Point and Shoot: Phonecam-based Interactions for Large Public Displays. In CHI ’05 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’05). ACM, New York, NY, USA, 1200–1203. DOI: http://dx.doi.org/10.1145/1056808.1056876 \ 5. Amartya Banerjee, Jesse Burstyn, Audrey Girouard, and Roel Vertegaal. 2012. MultiPoint: Comparing Laser and Manual Pointing As Remote Input in Large Display Interactions. Int. J. Hum.-Comput. Stud. 70, 10 (Oct. 2012), 690–702. DOI: http://dx.doi.org/10.1016/j.ijhcs.2012.05.009 \ 6. Ana M Bernardos, David Gómez, and José R Casar. 2016. A Comparison of Head Pose and Deictic Pointing Interaction Methods for Smart Environments. International Journal of Human-Computer Interaction 32, 4 (2016), 325–351. \ 7. Renaud Blanch, Yves Guiard, and Michel Beaudouin-Lafon. 2004. Semantic Pointing: Improving Target Acquisition with Control-display Ratio Adaptation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’04). ACM, New York, NY, USA, 519–526. DOI: http://dx.doi.org/10.1145/985692.985758 \ 8. Florian Block, Michael Haller, Hans Gellersen, Carl Gutwin, and Mark Billinghurst. 2008. VoodooSketch: Extending Interactive Surfaces with Adaptable Interface Palettes. In Proceedings of the 2Nd International Conference on Tangible and Embedded Interaction (TEI ’08). ACM, New York, NY, USA, 55–58. DOI: http://dx.doi.org/10.1145/1347390.1347404 \ 9. Richard A. Bolt. 1980. “Put-that-there”: Voice and Gesture at the Graphics Interface. SIGGRAPH Comput. Graph. 14, 3 (July 1980), 262–270. DOI: http://dx.doi.org/10.1145/965105.807503 \ 10. Sebastian Boring, Dominikus Baur, Andreas Butz, Sean Gustafson, and Patrick Baudisch. 2010. Touch Projector: Mobile Interaction Through Video. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’10). ACM, New York, NY, USA, 2287–2296. DOI: http://dx.doi.org/10.1145/1753326.1753671 \ 11. Jean-Yves Bouguet. 2000. Pyramidal implementation of the Lucas Kanade feature tracker. Intel Corporation, Microprocessor Research Labs (2000). \ 12. Michelle A Brown, Wolfgang Stuerzlinger, and others. 2014. The performance of un-instrumented in-air pointing. In Proceedings of Graphics Interface 2014. Canadian Information Processing Society, 59–66. \ 13. Marcus Carter, Eduardo Velloso, John Downs, Abigail Sellen, Kenton O’Hara, and Frank Vetere. 2016. PathSync: Multi-User Gestural Interaction with Touchless Rhythmic Path Mimicry. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 13. DOI: http://dx.doi.org/10.1145/2858036.2858284 \ 14. Ming-yu Chen, Lily Mummert, Padmanabhan Pillai, Alexander Hauptmann, and Rahul Sukthankar. 2010. Controlling Your TV with Gestures. In Proceedings of the International Conference on Multimedia Information Retrieval (MIR ’10). ACM, New York, NY, USA, 405–408. DOI: http://dx.doi.org/10.1145/1743384.1743453 \ 15. Ngip-Khean Chuan and Ashok Sivaji. 2012. Combining eye gaze and hand tracking for pointer control in HCI: Developing a more robust and accurate interaction system for pointer positioning and clicking. In Humanities, Science and Engineering (CHUSER), 2012 IEEE Colloquium on. IEEE, 172–176. \ 16. Christopher Clarke, Alessio Bellino, Augusto Esteves, and Hans Gellersen. 2017. Remote Control by Body Movement in Synchrony with Orbiting Widgets: an Evaluation of TraceMatch. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1, 3, Article 45 (Sept. 2017), 22 pages. DOI:http://dx.doi.org/10.1145/3130910 \ 17. Christopher Clarke, Alessio Bellino, Augusto Esteves, Eduardo Velloso, and Hans Gellersen. 2016. TraceMatch: A Computer Vision Technique for User Input by Tracing of Animated Controls. In Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp ’16). ACM, New York, NY, USA, 298–303. DOI: http://dx.doi.org/10.1145/2971648.2971714 \ 18. Martin Danelljan, Fahad Shahbaz Khan, Michael Felsberg, and Joost Van de Weijer. 2014. Adaptive color attributes for real-time visual tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 1090–1097. \ 19. Connor Dickie, Jamie Hart, Roel Vertegaal, and Alex Eiser. 2006. LookPoint: An Evaluation of Eye Input for Hands-free Switching of Input Devices Between Multiple Computers. In Proceedings of the 18th Australia Conference on Computer-Human Interaction: Design: Activities, Artefacts and Environments (OZCHI ’06). ACM, New York, NY, USA, 119–126. DOI: http://dx.doi.org/10.1145/1228175.1228198 \ 20. Ashley Dover, G. Michael Poor, Darren Guinness, and Alvin Jude. 2016. Improving Gestural Interaction With Augmented Cursors. In Proceedings of the 2016 Symposium on Spatial User Interaction (SUI ’16). ACM, New York, NY, USA, 135–138. DOI: http://dx.doi.org/10.1145/2983310.2985765 \ 21. Scott Elrod, Richard Bruce, Rich Gold, David Goldberg, Frank Halasz, William Janssen, David Lee, Kim McCall, Elin Pedersen, Ken Pier, John Tang, and Brent Welch. 1992. Liveboard: A Large Interactive Display Supporting Group Meetings, Presentations, and Remote Collaboration. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’92). ACM, New York, NY, USA, 599–607. DOI: http://dx.doi.org/10.1145/142750.143052 \ 22. Augusto Esteves, Eduardo Velloso, Andreas Bulling, and Hans Gellersen. 2015. Orbits: Gaze Interaction for Smart Watches using Smooth Pursuit Eye Movements. In Proc. of the 28th ACM Symposium on User Interface Software and Technology (UIST 2015) (2015-11-01). DOI: http://dx.doi.org/10.1145/2807442.2807499 \ 23. Gunnar Farnebäck. 2003. Two-frame Motion Estimation Based on Polynomial Expansion. In Proceedings of the 13th Scandinavian Conference on Image Analysis (SCIA’03). Springer-Verlag, Berlin, Heidelberg, 363–370. http://dl.acm.org/citation.cfm?id=1763974.1764031 \ 24. Jean-Daniel Fekete, Niklas Elmqvist, and Yves Guiard. 2009. Motion-pointing: Target Selection Using Elliptical Motions. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’09). ACM, New York, NY, USA, 289–298. DOI: http://dx.doi.org/10.1145/1518701.1518748 \ 25. George W. Fitzmaurice, Hiroshi Ishii, and William A. S. Buxton. 1995. Bricks: Laying the Foundations for Graspable User Interfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’95). ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 442–449. DOI: http://dx.doi.org/10.1145/223904.223964 \ 26. Helmut Grabner, Michael Grabner, and Horst Bischof. 2006. Real-time tracking via on-line boosting.. In Bmvc, Vol. 1. 6. \ 27. Saul Greenberg and Michael Boyle. 2002. Customizable Physical Interfaces for Interacting with Conventional Applications. In Proceedings of the 15th Annual ACM Symposium on User Interface Software and Technology (UIST ’02). ACM, New York, NY, USA, 31–40. DOI: http://dx.doi.org/10.1145/571985.571991 \ 28. Yves Guiard. 2009. The Problem of Consistency in the Design of Fitts’ Law Experiments: Consider Either Target Distance and Width or Movement Form and Scale. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’09). ACM, New York, NY, USA, 1809–1818. DOI: http://dx.doi.org/10.1145/1518701.1518980 \ 29. Darren Guinness, Alvin Jude, G. Michael Poor, and Ashley Dover. 2015. Models for Rested Touchless Gestural Interaction. In Proceedings of the 3rd ACM Symposium on Spatial User Interaction (SUI ’15). ACM, New York, NY, USA, 34–43. DOI: http://dx.doi.org/10.1145/2788940.2788948 \ 30. Ken Hinckley, Randy Pausch, John C. Goble, and Neal F. Kassell. 1994. Passive Real-world Interface Props for Neurosurgical Visualization. In Conference Companion on Human Factors in Computing Systems (CHI ’94). ACM, New York, NY, USA, 232–. DOI: http://dx.doi.org/10.1145/259963.260443 \ 31. Inwook Hwang, Hyun-Cheol Kim, Jihun Cha, Chunghyun Ahn, Karam Kim, and Jong-Il Park. 2015. A gesture based tv control interface for visually impaired: Initial design and user study. In Frontiers of Computer Vision (FCV), 2015 21st Korea-Japan Joint Workshop on. IEEE, 1–5. \ 32. Rados Javanovic and Ian MacKenzie. 2010. MarkerMouse: mouse cursor control using a head-mounted marker. Computers Helping People with Special Needs (2010), 49–56. \ 33. Soonmook Jeong, Jungdong Jin, Taehoun Song, Keyho Kwon, and Jae Wook Jeon. 2012. Single-camera dedicated television control system using gesture drawing. IEEE Transactions on Consumer Electronics 58, 4 (2012), 1129–1137. \ 34. Alvin Jude, G. Michael Poor, and Darren Guinness. 2014. Personal Space: User Deﬁned Gesture Space for GUI Interaction. In CHI ’14 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’14). ACM, New York, NY, USA, 1615–1620. DOI: http://dx.doi.org/10.1145/2559206.2581242 \ 35. Zdenek Kalal, Krystian Mikolajczyk, and Jiri Matas. 2010. Forward-backward error: Automatic detection of tracking failures. In Pattern recognition (ICPR), 2010 20th international conference on. IEEE, 2756–2759. \ 36. Zdenek Kalal, Krystian Mikolajczyk, and Jiri Matas. 2012. Tracking-learning-detection. IEEE transactions on pattern analysis and machine intelligence 34, 7 (2012), 1409–1422. \ 37. Shaun K. Kane, Daniel Avrahami, Jacob O. Wobbrock, Beverly Harrison, Adam D. Rea, Matthai Philipose, and Anthony LaMarca. 2009. Bonﬁre: A Nomadic System for Hybrid Laptop-tabletop Interaction. In Proceedings of the 22Nd Annual ACM Symposium on User Interface Software and Technology (UIST ’09). ACM, New York, NY, USA, 129–138. DOI: http://dx.doi.org/10.1145/1622176.1622202 \ 38. Rick Kjeldsen. 2006. Improvements in Vision-based Pointer Control. In Proceedings of the 8th International ACM SIGACCESS Conference on Computers and Accessibility (Assets ’06). ACM, New York, NY, USA, 189–196. DOI: http://dx.doi.org/10.1145/1168987.1169020 \ 39. Rick Kjeldsen and Jacob Hartman. 2001. Design Issues for Vision-based Computer Interaction Systems. In Proceedings of the 2001 Workshop on Perceptive User Interfaces (PUI ’01). ACM, New York, NY, USA, 1–8. DOI:http://dx.doi.org/10.1145/971478.971511 \ 40. Tiiu Koskela and Kaisa Väänänen-Vainio-Mattila. 2004. Evolution towards smart home environments: empirical evaluation of three user interfaces. Personal and Ubiquitous Computing 8, 3-4 (2004), 234–240. DOI: http://dx.doi.org/10.1007/s00779-004-0283-x \ 41. Georgios Kouroupetroglou, Alexandros Pino, Athanasios Balmpakakis, Dimitrios Chalastanis, Vasileios Golematis, Nikolaos Ioannou, and Ioannis Koutsoumpas. 2011. Using Wiimote for 2D and 3D pointing tasks: gesture performance evaluation. In International Gesture Workshop. Springer, 13–23. \ 42. Chiuhsiang Joe Lin, Sui-Hua Ho, and Yan-Jyun Chen. 2015. An investigation of pointing postures in a 3D stereoscopic environment. Applied ergonomics 48 (2015), 154–163. \ 43. Bruce D. Lucas and Takeo Kanade. 1981. An iterative image registration technique with an application to stereo vision. In In IJCAI81. 674–679. \ 44. I. Scott MacKenzie, Tatu Kauppinen, and Miika Silfverberg. 2001. Accuracy Measures for Evaluating Computer Pointing Devices. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’01). ACM, New York, NY, USA, 9–16. DOI:http://dx.doi.org/10.1145/365024.365028 \ 45. Rainer Malkewitz. 1998. Head Pointing and Speech Control As a Hands-free Interface to Desktop Computing. In Proceedings of the Third International ACM Conference on Assistive Technologies (Assets ’98). ACM, New York, NY, USA, 182–188. DOI: http://dx.doi.org/10.1145/274497.274531 \ 46. Kenton O’Hara, Gerardo Gonzalez, Abigail Sellen, Graeme Penney, Andreas Varnavas, Helena Mentis, Antonio Criminisi, Robert Corish, Mark Rounceﬁeld, Neville Dastur, and Tom Carrell. 2014. Touchless Interaction in Surgery. Commun. ACM 57, 1 (Jan. 2014), 70–77. DOI:http://dx.doi.org/10.1145/2541883.2541899 \ 47. Ken Pfeuffer, Mélodie Vidal, Jayson Turner, Andreas Bulling, and Hans Gellersen. 2013. Pursuit Calibration: Making Gaze Calibration Less Tedious and More Flexible. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST ’13). ACM, New York, NY, USA, 261–270. DOI: http://dx.doi.org/10.1145/2501988.2501998 \ 48. Alexandros Pino, Evangelos Tzemis, Nikolaos Ioannou, and Georgios Kouroupetroglou. 2013. Using kinect for 2D and 3D pointing tasks: performance evaluation. In International Conference on Human-Computer Interaction. Springer, 358–367. \ 49. Edward Rosten and Tom Drummond. 2006. Machine learning for high-speed corner detection. In European Conference on Computer Vision, Vol. 1. 430–443. DOI: http://dx.doi.org/10.1007/11744023_34 \ 50. Lawrence Sambrooks and Brett Wilkinson. 2013. Comparison of Gestural, Touch, and Mouse Interaction with Fitts’ Law. In Proceedings of the 25th Australian Computer-Human Interaction Conference: Augmentation, Application, Innovation, Collaboration (OzCHI ’13). ACM, New York, NY, USA, 119–122. DOI: http://dx.doi.org/10.1145/2541016.2541066 \ 51. Matthias Schwaller and Denis Lalanne. 2013. Pointing in the air: measuring the effect of hand selection strategies on performance and effort. In Human Factors in Computing and Informatics. Springer, 732–747. \ 52. Garth Shoemaker, Anthony Tang, and Kellogg S. Booth. 2007. Shadow Reaching: A New Perspective on Interaction for Large Displays. In Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology (UIST ’07). ACM, New York, NY, USA, 53–56. DOI:http://dx.doi.org/10.1145/1294211.1294221 \ 53. R. William Soukoreff and I. Scott MacKenzie. 2004. Towards a Standard for Pointing Device Evaluation, Perspectives on 27 Years of Fitts’ Law Research in HCI. Int. J. Hum.-Comput. Stud. 61, 6 (Dec. 2004), 751–789. DOI:http://dx.doi.org/10.1016/j.ijhcs.2004.09.001 \ 54. Rainer Stiefelhagen and Jie Zhu. 2002. Head orientation and gaze direction in meetings. In CHI’02 Extended Abstracts on Human Factors in Computing Systems. ACM, 858–859. \ 55. Radu-Daniel Vatavu. 2012. Point & Click Mediated Interactions for Large Home Entertainment Displays. Multimedia Tools Appl. 59, 1 (July 2012), 113–128. DOI: http://dx.doi.org/10.1007/s11042-010-0698-5 \ 56. Eduardo Velloso, Jason Alexander, Andreas Bulling, and Hans Gellersen. 2015. Interactions Under the Desk: A Characterisation of Foot Movements for Input in a Seated Position. Springer International Publishing, Cham, 384–401. DOI: http://dx.doi.org/10.1007/978-3-319-22701-6_29 \ 57. Eduardo Velloso, Marcus Carter, Joshua Newn, Augusto Esteves, Christopher Clarke, and Hans Gellersen. 2017. Motion Correlation: Selecting Objects by Matching Their Movement. ACM Trans. Comput.-Hum. Interact. 24, 3, Article 22 (April 2017), 35 pages. DOI: http://dx.doi.org/10.1145/3064937 \ 58. Eduardo Velloso, Dominik Schmidt, Jason Alexander, Hans Gellersen, and Andreas Bulling. 2015. The Feet in Human–Computer Interaction: A Survey of Foot-Based Interaction. ACM Comput. Surv. 48, 2, Article 21 (Sept. 2015), 35 pages. DOI:http://dx.doi.org/10.1145/2816455 \ 59. Mélodie Vidal, Andreas Bulling, and Hans Gellersen. 2013. Pursuits: Spontaneous Interaction with Displays Based on Smooth Pursuit Eye Movement and Moving Targets. In Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp ’13). ACM, New York, NY, USA, 439–448. DOI:http://dx.doi.org/10.1145/2493432.2493477 \ 60. Nicolas Villar and Hans Gellersen. 2007. A Malleable Control Structure for Softwired User Interfaces. In Proceedings of the 1st International Conference on Tangible and Embedded Interaction (TEI ’07). ACM, New York, NY, USA, 49–56. DOI: http://dx.doi.org/10.1145/1226969.1226980 \ 61. Daniel Vogel and Ravin Balakrishnan. 2005. Distant Freehand Pointing and Clicking on Very Large, High Resolution Displays. In Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology (UIST ’05). ACM, New York, NY, USA, 33–42. DOI:http://dx.doi.org/10.1145/1095034.1095041 \ 62. Edwin Walsh, Walter Daems, and Jan Steckel. 2015. An optical head-pose tracking sensor for pointing devices using IR-LED based markers and a low-cost camera. In SENSORS, 2015 IEEE. IEEE, 1–4. \ 63. John Williamson and Roderick Murray-Smith. 2004. Pointing Without a Pointer. In CHI ’04 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’04). ACM, New York, NY, USA, 1407–1410. DOI: http://dx.doi.org/10.1145/985921.986076 \ 64. Jacob O. Wobbrock, Kristen Shinohara, and Alex Jansen. 2011. The Effects of Task Dimensionality, Endpoint Deviation, Throughput Calculation, and Experiment Design on Pointing Measures and Models. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’11). ACM, New York, NY, USA, 1639–1648. DOI: http://dx.doi.org/10.1145/1978942.1979181 \ 65. Xing-Dong Yang, Khalad Hasan, Neil Bruce, and Pourang Irani. 2013. Surround-see: Enabling Peripheral Vision on Smartphones During Active Use. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST ’13). ACM, New York, NY, USA, 291–300. DOI: http://dx.doi.org/10.1145/2501988.2502049 \ ",User input; Input techniques; Motion-matching; Pointing; Gesture input; Touchless input; Bodily interaction; Vison-based interfaces; Computer vision,H.5.m,uistf2925-file1.zip,,uistf2925-file3.mp4,,,,"We present a novel input technique that allows any body movement, or tangible object, to be appropriated for touchless pointing on an ad hoc basis.","In addition to minor tweaks and tightening up of text we have made the following changes: \  \ Page 3 - Removed the bullet points describing the sub-sections (that were present before the ""Control-Display Gain"" sub-section) to make additional room. \  \ Page 3 - Changed spotantaneous spatial coupling section (after the five properties) to highlight that any trajectory can be used for motion matching. \  \ Page 3 - New sub-section on pointer termination.  \  \ Page 6 - Changed backgrounds of figures 3 and 6. \  \ Page 9 - Included sentence on false activations in results section (last sentence before study discussion). \  \ Page 9 - Included plot for movement time and reformatted plots. \  \ Page 10 - Reworded discussion and conclusion to better reflect the contributions. \  \ Page 10 - Extended discussion on limitations of MatchPoint to include the difficulties faced when multiple simultaneous motions from two users are detected for the same Orbit.",Christopher Clarke,Hans Gellersen,FormatComplete,,,,,,,Aug 7 6:12,
uistf2052,10/23,4,Eye Tracking,2:30:00 PM,1:00:00 PM,4,3:30:00 PM,3:50:00 PM,long,long,uistf2925,4,404,Honorable Mention,,uistf2052,A,Everyday Eye Contact Detection Using Unsupervised Gaze Target Discovery,Xucong,Zhang,xczhang@mpi-inf.mpg.de,uistf2052-paper.pdf,11,letter,,,"Xucong Zhang, Yusuke Sugano, Andreas Bulling","xczhang@mpi-inf.mpg.de, sugano@ist.osaka-u.ac.jp, bulling@mpi-inf.mpg.de",60314,Xucong,,Zhang,xczhang@mpi-inf.mpg.de,Max Planck Institute for Informatics,Saarland Informatics Campus,Saarbrücken,Saarland,Germany,,,,,,28124,Yusuke,,Sugano,sugano@ist.osaka-u.ac.jp,Graduate School of Information Science and Technology,Osaka University,Osaka,,Japan,,,,,,11540,Andreas,,Bulling,bulling@mpi-inf.mpg.de,Max Planck Institute for Informatics,Saarland Informatics Campus,Saarbrücken,,Germany,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Eye contact is an important non-verbal cue in social signal processing and promising as a measure of overt attention in human-object interactions and attentive user interfaces. However, robust detection of eye contact across different users, gaze targets, camera positions, and illumination conditions is notoriously challenging. We present a novel method for eye contact detection that combines a state-of-the-art appearance-based gaze estimator with a novel approach for unsupervised gaze target discovery, i.e. without the need for tedious and time-consuming manual data annotation. We evaluate our method in two real-world scenarios: detecting eye contact at the workplace, including on the main work display, from cameras mounted to target objects, as well as during everyday social interactions with the wearer of a head-mounted egocentric camera. We empirically evaluate the performance of our method in both scenarios and demonstrate its effectiveness for detecting eye contact independent of target object type and size, camera position, and user and recording environment.",xczhang@mpi-inf.mpg.de,"1. Michael F Land and Mary Hayhoe. In what ways do eye movements contribute to everyday activities? Vision research, 41(25):3559–3565, 2001. \ 2. Chris L Kleinke. Gaze and eye contact: a research review. Psychological bulletin, 100(1):78, 1986. \ 3. Jeffrey S Shell, Ted Selker, and Roel Vertegaal. Interacting with groups of computers. Communications of the ACM, 46(3):40–46, 2003. \ 4. Zhefan Ye, Yin Li, Alireza Fathi, Yi Han, Agata Rozga, Gregory D Abowd, and James M Rehg. Detecting eye contact using wearable eye-tracking glasses. In Proceedings of the 2012 ACM conference on ubiquitous computing, pages 699–704. ACM, 2012. \ 5. Connor Dickie, Roel Vertegaal, David Fono, Changuk Sohn, Daniel Chen, Daniel Cheng, Jeffrey S Shell, and Omar Aoudeh. Augmenting and sharing memory with eyeblog. In Proceedings of the the 1st ACM workshop on Continuous archival and retrieval of personal experiences, pages 105–109. ACM, 2004. \ 6. Michita Imai, Tetsuo Ono, and Hiroshi Ishiguro. Physical relation and expression: Joint attention for human-robot interaction. IEEE Transactions on Industrial Electronics, 50(4):636–643, 2003. \ 7. Jeffrey S Shell, Roel Vertegaal, Daniel Cheng, Alexander W Skaburskis, Changuk Sohn, A James Stewart, Omar Aoudeh, and Connor Dickie. Ecsglasses and eyepliances: using attention to open sociable windows of interaction. In Proceedings of the 2004 symposium on Eye tracking research & applications, pages 93–100. ACM, 2004. \ 8. Yanxia Zhang, Ming Ki Chong, Jörg Müller, Andreas Bulling, and Hans Gellersen. Eye tracking for public displays in the wild. Personal and Ubiquitous Computing, 19(5-6):967–981, 2015. \ 9. Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas Bulling. It’s written all over your face: Full-face appearance-based gaze estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2017. \ 10. Yusuke Sugano, Xucong Zhang, and Andreas Bulling. Aggregaze: Collective estimation of audience attention on public displays. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, pages 821–831. ACM, 2016. \ 11. Jeffrey S Shell, Roel Vertegaal, and Alexander W Skaburskis. Eyepliances: attention-seeking devices that respond to visual attention. In CHI’03 extended abstracts on Human factors in computing systems, pages 770–771. ACM, 2003. \ 12. Roel Vertegaal, Connor Dickie, Changuk Sohn, and Myron Flickner. Designing attentive cell phone using wearable eyecontact sensors. In CHI’02 extended abstracts on Human factors in computing systems, pages 646–647. ACM, 2002. \ 13. Sarah R Edmunds, Agata Rozga, Yin Li, Elizabeth A Karp, Lisa V Ibanez, James M Rehg, and Wendy L Stone. Brief report: Using a point-of-view camera to measure eye gaze in young children with autism spectrum disorder during naturalistic social interactions: A pilot study. Journal of Autism and Developmental Disorders, pages 1–7, 2017. \ 14. Brian A Smith, Qi Yin, Steven K Feiner, and Shree K Nayar. Gaze locking: passive eye contact detection for human-object interaction. In Proceedings of the 26th annual ACM symposium on User interface software and technology, pages 271–280. ACM, 2013. \ 15. Zhefan Ye, Yin Li, Yun Liu, Chanel Bridges, Agata Rozga, and James M Rehg. Detecting bids for eye contact using a wearable camera. In Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on, volume 1, pages 1–8. IEEE, 2015. \ 16. Antje Nuthmann and John M Henderson. Object-based attentional selection in scene viewing. Journal of vision, 10(8):20–20, 2010. \ 17. Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas Bulling. Appearance-based gaze estimation in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4511–4520, 2015. \ 18. Paul P Maglio, Rob Barrett, Christopher S Campbell, and Ted Selker. Suitor: An attentive information system. In Proceedings of the 5th international conference on Intelligent user interfaces, pages 169–176. ACM, 2000. \ 19. Paul P Maglio, Teenie Matlock, Christopher S Campbell, Shumin Zhai, and Barton A Smith. Gaze and speech in attentive user interfaces. In Advances in Multimodal Interfacesâ ˘AˇTICMI 2000, pages 1–7. Springer, 2000. \ 20. Alice Oh, Harold Fox, Max Van Kleek, Aaron Adler, Krzysztof Gajos, Louis-Philippe Morency, and Trevor Darrell. Evaluating look-to-talk: a gaze-aware interface in a collaborative environment. In CHI’02 Extended Abstracts on Human Factors in Computing Systems, pages 650–651. ACM, 2002. \ 21. Eric Horvitz, Carl Kadie, Tim Paek, and David Hovel. Models of attention in computing and communication: from principles to applications. Communications of the ACM, 46(3):52–59, 2003. \ 22. Roel Vertegaal and Jeffrey S Shell. Attentive user interfaces: the surveillance and sousveillance of gaze-aware objects. Social Science Information, 47(3):275–298, 2008. \ 23. Frederik Brudy, David Ledo, Saul Greenberg, and Andreas Butz. Is anyone looking? mitigating shoulder surﬁng on public displays through awareness and protection. In Proceedings of The International Symposium on Pervasive Displays, page 1. ACM, 2014. \ 24. Kevin Smith, Sileye O Ba, Jean-Marc Odobez, and Daniel Gatica-Perez. Tracking the visual focus of attention for a varying number of wandering people. IEEE transactions on pattern analysis and machine intelligence, 30(7):1212–1229, 2008. \ 25. Dan Witzner Hansen and Qiang Ji. In the eye of the beholder: A survey of models for eyes and gaze. IEEE transactions on pattern analysis and machine intelligence, 32(3):478–500, 2010. \ 26. Carlos Hitoshi Morimoto, Arnon Amir, and Myron Flickner. Detecting eye position and gaze from a single camera and 2 light sources. In Pattern Recognition, 2002. Proceedings. 16th International Conference on, volume 4, pages 314–317. IEEE, 2002. \ 27. Zhiwei Zhu and Qiang Ji. Eye gaze tracking under natural head movements. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 918–923. IEEE, 2005. \ 28. Zhiwei Zhu, Qiang Ji, and Kristin P Bennett. Nonlinear eye gaze mapping function estimation via support vector regression. In Pattern Recognition, 2006. ICPR 2006. 18th International Conference on, volume 1, pages 1132–1135. IEEE, 2006. \ 29. Dan Witzner Hansen and Arthur EC Pece. Eye tracking in the wild. Computer Vision and Image Understanding, 98(1):155–181, 2005. \ 30. Feng Lu, Yusuke Sugano, Takahiro Okabe, and Yoichi Sato. Inferring human gaze from appearance via adaptive linear regression. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 153–160. IEEE, 2011. \ 31. Erroll Wood and Andreas Bulling. Eyetab: Model-based gaze estimation on unmodiﬁed tablet computers. In Proceedings of the Symposium on Eye Tracking Research and Applications, pages 207–210. ACM, 2014. \ 32. Jixu Chen, Qiang Ji, et al. A probabilistic approach to online eye gaze tracking without personal calibration. IEEE Transactions on Image Processing, 2014. \ 33. Yusuke Sugano, Yasuyuki Matsushita, and Yoichi Sato. Appearance-based gaze estimation using visual saliency. IEEE transactions on pattern analysis and machine intelligence, 35(2):329–341, 2013. \ 34. Michael Xuelin Huang, Tiffany CK Kwok, Grace Ngai, Stephen CF Chan, and Hong Va Leong. Building a personalized, auto-calibrating eye tracker from user interactions. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pages 5169–5179. ACM, 2016. \ 35. Yusuke Sugano, Yasuyuki Matsushita, Yoichi Sato, and Hideki Koike. Appearance-based gaze estimation with online calibration from mouse operations. IEEE Transactions on Human-Machine Systems, 45(6):750–760, 2015. \ 36. Ben Benfold and Ian Reid. Unsupervised learning of a scene-speciﬁc coarse gaze estimator. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 2344–2351. IEEE, 2011. \ 37. Isarun Chamveha, Yusuke Sugano, Daisuke Sugimura, Teera Siriteerakul, Takahiro Okabe, Yoichi Sato, and Akihiro Sugimoto. Head direction estimation from low resolution images with scene adaptation. Computer Vision and Image Understanding, 117(10):1502–1511, 2013. \ 38. Stefan Duffner and Christophe Garcia. Visual focus of attention estimation with unsupervised incremental learning. IEEE Transactions on Circuits and Systems for Video Technology, 26(12):2264–2272, 2016. \ 39. Ted Selker, Andrea Lockerd, and Jorge Martinez. Eye-r, a glasses-mounted eye motion detection interface. In CHI’01 extended abstracts on Human factors in computing systems, pages 179–180. ACM, 2001. \ 40. Connor Dickie, Roel Vertegaal, Jeffrey S Shell, Changuk Sohn, Daniel Cheng, and Omar Aoudeh. Eye contact sensing glasses for attention-sensitive wearable video blogging. In CHI’04 extended abstracts on Human factors in computing systems, pages 769–770. ACM, 2004. \ 41. John D Smith, Roel Vertegaal, and Changuk Sohn. Viewpointer: lightweight calibration-free eye tracking for ubiquitous handsfree deixis. In Proceedings of the 18th annual ACM symposium on User interface software and technology, pages 53–61. ACM, 2005. \ 42. Adria Recasens, Aditya Khosla, Carl Vondrick, and Antonio Torralba. Where are they looking? In Advances in Neural Information Processing Systems, pages 199–207, 2015. \ 43. Adrià Recasens, Carl Vondrick, Aditya Khosla, and Antonio Torralba. Following gaze across views. arXiv preprint arXiv:1612.03094, 2016. \ 44. Kenneth Alberto Funes Mora, Florent Monay, and Jean-Marc Odobez. Eyediap: A database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras. In Proceedings of the Symposium on Eye Tracking Research and Applications, pages 255–258. ACM, 2014. \ 45. Davis E. King. Dlib-ml: A machine learning toolkit. Journal of Machine Learning Research, 10:1755–1758, 2009. \ 46. Tadas Baltrušaitis, Peter Robinson, and Louis-Philippe Morency. Openface: an open source facial behavior analysis toolkit. In Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on, pages 1–10. IEEE, 2016. \ 47. Mihael Ankerst, Markus M Breunig, Hans-Peter Kriegel, and Jörg Sander. Optics: ordering points to identify the clustering structure. In ACM Sigmod record, volume 28, pages 49–60. ACM, 1999. \ 48. Petros Xanthopoulos and Talayeh Razzaghi. A weighted support vector machine method for control chart pattern recognition. Computers & Industrial Engineering, 70:134–149, 2014. \ 49. Rainer Stiefelhagen, Jie Yang, and Alex Waibel. Modeling focus of attention for meeting indexing based on multiple cues. IEEE Transactions on Neural Networks, 13(4):928–938, 2002. \ 50. Michael Voit and Rainer Stiefelhagen. Deducing the visual focus of attention from head pose estimation in dynamic multi-view meeting scenarios. In Proceedings of the 10th international conference on Multimodal interfaces, pages 173–180. ACM, 2008. \ 51. ByungIn Yoo, Jae-Joon Han, Changkyu Choi, Kwonju Yi, Sungjoo Suh, Dusik Park, and Changyeong Kim. 3d user interface combining gaze and hand gestures for large-scale display. In CHI’10 Extended Abstracts on Human Factors in Computing Systems, pages 3709–3714. ACM, 2010. \ ",Eye Contact; Appearance-Based Gaze Estimation; Attentive User Interfaces; Social Signal Processing,H.5.m.,uistf2052-file1.zip,,uistf2052-file3.mp4,,,,"We present a novel method for eye contact detection with a webcam, which combines appearance-based gaze estimator with a novel unsupervised gaze target discovery without manual data annotation.",We added discussion and clarified the points raised by reviewers and AC. We modified the sentence in the introduction section suggested by AC.,Xucong Zhang,Yusuke Sugano,FormatComplete,,EXC-MMCI at Saarland University,JST CREST,JPMJCR14E1,,,Jul 30 8:55,
uistf3412,10/23,5,Physical Interfaces,4:20:00 PM,5:40:00 PM,4,4:20:00 PM,4:40:00 PM,long,long,none,1,501,,,uistf3412,A,A Modular Smartphone for Lending,Teddy,Seyed,teddy.seyed@ucalgary.ca,uistf3412-paper.pdf,11,letter,,,"Teddy Seyed, Xing-Dong Yang, Daniel Vogel","teddy.seyed@ucalgary.ca, xing-dong.yang@dartmouth.edu, dvogel@uwaterloo.ca",28271,Teddy,,Seyed,teddy.seyed@ucalgary.ca,Department of Computer Science,University of Calgary,Calgary,Alberta,Canada,,,,,,10966,Xing-Dong,,Yang,xing-dong.yang@dartmouth.edu,Department of Computer Science,Dartmouth College,Hanover,New Hampshire,United States,,,,,,3433,Daniel,,Vogel,dvogel@uwaterloo.ca,School of Computer Science,University of Waterloo,Waterloo,Ontario,Canada,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We motivate, design, and prototype a modular smartphone designed to make temporary device lending trustworthy and convenient. The concept is that the phone can be separated into pieces, so a child, friend, or even stranger can begin an access-controlled interaction with one piece, while the own-er retains another piece to continue their tasks and monitor activity. This is grounded in a survey capturing attitudes towards device lending, and an exploratory study probing how people might lend pieces of different kinds of modular smartphones. Design considerations are generated for a hardware form factor and software interface to support different lending scenarios. A functional prototype combining three smartphones into a single modular device is described and used to demonstrate a lending interaction design. A usability test validates the concept using the prototype.",teddy.seyed@ucalgary.ca,"1. Susanne Bødker and Ellen Christiansen. 2012. Poetry in motion: appropriation of the world of apps. In Proceedings of the 30th European Conference on Cognitive Ergonomics, 78–84. https://doi.org/10.1145/2448136.2448152 \ 2. Andrew Bragdon, Rob DeLine, Ken Hinckley, and Meredith Ringel Morris. 2011. Code space: touch + air gesture hybrid interactions for supporting developer meetings. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces, 212–221. \ 3. A.J. Bernheim Brush and Kori M Inkpen. 2007. Yours, mine and ours? Sharing and use of technology in domestic environments. In International Conference on Ubiquitous Computing, 109–126. \ 4. Raimund Dachselt and Robert Buchholz. 2009. Natural throw and tilt interaction between mobile phones and distant displays. In CHI ’09 Extended Abstracts on Human Factors in Computing Systems, 3253–3258. \ 5. David Pierce. 2016. Project Ara Lives: Google’s Modular Phone Is Ready for You Now. WIRED. Retrieved September 4, 2016 from https://www.wired.com/2016/05/project-ara-livesgoogles-modular-phone-is-ready/ \ 6. Devindra Hardawar. 2016. The Moto Z and Z Force are Motorola’s new modular flagships. Engadget. Retrieved August 24, 2016 from https://www.engadget.com/2016/06/09/moto-z-z-force-mods/ \ 7. Serge Egelman, A.J. Bernheim Brush, and Kori M. Inkpen. 2008. Family Accounts: A New Paradigm for User Accounts Within the Home Environment. In Proceedings of the 2008 ACM Conference on Computer Supported Cooperative Work (CSCW ’08), 669–678. https://doi.org/10.1145/1460563.1460666 \ 8. Alina Hang, Emanuel Von Zezschwitz, Alexander De Luca, and Heinrich Hussmann. 2012. Too much information!: user attitudes towards smartphone sharing. In Proceedings of the 7th Nordic Conference on HumanComputer Interaction: Making Sense Through Design, 284–287. \ 9. Eiji Hayashi, Oriana Riva, Karin Strauss, A.J. Brush, and Stuart Schechter. 2012. Goldilocks and the two mobile devices: going beyond all-or-nothing access to a device’s applications. In Proceedings of the Eighth Symposium on Usable Privacy and Security, 2. \ 10. Ken Hinckley, Morgan Dixon, Raman Sarin, François Guimbretière, and Ravin Balakrishnan. 2009. Codex: a dual screen tablet computer. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’09), 1933–1942. https://doi.org/10.1145/1518701.1518996 \ 11. Amy K. Karlson, A.J. Bernheim Brush, and Stuart Schechter. 2009. Can I borrow your phone?: understanding concerns when sharing mobile phones. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 1647–1650. https://doi.org/10.1145/1518701.1518953 \ 12. Yunxin Liu, Ahmad Rahmati, Yuanhe Huang, Hyukjae Jang, Lin Zhong, Yongguang Zhang, and Shensheng Zhang. 2009. xShare: supporting impromptu sharing of mobile phones. In Proceedings of the 7th international conference on Mobile systems, applications, and services, 15–28. \ 13. Tara Matthews, Kerwell Liao, Anna Turner, Marianne Berkovich, Robert Reeder, and Sunny Consolvo. 2016. “She’ll just grab any device that’s closer”: A Study of Everyday Device & Account Sharing in Households. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, 5921–5932. https://doi.org/10.1145/2858036.2858051 \ 14. Arion McNicoll. 2013. Phonebloks: The smartphone for the rest of your life. CNN. Retrieved April 4, 2017 from http://www.cnn.com/2013/09/19/tech/innovation/phonebloks-the-smartphone-for-life/index.html \ 15. David Merrill, Jeevan Kalanithi, and Pattie Maes. 2007. Siftables: towards sensor network user interfaces. In Proceedings of the 1st international conference on Tangible and embedded interaction, 75–78. https://doi.org/10.1145/1226969.1226984 \ 16. Hendrik Müller, Jennifer Gove, and John Webb. 2012. Understanding tablet use: a multi-method exploration. In Proceedings of the 14th international conference on Human-computer interaction with mobile devices and services, 1–10. https://doi.org/10.1145/2371574.2371576 \ 17. Laura L Murphy and Alexandra E Priebe. 2011. “My co-wife can borrow my mobile phone!” Gendered Geographies of Cell Phone Usage and Significance for Rural Kenyans. Gender, Technology and Development 15, 1: 1–23. \ 18. Jeni Paay, Dimitrios Raptis, Jesper Kjeldskov, Mikael B. Skov, Eric V. Ruder, and Bjarke M. Lauridsen. 2017. Investigating Cross-Device Interaction between a Handheld Device and a Large Display. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, 6608–6619. \ 19. Julian Seifert, Alexander De Luca, and Bettina Conradi. 2009. A Context-sensitive Security Model for Privacy Protection on Mobile Phones. In Proceedings of the 11th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI ’09), 68:1–68:2. https://doi.org/10.1145/1613858.1613940 \ 20. Julian Seifert, Alexander De Luca, Bettina Conradi, and Heinrich Hussmann. 2010. TreasurePhone: Contextsensitive User Data Protection on Mobile Phones. In Proceedings of the 8th International Conference on Pervasive Computing (Pervasive’10), 130–137. https://doi.org/10.1007/978-3-642-12654-3_8 \ 21. Teddy Seyed, Xing-Dong Yang, and Daniel Vogel. 2016. Doppio: A Reconfigurable Dual-Face Smartwatch for Tangible Interaction. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, 4675–4686. \ 22. Molly Steenson and Jonathan Donner. 2009. Beyond the personal and private: Modes of mobile phone sharing in urban India. The reconstruction of space and time: Mobile communication practices 1: 231–250. \ 23. Emma Tucker. 2015. Blocks launches “world’s first modular smartwatch.” Dezeen. Retrieved August 25, 2016 from https://www.dezeen.com/2015/10/27/blocksmodular-smartwatch-kickstarter/ \ 24. Chris Velazco. 2016. LG’s modular G5 is its most daring flagship phone ever. Engadget. Retrieved September 14, 2016 from https://www.engadget.com/2016/02/21/lg-g5-modular-official/ \ 25. Andrew Webster. 2014. Nex Band is a smart, modular charm bracelet for gaming on your wrist. The Verge. Retrieved October 2, 2016 from http://www.theverge.com/2014/2/13/5289404/nex-band-is-a-smartmodular-charm-bracelet \ 26. Merriam-Webster Dictionary. Retrieved March 4, 2017 from https://www.merriam-webster.com/ \ ",Smartphone; Lending smartphone; Modular smartphone,"H.5.2. Information Interfaces (e.g., HCI): Input devices",uistf3412-file1.docx,uistf3412-file2.jpg,uistf3412-file3.mp4,,"Modular smartphone for lending: (a) owner wishes to lend phone; (b) module slid out and configured for limited access; (b) handed to a child or to a friend, while keeping their phone.",,"We describe our concept modular smartphone that can be separated into pieces. It is motivated and designed to make temporary device lending trustworthy and convenient for family, friends, and strangers.","Revison Log \  \ Note the paper is now 9 pages due to the larger Figure 5 (Page 67) and extra content requested by reviewers. If the increased page length is a problem, Figure 5 could be moved into an appendix page at the end of the paper (though, this will make reading it a little less optimal).  \  \ HARDWARE PROTOTYPE \  \ In section “Lendable Smartphone” (first paragraph), we explained that it really wasn’t possible to do the interaction design without a working prototype (even the act of making the prototype helped us think through the interaction design) and it was critical to make the idea “real” for study 3 and for demonstrations (like in the video and live demos).  \  \ In section “Lendable Smartphone” (first paragraph), we briefly mention that making our fully-functional prototype was non-trivial and took several iterations of case designs, devices, software, and sensors. \  \ In the Introduction section (last paragraph), clarified our contribution is really the modular lendable device concept (motivated by study 1), the general modular form factor for lending (explored in study 2), and the interaction design (described on p6 to p8 and tested in study 3).   \  \ Expanded Limitations section about how a more ideal thin and light hardware design could be achieved if commercialized (for example, the AC suggests how a modular piece could be a thin-client display). \  \ RELATED WORK \  \ We revised the final section  to highlight key studies in this area of multi-device interaction and co-located social interaction.  \  \ STUDY 1 \  \ In the discussion of study 1, we clarified that  study 1 validates previous work and extends those results to current smartphone lending and gathered initial impressions about lending a future modular phone.  \  \ USAGE SCENARIOS \  \ In the Interaction Design section (Lending Scenarios subsection), we  clarified our  goal was to accommodate broad types of lending inspired by the conventional account sharing scenarios identified by Matthews et al. (e.g. borrowing, mutual use, setup, helping, …). We state that lending to occupy a child is likely one of the “killer apps”.  \ In the Interaction Design section (Lending Modes subsection), we briefly discuss  potential killer apps using a value prop lens for the lending modes, as well as brief discussion on the design implications if  a specific lending scenario was the sole focus. \  \ COST BENEFIT \  \ Added an  additional paragraph in Study 3 for a richer discussion based on feedback from participants.  \  \ OTHER MINOR REVISIONS \  \ Added details explaining methodology behind studies 1 and 3. \  \ Made Figure 5 full page so it is larger and easier to read with larger screenshots.  \  \ Fixed  various typos.",Teddy Seyed,Xing-Dong Yang,FormatComplete,402467,NSERC,NSF,1657141,,,Aug 8 23:34,
uistf1785,10/23,5,Physical Interfaces,4:20:00 PM,5:40:00 PM,4,4:40:00 PM,5:00:00 PM,long,long,uistf3412,2,502,,,uistf1785,A,You as a Puppet: Evaluation of Telepresence User Interface for Puppetry,Mose,Sakashita,sakasita86@gmail.com,uistf1785-paper.pdf,12,letter,,,"Mose Sakashita, Tatsuya Minagawa, Amy Koike, Ippei Suzuki, Keisuke Kawahara, Yoichi Ochiai","sakasita86@gmail.com, minatatu0901.happy@gmail.com, amy23kik@gmail.com, 1heisuzuki@gmail.com, kawahara@ai.iit.tsukuba.ac.jp, wizard@slis.tsukuba.ac.jp",53299,Mose,,Sakashita,sakasita86@gmail.com,Digital Nature Group,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,62149,Tatsuya,,Minagawa,minatatu0901.happy@gmail.com,Digital Nature Group,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,54819,Amy,,Koike,amy23kik@gmail.com,Digital Nature Group,University of TSUKUBA,Tsukuba,Ibaraki,Japan,,,,,,54822,Ippei,,Suzuki,1heisuzuki@gmail.com,Digital Nature Group,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,55236,Keisuke,,Kawahara,kawahara@ai.iit.tsukuba.ac.jp,Digital Nature Group,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,53649,Yoichi,,Ochiai,wizard@slis.tsukuba.ac.jp,Digital Nature Group,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose an immersive telepresence system for puppetry that transmits a human performer's body and facial movements into a puppet with audiovisual feedback to the performer. The cameras carried in place of puppet's eyes stream live video to the HMD worn by the performer, so that performers can see the images from the puppet's eyes with their own eyes and have a visual understanding of the puppet's ambience. In conventional methods to manipulate a puppet (a hand-puppet, a string-puppet, and a rod-puppet), there is a need to practice manipulating puppets, and there is difficulty carrying out interactions with the audience. Moreover, puppeteers must be positioned exactly where the puppet is. The proposed system addresses these issues by enabling a human performer to manipulate the puppet remotely using his or her body and facial movements. We conducted several user studies with both beginners and professional puppeteers. The results show that, unlike the conventional method, the proposed system facilitates the manipulation of puppets especially for beginners. Moreover, this system allows performers to enjoy puppetry and fascinate audiences.",sakasita86@gmail.com,"1. E. Blumenthal. 2005. Puppetry and Puppets: An Illustrated World Survey. Thames & Hudson. https://books.google.co.uk/books?id=VkV- QgAACAAJ \ 2. Leonardo Bonanni, Cati Vaucelle, Jeff Lieberman, and Orit Zuckerman. 2006. PlayPals: Tangible Interfaces for Remote Communication and Play. In CHI ’06 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’06). ACM, New York, NY, USA, 574–579. DOI: http://dx.doi.org/10.1145/1125451.1125572 \ 3. Matthew Brand. 1999. Voice Puppetry. In Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH ’99). ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 21–28. DOI: http://dx.doi.org/10.1145/311535.311537 \ 4. L. Fritsche, F. Unverzagt, J. Peters, and R. Calandra. 2015. First-Person Tele-Operation of a Humanoid Robot. In 15th IEEE-RAS International Conference on Humanoid Robots. 997–1002. http://www.ausy.tu-darmstadt.de/ uploads/Site/EditPublication/Fritsche_Humanoids15.pdf \ 5. J.C. Garlen and A.M. Graham. 2009. Kermit Culture: Critical Perspectives on Jim Henson’s Muppets. McFarland, Incorporated, Publishers. https://books.google.co.jp/books?id=Rac9xis0BIgC \ 6. A. Gruebler and K. Suzuki. 2010. Measurement of distal EMG signals using a wearable device for reading facial expressions. In 2010 Annual International Conference of the IEEE Engineering in Medicine and Biology. 4594–4597. DOI: http://dx.doi.org/10.1109/IEMBS.2010.5626504 \ 7. Robert Held, Ankit Gupta, Brian Curless, and Maneesh Agrawala. 2012. 3D Puppetry: A Kinect-based Interface for 3D Animation. In Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology (UIST ’12). ACM, New York, NY, USA, 423–434. DOI: http://dx.doi.org/10.1145/2380116.2380170 \ 8. Tamara Hunt and Nancy Renfro. 1982. Puppetry in early childhood education. N. Renfro Studios. \ 9. Keisuke Kawahara, Mose Sakashita, Amy Koike, Ippei Suzuki, Kenta Suzuki, and Yoichi Ochiai. 2016. Transformed Human Presence for Puppetry. In Proceedings of the 13th International Conference on Advances in Computer Entertainment Technology (ACE2016). ACM, New York, NY, USA, Article 38, 6 pages. DOI:http://dx.doi.org/10.1145/3001773.3001813 \ 10. Joseph J. LaViola, Jr. 2000. A Discussion of Cybersickness in Virtual Environments. SIGCHI Bull. 32, 1 (Jan. 2000), 47–56. DOI: http://dx.doi.org/10.1145/333329.333344 \ 11. Jun Ki Lee, R. L. Toscano, W. D. Stiehl, and C. Breazeal. 2008. The design of a semi-autonomous robot avatar for family communication and education. In RO-MAN 2008 - The 17th IEEE International Symposium on Robot and Human Interactive Communication. 166–173. DOI: http://dx.doi.org/10.1109/ROMAN.2008.4600661 \ 12. Luís Leite and Veronica Orvalho. 2012. Shape Your Body: Control a Virtual Silhouette Using Body Motion. In CHI ’12 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’12). ACM, New York, NY, USA, 1913–1918. DOI: http://dx.doi.org/10.1145/2212776.2223728 \ 13. Luis LEite and Veronica Orvalho. 2017. Mani-Pull-Action: Hand-based Digital Puppetry. Proc. ACM Hum.-Comput. Interact. 1, 1, Article 2 (June 2017), 16 pages. DOI:http://dx.doi.org/10.1145/3095804 \ 14. B. Y. L. Li, A. S. Mian, W. Liu, and A. Krishna. 2013. Using Kinect for face recognition under varying poses, expressions, illumination and disguise. In Applications of Computer Vision (WACV), 2013 IEEE Workshop on. 186–192. DOI: http://dx.doi.org/10.1109/WACV.2013.6475017 \ 15. H. Li, P. Roivainen, and R. Forcheimer. 1993. 3DD Motion Estimation in Model-Based Facial Image Coding. \ IEEE Trans. Pattern Anal. Mach. Intell. 15, 6 (June 1993), 545–555. DOI:http://dx.doi.org/10.1109/34.216724 \ 16. Hao Li, Laura Trutoiu, Kyle Olszewski, Lingyu Wei, Tristan Trutna, Pei-Lun Hsieh, Aaron Nicholls, and Chongyang Ma. 2015. Facial Performance Sensing Head-mounted Display. ACM Trans. Graph. 34, 4, Article 47 (July 2015), 9 pages. DOI: http://dx.doi.org/10.1145/2766939 \ 17. Peter Lincoln, Greg Welch, Andrew Nashel, Adrian Ilie, Andrei State, and Henry Fuchs. 2009. Animatronic Shader Lamps Avatars. In Proceedings of the 2009 8th IEEE International Symposium on Mixed and Augmented Reality (ISMAR ’09). IEEE Computer Society, Washington, DC, USA, 27–33. DOI: http://dx.doi.org/10.1109/ISMAR.2009.5336503 \ 18. Katsutoshi Masai, Yuta Sugiura, Masa Ogata, Katsuhiro Suzuki, Fumihiko Nakamura, Sho Shimamura, Kai Kunze, Masahiko Inami, and Maki Sugimoto. 2015. AffectiveWear: Toward Recognizing Facial Expression. In ACM SIGGRAPH 2015 Posters (SIGGRAPH ’15). ACM, New York, NY, USA, Article 16, 1 pages. DOI: http://dx.doi.org/10.1145/2787626.2792632 \ 19. Marvin Minsky. 1980. Telepresence. OMNI magazin, June 1980. (1980). \ 20. Kana Misawa and Jun Rekimoto. 2015. ChameleonMask: Embodied Physical and Social Telepresence Using Human Surrogates. In Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA ’15). ACM, New York, NY, USA, 401–411. DOI: http://dx.doi.org/10.1145/2702613.2732506 \ 21. Shohei Nagai, Shunichi Kasahara, and Jun Rekimoto. 2015. LiveSphere: Sharing the Surrounding Visual Environment for Immersive Experience in Remote Collaboration. In Proceedings of the Ninth International Conference on Tangible, Embedded, and Embodied Interaction (TEI ’15). ACM, New York, NY, USA, 113–116. DOI: http://dx.doi.org/10.1145/2677199.2680549 \ 22. Oculus.inc. 2014. Oculus Rift Develop kit 2 (last accessed August 27, 2016). (2014). https://www.oculus.com/en- us/dk2/ \ 23. E.M.B.J. O’Hare. 2005. Puppetry in Education and Therapy: Unlocking Doors to the Mind and Heart. AuthorHouse. https://books.google.co.jp/books?id=7X0CodZfjrcC \ 24. Masaki Oshita, Yuta Senju, and Syun Morishige. 2013. Character Motion Control Interface with Hand Manipulation Inspired by Puppet Mechanism. In Proceedings of the 12th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry (VRCAI ’13). ACM, New York, NY, USA, 131–138. DOI: http://dx.doi.org/10.1145/2534329.2534360 \ 25. Emil Polyak. 2012. Virtual Impersonation Using Interactive Glove Puppets. In SIGGRAPH Asia 2012 Posters (SA ’12). ACM, New York, NY, USA, Article 31, 1 pages. DOI:http://dx.doi.org/10.1145/2407156.2407191 \ 26. Daisuke Sakamoto, Takayuki Kanda, Tetsuo Ono, Hiroshi Ishiguro, and Norihiro Hagita. 2007. Android As a Telecommunication Medium with a Human-like Presence. In Proceedings of the ACM/IEEE International Conference on Human-robot Interaction (HRI ’07). ACM, New York, NY, USA, 193–200. DOI: http://dx.doi.org/10.1145/1228716.1228743 \ 27. Mose Sakashita, Keisuke Kawahara, Amy Koike, Kenta Suzuki, Ippei Suzuki, and Yoichi Ochiai. 2016. Yadori: Mask-type User Interface for Manipulation of Puppets. In ACM SIGGRAPH 2016 Emerging Technologies (SIGGRAPH ’16). ACM, New York, NY, USA, Article 23, 1 pages. DOI: http://dx.doi.org/10.1145/2929464.2929478 \ 28. C.E. Schaefer and L.J. Carey. 1994. Family Play Therapy. J. Aronson. \ https://books.google.co.jp/books?id=kElcV84iH- 4C \ 29. Dairoku Sekiguchi, Masahiko Inami, and Susumu Tachi. 2001. RobotPHONE: RUI for interpersonal communication. In CHI’01 Extended Abstracts on Human Factors in Computing Systems. ACM, 277–278. \ 30. Rika Shoji, Toshiki Yoshiike, Yuya Kikukawa, Tadahiro Nishikawa, Taigetsu Saori, Suketomo Ayaka, Tetsuaki Baba, and Kumiko Kushiyama. 2012. Mimicat: Face Input Interface Supporting Animatronics Costume Performer’s Facial Expression. In ACM SIGGRAPH 2012 Posters (SIGGRAPH ’12). ACM, New York, NY, USA, Article 72, 1 pages. DOI: http://dx.doi.org/10.1145/2342896.2342983 \ 31. Ronit Slyper, Guy Hoffman, and Ariel Shamir. 2015. Mirror Puppeteering: Animating Toy Robots in Front of a Webcam. In Proceedings of the Ninth International Conference on Tangible, Embedded, and Embodied Interaction (TEI ’15). ACM, New York, NY, USA, 241–248. DOI: http://dx.doi.org/10.1145/2677199.2680548 \ 32. Ronit Slyper, Jill Lehman, Jodi Forlizzi, and Jessica Hodgins. 2011. A Tongue Input Device for Creating Conversations. In Proceedings of the 24th annual ACM Symposium on User Interface Software and Technology (UIST ’11). ACM, 117–126. \ 33. Yuta Sugiura, Calista Lee, Masayasu Ogata, Anusha Withana, Yasutoshi Makino, Daisuke Sakamoto, Masahiko Inami, and Takeo Igarashi. 2012. PINOKY: A Ring That Animates Your Plush Toys. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’12). ACM, New York, NY, USA, 725–734. DOI:http://dx.doi.org/10.1145/2207676.2207780 \ 34. Thibaut Weise, Hao Li, Luc Van Gool, and Mark Pauly. 2009. Face/Off: Live Facial Puppetry. In Proceedings of \ the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA ’09). ACM, New York, NY, USA, 7–16. DOI: http://dx.doi.org/10.1145/1599470.1599472 \ 35. John P Whitney, Tianyao Chen, John Mars, and Jessica K Hodgins. 2016. A hybrid hydrostatic transmission and human-safe haptic telepresence robot. In Robotics and Automation (ICRA), 2016 IEEE International Conference on. IEEE, 690–695. \ 36. Tomoko Yonezawa and Kenji Mase. 2002. Musically Expressive Doll in Face-to-Face Communication. In Proceedings of the 4th IEEE International Conference on Multimodal Interfaces (ICMI ’02). IEEE Computer Society, Washington, DC, USA, 417–. DOI: http://dx.doi.org/10.1109/ICMI.2002.1167031",Animatronics; puppet; telepresence; head-mounted display;,H.5.2,uistf1785-file1.zip,uistf1785-file2.jpg,uistf1785-file3.mp4,,Telepresence system for puppetry that transmits a human performer's body and facial movements into a puppet with audiovisual feedback to the performer.,,We propose an immersive telepresence system for puppetry that transmits a human performer's body and facial movements into a puppet with audiovisual feedback to the performer. ,"Main changes \ - Modification of typos and english grammatical errors \ - Modification of wrong numbers in the figures and sentences \ - Change of colors in the figures (fig 9, 10, 11, and 13) \ - Capitalization of the subtitles \ ",Mose Sakashita,Tatsuya Minagawa,FormatComplete,,FUJITSU SOCIAL SCIENCE LABORATORY LIMITED,,,,,Aug 10 8:46,
uistf4432,10/23,5,Physical Interfaces,4:20:00 PM,5:40:00 PM,4,5:00:00 PM,5:20:00 PM,long,long,uistf1785,3,503,,,uistf4432,A,Reinventing the Wheel: Transforming Steering Wheel Systems for Autonomous Vehicles,Brian,Mok,brianmok@stanford.edu,uistf4432-paper.pdf,13,letter,,,"Brian Mok, Mishel Johns, Stephen Yang, Wendy Ju","brianmok@stanford.edu, mishel@stanford.edu, syang0@stanford.edu, wendyju@stanford.edu",44451,Brian,,Mok,brianmok@stanford.edu,Center for design research,Stanford,Stanford,California,United States,,,,,,47269,Mishel,,Johns,mishel@stanford.edu,Center for Design Research,Stanford University,Stanford,CA,USA,,,,,,43183,Stephen,,Yang,syang0@stanford.edu,Center of Design Research,Stanford University,Stanford,California,United States,,,,,,1438,Wendy,,Ju,wendyju@stanford.edu,,Stanford University,Palo Alto,CA,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this paper, we introduce two different transforming steering wheel systems that can be utilized to augment user experience for future partially autonomous and fully autonomous vehicles. The first one is a robotic steering wheel that can mechanically transform by using its actuators to move the various components into different positions. The second system is a LED steering wheel that can visually transform by using LEDs embedded along the rim of wheel to change colors. Both steering wheel systems contain onboard microcontrollers developed to interface with our driving simulator. The main function of these two systems is to provide emergency warnings to drivers in a variety of safety critical scenarios, although the design space that we propose for these steering wheel systems also includes the use as interactive user interfaces.  \  \ To evaluate the effectiveness of the emergency alerts, we conducted a driving simulator study examining the performance of participants (N=56) after an abrupt loss of autonomous vehicle control. Drivers who experienced the robotic steering wheel performed significantly better than those who experienced the LED steering wheel. The results of this study suggest that alerts utilizing mechanical movement are more effective than purely visual warnings. \ ",brianmok@stanford.edu,"1. David A. Abbink, Mark Mulder, and Erwin R. Boer. 2012. Haptic shared control: smoothly shifting control authority? Cognition, Technology & Work 14, 1: 19– 28. https://doi.org/10.1007/s10111-011-0192-5 \ 2. Dillis V. Allen. 1994. Steering wheel assembly with communication keyboard. Google Patents. Retrieved from https://www.google.com/patents/US5319803 \ 3. Avinash Balachandran and J. Christian Gerdes. 2015. Designing Steering Feel for Steer-by-Wire Vehicles Using Objective Measures. IEEE/ASME Transactions on Mechatronics 20, 1: 373–383. https://doi.org/10.1109/TMECH.2014.2324593 \ 4. Frank Beruscha, Klaus Augsburg, and Dietrich Manstetten. 2011. Haptic warning signals at the steering wheel: A literature survey regarding lane departure warning systems. Retrieved from https://digital.lib.washington.edu/researchworks/handle /1773/34898 \ 5. Arie P. van den Beukel and Mascha C. van der Voort. 2013. The influence of time-criticality on Situation Awareness when retrieving human control after automated driving. In 16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013), 2000–2005. https://doi.org/10.1109/ITSC.2013.6728523 \ 6. BMW Group. The Next 100 Years - Brand Visions. Retrieved from https://www.bmwgroup.com/en/next100/brandvisions. html \ 7. Avinoam Borowsky and Tal Oron-Gilad. 2016. The effects of automation failure and secondary task on drivers’ ability to mitigate hazards in highly or semiautomated vehicles. Advances in Transportation Studies, 1. \ 8. Karel A. Brookhuis and Dick de Waard. 2010. Monitoring drivers’ mental workload in driving simulators using physiological measures. Accident Analysis & Prevention 42, 3: 898–903. https://doi.org/10.1016/j.aap.2009.06.001 \ 9. Marcelo Coelho, Hiroshi Ishii, and Pattie Maes. 2008. Surflex: a programmable surface for the design of tangible interfaces. In CHI’08 extended abstracts on Human factors in computing systems, 3429–3434. http://dl.acm.org/citation.cfm?id=1358869 \ 10. Marcelo Coelho and Jamie Zigelbaum. 2011. Shapechanging interfaces. Personal and Ubiquitous Computing 15, 2: 161–173. \ 11. Daniel Damböck and Klaus Bengler. Übernahmezeiten beim hochautomatisierten Fahren. Retrieved from http://www.ftm.mw.tum.de/uploads/media/24_Dambo eck.pdf \ 12. Tanja Döring, Dagmar Kern, Paul Marshall, Max Pfeiffer, Johannes Schöning, Volker Gruhn, and Albrecht Schmidt. 2011. Gestural Interaction on the Steering Wheel: Reducing the Visual Demand. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’11), 483–492. https://doi.org/10.1145/1978942.1979010 \ 13. H. O. Duncan. 1927. The world on wheels, by H. O. Duncan. \ 14. Enes Selman Ege, Furkan Cetin, and Cagatay Basdogan. 2011. Vibrotactile feedback in steering wheel reduces navigation errors during GPS-guided car driving. In World Haptics Conference (WHC), 2011 IEEE, 345–348. http://ieeexplore.ieee.org/abstract/document/5945510/ \ 15. Gregory M. Fitch, Jonathan M. Hankey, Brian M. Kleiner, and Thomas A. Dingus. 2011. Driver comprehension of multiple haptic seat alerts intended for use in an integrated collision avoidance system. Transportation Research Part F: Traffic Psychology and Behaviour 14, 4: 278–290. https://doi.org/10.1016/j.trf.2011.02.001 \ 16. Sean Follmer, Daniel Leithinger, Alex Olwal, Nadia Cheng, and Hiroshi Ishii. 2012. Jamming user interfaces: programmable particle stiffness and sensing for malleable and shape-changing devices. In Proceedings of the 25th annual ACM symposium on User interface software and technology, 519–528. http://dl.acm.org/citation.cfm?id=2380181 \ 17. Sean Follmer, Daniel Leithinger, Alex Olwal, Akimitsu Hogge, and Hiroshi Ishii. 2013. inFORM: dynamic physical affordances and constraints through shape and object actuation. In Uist, 417–426. http://dl.acm.org/citation.cfm?doid=2501988.2502032 \ 18. Terrence Fong and Charles Thorpe. 2001. Vehicle Teleoperation Interfaces. Autonomous Robots 11, 1: 9– 18. https://doi.org/10.1023/A:1011295826834 \ 19. Terrence W. Fong, Francois Conti, Sébastien Grange, and Charles Baur. 2001. Novel interfaces for remote driving: gesture, haptic, and PDA. In Intelligent Systems and Smart Manufacturing, 300–311. http://proceedings.spiedigitallibrary.org/data/Conferen ces/SPIEP/39298/300_1.pdf \ 20. Christian Gold, Daniel Damböck, Lutz Lorenz, and Klaus Bengler. 2013. “Take over!” How long does it take to get the driver back into the loop? In Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 1938–1942. http://pro.sagepub.com/content/57/1/1938.short \ 21. Sungjae Hwang and Jung-hee Ryu. 2010. The Haptic steering Wheel: Vibro-tactile based navigation for the driving environment. In Pervasive Computing and Communications Workshops (PERCOM Workshops), 2010 8th IEEE International Conference on, 660–665. http://ieeexplore.ieee.org/abstract/document/5470517/ \ 22. Wijnand A. IJsselsteijn, Huib de Ridder, Jonathan Freeman, and Steve E. Avons. 2000. Presence: concept, determinants, and measurement. 520–529. https://doi.org/10.1117/12.387188 \ 23. International Organization for Standards. ISO 22324:2015(en), Societal security — Emergency management — Guidelines for colour-coded alerts. Retrieved from https://www.iso.org/obp/ui/#iso:std:50061:en \ 24. Toshio Ito, Arata Takata, and Kenta Oosawa. 2016. Time Required for Take-over from Automated to Manual Driving. https://doi.org/10.4271/2016-01-0158 \ 25. Dagmar Kern, Paul Marshall, Eva Hornecker, Yvonne Rogers, and Albrecht Schmidt. 2009. Enhancing navigation information with tactile output embedded into the steering wheel. Pervasive Computing: 42–58. \ 26. Miltos Kyriakidis, Joost C. F. de Winter, Neville Stanton, Thierry Bellet, Bart van Arem, Karel Brookhuis, Marieke Martens, Klaus Bengler, Jan Andersson, Natasha Merat, Nick Reed, Matt Flament, Marjan Hagenzieker, and Riender Happee. 2017. A human factors perspective on automated driving. Theoretical Issues in Ergonomics Science: 1–27. https://doi.org/10.1080/1463922X.2017.1293187 \ 27. John E. Lahiff. 1997. Vehicle information display on steering wheel surface. Google Patents. Retrieved from https://www.google.com/patents/US5691695 \ 28. David Lakatos and Hiroshi Ishii. 2012. Towards Radical Atoms—Form-giving to transformable materials. In Cognitive Infocommunications (CogInfoCom), 2012 IEEE 3rd International Conference on, 37–40. http://ieeexplore.ieee.org/abstract/document/6422023/ \ 29. Tin Lun Lam, Huihuan Qian, and Yangsheng Xu. 2010. Omnidirectional Steering Interface and Control for a Four-Wheel Independent Steering Vehicle. IEEE/ASME Transactions on Mechatronics 15, 3: 329–338. https://doi.org/10.1109/TMECH.2009.2024938 \ 30. Vivien Melcher, Stefan Rauh, Frederik Diederichs, Harald Widlroither, and Wilhelm Bauer. 2015. TakeOver Requests for Automated Driving. Procedia Manufacturing 3: 2867–2873. https://doi.org/10.1016/j.promfg.2015.07.788 \ 31. Mercedes-Benz USA. The Mercedes-Benz F 015 Luxury in Motion. Retrieved July 16, 2017 from https://www.mbusa.com/vcm/MB/DigitalAssets/About Us/PressReleases/F_015_Luxury_in_Motion_enus.pdf \ 32. Georg Michelitsch, Jason Williams, Martin Osen, Beatriz Jimenez, and Stefan Rapp. 2004. Haptic chameleon: a new concept of shape-changing user interface controls with force feedback. In CHI’04 extended abstracts on Human factors in computing systems, 1305–1308. http://dl.acm.org/citation.cfm?id=986050 \ 33. David Miller, Annabel Sun, Mishel Johns, Hillary Page Ive, David Sirkin, Sudipto Aich, and Wendy Ju. 2015. Distraction Becomes Engagement in Automated Driving. In Proceedings of the Human Factors and Ergonomics Society Annual Meeting, October 2015. \ 34. Brian Mok, Mishel Johns, Key Jung Lee, Hillary Page Ive, David Miller, and Wendy Ju. 2015. Timing of unstructured transitions of control in automated driving. In 2015 IEEE Intelligent Vehicles Symposium (IV), 1167–1172. https://doi.org/10.1109/IVS.2015.7225841 \ 35. Brian Mok, Mishel Johns, Key Jung Lee, David Miller, David Sirkin, Page Ive, and Wendy Ju. 2015. Emergency, Automation Off: Unstructured Transition Timing for Distracted Drivers of Automated Vehicles. In Intelligent Transportation Systems (ITSC), 2015 IEEE 18th International Conference on, 2458–2464. http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7 \ 36. Brian Mok, Mishel Johns, David Miller, and Wendy Ju. 2017. Tunneled In: Drivers with Active Secondary Tasks Need More Time to Transition from Automation. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, 2840–2844. http://dl.acm.org/citation.cfm?id=3025713 \ 37. Brian Mok, Mishel Johns, Stephen Yang, and Wendy Ju. 2017. Actions Speak Louder: Effects of a Transforming Steering Wheel on Post-Transition Driver Performance. In IEEE 20th International Conference on Intelligent Transportation Systems. \ 38. MOTOR1. Audi James 2025 virtual cockpit of the future. Retrieved from https://www.youtube.com/watch?v=LBiVk31rvjA \ 39. Mark Mulder, David A. Abbink, and Erwin R. Boer. 2008. The effect of haptic guidance on curve negotiation behavior of young, experienced drivers. In Systems, Man and Cybernetics, 2008. SMC 2008. IEEE International Conference on, 804–809. http://ieeexplore.ieee.org/abstract/document/4811377/ \ 40. Yoshitoshi Murata and Kazuhiro Yoshida. Automobile Driving Interface Using Gesture Operations for Disabled People. Retrieved April 4, 2017 from http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1 .1.679.2206 \ 41. Nissan USA. Nissan IDS Concept: Nissan’s vision for the future of EVs and autonomous driving. Nissan Online Newsroom. Retrieved July 16, 2017 from http://nissannews.com/enUS/nissan/usa/releases/nissan-ids-concept-nissan-svision-for-the-future-of-evs-and-autonomous-driving \ 42. S. M. Petermeijer, S. Cieler, and J. C. F. de Winter. 2017. Comparing spatially static and dynamic vibrotactile take-over requests in the driver seat. Accident Analysis & Prevention 99, Part A: 218–227. https://doi.org/10.1016/j.aap.2016.12.001 \ 43. Bastian Pfleging, Stefan Schneegass, and Albrecht Schmidt. 2012. Multimodal Interaction in the Car: Combining Speech and Gestures on the Steering Wheel. In Proceedings of the 4th International Conference on Automotive User Interfaces and Interactive Vehicular Applications (AutomotiveUI ’12), 155–162. https://doi.org/10.1145/2390256.2390282 \ 44. Christopher J. Ploch, Jung Hwa Bae, Wendy Ju, and Mark Cutkosky. 2016. Haptic skin stretch on a steering wheel for displaying preview information in autonomous cars. In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on, 60–65. http://ieeexplore.ieee.org/abstract/document/7759035/ \ 45. Jonas Radlmayr, Christian Gold, Lutz Lorenz, Mehdi Farid, and Klaus Bengler. 2014. How traffic situations and non-driving related tasks affect the take-over quality in highly automated driving. In Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 2063–2067. http://pro.sagepub.com/content/58/1/2063.short \ 46. Majken K. Rasmussen, Esben W. Pedersen, Marianne G. Petersen, and Kasper Hornb\\a ek. 2012. Shapechanging Interfaces: A Review of the Design Space and Open Research Questions. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’12), 735–744. https://doi.org/10.1145/2207676.2207781 \ 47. SAE International. Taxonomy and Definitions for Terms Related to Driving Automation Systems for OnRoad Motor Vehicles. SAE International. Retrieved January 21, 2017 from http://standards.sae.org/j3016_201609/ \ 48. SAE International. J2944: Operational Definitions of Driving Performance Measures and Statistics - SAE International. Retrieved from http://standards.sae.org/j2944_201506/ \ 49. Gözel Shakeri, Alexander Ng, John H. Williamson, and Stephen A. Brewster. 2016. Evaluation of Haptic Patterns on a Steering Wheel. In Proceedings of the 8th International Conference on Automotive User Interfaces and Interactive Vehicular Applications (Automotive’UI 16), 129–136. https://doi.org/10.1145/3003715.3005417 \ 50. David Sirkin and Wendy Ju. 2012. Consistency in physical and on-screen action improves perceptions of telepresence robots. In Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction, 57–64. http://dl.acm.org/citation.cfm?id=2157699 \ 51. Micah Steele and R. Brent Gillespie. 2001. Shared control between human and machine: Using a haptic steering wheel to aid in land vehicle guidance. In Proceedings of the human factors and ergonomics society annual meeting, 1671–1675. http://journals.sagepub.com/doi/abs/10.1177/15419312 0104502323 \ 52. Joris C Verster and Thomas Roth. 2011. Standard operation procedures for conducting the on-the-road driving test, and measurement of the standard deviation of lateral position (SDLP). International Journal of General Medicine 4: 359–371. https://doi.org/10.2147/IJGM.S19639 \ 53. Volvo Cars. Volvo Concept 26 | Intillisafe | Volvo Cars. Retrieved from http://www.volvocars.com/intl/about/our-innovationbrands/intellisafe/autonomous-driving/c26 \ 54. Volvo Cars. Volvo Cars conducts research into driver sensors in order to create cars that get to know their drivers. Retrieved from https://www.media.volvocars.com/global/engb/media/pressreleases/140898/volvo-cars-conductsresearch-into-driver-sensors-in-order-to-create-carsthat-get-to-know-their-driv \ 55. David Wilfinger, Martin Murer, Sebastian Osswald, Alexander Meschtscherjakov, and Manfred Tscheligi. 2013. The Wheels Are Turning: Content Rotation on Steering Wheel Displays. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’13), 1809–1812. https://doi.org/10.1145/2470654.2466238 \ 56. Yanfeng Automotive Interiors. XiM17: “the next living space” interior autonomous concept car debuts at NAIAS 2017. Retrieved from https://www.youtube.com/watch?v=56Ow8cw1m6o \ 57. Lining Yao, Ryuma Niiyama, Jifei Ou, Sean Follmer, Clark Della Silva, and Hiroshi Ishii. 2013. PneUI: pneumatically actuated soft composite materials for shape changing interfaces. In Proceedings of the 26th annual ACM symposium on User interface software and Technology, 13–22. http://dl.acm.org/citation.cfm?id=2502037 \ 58. Kathrin Zeeb, Axel Buchner, and Michael Schrauf. 2015. What determines the take-over time? An integrated model approach of driver take-over after automated driving. Accident Analysis & Prevention 78: 212–221. https://doi.org/10.1016/j.aap.2015.02.023 \ ",Transforming Steering Wheel; Autonomous Vehicles; Transition of Control; Human Machine Interaction. ,H.1.2. User/Machine Systems.,uistf4432-file1.docx,uistf4432-file2.jpg,uistf4432-file3.mp4,,"The mechanically transforming robotic steering wheel, one of the two transforming steering wheel systems that were developed and evaluated in this paper.",,We developed and evaluated two transforming steering wheel systems (a mechanically transforming robotic steering wheel and a visually transforming LED steering wheel) that can be utilized for future autonomous vehicles. ,"1) Background Section \ Based on the reviewers' suggestions, we have added a substantial number of references concerning TUI and SCI in Background Section. \  \ The TUI literature mainly focuses on haptic steering wheel and also steering wheels as a touch gesture input.  \  \ The SCI literature focuses on the ability of SCI to provide dynamic affordances. It also discusses shape changing steering wheel concepts from the automotive industry.  \  \ Overall, we have edited the entire background section for readability and to provide a more complete depiction of the state of the art. \  \  \ 2) Steering Wheel Systems Section \ As the comparison of the two systems is the main purpose of this paper, we have edited the systems section to better focus on the two transforming steering wheel systems.  \  \ We have added additional content (development, choice of color, and other details regarding the final prototype) to the visually transforming LED steering wheel system so that the two subsections (mechanically robotic steering wheel system and visually transforming LED steering wheel system) are comparable. \  \ The simulator subsection has subsequently been moved back into the evaluation section. We have decided to keep sections such as the “Wizard of Oz Mode” section because it is referred later in other applications. \  \  \ 3) Analysis Section \ In response to reviewer requests, we have added the effect size for the relevant statistical tests and have removed anova reference for the attitudinal data in the analysis section.  \  \  \ 4) Discussion  Section \ We have added a section discussing form factor in the Discussion section. We have  mentioned potential ways the form factor could affect drivers' responses. In response to a reviewer query,  we have referenced our previous work (accepted, not yet published)  showing that the form factor of the robotic steering wheel does not cause a statistically significantly increase / decrease in performance with the metrics we used or with time to evasive action. \  \  \ 5) Application as a Robotic Avatar \ As we promised in the rebuttal, we have added additional content regarding the steering wheels application as a robotic avatar. \  \  \ 6) Safety Claims \ We have used words such as ""suggests"" and ""could"" to soften the safety claims. For the sections looking at statistical comparison, we have kept phrases such as ""significantly better"" as they refer to statistical significance.  \  \  \ 7) Updated Figures \ We have updated the figures to be more easily readable and interpretable. \  \  \ 8) Paper Length (Slightly Over 10 Pages) \ Due to the the reviewers's suggestions and the changes made to improve the continuity and quality, the revised paper has increased in length (slightly over 10 pages). We plan to cut the “Indicator of Vehicle Automation’s Intentions” Section so that the paper will be exactly be 10 pages (excluding references).  \  \ To get feedback for the final camera ready submission, we have submitted longer full length work, but the 10 page version of the paper is included in the auxiliary materials for reference. \  \  \ 9) Edited Video Figures \ We have edited the video figures to show the two participants who performed equally well on the critical event (so that it will not bias viewers towards a particular steering wheel system). \  \ Thank you.",Brian Mok,Mishel Johns,FormatComplete,,,,,,,Aug 8 16:33,
uistf1353,10/23,5,Physical Interfaces,4:20:00 PM,5:40:00 PM,4,5:20:00 PM,5:40:00 PM,long,long,uistf4432,4,504,,,uistf1353,A,Thermal-Comfort Design of Personalized Casts,Xiaoting,Zhang,narheas@gmail.com,uistf1353-paper.pdf,12,letter,,,"Xiaoting Zhang, Guoxin Fang, Chengkai Dai, Jouke Verlinden, Jun Wu, Emily Whiting, Charlie C. L. Wang","narheas@gmail.com, G.Fang-1@tudelft.nl, c.dai@tudelft.nl, j.c.verlinden@tudelft.nl, j.wu-1@tudelft.nl, emily@cs.dartmouth.edu, c.c.wang@tudelft.nl",71590,Xiaoting,,Zhang,narheas@gmail.com,Computer Science,Boston University,Boston,Massachusetts,United States,,,,,,71727,Guoxin,,Fang,G.Fang-1@tudelft.nl,advanced manufacturing,Delft University of Technology,Delft,South Holland,Netherlands,,,,,,71699,Chengkai,,Dai,c.dai@tudelft.nl,Advanced Manufacturing,Delft University of Technology,DELFT,South Holland,Netherlands,,,,,,66045,Jouke,,Verlinden,j.c.verlinden@tudelft.nl,Industrial Design Engineering,TU Delft,Delft,,the Netherlands,,,,,,71688,Jun,,Wu,j.wu-1@tudelft.nl,Department of Design Engineering,TU Delft,Delft,South Holland,The Netherlands,,,,,,43494,Emily,,Whiting,emily@cs.dartmouth.edu,Computer Science,Boston University,Boston,Massachusetts,United States,,,,,,71692,Charlie C. L.,,Wang,c.c.wang@tudelft.nl,Dept. of Design Engineering,Delft University of Technology,Delft,,The Netherlands,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper introduces a novel method for designing personalized orthopedic casts which are aware of thermal-comfort while satisfying mechanical requirements. Our pipeline starts from thermal images taken by an infrared camera, by which the distribution of thermal-comfort sensitivity is generated on the surface of a 3D scanned model. We formulate a hollowed Voronoi tessellation pattern to represent the covered region for a web-like cast design. The pattern is further optimized according to the thermal-comfort sensitivity calculated from thermal images. Working together with a thickness variation method, we generate a solid model for a personalized cast maximizing both thermal comfort and mechanical stiffness. To demonstrate the effectiveness of our approach, 3D printed models of personalized casts are tested on body parts of different individuals.",narheas@gmail.com,"1. E. Arens and H. Zhang. 2006. The skin’s role in human thermoregulation and comfort. Center for the Built Environment (2006). \ 2. E. Arens, H. Zhang, and C. Huizenga. 2006. Partial-and whole-body thermal sensation and comfort – Part I: Uniform environmental conditions. Journal of thermal Biology 31, 1 (2006), 53–59. \ 3. ASHRAE. 2001. Fundamentals. American Society of Heating, Refrigerating and Air Conditioning Engineers, Atlanta 111 (2001). \ 4. M. Attia and P. Engel. 1981. Thermal alliesthesial response in man is independent of skin location stimulated. Physiology & behavior 27, 3 (1981), 439–444. \ 5. S. Axler, P. Bourdon, and R. Wade. 2013. Harmonic function theory. Vol. 137. Springer Science & Business Media. \ 6. P. Baudisch and S. Mueller. 2016. Personal Fabrication: State of the Art and Future Research. In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems. ACM, 936–939. \ 7. T.N. Bitzer. 2012. Honeycomb technology: materials, design, manufacturing, applications and testing. Springer Science & Business Media. \ 8. L. Bonanni, C. Vaucelle, J. Lieberman, and O. Zuckerman. 2006. TapTap: a haptic wearable for asynchronous distributed touch therapy. In CHI’06 extended abstracts on Human factors in computing systems. ACM, 580–585. \ 9. D. Chen, D. I. W. Levin, S. Sueda, and W. Matusik. 2015. Data-driven Finite Elements for Geometry and Material Design. ACM Trans. Graph. 34, 4, Article 74 (July 2015), \ 10. pages. 10. W. Chen, X. Zhang, S. Xin, Y. Xia, S. Lefebvre, and W. Wang. 2016. Synthesis of ﬁligrees for digital fabrication. ACM Transactions on Graphics (TOG) 35, 4 (2016), 98. \ 11. K. Dai, M. Yan, Z. Zhu, and Y. Sun. 2007. Computer-aided custom-made hemipelvic prosthesis used in extensive pelvic lesions. The Journal of arthroplasty 22, 7 (2007), 981–986. \ 12. Q. Du, V. Faber, and M. Gunzburger. 1999. Centroidal Voronoi tessellations: applications and algorithms. SIAM review 41, 4 (1999), 637–676. \ 13. J. Dumas, A. Lu, S. Lefebvre, J. Wu, and C. Dick. 2015. By-example synthesis of structurally sound patterns. ACM Transactions on Graphics (TOG) 34, 4 (2015), 137. \ 14. J. Evill. 2016. Cortex: Exoskeletal Cast. http://www.evilldesign.com/cortex. (2016). \ 15. A.P. Gagge, J.A.J. Stolwijk, and J.D. Hardy. 1967. Comfort and thermal sensations and associated physiological responses at various ambient temperatures. Environmental research 1, 1 (1967), 1–20. \ 16. M. Gannon, T. Grossman, and G. Fitzmaurice. 2015. Tactum: a skin-centric approach to digital design and fabrication. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 1779–1788. \ 17. M. Gannon, T. Grossman, and G. Fitzmaurice. 2016. ExoSkin: On-body fabrication. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 5996–6007. \ 18. C. Hurson, A. Tansey, B. O’donnchadha, P. Nicholson, J. Rice, and J. McElwain. 2007. Rapid prototyping in the assessment, classiﬁcation and preoperative planning of acetabular fractures. Injury 38, 10 (2007), 1158–1162. \ 19. T. Ju, F. Losasso, S. Schaefer, and J. Warren. 2002. Dual Contouring of Hermite Data. ACM Trans. Graph. 21, 3 (July 2002), 339–346. \ 20. D. Karasahin. 2014. Osteoid Medical cast. https: //competition.adesignaward.com/design.php?ID=34151. (2014). \ 21. H. Kim and S. Jeong. 2015. Case study: Hybrid model for the customized wrist orthosis using 3D printing. Journal of Mechanical Science and Technology 29, 12 (2015), 5151–5156. \ 22. T. Langlois, A. Shamir, D. Dror, W. Matusik, and D. I. W. Levin. 2016. Stochastic Structural Analysis for Context-aware Design and Fabrication. ACM Trans. Graph. 35, 6, Article 226 (Nov. 2016), 13 pages. \ 23. G. Laput, X. Chen, and C. Harrison. 2015. 3D printed hair: Fused deposition modeling of soft strands, ﬁbers, and bristles. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology. ACM, 593–597. \ 24. S. Lautenbacher and F. Strian. 1991. Sex differences in pain and thermal sensitivity: the role of body size. Attention, Perception, & Psychophysics 50, 2 (1991), 179–183. \ 25. B. Lévy and Y. Liu. 2010. Lp Centroidal Voronoi Tessellation and Its Applications. ACM Trans. Graph. 29, 4, Article 119 (July 2010), 11 pages. \ 26. Lexus. 2016. Lexus Kinetic Seat Concept World Premiere at the 2016 Paris Motor Show. http://pressroom.lexus.com/releases/ lexus-kinetic-seat-concept-premiere-2016-paris-motor-show. htm. (2016). \ 27. H. Lin, L. Shi, and D. Wang. 2016. A rapid and intelligent designing technique for patient-speciﬁc and 3D-printed orthopedic cast. 3D Printing in Medicine 2, 1 (2016), 4. \ 28. S. Lloyd. 1982. Least squares quantization in PCM. IEEE transactions on information theory 28, 2 (1982), 129–137. \ 29. W.E. Lorensen and H.E. Cline. 1987. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. In Proceedings of SIGGRAPH ’87. 7. \ 30. L. Lu, A. Sharf, H. Zhao, Y. Wei, Q. Fan, X. Chen, Y. Savoye, C. Tu, D. Cohen-Or, and B. Chen. 2014. Build-to-last: strength to weight 3D printed objects. ACM Transactions on Graphics (TOG) 33, 4 (2014), 97. \ 31. J. Martínez, J. Dumas, and S. Lefebvre. 2016. Procedural Voronoi Foams for Additive Manufacturing. ACM Trans. Graph. 35, 4, Article 44 (July 2016), 12 pages. \ 32. J. Martínez, J. Dumas, S. Lefebvre, and L. Wei. 2015. Structure and appearance optimization for controllable shape design. ACM Transactions on Graphics (TOG) 34, 6 (2015), 229. \ 33. C. Mavroidis, R.G. Ranky, M.L. Sivak, B.L. Patritti, J. DiPisa, A. Caddle, K. Gilhooly, L. Govoni, S. Sivak, M. Lancia, and others. 2011. Patient speciﬁc ankle-foot orthoses using rapid prototyping. Journal of neuroengineering and rehabilitation 8, 1 (2011), 1. \ 34. G.D. Mower. 1976. Perceived intensity of peripheral thermal stimuli is independent of internal body temperature. Journal of comparative and physiological psychology 90, 12 (1976), 1152. \ 35. S. Mueller, S. Im, S. Gurevich, A. Teibrich, L. Pﬁsterer, F. Guimbretière, and P. Baudisch. 2014a. WirePrint: Fast 3D printed previews. Proc. UISTâ ˘A ´Z14 (2014), 273–280. \ 36. S. Mueller, T. Mohr, K. Guenther, J. Frohnhofen, and P. Baudisch. 2014b. faBrickation: fast 3D printing of functional objects by integrating construction kit building blocks. In Proceedings of the 32nd annual ACM conference on Human factors in computing systems. ACM, 3827–3834. \ 37. P. Musialski, C. Hafner, F. Rist, M. Birsak, M. Wimmer, and L. Kobbelt. 2016. Non-linear Shape Optimization Using Local Subspace Projections. ACM Trans. Graph. 35, 4, Article 87 (July 2016), 13 pages. \ 38. J. Panetta, Q. Zhou, L. Malomo, N. Pietroni, P. Cignoni, and D. Zorin. 2015. Elastic textures for additive fabrication. ACM Transactions on Graphics (TOG) 34, 4 (2015), 135. \ 39. V. Sastri. 2013. Plastics in medical devices: properties, requirements, and applications. William Andrew. \ 40. V. Savage, S. Follmer, J. Li, and B. Hartmann. 2015. Makers’ Marks: Physical markup for designing and fabricating functional objects. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology. ACM, 103–108. \ 41. C. Schumacher, B. Bickel, J. Rys, S. Marschner, C. Daraio, and M. Gross. 2015. Microstructures to control elasticity in 3D printing. ACM Transactions on Graphics (TOG) 34, 4 (2015), 136. \ 42. C. Schumacher, B. Thomaszewski, and M. Gross. 2016. Stenciling: Designing Structurally-Sound Surfaces with Decorative Patterns. Computer Graphics Forum 35, 5 (2016), 101–110. \ 43. J. Schuren. 1994. Working with soft cast: a manual on semi-rigid immobilisation. Minnesota Mining & Manufacturing. \ 44. M. Skouras, B. Thomaszewski, S. Coros, B. Bickel, and M. Gross. 2013. Computational design of actuated deformable characters. ACM Transactions on Graphics (TOG) 32, 4 (2013), 82. \ 45. A. Teibrich, S. Mueller, F. Guimbretière, R. Kovacs, S. Neubert, and P. Baudisch. 2015. Patching physical objects. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology. ACM, 83–91. \ 46. C. Torres, T. Campbell, N. Kumar, and E. Paulos. 2015. HapticPrint: Designing feel aesthetics for digital fabrication. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology. ACM, 583–591. \ 47. C.C.L. Wang and Y. Chen. 2013. Thickening freeform surfaces for solid fabrication. Rapid Prototyping Journal 19, 6 (2013), 395–406. \ 48. W. Wang, T.Y. Wang, Z. Yang, L. Liu, X. Tong, W. Tong, J. Deng, F. Chen, and X. Liu. 2013. Cost-effective printing of 3D objects with skin-frame structures. ACM Transactions on Graphics (TOG) 32, 6 (2013), 177. \ 49. A. Wibowo, D. Sakamoto, J. Mitani, and T. Igarashi. 2012. DressUp: a 3D interface for clothing design with a physical mannequin. In Proceedings of the Sixth International Conference on Tangible, Embedded and Embodied Interaction. ACM, 99–102. \ 50. K. Wong, S. Kumta, K. Sze, and C. Wong. 2012. Use of a patient-speciﬁc CAD/CAM surgical jig in extremity bone tumor resection and custom prosthetic reconstruction. Computer Aided Surgery 17, 6 (2012), 284–293. \ 51. J. Wu, C. Dick, and R. Westermann. 2016a. A System for High-Resolution Topology Optimization. IEEE Transactions on Visualization and Computer Graphics 22, 3 (2016), 1195–1208. \ 52. J. Wu, C.C.L. Wang, X. Zhang, and R. Westermann. 2016b. Self-supporting rhombic inﬁll structures for additive manufacturing. Computer-Aided Design 80 (2016), 32 – 42. \ 53. J. Zehnder, S. Coros, and B. Thomaszewski. 2016. Designing structurally-sound ornamental curve networks. ACM Transactions on Graphics (TOG) 35, 4 (2016), 99. \ 54. H. Zhang, E. Arens, C. Huizenga, and T. Han. 2010a. Thermal sensation and comfort models for non-uniform and transient environments: Part I: Local sensation of individual body parts. Building and Environment 45, 2 (2010), 380–388. \ 55. H. Zhang, E. Arens, C. Huizenga, and T. Han. 2010b. Thermal sensation and comfort models for non-uniform and transient environments, part II: Local comfort of individual body parts. Building and Environment 45, 2 (2010), 389–398. \ 56. Q. Zhou, J. Panetta, and D. Zorin. 2013. Worst-case Structural Analysis. ACM Trans. Graph. 32, 4, Article 137 (July 2013), 12 pages. \ ",personalized cast; thermal-comfort; 3D printing; pattern optimization; structural analysis,D.2.2; J.6,uistf1353-file1.zip,uistf1353-file2.jpg,uistf1353-file3.mp4,,We propose a new method for thermal-comfort design of personalized casts. (Left) Physical-data acquisition by a 3D scanner and a thermal camera. (Right) Our optimized cast.,,We present a novel computational paradigm for personal fabrication by considering thermal-comfort of 3D printed wearable instruments. ,"-	Fit for CHI/UIST community \ Revision:  \ 1.	We added a Personal Fabrication section in Related Work including UIST/CHI papers. (Related Work) \ 2.	We provided a more detailed user study including the full questionnaire and feedback description from the participants. (User Feedback) \ 3.	We discussed the potential of our work to be extended to other personal products and interactive design tools. (Future Work) \ 4.	We changed the ACM Classification Keywords \  \ -	Algorithm Description  \ Revision: We simplified the language for the algorithm description using more high level concepts. We rewrote the Hollowed Voronoi Tessellation and Framework of Computation sections to include more descriptive text. \  \ -	Clinic Consultation \ Revision: We conducted an interview with an orthopedic surgeon, added a paragraph of the discussion and suggestion in the Heuristic Evaluation section. \  \ -	We addressed all the miscellaneous requests in the revision. \ ",Xiaoting Zhang,Emily Whiting,FormatComplete,1464267,National Science Foundation,Natural Science Foundation of China,61628211,,,Jul 27 16:58,
uistf4474,10/23,6,360 Video,4:20:00 PM,5:40:00 PM,3+1,4:20:00 PM,4:40:00 PM,long,long,none,1,601,,,uistf4474,A,Outside-In: Visualizing Out-of-Sight Regions-of-Interest in a 360 Video Using Spatial Picture-in-Picture Previews,Yung-Ta,Lin,lynda0214@gmail.com,uistf4474-paper.pdf,11,letter,,,"Yung-Ta Lin, Yi-Chi Liao, Shan-Yuan Teng, Yi-Ju Chung, Liwei Chan, Bing-Yu Chen","lynda0214@gmail.com, imetliao@gmail.com, tsy754@gmail.com, violetayjchung@gmail.com, liwei.name@gmail.com, robin@ntu.edu.tw",64512,Yung-Ta,,Lin,lynda0214@gmail.com,,National Taiwan University,Taipei,,Taiwan,,,,,,47176,Yi-Chi,,Liao,imetliao@gmail.com,,National Taiwan University,Taipei,,Taiwan,,,,,,64509,Shan-Yuan,,Teng,tsy754@gmail.com,,National Taiwan University,Taipei,,Taiwan,,,,,,55376,Yi-Ju,,Chung,violetayjchung@gmail.com,,National Taiwan University,Taipei,,Taiwan,,,,,,19308,Liwei,,Chan,liwei.name@gmail.com,Computer Science,National Chiao Tung University,Hsinchu,,Taiwan,,,,,,11664,Bing-Yu,,Chen,robin@ntu.edu.tw,,National Taiwan University,Taipei,,Taiwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"360-degree video contains a full field of environmental content. However, browsing these videos, either on screens or through head-mounted displays (HMDs), users consume only a subset of the full field of view per a natural viewing experience. This causes a search problem when a region-of-interest (ROI) in a video is outside of the current field of view (FOV) on the screen, or users may search for non-existing ROIs.  \  \ We propose Outside-In, a visualization technique which re-introduces off-screen regions-of-interest (ROIs) into the main screen as spatial picture-in-picture (PIP) previews. The geometry of the preview windows further encodes a ROI's relative location vis-à-vis the main screen view, allowing for effective navigation. In an 18-participant study, we compare Outside-In with traditional arrow-based guidance within three types of 360-degree video. Results show that Outside-In outperforms in regard to understanding spatial relationship, the storyline of the content and overall preference. Two applications are demonstrated for use with Outside-In in 360-degree video navigation with touchscreens, and live telepresence.",lynda0214@gmail.com,"1. J´erˆome Ardouin, Anatole L´ecuyer, Maud Marchal, Cl´ement Riant, and Eric Marchand. 2012. FlyVIZ: A Novel Display Device to Provide Humans with 360◦ Vision by Coupling Catadioptric Camera with Hmd. In Proceedings of the 18th ACM Symposium on Virtual Reality Software and Technology (VRST ’12). ACM, New York, NY, USA, 41–44. DOI: http://dx.doi.org/10.1145/2407336.2407344 \ 2. Patrick Baudisch and Ruth Rosenholtz. 2003. Halo: A Technique for Visualizing Off-screen Objects. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’03). ACM, New York, NY, USA, 481–488. DOI: http://dx.doi.org/10.1145/642611.642695 \ 3. Andreas Girgensohn, Frank Shipman, Thea Turner, and Lynn Wilcox. 2007. Effects of Presenting Geographic Context on Tracking Activity Between Cameras. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’07). ACM, New York, NY, USA, 1167–1176. DOI: http://dx.doi.org/10.1145/1240624.1240801 \ 4. Jan Gugenheimer, Dennis Wolf, Gabriel Haas, Sebastian Krebs, and Enrico Rukzio. 2016. SwiVRChair: A Motorized Swivel Chair to Nudge Users’ Orientation for 360 Degree Storytelling in Virtual Reality. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 1996–2000. DOI: http://dx.doi.org/10.1145/2858036.2858040 \ 5. Sean Gustafson, Patrick Baudisch, Carl Gutwin, and Pourang Irani. 2008. Wedge: Clutter-free Visualization of Off-screen Locations. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’08). ACM, New York, NY, USA, 787–796. DOI: http://dx.doi.org/10.1145/1357054.1357179 \ 6. Sean G. Gustafson and Pourang P. Irani. 2007. Comparing Visualizations for Tracking Off-screen Moving Targets. In CHI ’07 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’07). ACM, New York, NY, USA, 2399–2404. DOI: http://dx.doi.org/10.1145/1240866.1241014 \ 7. Hou-Ning Hu, Yen-Chen Lin, Ming-Yu Liu, Hsien-Tzu Cheng, Yung-Ju Chang, and Min Sun. 2017. Deep 360 Pilot: Learning a Deep Agent for Piloting through 360◦ Sports Video. CoRR abs/1705.01759 (2017). http://arxiv.org/abs/1705.01759 \ 8. Laurent Itti and Pierre F. Baldi. 2006. Bayesian Surprise Attracts Human Attention. In Advances in Neural Information Processing Systems 18, Y. Weiss, P. B. Sch¨olkopf, and J. C. Platt (Eds.). MIT Press, 547–554. http://papers.nips.cc/paper/ 2822-bayesian-surprise-attracts-human-attention. pdf \ 9. Shunichi Kasahara and Jun Rekimoto. 2014. JackIn: Integrating First-person View with Out-of-body Vision Generation for Human-human Augmentation. In Proceedings of the 5th Augmented Human International Conference (AH ’14). ACM, New York, NY, USA, Article 46, 8 pages. DOI: http://dx.doi.org/10.1145/2582051.2582097 \ 10. Yen-Chen Lin, Yung-Ju Chang, Hou-Ning Hu, Hsien-Tzu Cheng, Chi-Wen Huang, and Min Sun. 2017. Tell Me Where to Look: Investigating Ways for Assisting Focus in 360◦ Video. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17). ACM, New York, NY, USA, 2535–2545. DOI: http://dx.doi.org/10.1145/3025453.3025757 \ 11. Hajime Nagahara, Yasushi Yagi, and Masahiko Yachida. 2003. Wide Field of View Head Mounted Display for Tele-presence with An Omnidirectional Image Sensor. In 2003 Conference on Computer Vision and Pattern Recognition Workshop, Vol. 7. 86–86. DOI: http://dx.doi.org/10.1109/CVPRW.2003.10070 \ 12. School of Rock the Musical. 2015. SCHOOL OF ROCK: The Musical - “You’re in the Band” (360 Video). https://www.youtube.com/watch?v= GFRPXRhBYOI&feature=youtu.be. (2015). \ 13. Amy Pavel, Maneesh Agrawala, and Bjorn Hartmann. 2017. Shot Orientation Controls for Interactive Cinematography with 360 Video. In In proceedings of the 30th annual ACM symposium on User Interface Software and Technology (UIST ’17). New York, NY, USA. DOI:http://dx.doi.org/https: //doi.org/10.1145/3126594.3126636 \ 14. Freakin Rad. 2016. Pok´emon 360 - CATCH ’EM ALL in VR! https://www.youtube.com/watch?v=pHUVS_ GrIeM&feature=youtu.be. (2016). \ 15. Google Spotlight Stories. 2016. 360 Google Spotlight Story: HELP. https://www.youtube.com/watch?v= G-XZhKqQAHU&feature=youtu.be. (2016). \ 16. Baobab Studios. 2016. INVASION! 360 VR Sneak Peek. https: //www.youtube.com/watch?v=gPUDZPWhiiE&t=12s. (2016). \ 17. Laura A. Teodosio and Michael Mills. 1993. Panoramic Overviews for Navigating Real-world Scenes. In Proceedings of the First ACM International Conference on Multimedia (MULTIMEDIA ’93). ACM, New York, NY, USA, 359–364. DOI: http://dx.doi.org/10.1145/166266.168422 \ 18. Verest 360 VR. 2015. Bambino Down Mode. https://www.youtube.com/watch?v=ujYyE0lfSUk. (2015). \ 19. Yi Wang, David M. Krum, Enylton M. Coelho, and Doug A. Bowman. 2007. Contextualized Videos: Combining Videos with Environment Models to Support Situational Understanding. IEEE Transactions on Visualization and Computer Graphics 13, 6 (Nov. 2007), 1568–1575. DOI: http://dx.doi.org/10.1109/TVCG.2007.70544 \ 20. Robert Xiao and Hrvoje Benko. 2016. Augmenting the Field-of-View of Head-Mounted Displays with Sparse Peripheral Displays. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 1221–1232. DOI:http://dx.doi.org/10.1145/2858036.2858212 \ 21. Yun Zhai and Mubarak Shah. 2006. Visual Attention Detection in Video Sequences Using Spatiotemporal Cues. In Proceedings of the 14th ACM International Conference on Multimedia (MM ’06). ACM, New York, NY, USA, 815–824. DOI: http://dx.doi.org/10.1145/1180639.1180824 \ ",360 Video; Telepresence; Picture-in-picture; Off-screen Targets;,H.5.m,uistf4474-file1.zip,uistf4474-file2.jpg,uistf4474-file3.mp4,,Outside-In reintroduces off-screen regions-of-interest in 360 videos into the main screen as spatial picture-in-picture previews. The geometry of PIPs further encodes a ROI’s relative location vis-à-vis the main screen view.   \  \ ,,Outside-In reintroduces off-screen regions-of-interest in 360 videos into the main screen as spatial picture-in-picture previews. The geometry of PIPs further encodes a ROI’s relative location vis-à-vis the main screen view.   \  \ ,"Followed by the reviewers’ comments and our claim in rebuttal, the camera-ready version includes these changes: (all the following changes are highlighted in the uploaded pdf) \  \ [TITLE]  \ change from ""Region-of-Interests"" to ""Regions-of-Interest"" \  \ [INTRODUCTION]  \ Remove the description of amateur contents, while introducing multi-ROIs challenges instead.  \  \ [RELATED WORK] \ Add some more works related to 360, including CHI '17 and UIST '17 \  \ [OUTSIDE-IN] \ In the paragraph of ""The Tilt of PIP"", more details are added to describe the necessity of scaled tilt. Besides, a comparison figure is added as well.  \ We also removed the description of pilot study determining maximal tilt angle.  \  \ [STUDY CONSIDERATIONS] \ For the video categorization, video examples are enumerated to better understand these types.  \  \ [SUBJECTIVE RATING ANALYSIS AND RESULTS] \ Section title changed from ""Quantitative"" to ""Subjective Rating"".  \  \ [DISCUSSION AND FUTURE WORKS] \ ROI labeling is discussed in the first subsection, also the details of improper video types are added.  \  \ [LIMITATIONS] \ For the reviewers' concern that there's no objective performance data, the limitation of study is discussed in the first paragraph.  \ Second paragraph discussed the limitation of the proposed method itself: the distraction.  \  \ [CONCLUSION] \ We put the possible future application of social VR in the last paragraph of conclusion.  \  \ Lastly, this paper has gone through a thorough proof-reading with a professional native editor, and has fixed the grammar problems.",Yung-Ta Lin,Yi-Chi Liao,FormatComplete,,,,,,,Aug 8 0:14,
uistf4597,10/23,6,360 Video,4:20:00 PM,5:40:00 PM,3+1,4:40:00 PM,5:00:00 PM,long,long,uistf4474,2,602,,,uistf4597,A,CollaVR: Collaborative In-Headset Review for VR Video,Cuong,Nguyen,cuong3@pdx.edu,uistf4597-paper.pdf,11,letter,,,"Cuong Nguyen, Stephen DiVerdi, Aaron Hertzmann, Feng Liu","cuong3@pdx.edu, stephen.diverdi@gmail.com, hertzman@dgp.toronto.edu, fliu@cs.pdx.edu",22178,Cuong,,Nguyen,cuong3@pdx.edu,,Portland State University,Portland,Oregon,United States,,,,,,7232,Stephen,,DiVerdi,stephen.diverdi@gmail.com,,Adobe Research,San Francisco,California,United States,,,,,,9020,Aaron,,Hertzmann,hertzman@dgp.toronto.edu,Adobe Research,Adobe Research,San Francisco,CA,USA,,,,,,5695,Feng,,Liu,fliu@cs.pdx.edu,,Portland State University,Portland,Oregon,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Collaborative review and feedback is an important part of conventional filmmaking and now Virtual Reality (VR) video production as well. However, conventional collaborative review practices do not easily translate to VR video because VR video is normally viewed in a headset, which makes it difficult to align gaze, share context, and take notes. This paper presents CollaVR, an application that enables multiple users to review a VR video together while wearing headsets. We interviewed VR video professionals to distill key considerations in reviewing VR video. Based on these insights, we developed a set of networked tools that enable filmmakers to collaborate and review video in real-time. We conducted a preliminary expert study to solicit feedback from VR video professionals about our system and assess their usage of the system with and without collaboration features.",cuong3@pdx.edu,"1. Ellen Baker, John Geirland, Tom Fisher, and Annmarie Chandler. 1999. Media Production: Towards Creative Collaboration Using Communication Networks. Computer Supported Cooperative Work (CSCW) 8, 4 (dec 1999), 303–332. DOI: http://dx.doi.org/10.1023/A:1008616002814 \ 2. Paulo Bala, Mara Dionisio, Valentina Nisi, and Nuno Nunes. 2016. IVRUX: A tool for analyzing immersive narratives in virtual reality. In Interactive Storytelling, Ulrike Spierling and Nicolas Szilas (Eds.). Lecture Notes in Computer Science, Vol. 5334. Springer Berlin Heidelberg, 3–11. DOI: http://dx.doi.org/10.1007/978-3-319-48279-8_1 \ 3. Jessica J. Baldis. 2001. Effects of spatial audio on memory, comprehension, and preference during desktop conferences. In Proceedings of the SIGCHI conference on Human factors in computing systems. 166–173. DOI: http://dx.doi.org/10.1145/365024.365092 \ 4. Steve Benford, Chris Greenhalgh, Tom Rodden, and James Pycock. 2001. Collaborative Virtual Environments. Commun. ACM 44, 7 (July 2001), 79–85. DOI: http://dx.doi.org/10.1145/379300.379322 \ 5. M. L. Brown, S. L. Newsome, and E. P. Glinert. 1989. An experiment into the use of auditory cues to reduce visual workload. In Proceedings of the SIGCHI conference on Human factors in computing systems. 339–346. DOI: http://dx.doi.org/10.1145/67449.67515 \ 6. Gerd Bruder, Frank Steinicke, Phil Wieland, and Markus Lappe. 2012. Tuning self-motion perception in virtual reality with visual illusions. IEEE Transactions on Visualization and Computer Graphics 18, 7 (July 2012), 1068–1078. DOI: http://dx.doi.org/10.1109/TVCG.2011.274 \ 7. Mauro Cherubini, Marc-Antoine Nüssli, and Pierre Dillenbourg. 2008. Deixis and gaze in collaborative work at a distance (over a shared map): a computational model to detect misunderstandings. In Proceedings of the Symposium on Eye Tracking Research & Applications. 8. DOI:http://dx.doi.org/10.1145/1344471.1344515 \ 8. cineSync. 2005. Retrieved 2017-04-03 from https://cospective.com/cinesync/ \ 9. Maxime Cordeil, Tim Dwyer, Karsten Klein, Bireswar Laha, Kim Marriott, and Bruce H. Thomas. 2017. Immersive collaborative analysis of network connectivity: CAVE-style or head-mounted display? IEEE Transactions on Visualization and Computer Graphics 23, 1 (Jan. 2017), 441–450. DOI: http://dx.doi.org/10.1109/TVCG.2016.2599107 \ 10. Paul Dourish and Victoria Bellotti. 1992. Awareness and coordination in shared workspaces. In Proceedings of the ACM conference on Computer-supported cooperative work. 107–114. DOI: http://dx.doi.org/10.1145/143457.143468 \ 11. Ajoy S. Fernandes and Steven K. Feiner. 2016. Combating VR sickness through subtle dynamic ﬁeld-of-view modiﬁcation. In Proceedings of the IEEE Symposium on 3D User Interfaces. 201–210. DOI: http://dx.doi.org/10.1109/3DUI.2016.7460053 \ 12. Mike Fraser, Steve Benford, Jon Hindmarsh, and Christian Heath. 1999. Supporting awareness and interaction through collaborative virtual interfaces. In Proceedings of the ACM symposium on User interface software and technology, Vol. 1. 27–36. DOI: http://dx.doi.org/10.1145/320719.322580 \ 13. Jan Gugenheimer, Evgeny Stemasov, Julian Frommel, and Enrico Rukzio. 2017. ShareVR: Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI ’17. ACM Press, 4021–4033. DOI: http://dx.doi.org/10.1145/3025453.3025683 \ 14. Carl Gutwin and Saul Greenberg. Design for individuals, design for groups: tradeoffs between power and workspace awareness. In Proceedings of the ACM conference on Computer supported cooperative work. 207–216. DOI:http://dx.doi.org/10.1145/289444.289495 \ 15. Jefferson Han and Brian Smith. 1996. CU-SeeMe VR immersive desktop teleconferencing. In Proceedings of the ACM international conference on Multimedia. 199–207. DOI:http://dx.doi.org/10.1145/244130.244199 \ 16. Jeffrey Heer and Maneesh Agrawala. 2008. Design considerations for collaborative visual analytics. Information Visualization 7, 1 (Jan. 2008), 49–62. DOI: http://dx.doi.org/10.1057/palgrave.ivs.9500167 \ 17. Rorik Henrikson, Bruno De Araujo, Fanny Chevalier, Karan Singh, and Ravin Balakrishnan. 2016a. Multi-device storyboards for cinematic narratives in VR. Proceedings of the ACM Symposium on User interface software and technology (2016), 787–796. http://doi.acm.org/10.1145/2984511.2984539 \ 18. Rorik Henrikson, Bruno De Araujo, Fanny Chevalier, Karan Singh, and Ravin Balakrishnan. 2016b. Storeoboard: sketching stereoscopic storyboards. In Proceedings of the SIGCHI conference on Human factors in computing systems. 4587–4598. DOI: http://dx.doi.org/10.1145/2858036.2858079 \ 19. Eugenia M. Kolasinski. 1995. Simulator sickness in virtual environments. Technical Report ARI-TR-1027. Army Research Institute for the Behavioral and Social Sciences. DOI:http://dx.doi.org/10.1121/1.404501 \ 20. Hao Li, Laura Trutoiu, Kyle Olszewski, Lingyu Wei, Tristan Trutna, Pei-Lun Hsieh, Aaron Nicholls, and Chongyang Ma. 2015. Facial performance sensing head-mounted display. ACM Transactions on Graphics (Proceedings of SIGGRAPH) 34, 4 (July 2015). \ 21. LookAt. 2017. Retrieved 2017-04-03 from https://www.lookat.io/ \ 22. Thomas Löwe, Michael Stengel, Förster Emmy-Charlotte, Steve Grogorick, and Marcus Magnor. 2015. Visualization and analysis of head movement and gaze data for immersive video in head-mounted displays. Proceedigns of the Workshop on Eye Tracking and Visualization 1 (2015), 1–5. \ 23. Andrew MacQuarrie and Anthony Steed. 2017. Cinematic virtual reality: Evaluating the effect of display type on the viewing experience for panoramic video. In 2017 IEEE Virtual Reality (VR). IEEE, 45–54. DOI: http://dx.doi.org/10.1109/VR.2017.7892230 \ 24. Charles Malleson, Maggie Kosek, Martin Klaudiny, Ivan Huerta, Jean-Charles Bazin, Alexander Sorkine-Hornung, Mark Mine, and Kenny Mitchell. 2017. Rapid one-shot acquisition of dynamic VR avatars. In 2017 IEEE Virtual Reality (VR). 131–140. DOI: http://dx.doi.org/10.1109/VR.2017.7892240 \ 25. Mark McGill, John H Williamson, and Stephen Brewster. 2016. Examining the role of smart TVs and VR HMDs in synchronous at-a-distance media consumption. ACM Transactions on Computer-Human Interaction 23, 5 (Nov. 2016), 1–57. DOI:http://dx.doi.org/10.1145/2983530 \ 26. Meredith Ringel Morris and Eric Horvitz. 2007. SearchTogether: an interface for collaborative web search. In Proceedings of the ACM symposium on User interface software and technology. 3. DOI: http://dx.doi.org/10.1145/1294211.1294215 \ 27. Cuong Nguyen, Stephen DiVerdi, Aaron Hertzmann, and Feng Liu. 2017. Vremiere: In-Headset Virtual Reality Video Editing. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. DOI:http://dx.doi.org/10.1145/3025453.3025675 \ 28. Costas Panagiotakis and George Tziritas. 2005. A speech/music discriminator based on RMS and zero-crossings. IEEE Transactions on Multimedia 7, 1 (Feb. 2005), 155–166. DOI: http://dx.doi.org/10.1109/TMM.2004.840604 \ 29. Amy Pavel, Dan B Goldman, Björn Hartmann, and Maneesh Agrawala. 2016. VidCrit: video-based asynchronous video review. In Proceedings of the Symposium on User interface software and technology. 517–528. DOI: http://dx.doi.org/10.1145/2984511.2984552 \ 30. Julien Phalip, Ernest a. Edmonds, and David Jean. 2009. Supporting remote creative collaboration in ﬁlm scoring. In Proceeding of the ACM conference on Creativity and cognition. 211. DOI: http://dx.doi.org/10.1145/1640233.1640266 \ 31. Road to VR. 2016. Facebook social VR demo. Retrieved 2017-04-03 from https://www.youtube.com/watch?v=YuIgyKLPt3s \ 32. Anne Sèdes, Pierre Guillot, and Eliott Paris. 2014. The HOA library, review and prospects. In International Computer Music Conference | Sound and Music Computing. 855 –860. https://hal.archives-ouvertes.fr/hal-01196453 \ 33. Dave Snowdon, Elizabeth F Churchill, and Alan J Munro. 2000. Collaborative virtual environments: digital spaces and places for CSCW. Collaborative Virtual Environments (2000), 1–34. DOI: http://dx.doi.org/10.1.1.114.9226 \ 34. Anthony Tang and Omid Fakourfar. 2017. Watching 360◦ videos together. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. DOI:http://dx.doi.org/10.1145/3025453.3025519 \ 35. James R. Wallace, Stacey D. Scott, Eugene Lai, and Deon Jajalla. 2011. Investigating the role of a large, shared display in multi-display environments. Computer Supported Cooperative Work 20, 6 (Dec. 2011), 529–561. DOI:http://dx.doi.org/10.1007/s10606-011-9149-8 \ 36. Robert Xiao and Hrvoje Benko. 2016. Augmenting the ﬁeld-of-view of head-mounted displays with sparse peripheral displays. In Proceedings of the SIGCHI conference on Human factors in computing systems. 1221–1232. DOI: http://dx.doi.org/10.1145/2858036.2858212 \ 37. Dongwook Yoon, Nicholas Chen, François Guimbretière, and Abigail Sellen. 2014. RichReview: blending ink, speech, and gesture to support collaborative document review. In Proceedings of the ACM symposium on User interface software and technology. 481–490. DOI: http://dx.doi.org/10.1145/2642918.2647390 \ ",Virtual reality; collaboration; video reviewing; video editing,H.5.1 Information Interfaces and Presentation:  Multimedia Information Systems,uistf4597-file1.zip,uistf4597-file2.jpg,uistf4597-file3.mp4,,CollaVR is an application that enables multiple users to collaborate and review VR video together in VR.,,We present the CollaVR system that enables multiple filmmakers to collaborate and review VR video together while fully immersed in VR.,"We'd like to thank the chairs and reviewers for supporting and helping us to improve our paper. We revised our paper to incorporate the reviews. In particular, we integrated the twelve (12) changes described in the rebuttal. We provide more details below. \   \ 1. R4, R2, R1: Lack of 3D interaction support \ We added the discussion of our design choice in the last Paragraph in Section “Discussion”. We also acknowledge the potential limitation of this design choice as suggested by R2 post-rebuttal. \  \ 2. ""the authors found that it is important to support natural interactions such as gesture… it is surprising to read that CollaVR uses mouse+keyboard."" \ We revised our “Formative interviews” and “Design goals” sections to clarify this point. \  \ 3.  R4: Clarify main contributions \ We added Paragraph 5 in Section “Introduction” to clarify our main contribution. \  \ 4. R3, R4: clarify generality: how it could be used outside the specific setup and/or application domain.” \ We revised Paragraph 4 in Section “Discussion” to clarify how our features could be used in other applications. \  \ 5. R4 Discuss scalability \ We added Paragraph 5 in Section “Discussion” to discuss scalability of our system. \  \ 6. R4 R2 Explain the similarity metric \ We revised Paragraph 3 of subsection “Measures” to clarify the similarity metrics used in the user study. \  \ 7. R4, R3: Clarify vocabulary \ We revised Paragraph 2 in Section “Introduction” to clarify the type of VR video and the type of headset used in the paper. \  \ 8. R4 “report participant discussion topics if they are indeed interesting” \ We removed the discussion of topics to avoid confusing the readers. \  \ 9. R4: add references to the Introduction \ We added some references to Paragraph 1 and 2 in Section “Introduction” to support some of the claims. \  \ 10. Acknowledge the small sample size limitation \ We revised Paragraph 2 in Section “Discussion” to acknowledge this limitation. \  \ 11. R2 Clarify the research question \ We revised Paragraph 1 in Section “User Study” to clarify our research question.  \  \ 12. Add more technical details and study procedure \ We revised Paragraph 2, 4, and 5 in Section “User Study” to add details about the video resolution, training procedure, and the latency of the spatialized audio and note recording features.  \  \ Thank you. \ Best regards, \ Authors \  \ ",Cuong Nguyen,Stephen DiVerdi,FormatComplete,IIS-1321119,National Science Foundation (NSF),,,,,Aug 2 13:16,
uistf2291,10/23,6,360 Video,4:20:00 PM,5:40:00 PM,3+1,5:00:00 PM,5:20:00 PM,long,long,uistf4597,3,603,,,uistf2291,A,Panning and Zooming High-Resolution Panoramas in Virtual Reality Devices,Huiwen,Chang,huiwenc@cs.princeton.edu,uistf2291-paper.pdf,10,letter,,,"Huiwen Chang, Michael Cohen","huiwenc@cs.princeton.edu, mcohen1@fb.com",71830,Huiwen,,Chang,huiwenc@cs.princeton.edu,Computer Science,Princeton University,Princeton,NJ,United States,,,,,,71851,Michael,,Cohen,mcohen1@fb.com,,Facebook Research,Seattle,Washington,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Two recent innovations in immersive media include the ability to capture very high resolution panoramic imagery, and the rise of consumer level heads-up displays for virtual reality. Unfortunately, zooming to examine the high resolution in VR breaks the basic contract with the user, that the FOV of the visual field matches the FOV of the imagery. In this paper, we study methods to overcome this restriction to allow high resolution panoramic imagery to be able to be explored in VR. \  \ We introduce and test new interface modalities for exploring high resolution panoramic imagery in VR. In particular, we demonstrate that limiting the visual FOV of the zoomed in imagery to the central portion of the visual field, and modulating the transparency or zoom level of the imagery during rapid panning, reduce simulator sickness and help with targeting tasks. ",huiwenc@cs.princeton.edu,"1. Daniel Ambrosi. 2015. Dreamscapes. http://www.danielambrosi.com/Dreamscapes/. (2015). \ 2. Caroline Appert, Olivier Chapuis, and Emmanuel Pietriga. 2010. High-precision magniﬁcation lenses. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 273–282. \ 3. Patrick Baudisch, Nathaniel Good, Victoria Bellotti, and Pamela Schraedley. 2002. Keeping things in context: a comparative evaluation of focus plus context screens, overviews, and zooming. In Proceedings of the SIGCHI conference on Human factors in computing systems. ACM, 259–266. Figure 7: A high resolution Deep Dream photograph and a zoomed-in detail. Courtesy of Daniel Ambrosi. \ 4. Eric A. Bier, Maureen C. Stone, Ken Pier, William Buxton, and Tony D. DeRose. 1993. Toolglass and Magic Lenses: The See-through Interface. In Proceedings of the 20th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH ’93). ACM, New York, NY, USA, 73–80. DOI: http://dx.doi.org/10.1145/166117.166126 \ 5. Marianne Sheelagh Therese Carpendale. 1999. A framework for elastic presentation space. Ph.D. Dissertation. Simon Fraser University. \ 6. Shenchang Eric Chen. 1995. QuickTime VR: An Image-based Approach to Virtual Environment Navigation. In Proceedings of the 22Nd Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH ’95). ACM, New York, NY, USA, 29–38. DOI: http://dx.doi.org/10.1145/218380.218395 \ 7. Andy Cockburn, Amy Karlson, and Benjamin B. Bederson. 2008. A Review of Overview+Detail, Zooming, and Focus+Context Interfaces, In ACM Surveys. Comput. Surveys (January 2008). https: //www.microsoft.com/en-us/research/publication/ a-review-of-overviewdetail-zooming-and-focuscontext-interfaces/ \ 8. Mark H Draper, Erik S Viirre, Thomas A Furness, and Valerie J Gawron. 2001. Effects of image scale and system time delay on simulator sickness within head-coupled virtual environments. Human Factors: The Journal of the Human Factors and Ergonomics Society 43, 1 (2001), 129–146. \ 9. Facebook. 2016. 360. https://facebook360.fb.com/. (05 2016). \ 10. Ajoy S Fernandes and Steven K Feiner. 2016. Combating VR sickness through subtle dynamic ﬁeld-of-view modiﬁcation. In 2016 IEEE Symposium on 3D User Interfaces (3DUI). IEEE, 201–210. \ 11. Karen A. Frenkel. 2010. Panning for Science. Science 330, 6005 (2010), 748–749. DOI: http://dx.doi.org/10.1126/science.330.6005.748 \ 12. Google+. 2015. 360 Panoramas. https://plus.google.com/s/%23Panorama_360. (2015). \ 13. C.Y. Ip and A. Varshney. 2011. Saliency-assisted navigation of very large landscape images. IEEE Transactions on Visualization and Computer Graphics 17, 12 (2011), 1737–1746. \ 14. Robert S Kennedy, Norman E Lane, Kevin S Berbaum, and Michael G Lilienthal. 1993. Simulator sickness questionnaire: An enhanced method for quantifying simulator sickness. (1993). \ 15. Eugenia M Kolasinski. 1995. Simulator Sickness in Virtual Environments. Technical Report. DTIC Document. \ 16. Johannes Kopf, Billy Chen, Richard Szeliski, and Michael Cohen. 2010. Street Slide: Browsing Street Level Imagery. In ACM SIGGRAPH 2010 Papers (SIGGRAPH ’10). ACM, New York, NY, USA, Article 96, 8 pages. DOI: http://dx.doi.org/10.1145/1833349.1778833 \ 17. Johannes Kopf, Matt Uyttendaele, Oliver Deussen, and Michael F. Cohen. 2007. Capturing and Viewing Gigapixel Images. In ACM SIGGRAPH 2007 Papers (SIGGRAPH ’07). ACM, New York, NY, USA, Article 93. DOI:http://dx.doi.org/10.1145/1275808.1276494 \ 18. Qing Luan, Steven M Drucker, Johannes Kopf, Ying-Qing Xu, and Michael F Cohen. 2008. Annotating gigapixel images. In Proceedings of the 21st annual ACM symposium on User interface software and technology. ACM, 33–36. \ 19. Sylvain Malacria, Eric Lecolinet, and Yves Guiard. 2010. Clutch-free Panning and Integrated Pan-zoom Control on Touch-sensitive Surfaces: The Cyclostar Approach. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’10). ACM, New York, NY, USA, 2615–2624. DOI: http://dx.doi.org/10.1145/1753326.1753724 \ 20. Leonard McMillan and Gary Bishop. 1995. Plenoptic modeling: An image-based rendering system. In Proceedings of the 22nd annual conference on Computer graphics and interactive techniques. ACM, 39–46. \ 21. Mathieu Nancel, Julie Wagner, Emmanuel Pietriga, Olivier Chapuis, and Wendy Mackay. 2011. Mid-air Pan-and-zoom on Wall-sized Displays. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’11). ACM, New York, NY, USA, 177–186. DOI:http://dx.doi.org/10.1145/1978942.1978969 \ 22. Oculus. 2016. Simulator Sickness in Oculus Best Practice. https: //developer3.oculus.com/documentation/intro-vr/ latest/concepts/bp_app_simulator_sickness/. (2016). \ 23. Emilee Patrick, Dennis Cosgrove, Aleksandra Slavkovic, Jennifer A Rode, Thom Verratti, and Greg Chiselko. 2000. Using a large projection screen as an alternative to head-mounted displays for virtual environments. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems. ACM, 478–485. \ 24. Kaloian Petkov, Charilaos Papadopoulos, and Arie E Kaufman. 2013. Visual exploration of the inﬁnite canvas. In Virtual Reality (VR), 2013 IEEE. IEEE, 11–14. \ 25. Emmanuel Pietriga and Caroline Appert. 2008. Sigma lenses: focus-context transitions combining space, time and translucence. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1343–1352. \ 26. JD Prothero and HG Hoffman. 1995. Widening the ﬁeld-of-view increases the sense of presence in immersive virtual environments. Human Interface Technology Laboratory Technical Report TR-95 2 (1995). \ 27. Sarah Sharples, Sue Cobb, Amanda Moody, and John R Wilson. 2008. Virtual reality induced symptoms and effects (VRISE): Comparison of head mounted display (HMD), desktop and projection display systems. Displays 29, 2 (2008), 58–69. \ 28. Kay M Stanney, Robert S Kennedy, and Julie M Drexler. 1997. Cybersickness is not simulator sickness. In Proceedings of the Human Factors and Ergonomics Society annual meeting, Vol. 41. SAGE Publications, 1138–1142. \ 29. Thomas A Stoffregen, Mark H Draper, Robert S Kennedy, and Daniel Compton. 2002. Vestibular adaptation and aftereffects. Handbook of virtual environments: Design, implementation, and applications (2002), 773–790. \ 30. Luc Vincent. 2007. Taking online maps down to street level. Computer 40, 12 (2007), 118–120. \ 31. DM Whittinghill, Bradley Ziegler, T Case, and B Moore. 2015. Nasum Virtualis: A Simple Technique for Reducing Simulator Sickness. In Games Developers Conference (GDC). \ 32. Wikipedia. 2010. Gigapan. http://en.wikipedia.org/wiki/Gigapan. (2010). \ ",Virtual Reality; Gigapixel Resolution,H.5.m,uistf2291-file1.zip,,,,,,"In this paper, we study interfaces to pan and zoom high resolution panoramic imagery in VR devices. Please come to see our live demo!","All major changes were highlighted in blue, such as adding subsections for slow mode and location mode. Besides, we also add selected references according to reviews, made the statement clearer suggested in the reviews. ",Huiwen Chang,Michael Cohen,FormatComplete,,,,,,,Aug 10 1:56,
uistf3506,10/23,6,360 Video,4:20:00 PM,5:40:00 PM,3+1,5:20:00 PM,5:30:00 PM,short,short,uistf2291,4,604,,"shouldn't have first paper in a session short.. Only 6 pages long

please move to the end --KZG",uistf3506,A,Shot Orientation Controls for Interactive Cinematography with 360 Video,Amy,Pavel,amypavel@berkeley.edu,uistf3506-paper.pdf,9,letter,,,"Amy Pavel, Bjoern Hartmann, Maneesh Agrawala","amypavel@berkeley.edu, bjoern@eecs.berkeley.edu, maneesh@cs.stanford.edu",27237,Amy,,Pavel,amypavel@berkeley.edu,University of California,Berkeley,Berkeley,California,United States,,,,,,5798,Bjoern,,Hartmann,bjoern@eecs.berkeley.edu,,"University of California, Berkeley",Berkeley,California,United States,,,,,,53076,Maneesh,,Agrawala,maneesh@cs.stanford.edu,Computer Science,Stanford University,Palo Alto,California,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Virtual reality filmmakers creating 360-degree video currently rely on \ cinematography techniques that were developed for traditional narrow \ field of view film. They typically edit together a sequence of shots \ so that they appear at a fixed-orientation irrespective of the viewer's \ field of view. But because viewers set their own camera orientation \ they may miss important story content while looking in the wrong \ direction. \ We present new interactive shot orientation techniques that are \ designed to help viewers see all of the important content in \ 360-degree video stories.  Our viewpoint-oriented technique \ reorients the shot at each cut so that the most \ important content lies in the the viewer's current field of view. Our \ active reorientation technique, lets the viewer press a button to \ immediately reorient the shot so that important content lies in their \ field of view. \ We present a 360-degree video player which implements these techniques \ and conduct a user study which finds that users spend \ 5.2-9.5% more time viewing the important points (manually labelled) of the scene with our techniques \ compared to the traditional fixed-orientation cuts.  \ In practice, 360-degree video creators may label important content, but we also provide an automatic method for determining important content in existing 360-degree videos.",amypavel@berkeley.edu,"1. 2017. GoPro Youtube Channel. (2017). https://www.youtube.com/user/GoProCamera. \ 2. 2017. Hufﬁngton Post Youtube Channel. (2017). https://www.youtube.com/user/HuffingtonPost. \ 3. 2017. National Geographic Youtube Channel. (2017). https://www.youtube.com/user/NationalGeographic. \ 4. 2017. The New York Times YouTube channel, 360 VR Playlist. (June 2017). https://www.youtube.com/playlist? list=PL4CGYNsoW2iCGZa3_Pes8LP_jQ_GPTW8w. \ 5. 2017. YouTube Virtual Reality Channel, Best of 360 Playlist. (June 2017). https://www.youtube.com/playlist? list=PLU8wpH_LfhmsSVRA8bSknO4-2wXvYXS4C. \ 6. Aseem Agarwala, Ke Colin Zheng, Chris Pal, Maneesh Agrawala, Michael Cohen, Brian Curless, David Salesin, and Richard Szeliski. 2005. Panoramic video textures. In Proc. TOG’05. ACM. \ 7. Simon Baker and Iain Matthews. 2004. Lucas-kanade 20 years on: A unifying framework. In International Journal of Computer Vision. Springer. \ 8. Tadas Baltrušaitis, Peter Robinson, and Louis-Philippe Morency. 2016. Openface: an open source facial behavior analysis toolkit. In Proc. WACV’16. IEEE, 1–10. \ 9. Paul Beddoe-Stephens. 2016. New Publisher Tools for 360 Video. (August 2016). https://media.fb.com/2016/ 08/10/new-publisher-tools-for-360-video/. \ 10. Jessica Brillhart. 2016a. In the Blink of a Mind: Engagement, Part 1. (2016). https://medium.com/the-language-of-vr/ in-the-blink-of-a-mind-engagement-part-1-eda16ee3c0d8. \ 11. Jessica Brillhart. 2016b. The Language of VR - Blog. (2016). https://medium.com/the-language-of-vr. \ 12. Red Bull. 2016. Expedition to the Heart of an Active Volcano | 360 Video. (2016). https://www.youtube.com/watch?v=OBp2EWPjotk. \ 13. Adam Cusco. 2017. Knives. (2017). http://adamcosco.com/blog/2016/10/6/ bec9blqspab3enlckac7ij6ij82d5a?rq=knives. \ 14. James E Cutting, Jordan E DeLong, and Christine E Nothelfer. 2010. Attention and the evolution of Hollywood ﬁlm. In Psychological Science, Vol. 21. Sage Publications, 432–439. \ 15. David K Elson and Mark O Riedl. 2007. A Lightweight Intelligent Virtual Cinematography System for Machinima Production.. In Proc. AIIDE’07. 8–13. \ 16. Jonathan Foote and Don Kimber. 2000. Flycam: Practical panoramic video and automatic camera control. In Proc. ICME’00. IEEE. \ 17. GoPro. 2016. GoPro VR: Mentos and Coke experiment with Untamed Science. (2016). https://www.youtube.com/watch?v=qiKqCWNelB0. \ 18. GoPro. 2017. GoPro Fusion: Relive Reality. (2017). https://www.youtube.com/watch?v=PygsKZXpYrI. \ 19. Jan Gugenheimer, Dennis Wolf, Gabriel Haas, Sebastian Krebs, and Enrico Rukzio. 2016. Swivrchair: A motorized swivel chair to nudge users’ orientation for 360 degree storytelling in virtual reality. In Proc. CHI’16. ACM. \ 20. Chris Harris and Mike Stephens. 1988. A combined corner and edge detector.. In Proc. Alvey Vision Conference, Vol. 15. Manchester, UK, 10–5244. \ 21. Li-wei He, Michael F. Cohen, and David H. Salesin. The Virtual Cinematographer: A Paradigm for Automatic Real-time Camera Control and Directing. In SIGGRAPH ’96. ACM, 217–224. \ 22. Eugenia M Kolasinski. 1995. Simulator Sickness in Virtual Environments. Technical Report. DTIC Document. \ 23. Philipp Krähenbühl, Manuel Lang, Alexander Hornung, and Markus Gross. 2009. A system for retargeting of streaming video. In Proc. TOG’09, Vol. 28. ACM, 126. \ 24. Rainer Lienhart. 2001. Reliable transition detection in videos: A survey and practitioner’s guide. 1, 03 (2001), 469–486. \ 25. Yung-Ta Lin, Yi-Chi Liao, Shan-Yuan Teng, Yi-Ju Chung, Liwei Chan, and Bing-Yu Chen. 2017. Outside-In: Visualizing Out-of-Sight Regions-of-Interest in a 360 Video Using Spatial Picture-in-Picture Previews. In Proc. UIST’17. ACM. \ 26. Feng Liu and Michael Gleicher. 2006. Video retargeting: automating pan and scan. In Proc. MM’06. ACM, 241–250. \ 27. Christopher G Morris. 1992. Academic Press dictionary of science and technology. \ 28. Cuong Nguyen, Stephen DiVerdi, Aaron Hertzmann, and Feng Liu. 2017. Vremiere: In-headset Virtual Reality Video Editing. In Proc. CHI’17. \ 29. Abhishek Ranjan, Rorik Henrikson, Jeremy Birnholtz, Ravin Balakrishnan, and Dana Lee. 2010. Automatic camera control using unobtrusive vision and audio tracking. In Proc. GI’10. Canadian Information Processing Society, 47–54. \ 30. Michael Rubinstein, Ariel Shamir, and Shai Avidan. 2008. Improved seam carving for video retargeting. In Proc. TOG’08, Vol. 27. ACM, 16. \ 31. Ana Serrano, Vincent Sitzmann, Jaime Ruiz-Borau, Gordon Wetzstein, Diego Gutierrez, and Belen Masia. 2017. Movie editing and cognitive event segmentation in virtual reality video. In Proc. SIGGRAPH’17. \ 32. Bobab Studios. 2017. Invasion. (2017). http://www.baobabstudios.com/. \ 33. Yu-Chuan Su and Kristen Grauman. 2017. Making 360◦ Video Watchable in 2D: Learning Videography for Click Free Viewing. In arXiv preprint arXiv:1703.00495. \ 34. Yu-Chuan Su, Dinesh Jayaraman, and Kristen Grauman. 2016. Pano2Vid: Automatic Cinematography for Watching 360◦ Videos. In arXiv preprint arXiv:1612.02335. \ 35. New York Times. 2017a. A Chilly Walk Amid China’s Ice Art. (2017). https: //www.nytimes.com/video/world/asia/100000004868768/ a-chilly-walk-amid-chinas-ice-art.html. \ 36. New York Times. 2017b. Dining at the Met. (2017). https://www.nytimes.com/video/dining/100000004855665/ dining-in-at-the-met-breuer.html. \ 37. The New York Times. 2017c. 36 Hours: Tokyo | The Daily 360. (2017). https://www.youtube.com/watch?v=S6dYUOyx880. \ 38. The New York Times. 2017d. 52 Places to Go: Grand Teton | The Daily 360. (2017). https://www.youtube.com/watch?v=y19GY195Qnc. \ 39. Oculus Story Studio Blog Saschka Unseld. 2015. 5 Lessons Learned While Making Lost. (2015). https://www.oculus.com/story-studio/blog/ 5-lessons-learned-while-making-lost/. \ 40. Jianming Zhang, Stan Sclaroff, Zhe Lin, Xiaohui Shen, Brian Price, and Radomir Mech. 2015. Minimum barrier salient object detection at 80 fps. In Proc. ICCV’15. 1404–1412. \ ",360 video; cinematography,H.5.m,uistf3506-file1.zip,uistf3506-file2.jpg,uistf3506-file3.mp4,,Our system lets users locate important points within 360-degree videos.,,We present three techniques for interactive 360-degree cinematography. Our techniques help users locate important content.,"revision-log.txt \ We have made the following changes promised in the rebuttal: \  \ * Added takeaway messages as described in the rebuttal (1AC, R3, R2) -- see subsection ""Discussion"" \  \ * Describe a Hybrid VO and ARO technique (1AC, R3) -- see section ""Viewpoint-oriented cuts with active reorientation."" \  \ * How do video length and shot length influence behavior? (R3, R2) -- We included the discussion from the rebuttal - see subsection ""Discussion"", and Table 1. \  \ * Added automatic importance labeling and evaluation (R1,R2) - see the ALGORITHM section. \  \ * Added more statistics for selected videos \  \ Additional changes from post-PC meeting comments: \ * add discussion of appropriateness of evaluation metrics to the paper (R1) -- Discussion \ * add discussion of the new Facebook 360 pre-selected direction technique (R1) - RELATED WORK \  \  \ ADDITIONAL CHANGES FOR CAMERA READY: \ The changes we made have currently pushed the manuscript past 6 pages of text (excluding references).  \ --EITHER-- \ We will condense the paper back to 6 pages + refs by the camera ready deadline. \ --OR-- \ We would like to ask the 1AC if extending to 7 papers is possible as this extra content (requested by reviewers) can make the paper significantly stronger. We look forward to a reply so we can prepare either an extended or shortened version in time for the camera-ready deadline.  \  \ OTHER ADDITIONS: \ Acknowledgements, the full video and thumbnail will be added to this submission as well. We will also make the submission accessible. \  \  \  \ ",Amy Pavel,Bjoern Hartmann,FormatComplete,NDSEG,Office of Naval Research,,,,,Aug 9 19:05,
uistf4553,10/24,7,Circuits,9:00:00 AM,10:30:00 AM,4+1,9:00:00 AM,9:20:00 AM,long,long,none,1,701,,,uistf4553,A,Bifröst : Visualizing and Checking Behavior of Embedded Systems across Hardware and Software,William,McGrath,wmcgrath@stanford.edu,uistf4553-paper.pdf,12,letter,Times-Roman,,"Will McGrath, Daniel Drew, Jeremy Warner, Majeed Kazemitabaar, Mitchell Karchemsky, David A Mellis, Bjoern Hartmann","wmcgrath@stanford.edu, ddrew73@berkeley.edu, jeremy.warner@berkeley.edu, majeed@cs.umd.edu, mkarch@berkeley.edu, mellis@berkeley.edu, bjoern@eecs.berkeley.edu",38024,Will,,McGrath,wmcgrath@stanford.edu,HCI Department,Stanford University,Stanford,California,United States,EECS,UC Berkeley,Berkeley,California,United States,60256,Daniel,,Drew,ddrew73@berkeley.edu,Electrical Engineering & Computer Science,"University of California, Berkeley",Berkeley,CA,USA,,,,,,52379,Jeremy,,Warner,jeremy.warner@berkeley.edu,Electrical Engineering & Computer Science,UC Berkeley,Berkeley,California,United States,,,,,,46991,Majeed,,Kazemitabaar,majeed@cs.umd.edu,"Computer Science, Makeability Lab","University of Maryland, College Park",College Park,Maryland,United States,EECS,UC Berkeley,Berkeley,California,United States,60329,Mitchell,,Karchemsky,mkarch@berkeley.edu,EECS,UC Berkeley,Berkeley,California,United States,,,,,,54260,David,A,Mellis,mellis@berkeley.edu,EECS,UC Berkeley,Berkeley,CA,USA,,,,,,5798,Bjoern,,Hartmann,bjoern@eecs.berkeley.edu,Electrical Engineering & Computer Sciences,"University of California, Berkeley",Berkeley,California,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The Maker movement has encouraged more people to start working with electronics and embedded processors. A key challenge in developing and debugging custom embedded systems is understanding their behavior, particularly at the boundary between hardware and software. Existing tools such as step debuggers and logic analyzers only focus on software or hardware, respectively. This paper presents a new development environment designed to illuminate the boundary between embedded code and circuits. Bifröst automatically instruments and captures the progress of the user’s code, variable values, and the electrical and bus activity occurring at the interface between the processor and the circuit it operates in. This data is displayed in a linked visualization that allows navigation through time and program execution, enabling comparisons between variables in code and signals in circuits. Automatic checks can detect low-level hardware configuration and protocol issues, while user-authored checks can test particular application semantics. In an exploratory study with ten participants, we investigated how Bifröst influences debugging workflows.",wmcgrath@stanford.edu,"1. Marzieh Ahmadzadeh, Dave Elliman, and Colin Higgins. 2005. An analysis of patterns of debugging among novice computer science students. ACM SIGCSE Bulletin 37, 3 (2005), 84–88. \ 2. Arduino. 2017. Arduino Zero. https://www.arduino.cc/en/Main/ArduinoBoardZero. (2017). \ 3. Tracey Booth, Simone Stumpf, Jon Bird, and Sara Jones. 2016. Crossed Wires: Investigating the Problems of End-User Developers in a Physical Computing Task. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 3485–3497. DOI: http://dx.doi.org/10.1145/2858036.2858533 \ 4. Michael Bostock, Vadim Ogievetsky, and Jeffrey Heer. 2011. D3 data-driven documents. IEEE transactions on visualization and computer graphics 17, 12 (2011), 2301–2309. \ 5. Cloud9. 2017. Ace - The High Performance Code Editor for the Web. https://ace.c9.io/. (2017). \ 6. Digilent. 2016. Digilent Electronics Explorer Board. (2016). https://reference.digilentinc.com/electronics_ explorer:electronics_explorer Online; accessed 30-March-2016. \ 7. Daniel Drew, Julie L. Newcomb, William McGrath, Filip Maksimovic, David Mellis, and Björn Hartmann. 2016. The Toastboard: Ubiquitous Instrumentation and Automated Checking of Breadboarded Circuits. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 677–686. DOI: http://dx.doi.org/10.1145/2984511.2984566 \ 8. Mireille Ducassé. 1993. A pragmatic survey of automated debugging. Automated and Algorithmic Debugging (1993), 1–15. \ 9. L. Gugerty and G. Olson. 1986. Debugging by Skilled and Novice Programmers. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’86). ACM, New York, NY, USA, 171–174. DOI: http://dx.doi.org/10.1145/22627.22367 \ 10. Björn Hartmann, Scott R Klemmer, and Michael Bernstein. 2005. d. tools: Integrated prototyping for physical interaction design. IEEE Pervasive Computing 4 (2005). \ 11. Joshua Hibschman and Haoqi Zhang. 2015. Unravel: Rapid Web Application Reverse Engineering via Interaction Recording, Source Tracing, and Library Detection. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST ’15). ACM, New York, NY, USA, 270–279. DOI: http://dx.doi.org/10.1145/2807442.2807468 \ 12. Joshua Hibschman and Haoqi Zhang. 2016. Telescope: Fine-Tuned Discovery of Interactive Web UI Feature Implementation. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology. ACM, 233–245. \ 13. Jane Hoffswell, Arvind Satyanarayan, and Jeffrey Heer. 2016. Visual debugging techniques for reactive data visualization. In Computer Graphics Forum, Vol. 35. Wiley Online Library, 271–280. \ 14. Jun Kato, Sean McDirmid, and Xiang Cao. 2012. DejaVu: Integrated Support for Developing Interactive Camera-based Programs. In Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology (UIST ’12). ACM, New York, NY, USA, 189–196. DOI: http://dx.doi.org/10.1145/2380116.2380142 \ 15. Cory Kissinger, Margaret Burnett, Simone Stumpf, Neeraja Subrahmaniyan, Laura Beckwith, Sherry Yang, and Mary Beth Rosson. 2006. Supporting End-user Debugging: What Do Users Want to Know?. In Proceedings of the Working Conference on Advanced Visual Interfaces (AVI ’06). ACM, New York, NY, USA, 135–142. DOI: http://dx.doi.org/10.1145/1133265.1133293 \ 16. Andrew J Ko, Robin Abraham, Laura Beckwith, Alan Blackwell, Margaret Burnett, Martin Erwig, Chris Scafﬁdi, Joseph Lawrance, Henry Lieberman, Brad Myers, and others. 2011. The state of the art in end-user software engineering. ACM Computing Surveys (CSUR) 43, 3 (2011), 21. \ 17. Andrew J. Ko and Brad A. Myers. 2008. Debugging Reinvented: Asking and Answering Why and Why Not Questions About Program Behavior. In Proceedings of the 30th International Conference on Software Engineering (ICSE ’08). ACM, New York, NY, USA, 301–310. DOI: http://dx.doi.org/10.1145/1368088.1368130 \ 18. Andrew J Ko, Brad A Myers, and Htet Htet Aung. 2004. Six learning barriers in end-user programming systems. In Visual Languages and Human Centric Computing, 2004 IEEE Symposium on. IEEE, 199–206. DOI: http://dx.doi.org/10.1109/VLHCC.2004.47 \ 19. Thomas D LaToza and Brad A Myers. 2010. Developers ask reachability questions. In Proceedings of the 32Nd ACM/IEEE International Conference on Software Engineering-Volume 1. ACM, 185–194. \ 20. Bil Lewis. 2003. Debugging backwards in time. arXiv preprint cs/0310016 (2003). \ 21. Tom Lieber, Joel R. Brandt, and Rob C. Miller. 2014. Addressing Misconceptions About Code with Always-on Programming Visualizations. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’14). ACM, New York, NY, USA, 2481–2490. DOI: http://dx.doi.org/10.1145/2556288.2557409 \ 22. D Mellis, Massimo Banzi, David Cuartielles, and Tom Igoe. 2007. Arduino: An open electronic prototyping platform. In Proc. CHI, Vol. 2007. \ 23. Bernhard Plattner and Juerg Nievergelt. 1981. Special feature: Monitoring program execution: A survey. Computer 14, 11 (1981), 76–93. \ 24. Guillaume Pothier, Éric Tanter, and José Piquer. 2007. Scalable omniscient debugging. ACM SIGPLAN Notices 42, 10 (2007), 535–552. \ 25. Markus Stumptner and Franz Wotawa. 1998. A survey of intelligent debugging. AI Communications 11, 1 (1998), 35–51. \ 26. Daniel Tetteroo, Iris Soute, and Panos Markopoulos. 2013. Five key challenges in end-user development for tangible and embodied interaction. In Proceedings of the 15th ACM on International conference on multimodal interaction. ACM, 247–254. DOI: http://dx.doi.org/10.1145/2522848.2522887 \ 27. Twitter. 2017. Bootstrap. http://getbootstrap.com. (2017). \ 28. David Ungar, Henry Lieberman, and Christopher Fry. 1997. Debugging and the Experience of Immediacy. Commun. ACM 40, 4 (April 1997), 38–43. DOI: http://dx.doi.org/10.1145/248448.248457 \ 29. Bret Victor. 2012. Learnable programming. (2012). http://worrydream.com/LearnableProgramming/ \ 30. Bret Victor. 2014. Seeing Spaces. (2014). http://worrydream.com/SeeingSpaces/ \ 31. W Eric Wong, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016. A survey on software fault localization. IEEE Transactions on Software Engineering 42, 8 (2016), 707–740. \ 32. Andreas Zeller. 2009. Why programs fail: a guide to systematic debugging. Elsevier. \ ",embedded systems; debugging; IDE; visualization,H.5.m,uistf4553-file1.zip,,uistf4553-file3.mp4,,,Video figure,This project makes it easier to understand the behavior of embedded systems using automatic instrumentation and a linked visualization that enables comparisons between variables in code and signals in circuits.,"We have submitted a version of our revised paper where  revisions promised in the rebuttal are highlighted in red text for easier checking by the 1AC. \  \ In summary, we changed: \  \ RELATED WORK: \ * Added discussion of additional references in ""Techniques and Interfaces for Software Debugging"" section (R1, R4) \  \ IMPLEMENTATION: \ * Clarified the assumption of a loop() architecture in the Software section on page 7 (R2) \ * Clarified the role of the custom shield PCB (R3) \ * Revised the description of checker functions. (R3) \  \ STUDY: \ * We now describe the study as exploratory in abstract, introduction and main study section and de-emphasize the quantitative comparison of Bifrost/no-Bifrost tasks. (R1,R2,R4) \  \ * Changed the description of T6 to focus on analysis over raw results \  \ * User Study: New discussion of sources of confusion for participants (pg 10) \  \ * Improved labeling of figure 8. \  \ LIMITATIONS: \ * Added a concrete example of the potential impacts of instrumentation delays (R2, R3, R4) \  \ For the final camera-ready version, we also plan to make the following changes: \  \ * Some of the revisions have added length to the paper, and several paragraphs currently extend onto page 11. We will wordsmith and improve figure placement and whitespace usage to make sure that the final text (without references) fits onto 10 pages. We will make the document accessible after these edits. \  \ * We will improve the visual design of some figures, especially Figure 6. \  \ * We are aware of several related papers that will also appear at UIST 2017. We will obtain proper citations for these papers.",Will McGrath,Jeremy Warner,FormatComplete,,TerraSwarm,NSF,"CNS 1505728, IIS 1149799",,,Aug 9 17:39,
uistf3450,10/24,7,Circuits,9:00:00 AM,10:30:00 AM,4+1,9:20:00 AM,9:40:00 AM,long,short,uistf4553,2,702,,,uistf3450,A,CircuitSense: Automatic Sensing of Physical Circuits and Generation of Virtual Circuits to Support Software Tools.,Te-Yen,Wu,teyanwu@gmail.com,uistf3450-paper.pdf,9,letter,Times-Roman,,"Te-Yen Wu, Bryan Wang, Jiun-Yu Lee, Hao-Ping Shen, Yu-Chian Wu, Yu-An Chen, Pin-sung Ku, MING-WEI HSU, Yu-Chih Lin, Mike Y. Chen","teyanwu@gmail.com, b02902096@ntu.edu.tw, b02902083@ntu.edu.tw, gp3wup6m3@gmail.com, andrew90622@gmail.com, tommy149347@gmail.com, scott201222@gmail.com, chad1023@gmail.com, b01902044@ntu.edu.tw, mikechen@csie.ntu.edu.tw",53801,Te-Yen,,Wu,teyanwu@gmail.com,Department of Computer Science & Information Engineering/National Taiwan University/MHCI Lab ,National Taiwan University,Taipei,,R.O.C,,,,,,60238,Bryan,,Wang,b02902096@ntu.edu.tw,Computer Science and Information Engineering,National Taiwan University,Taipei,Taipei,Taiwan,,,,,,71854,Jiun-Yu,,Lee,b02902083@ntu.edu.tw,CSIE,National Taiwan University,Taipei,,Taiwan,,,,,,58984,Hao-Ping,,Shen,gp3wup6m3@gmail.com,National Taiwan University,National Taipei University,Taipei,,Taiwan,,,,,,62796,Yu-Chian,,Wu,andrew90622@gmail.com,"Computer Science And Information Engineering, National Taiwan University,Taipei,Taipei,Taiwan",National Taiwan University,Taipei,,Taiwan,,,,,,62795,Yu-An,,Chen,tommy149347@gmail.com,"Computer Science And Information Engineering, National Taiwan University, Taipei, Taiwan",National Taiwan University,Taipei,,Taiwan,,,,,,63105,Pin-sung,,Ku,scott201222@gmail.com,Computer Science And Information Engineering,National Taiwan University,Taipei,Taipei,Taiwan,,,,,,58465,MING-WEI,,HSU,chad1023@gmail.com,Graduate Institute of Networking and Multimedia ,National Taiwan University,Taipei,Taiwan,Taiwan,,,,,,52041,Yu-Chih,,Lin,b01902044@ntu.edu.tw,Computer Science and Information Engineering,National Taiwan University,Taipei,,Taiwan,,,,,,6300,Mike,Y.,Chen,mikechen@csie.ntu.edu.tw,Computer Science and Information Engineering,National Taiwan University,Taipei,,Taiwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The rise of Maker communities and open-source electronic prototyping platforms have made electronic circuit projects increasingly popular around the world.  \ Although there are software tools that support the debugging and sharing of circuits, they require users to manually create the virtual circuits in software, which can be time-consuming and error-prone. \ We present CircuitSense, a system that automatically recognizes the wires and electronic components placed on breadboards. It uses a combination of passive sensing and active probing to detect and generate the corresponding circuit representation in software in real-time. CircuitSense bridges the gap between the physical and virtual representations of circuits. It enables users to interactively construct and experiment with physical circuits while gaining the benefits of using software tools. It also dramatically simplifies the sharing of circuit designs with online communities.",teyanwu@gmail.com,"1. Arduino Studio. https://www.arduino.cc/en/Main/Donate/ \ 2. AutoDesk Circuits. https://circuits.io \ 3. AutoDesk Eagle. http://www.autodesk.com/products/eagle/overview \ 4. CircuitLab. https://www.circuitlab.com \ 5. Digital IC Tester Model 575A. http://www.bkprecision.com/products/ component-testers/575A-digital-ic-tester.html \ 6. Electronics Explorer:All-in-one USB Oscilloscope,Multimeter and Workstation. http://store.digilentinc.com \ 7. LCR and Impedance Meter - Model LCR45. http://www.peakelec.co.uk/acatalog/lcr45.html \ 8. Maker Faire. http: //www.makerfairerome.eu/en/what-is-maker-faire/ \ 9. Multi-function Tester-TC1. http://vi.vipr.ebaydesc.com/ws/eBayISAPI.dll? ViewItemDescV4&item=322315868253&t= 1483418266000&tid=10&category=25421&seller= kwotop2014&excSoj=1&excTrk=1&lsite=0&ittenable= false&domain=ebay.com&descgauge=1# \ 10. OrCAD. http://www.orcad.com \ 11. PSpice. http://www.pspice.com \ 12. SparkFun. https://www.sparkfun.com/ \ 13. Tracey Booth, Simone Stumpf, Jon Bird, and Sara Jones. 2016. Crossed Wires: Investigating the Problems of End-User Developers in a Physical Computing Task. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 3485–3497. DOI: http://dx.doi.org/10.1145/2858036.2858533 \ 14. D. G. Childers, D. P. Skinner, and R. C. Kemerait. 1977. The cepstrum: A guide to processing. Proc. IEEE 65, 10 (Oct 1977), 1428–1443. DOI: http://dx.doi.org/10.1109/PROC.1977.10747 \ 15. Daniel Drew, Julie L. Newcomb, William McGrath, Filip Maksimovic, David Mellis, and Björn Hartmann. 2016. The Toastboard: Ubiquitous Instrumentation and Automated Checking of Breadboarded Circuits. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 677–686. DOI: http://dx.doi.org/10.1145/2984511.2984566 \ 16. André Knörig, Reto Wettach, and Jonathan Cohen. 2009. Fritzing: A Tool for Advancing Electronic Prototyping for Designers. In Proceedings of the 3rd International Conference on Tangible and Embedded Interaction (TEI ’09). ACM, New York, NY, USA, 351–358. DOI: http://dx.doi.org/10.1145/1517664.1517735 \ 17. M. Tawﬁk, E. Sancristobal, S. Martin, R. Gil, G. Diaz, A. Colmenar, J. Peire, M. Castro, K. Nilsson, J. Zackrisson, L. Hakansson, and I. Gustavsson. 2013. Virtual Instrument Systems in Reality (VISIR) for Remote Wiring and Measurement of Electronic Circuits on Breadboard. IEEE Transactions on Learning Technologies 6, 1 (Jan 2013), 60–72. DOI: http://dx.doi.org/10.1109/TLT.2012.20 \ 18. Nirzaree Vadgama and Jürgen Steimle. 2017. Flexy: Shape-Customizable, Single-Layer, Inkjet Printable Patterns for 1D and 2D Flex Sensing. In Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction (TEI ’17). ACM, New York, NY, USA, 153–162. DOI: http://dx.doi.org/10.1145/3024969.3024989 \ 19. Chiuan Wang, Hsuan-Ming Yeh, Bryan Wang, Te-Yen Wu, Hsin-Ruey Tsai, Rong-Hao Liang, Yi-Ping Hung, and Mike Y. Chen. 2016. CircuitStack: Supporting Rapid Prototyping and Evolution of Electronic Circuits. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 687–695. DOI: http://dx.doi.org/10.1145/2984511.2984527 \ ",Electric Circuits; Component Recognition; Circuit Virtualization,H.5.2. Prototyping,uistf3450-file1.zip,uistf3450-file2.jpg,uistf3450-file3.mp4,,CircuitSense automatically virtualizes the wires and electronic components placed on breadboards.,,CircuitSense automatically virtualizes the wires and electronic components placed on breadboards.,1.  Add a section about user evaluation \ 2. Add a paragraph about parallel circuits in Discussion \ 3. Revise some grammar mistakes \ ,Te-Yen Wu,Bryan Wang,FormatComplete,,,,,,,Aug 8 3:11,
uistf2357,10/24,7,Circuits,9:00:00 AM,10:30:00 AM,4+1,9:40:00 AM,10:00:00 AM,long,long,uistf3450,3,703,Honorable Mention,,uistf2357,A,Scanalog: Interactive Design and Debugging of Analog Circuits with Programmable Hardware,Evan,Strasnick,estrasnick@gmail.com,uistf2357-paper.pdf,10,letter,,,"Evan N Strasnick, Maneesh Agrawala, Sean Follmer","estrasni@stanford.edu, maneesh@cs.stanford.edu, sfollmer@stanford.edu",60322,Evan,N,Strasnick,estrasni@stanford.edu,"Department of Computer Science, Stanford University, Stanford, CA, United States",Stanford University,Stanford,California,United States,,,,,,53076,Maneesh,,Agrawala,maneesh@cs.stanford.edu,Computer Science,Stanford University,Stanford,California,United States,,,,,,15323,Sean,,Follmer,sfollmer@stanford.edu,Mechanical Engineering,Stanford University,Palo Alto,California,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Analog circuit design is a complex, error-prone task in which the processes of gathering observations, formulating reasonable hypotheses, and manually adjusting the circuit raise significant barriers to an iterative workflow. We present Scanalog, a tool built on programmable analog hardware that enables users to rapidly explore different circuit designs using direct manipulation, and receive immediate feedback on the resulting behaviors without manual assembly, calculation, or probing. Users can interactively tune modular signal transformations on hardware with real inputs, while observing real-time changes at all points in the circuit. They can create custom unit tests and assertions to detect potential issues. We describe three interactive applications demonstrating the expressive potential of Scanalog. In an informal evaluation, users successfully conditioned analog sensors and described Scanalog as both enjoyable and easy to use. ",estrasni@stanford.edu,"1. Yoh Akiyama and Homei Miyashita. 2014. Projectron Mapping: The Exercise and Extension of Augmented Workspaces for Learning Electronic Modeling Through Projection Mapping. In Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST’14 Adjunct). ACM, New York, NY, USA, 57–58. DOI: http://dx.doi.org/10.1145/2658779.2659113 \ 2. Altium. 2017. Altium Designer 17 Overview. (2017). http://www.altium.com/altium-designer/overview. \ 3. Anadigm. 2017a. AN231E04 dpASP. (2017). http://anadigm.com/an231e04.asp. \ 4. Anadigm. 2017b. Anadigm Designer2 Software. (2017). http://anadigm.com/anadigmdesigner2.asp. \ 5. Autodesk. 2017. EAGLE PCB Design and Schematic Software. (2017). http://www.autodesk.com/products/eagle/overview. \ 6. Ayah Bdeir. 2009. Electronics As Material: LittleBits. In Proceedings of the 3rd International Conference on Tangible and Embedded Interaction (TEI ’09). ACM, New York, NY, USA, 397–400. DOI: http://dx.doi.org/10.1145/1517664.1517743 \ 7. Tracey Booth, Simone Stumpf, Jon Bird, and Sara Jones. 2016. Crossed Wires: Investigating the Problems of End-User Developers in a Physical Computing Task. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 3485–3497. DOI: http://dx.doi.org/10.1145/2858036.2858533 \ 8. Joshua Chan, Tarun Pondicherry, and Paulo Blikstein. 2013. LightUp: An Augmented, Learning Platform for Electronics. In Proceedings of the 12th International Conference on Interaction Design and Children (IDC ’13). ACM, New York, NY, USA, 491–494. DOI: http://dx.doi.org/10.1145/2485760.2485812 \ 9. Bettina Conradi, Verena Lerch, Martin Hommer, Robert Kowalski, Ioanna Vletsou, and Heinrich Hussmann. 2011. Flow of Electrons: An Augmented Workspace for Learning Physical Computing Experientially. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces (ITS ’11). ACM, New York, NY, USA, 182–191. DOI: http://dx.doi.org/10.1145/2076354.2076389 \ 10. Cypress Semiconductor Corporation. 2017. PSoC Software. (2017). http://www.cypress.com/products/psoc-software. \ 11. BitScope Designs. 2017. BitScope Micro Model 5. (2017). http://www.bitscope.com/product/BS05/. \ 12. Daniel Drew, Julie L. Newcomb, William McGrath, Filip Maksimovic, David Mellis, and Björn Hartmann. 2016. The Toastboard: Ubiquitous Instrumentation and Automated Checking of Breadboarded Circuits. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 677–686. DOI: http://dx.doi.org/10.1145/2984511.2984566 \ 13. Adam Fourney and Michael Terry. 2012. PICL: Portable In-circuit Learner. In Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology (UIST ’12). ACM, New York, NY, USA, 569–578. DOI: http://dx.doi.org/10.1145/2380116.2380188 \ 14. Saul Greenberg and Chester Fitchett. 2001. Phidgets: Easy Development of Physical Interfaces Through Physical Widgets. In Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology (UIST ’01). ACM, New York, NY, USA, 209–218. DOI:http://dx.doi.org/10.1145/502348.502388 \ 15. Björn Hartmann, Leith Abdulla, Manas Mittal, and Scott R. Klemmer. 2007. Authoring Sensor-based Interactions by Demonstration with Direct Manipulation and Pattern Recognition. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’07). ACM, New York, NY, USA, 145–154. DOI: http://dx.doi.org/10.1145/1240624.1240646 \ 16. National Instruments. 2017. LabVIEW System Design Software. (2017). http://www.ni.com/labview/. \ 17. André Knörig, Reto Wettach, and Jonathan Cohen. 2009. Fritzing: A Tool for Advancing Electronic Prototyping for Designers. In Proceedings of the 3rd International Conference on Tangible and Embedded Interaction (TEI ’09). ACM, New York, NY, USA, 351–358. DOI: http://dx.doi.org/10.1145/1517664.1517735 \ 18. Johnny C. Lee, Daniel Avrahami, Scott E. Hudson, Jodi Forlizzi, Paul H. Dietz, and Darren Leigh. 2004. The Calder Toolkit: Wired and Wireless Components for Rapidly Prototyping Interactive Devices. In Proceedings of the 5th Conference on Designing Interactive Systems: Processes, Practices, Methods, and Techniques (DIS ’04). ACM, New York, NY, USA, 167–175. DOI: http://dx.doi.org/10.1145/1013115.1013139 \ 19. David A. Mellis, Leah Buechley, Mitchel Resnick, and Björn Hartmann. 2016. Engaging Amateurs in the Design, Fabrication, and Assembly of Electronic Devices. In Proceedings of the 2016 ACM Conference on Designing Interactive Systems (DIS ’16). ACM, New York, NY, USA, 1270–1281. DOI: http://dx.doi.org/10.1145/2901790.2901833 \ 20. Laurence William Nagel and Donald O Pederson. 1973. SPICE: Simulation program with integrated circuit emphasis. Electronics Research Laboratory, College of Engineering, University of California. \ 21. Yoichi Ochiai. 2010. The Visible Electricity Device: Visible Breadboard. In ACM SIGGRAPH 2010 Posters (SIGGRAPH ’10). ACM, New York, NY, USA, Article 98, 1 pages. DOI: http://dx.doi.org/10.1145/1836845.1836950 \ 22. Janneke Verhaegh, Willem Fontijn, and Jettie Hoonhout. 2007. TagTiles: Optimal Challenge in Educational Electronics. In Proceedings of the 1st International Conference on Tangible and Embedded Interaction (TEI ’07). ACM, New York, NY, USA, 187–190. DOI: http://dx.doi.org/10.1145/1226969.1227008 \ 23. Nicolas Villar, James Scott, Steve Hodges, Kerry Hammil, and Colin Miller. 2012. .NET Gadgeteer: A Platform for Custom Devices. Springer Berlin Heidelberg, Berlin, Heidelberg, 216–233. DOI: http://dx.doi.org/10.1007/978-3-642-31205-2_14 \ 24. Chiuan Wang, Hsuan-Ming Yeh, Bryan Wang, Te-Yen Wu, Hsin-Ruey Tsai, Rong-Hao Liang, Yi-Ping Hung, and Mike Y. Chen. 2016. CircuitStack: Supporting Rapid Prototyping and Evolution of Electronic Circuits. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 687–695. DOI: http://dx.doi.org/10.1145/2984511.2984527 \ ",Scanalog; Scanalogue; design tools; debugging; analog circuits; electronics; prototyping; FPAA; Field Programmable Analog Array; programmable analog hardware,H.5.m.,uistf2357-file1.zip,,uistf2357-file3.mp4,,,,Scanalog uses programmable hardware to facilitate the design and debugging of analog circuits by enabling users to directly manipulate circuit parameters while observing internal signals in real time.,"-Revised related work section to better show how our contributions relate to existing systems \ -Added an explanation of the possible impact that Scanalogue could have on various non-research domains \ -Added more specific implementation details \ -Added additional user feedback and context to the evaluation section \ -Added a description of the modules implemented and our rationale for choosing them \ -Included an explanation that new modules can be added, and that individual component-level design can be enabled \ -Tightened up the introduction and related work sections \ -Fleshed out conclusion section \ -Improved most figures \  \ ",Evan Strasnick,Sean Follmer,FormatComplete,,,,,,,Aug 8 19:06,
uistf3567,10/24,7,Circuits,9:00:00 AM,10:30:00 AM,4+1,10:00:00 AM,10:20:00 AM,long,long,uistf2357,4,704,,,uistf3567,A,Trigger-Action-Circuits: Leveraging Generative Design to Enable Novices to Design and Build Circuitry,Fraser,Anderson,fraser.anderson@autodesk.com,uistf3567-paper.pdf,12,letter,,,"Fraser Anderson, Tovi Grossman, George Fitzmaurice","fraser.anderson@autodesk.com, tovi.grossman@autodesk.com, George.Fitzmaurice@autodesk.com",25365,Fraser,,Anderson,fraser.anderson@autodesk.com,,Autodesk Research,Toronto,Ontario,Canada,,,,,,1476,Tovi,,Grossman,tovi.grossman@autodesk.com,,Autodesk Research,Toronto,Ontario,Canada,,,,,,1229,George,,Fitzmaurice,George.Fitzmaurice@autodesk.com,,Autodesk Research,Toronto,Ontario,Canada,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The dramatic decrease in price and increase in availability of hobbyist electronics has led to a wide array of embedded and interactive devices. While electronics have become more widespread, developing and prototyping the required circuitry for these devices is still difficult, requiring knowledge of electronics, components, and programming. In this paper, we present Trigger-Action-Circuits (TAC), an interactive system that leverages generative design to produce circuitry, firmware, and assembly instructions, based on high-level, behavioural descriptions. TAC is able to generate multiple candidate circuits from a behavioural description, giving the user a number of alternative circuits that may be best suited to their use case (e.g., based on cost, component availability or ease of assembly). The generated circuitry uses off-the-shelf, commodity electronics, not specialized hardware components, enabling scalability and extensibility. TAC supports a range of common components and behaviors that are frequently required for prototyping electronic circuits. A user study demonstrated that TAC helps users avoid problems encountered during circuit design and assembly, with users completing their circuits significantly faster than with traditional methods.",fraser.anderson@autodesk.com,"1. Martin Philip Bendsoe and Ole Sigmund. 2013. Topology optimization: theory, methods, and applications. Springer Science & Business Media. Retrieved March 10, 2017 from https://books.google.ca/books?hl=en&lr=&id=ZCjsCA AAQBAJ&oi=fnd&pg=PA1&dq=topology+optimizati on&ots=y0ffg_7F5J&sig=q5YI0tzfMgsMoph42afZ0b p0HKs \ 2. Tracey Booth, Simone Stumpf, Jon Bird, and Sara Jones. 2016. Crossed Wires: Investigating the Problems of End-User Developers in a Physical Computing Task. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16), ACM, 3485–3497. http://doi.org/10.1145/2858036.2858533 \ 3. Luisa Caldas. 2008. Generation of energy-efficient architecture solutions applying GENE_ARCH: An evolution-based generative design system. Advanced Engineering Informatics 22, 1: 59–70. \ 4. Amaresh Chakrabarti, Kristina Shea, Robert Stone, et al. 2011. Computer-Based Design Synthesis Research: An Overview. Journal of Computing and Information Science in Engineering 11, 2: 021003-021003-10. http://doi.org/10.1115/1.3593409 \ 5. Scott C. Chase. 2005. Generative design tools for novice designers: Issues for selection. Automation in Construction 14, 6: 689–698. \ 6. Anind K. Dey, Gregory D. Abowd, and Daniel Salber. 2001. A Conceptual Framework and a Toolkit for Supporting the Rapid Prototyping of Context-aware Applications. Hum.-Comput. Interact. 16, 2: 97–166. http://doi.org/10.1207/S15327051HCI16234_02 \ 7. Anind K. Dey, Raffay Hamid, Chris Beckmann, Ian Li, and Daniel Hsu. 2004. A CAPpella: Programming by Demonstration of Context-aware Applications. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’04), ACM, 33– 40. http://doi.org/10.1145/985692.985697 \ 8. Anind K. Dey, Timothy Sohn, Sara Streng, and Justin Kodama. 2006. iCAP: Interactive Prototyping of Context-Aware Applications. In Pervasive Computing, Kenneth P. Fishkin, Bernt Schiele, Paddy Nixon and Aaron Quigley (eds.). Springer Berlin Heidelberg, 254–271. Retrieved November 12, 2014 from http://link.springer.com/chapter/10.1007/11748625_16 \ 9. Daniel Drew, Julie L. Newcomb, William McGrath, Filip Maksimovic, David Mellis, and Björn Hartmann. 2016. The Toastboard: Ubiquitous Instrumentation and Automated Checking of Breadboarded Circuits. Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16), ACM, 677–686. http://doi.org/10.1145/2984511.2984566 \ 10. Bruno Ferreira and António Leitão. 2015. Generative Design for Building Information Modeling. Real TimeProceedings of the 33rd eCAADe Conference, 635– 644. Retrieved March 10, 2017 from http://papers.cumincad.org/data/works/att/ecaade2015_ 118.content.pdf \ 11. S. Greenberg and C. Fitchett. 2001. Phidgets: easy development of physical interfaces through physical widgets. ACM, 209–218. \ 12. Björn Hartmann, Leith Abdulla, Manas Mittal, and Scott R. Klemmer. 2007. Authoring Sensor-based Interactions by Demonstration with Direct Manipulation and Pattern Recognition. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’07), ACM, 145–154. http://doi.org/10.1145/1240624.1240646 \ 13. Björn Hartmann, Scott R. Klemmer, Michael Bernstein, et al. 2006. Reflective Physical Prototyping Through Integrated Design, Test, and Analysis. Proceedings of the 19th Annual ACM Symposium on User Interface Software and Technology (UIST ’06), ACM, 299–308. http://doi.org/10.1145/1166253.1166300 \ 14. Steve Hodges, Nicolas Villar, Nicholas Chen, et al. 2014. Circuit Stickers: Peel-and-stick Construction of Interactive Electronic Prototypes. Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI ’14), ACM, 1743–1746. http://doi.org/10.1145/2556288.2557150 \ 15. Steven Houben, Connie Golsteijn, Sarah Gallacher, et al. 2016. Physikit: Data Engagement Through Physical Ambient Visualizations in the Home. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16), ACM, 1608–1619. http://doi.org/10.1145/2858036.2858059 \ 16. Jan Humble, Andy Crabtree, Terry Hemmings, et al. 2003. “Playing with the Bits” User-Configuration of Ubiquitous Domestic Environments. In UbiComp 2003: Ubiquitous Computing, Anind K. Dey, Albrecht Schmidt and Joseph F. McCarthy (eds.). Springer Berlin Heidelberg, 256–263. Retrieved November 14, 2014 from http://link.springer.com/chapter/10.1007/978-3-54039653-6_20 \ 17. Yoshihiro Kawahara, Steve Hodges, Benjamin S. Cook, Cheng Zhang, and Gregory D. Abowd. 2013. Instant Inkjet Circuits: Lab-based Inkjet Printing to Support Rapid Prototyping of UbiComp Devices. Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp ’13), ACM, 363–372. http://doi.org/10.1145/2493432.2493486 \ 18. Majeed Kazemitabaar, Jason McPeak, Alexander Jiao, Liang He, Thomas Outing, and Jon E. Froehlich. 2017. MakerWear: A Tangible Approach to Interactive Wearable Creation for Children. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17), ACM, 133–145. http://doi.org/10.1145/3025453.3025887 \ 19. Sivam Krish. 2011. A practical generative design method. Computer-Aided Design 43, 1: 88–100. \ 20. David Ledo, Fraser Anderson, Ryan Schmidt, Lora Oehlberg, Saul Greenberg, and Tovi Grossman. 2017. Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17), ACM, 2583–2593. http://doi.org/10.1145/3025453.3025652 \ 21. Joanne Lo, Cesar Torres, Isabel Yang, et al. 2016. Aesthetic Electronics: Designing, Sketching, and Fabricating Circuits Through Digital Exploration. Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16), ACM, 665–676. http://doi.org/10.1145/2984511.2984579 \ 22. Don MacMillen, Raul Camposano, D. Hill, and Thomas W. Williams. 2000. An industrial view of electronic design automation. IEEE transactions on computer-aided design of integrated circuits and systems 19, 12: 1428–1448. \ 23. Will McGrath, Daniel Drew, Jeremy Warner, et al. 2017. Bifrost: An Interface for Visualizing and Debugging the Behavior of Embedded Systems. Proceedings of ACM User Interface and Software Technology, ACM. \ 24. David A. Mellis, Leah Buechley, Mitchel Resnick, and Björn Hartmann. 2016. Engaging Amateurs in the Design, Fabrication, and Assembly of Electronic Devices. Proceedings of the 2016 ACM Conference on Designing Interactive Systems (DIS ’16), ACM, 1270– 1281. http://doi.org/10.1145/2901790.2901833 \ 25. Raf Ramakers, Fraser Anderson, Tovi Grossman, and George Fitzmaurice. 2016. RetroFab: A Design Tool for Retrofitting Physical Interfaces Using Actuators, Sensors and 3D Printing. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16), ACM, 409–419. http://doi.org/10.1145/2858036.2858485 \ 26. Raf Ramakers, Kashyap Todi, and Kris Luyten. 2015. PaperPulse: An Integrated Approach for Embedding Electronics in Paper Designs. ACM SIGGRAPH 2015 Posters (SIGGRAPH ’15), ACM, 9:1–9:1. http://doi.org/10.1145/2787626.2792650 \ 27. Rohit Ramesh, Richard Lin, Antonio Iannopollo, Alberto Sangiovanni-Vincentelli, Björn Hartmann, and Prabal Dutta. 2017. Turning Coders into Makers: The Promise of Embedded Design Generation. Proceedings of the 1st Annual ACM Symposium on Computational Fabrication (SCF ’17), ACM, 4:1–4:10. http://doi.org/10.1145/3083157.3083159 \ 28. Mitchel Resnick, Brad Myers, Kumiyo Nakakoji, et al. 2005. Design principles for tools to support creative thinking. \ 29. Daniel Salber, Anind K. Dey, and Gregory D. Abowd. 1999. The Context Toolkit: Aiding the Development of Context-enabled Applications. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’99), ACM, 434–441. http://doi.org/10.1145/302979.303126 \ 30. Adrian Thompson, Paul Layzell, and Ricardo Salem Zebulum. 1999. Explorations in design space: Unconventional electronics design through artificial evolution. IEEE Transactions on Evolutionary Computation 3, 3: 167–196. \ 31. Vesselin K. Vassilev, Dominic Job, and Julian F. Miller. 2000. Towards the automatic design of more efficient digital circuits. Evolvable Hardware, 2000. Proceedings. The Second NASA/DoD Workshop on, IEEE, 151–160. Retrieved March 25, 2017 from http://ieeexplore.ieee.org/abstract/document/869353/ \ 32. Loutfouz Zaman, Wolfgang Stuerzlinger, Christian Neugebauer, et al. 2015. Gem-ni: A system for creating and managing alternatives in generative design. Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, ACM, 1201–1210. Retrieved March 10, 2017 from http://dl.acm.org/citation.cfm?id=2702398 \ ",Prototyping; Circuitry; Generative Design; Circuit Generation,H.5.2,uistf3567-file1.docx,uistf3567-file2.jpg,uistf3567-file3.mp4,,"Using a simple visual-programming-language, Trigger-Action-Circuits enables novices to design circuitry. From the high-level description, the system synthesizes a number of circuits that match their desired behaviour, and generates the necessary firmware and interactive assembly instructions.",,"Using a simple visual-programming-language, Trigger-Action-Circuits enables novices to design circuitry. From the high-level description, the system synthesizes a number of circuits that match their desired behaviour, and generates the necessary firmware and interactive assembly instructions.","Added relevant/requested references to better contrast the work with prior work. \  \ Compressed description of authoring in workflow to minimize perception that the VPL is a contribution. \  \ Removed claim that IFTTT does not support custom hardware devices. \  \ Renamed Future work section to 'Limitations and Future work' \  \ Added limitation on complex software authoring concepts, and the inability to reflect firmware changes back in the visual interface. \  \ Added benefits of generative design approach, as well as challenges in enabling this approach for more expert users (e.g., supporting equivalent circuit exchange). \  \ Added limitation regarding learnability of generated circuits and trade-offs that need to be explored. \  \ Added limitation of circuit explorer and need for more work in exploring generative design data sets. \  \ Minor changes to wording to meet space limitations. \ ",Fraser Anderson,Tovi Grossman,FormatComplete,,,,,,,Jul 24 16:42,
uistf3948,10/24,7,Circuits,9:00:00 AM,10:30:00 AM,4+1,10:20:00 AM,10:30:00 AM,short,short,uistf3567,5,705,,"shouldn't have second paper in a session short

please move to the end --KZG",uistf3948,A,CurrentViz: Sensing and Visualizing Electric Current of Breadboarded Circuits,Hao-Ping,Shen,gp3wup6m3@gmail.com,uistf3948-paper.pdf,7,letter,,,"Te-Yen Wu, Hao-Ping Shen, Yu-Chian Wu, Yu-An Chen, Pin-sung Ku, MING-WEI HSU, Jun-You Liu, Yu-Chih Lin, Mike Y. Chen","teyanwu@gmail.com, gp3wup6m3@gmail.com, andrew90622@gmail.com, tommy149347@gmail.com, scott201222@gmail.com, chad1023@gmail.com, junyouliu9@gmail.com, b01902044@ntu.edu.tw, mikechen@csie.ntu.edu.tw",53801,Te-Yen,,Wu,teyanwu@gmail.com,Department of Computer Science & Information Engineering/National Taiwan University/MHCI Lab ,National Taiwan University,Taipei,,R.O.C,,,,,,58984,Hao-Ping,,Shen,gp3wup6m3@gmail.com,National Taiwan University,National Taipei University,Taipei,,Taiwan,,,,,,62796,Yu-Chian,,Wu,andrew90622@gmail.com,"Computer Science And Information Engineering, National Taiwan University,Taipei,Taipei,Taiwan",National Taiwan University,Taipei,,Taiwan,,,,,,62795,Yu-An,,Chen,tommy149347@gmail.com,"Computer Science And Information Engineering, National Taiwan University, Taipei, Taiwan",National Taiwan University,Taipei,,Taiwan,,,,,,63105,Pin-sung,,Ku,scott201222@gmail.com,Computer Science And Information Engineering,National Taiwan University,Taipei,Taipei,Taiwan,,,,,,58465,MING-WEI,,HSU,chad1023@gmail.com,Graduate Institute of Networking and Multimedia ,National Taiwan University,Taipei,Taiwan,Taiwan,,,,,,73397,Jun-You,,Liu,junyouliu9@gmail.com,Computer Science & Information Engineering Mobile HCI lab,National Taiwan University,Taipei,,Taiwan,,,,,,52041,Yu-Chih,,Lin,b01902044@ntu.edu.tw,Computer Science and Information Engineering,National Taiwan University,Taipei,,Taiwan,,,,,,6300,Mike,Y.,Chen,mikechen@csie.ntu.edu.tw,Computer Science and Information Engineering,National Taiwan University,Taipei,,Taiwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Electric current and voltage are fundamental to learning, understanding, and debugging circuits. Although both can be measured using tools such as multimeters and oscilloscopes, electric current is much more difficult to measure because users have to unplug parts of a circuit and then insert the measuring tools in serial. Furthermore, users need to restore the circuits back to its original state after measurements have been taken. In practice, this cumbersome process poses a formidable barrier to knowing how current flows throughout a circuit. We present CurrentViz, a system that can sense and visualize the electric current flowing through a circuit, which helps users quickly understand otherwise invisible circuit behavior. It supports fully automatic, ubiquitous, and real-time collection of amperage information of breadboarded circuits. It also supports visualization of the amperage data on a circuit schematic to provide an intuitive view into the current state of a circuit. \ ",gp3wup6m3@gmail.com,"1. AutoDesk Circuits. https://circuits.io \ 2. CircuitLab. https://www.circuitlab.com \ 3. Fritzing. http://fritzing.org/ \ 4. OrCAD. http://www.orcad.com \ 5. PSpice. http://www.pspice.com \ 6. Yoh Akiyama and Homei Miyashita. 2014. Projectron Mapping: The Exercise and Extension of Augmented Workspaces for Learning Electronic Modeling Through Projection Mapping. In Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST’14 Adjunct). ACM, New York, NY, USA, 57–58. DOI: http://dx.doi.org/10.1145/2658779.2659113 \ 7. Ayah Bdeir and Paul Rothman. 2012. Electronics As Material: LittleBits. In Proc. of TEI ’12. ACM, New York, NY, USA, 371–374. DOI: http://dx.doi.org/10.1145/2148131.2148220 \ 8. Joshua Chan, Tarun Pondicherry, and Paulo Blikstein. 2013. LightUp: An Augmented, Learning Platform for Electronics. In Proceedings of the 12th International Conference on Interaction Design and Children (IDC ’13). ACM, New York, NY, USA, 491–494. DOI: http://dx.doi.org/10.1145/2485760.2485812 \ 9. Bettina Conradi, Martin Hommer, and Robert Kowalski. 2010. From Digital to Physical: Learning Physical Computing on Interactive Surfaces. In ACM International Conference on Interactive Tabletops and Surfaces (ITS ’10). ACM, New York, NY, USA, 249–250. DOI: http://dx.doi.org/10.1145/1936652.1936700 \ 10. Kayla DesPortes, Aditya Anupam, Neeti Pathak, and Betsy DiSalvo. 2016. BitBlox: A Redesign of the Breadboard. In Proceedings of the The 15th International Conference on Interaction Design and Children (IDC ’16). ACM, New York, NY, USA, 255–261. DOI: http://dx.doi.org/10.1145/2930674.2930708 \ 11. Daniel Drew, Julie L. Newcomb, William McGrath, Filip Maksimovic, David Mellis, and Björn Hartmann. 2016. The Toastboard: Ubiquitous Instrumentation and Automated Checking of Breadboarded Circuits. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 677–686. DOI: http://dx.doi.org/10.1145/2984511.2984566 \ 12. Yoichi Ochiai. 2014. Visible Breadboard: System for Dynamic, Programmable, and Tangible Circuit Prototyping with Visible Electricity. In Virtual, Augmented and Mixed Reality. Applications of Virtual and Augmented Reality. Lecture Notes in Computer Science, Vol. 8526. 73–84. \ ",Electric Current; Breadboarded Circuit; Debugging; Visualization;,H.5.2. Prototyping,uistf3948-file1.zip,uistf3948-file2.jpg,uistf3948-file3.mp4,,CurrentViz device and accompanying software which can sense and \ visualize the electric current flowing.,,"CurrentViz is a system that can sense and visualize the electric current flowing through a circuit, which helps users quickly understand otherwise invisible circuit behavior. \ ",1. Add a section about Informal User Feedback \ 2. Change original examples to another 3 which can better demonstrate the usefulness of current \ 3. Optimizing space usage of figures and condensing the MUX discussion \ 4. Revise some grammar mistakes,Jun-You Liu,Te-Yen Wu,FormatComplete,,,,,,,Aug 8 6:38,
uistf4922,10/24,8,"Pointing, Pens & Text",9:00:00 AM,10:30:00 AM,4,9:00:00 AM,9:20:00 AM,long,long,none,1,801,,,uistf4922,A,"Reflector: Distance-Independent, Private Pointing on a Reflective Screen",Byungjoo,Lee,www.kiml.org@gmail.com,uistf4922-paper.pdf,14,letter,,,"JONG-IN LEE, Sunjun Kim, Masaaki Fukumoto, Byungjoo Lee","yi-jong-in@kaist.ac.kr, kuaa.net@gmail.com, fukumoto@microsoft.com, www.kiml.org@gmail.com",71846,JONG-IN,,LEE,yi-jong-in@kaist.ac.kr,Graduate School of Culture Technology,KAIST,Daejeon,Daejeon Metropolitan City,Republic of Korea,,,,,,15397,Sunjun,,Kim,kuaa.net@gmail.com,,KAIST,Daejeon,,"Korea, Republic of",,,,,,45775,Masaaki,,Fukumoto,fukumoto@microsoft.com,,Microsoft Research,Beijing,,China,,,,,,51724,Byungjoo,,Lee,www.kiml.org@gmail.com,Graduate School of Culture Technology,KAIST,Daejeon,,"Korea, Republic of",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Reflector is a novel direct pointing method that utilizes hidden design space on reflective screens. By aligning a part of the user’s onscreen reflection with objects rendered on the screen, Reflector enables (1) distance-independent and (2) private pointing on commodity screens. Reflector can be implemented easily in both desktop and mobile conditions through a single camera installed at the edge of the screen. Reflector’s pointing performance was compared to today’s major direct input devices: eye trackers and touchscreens. We demonstrate that Reflector allows the user to point more reliably, regardless of distance from the screen, compared to an eye tracker. Further, due to the private nature of an onscreen reflection, Reflector shows a shoulder surfing success rate 20 times lower than that of touchscreens for the task of entering a 4-digit PIN.",byungjoo.lee@kaist.ac.kr,"1. Pär-Anders Albinsson and Shumin Zhai. 2003. High precision touch screen interaction. In Proceedings of the SIGCHI conference on Human factors in computing systems. ACM, 105–112. \ 2. Fraser Anderson, Tovi Grossman, Justin Matejka, and George Fitzmaurice. 2013. YouMove: enhancing movement training with an augmented reality mirror. In Proceedings of the 26th annual ACM symposium on User interface software and technology. ACM, 311–320. \ 3. Amartya Banerjee, Jesse Burstyn, Audrey Girouard, and Roel Vertegaal. 2011. Pointable: an in-air pointing technique to manipulate out-of-reach targets on tabletops. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces. ACM, 11–20. \ 4. Florian Block, Daniel Wigdor, Brenda Caldwell Phillips, Michael S Horn, and Chia Shen. 2012. FlowBlocks: a multi-touch ui for crowd interaction. In Proceedings of the 25th annual ACM symposium on User interface software and technology. ACM, 497–508. \ 5. Gary Bradski and Adrian Kaehler. 2008. Learning OpenCV: Computer vision with the OpenCV library. "" O’Reilly Media, Inc."". \ 6. Kjell Brunnström, Börje Andrén, Zacharias Konstantinides, and Lukas Nordström. 2007. Visual ergonomic aspects of computer displays: glossy screens and angular dependence. Proc. of SPIE-IS&T Human Vision and Electronic Imaging XII 6492 (2007). \ 7. Kjell Brunnström, Katarina Josefsson, and Börje Andrén. 2008. The effects of glossy screens on the acceptance of ﬂat-panel displays. Journal of the Society for Information Display 16, 10 (2008), 1041–1049. \ 8. Marcus Carter, Joshua Newn, Eduardo Velloso, and Frank Vetere. 2015. Remote gaze and gesture tracking on the microsoft kinect: Investigating the role of feedback. In Proceedings of the Annual Meeting of the Australian Special Interest Group for Computer Human Interaction. ACM, 167–176. \ 9. Géry Casiez, Nicolas Roussel, and Daniel Vogel. 2012. 1C ﬁlter: a simple speed-based low-pass ﬁlter for noisy input in interactive systems. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2527–2530. \ 10. Ishan Chatterjee, Robert Xiao, and Chris Harrison. 2015. Gaze+ Gesture: Expressive, Precise and Targeted Free-Space Interactions. In Proceedings of the 2015 ACM on International Conference on Multimodal Interaction. ACM, 131–138. \ 11. Michael Chau and Margrit Betke. 2005. Real time eye tracking and blink detection with usb cameras. \ 12. Alexander De Luca, Marian Harbach, Emanuel von Zezschwitz, Max-Emanuel Maurer, Bernhard Ewald Slawik, Heinrich Hussmann, and Matthew Smith. 2014. Now you see me, now you don’t: protecting smartphone authentication from shoulder surfers. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2937–2946. \ 13. Alexander De Luca, Emanuel Von Zezschwitz, and Heinrich Hußmann. 2009. Vibrapass: secure authentication based on shared lies. In Proceedings of the SIGCHI conference on human factors in computing systems. ACM, 913–916. \ 14. Alexander De Luca, Emanuel Von Zezschwitz, Ngo Dieu Huong Nguyen, Max-Emanuel Maurer, Elisa Rubegni, Marcello Paolo Scipioni, and Marc Langheinrich. 2013. Back-of-device authentication on smartphones. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2389–2398. \ 15. Paul Dietz and Darren Leigh. 2001. DiamondTouch: a multi-user touch technology. In Proceedings of the 14th annual ACM symposium on User interface software and technology. ACM, 219–226. \ 16. Sarah A Douglas, Arthur E Kirkpatrick, and I Scott MacKenzie. 1999. Testing pointing device performance and user assessment with the ISO 9241, Part 9 standard. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems. ACM, 215–222. \ 17. Augusto Esteves, Eduardo Velloso, Andreas Bulling, and Hans Gellersen. 2015. Orbits: Gaze interaction for smart watches using smooth pursuit eye movements. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology. ACM, 457–466. \ 18. Anna Maria Feit, Shane Williams, Arturo Toledo, Ann Paradiso, Harish S. Kulkarni, Shaun Kane, and Meredith Ringel Morris. 2017. Toward Everyday Gaze Input: Accuracy and Precision of Eye Tracking and Implications for Design. (May 2017). \ 19. Alexandre François, Elaine Kang, and Umberto Malesci. 2002. A handheld virtual mirror. In ACM SIGGRAPH 2002 conference abstracts and applications. ACM, 140–140. \ 20. Alexandre RJ François and E-YE Kang. 2003. A handheld mirror simulation. In Multimedia and Expo, 2003. ICME’03. Proceedings. 2003 International Conference on, Vol. 2. IEEE, II–745. \ 21. Kaori Fujinami, Fahim Kawsar, and Tatsuo Nakajima. 2005. AwareMirror: a personalized display using a mirror. In International Conference on Pervasive Computing. Springer, 315–332. \ 22. Mayank Goel, Jacob Wobbrock, and Shwetak Patel. 2012. GripSense: using built-in sensors to detect hand posture and pressure on commodity mobile phones. In Proceedings of the 25th annual ACM symposium on User interface software and technology. ACM, 545–554. \ 23. Alix Goguey, Daniel Vogel, Fanny Chevalier, Thomas Pietrzak, Nicolas Roussel, and Géry Casiez. 2017. Leveraging ﬁnger identiﬁcation to integrate multi-touch command selection and parameter manipulation. International Journal of Human-Computer Studies 99 (2017), 21–36. \ 24. Martin Hachet, Benoit Bossavit, Aurélie Cohé, and Jean-Baptiste de la Rivière. 2011. Toucheo: multitouch and stereo combined in a seamless workspace. In Proceedings of the 24th annual ACM symposium on User interface software and technology. ACM, 587–592. \ 25. Jaehyun Han and Geehyuk Lee. 2015. Push-push: A drag-like operation overlapped with a page transition operation on touch interfaces. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology. acm, 313–322. \ 26. Jefferson Y Han. 2005. Low-cost multi-touch sensing through frustrated total internal reﬂection. In Proceedings of the 18th annual ACM symposium on User interface software and technology. ACM, 115–118. \ 27. Faizan Haque, Mathieu Nancel, and Daniel Vogel. 2015. Myopoint: Pointing and clicking using forearm mounted electromyography and inertial motion sensors. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 3653–3656. \ 28. Chris Harrison and Scott Hudson. 2012. Using shear as a supplemental two-dimensional input channel for rich touchscreen interaction. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 3149–3152. \ 29. Seongkook Heo, Jiseong Gu, and Geehyuk Lee. 2014. Expanding touch input vocabulary by using consecutive distant taps. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2597–2606. \ 30. Luke Hespanhol and Martin Tomitsch. 2012. Designing for collective participation with media installations in public spaces. In Proceedings of the 4th Media Architecture Biennale Conference: Participation. ACM, 33–42. \ 31. Otmar Hilliges, David Kim, Shahram Izadi, Malte Weiss, and Andrew Wilson. 2012. HoloDesk: direct 3d interactions with a situated see-through display. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2421–2430. \ 32. Ken Hinckley and Hyunyoung Song. 2011. Sensor synaesthesia: touch in motion, and motion in touch. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 801–810. \ 33. Thomas E Hutchinson, K Preston White, Worthy N Martin, Kelly C Reichert, and Lisa A Frey. 1989. Human-computer interaction using eye-gaze input. IEEE Transactions on systems, man, and cybernetics 19, 6 (1989), 1527–1534. \ 34. Petra Isenberg, Sheelegh Carpendale, Anastasia Bezerianos, Nathalie Henry, and Jean-Daniel Fekete. 2009. Coconuttrix: Collaborative retroﬁtting for information visualization. IEEE Computer Graphics and Applications 29, 5 (2009), 44–57. \ 35. Robert JK Jacob. 1995. Eye tracking in advanced interface design. Virtual environments and advanced interface design (1995), 258–288. \ 36. Mikkel R Jakobsen and Kasper Hornbæk. 2016. Negotiating for Space?: Collaborative Work Using a Wall Display with Mouse and Touch Input.. In CHI. 2050–2061. \ 37. Ricardo Jota, Miguel A Nacenta, Joaquim A Jorge, Sheelagh Carpendale, and Saul Greenberg. 2010. A comparison of ray pointing techniques for very large displays. In Proceedings of Graphics Interface 2010. Canadian Information Processing Society, 269–276. \ 38. Vahid Kazemi and Josephine Sullivan. 2014. One millisecond face alignment with an ensemble of regression trees. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 1867–1874. \ 39. Mohamed Khamis, Florian Alt, Mariam Hassib, Emanuel von Zezschwitz, Regina Hasholzner, and Andreas Bulling. 2016. Gazetouchpass: Multimodal authentication using gaze and touch on mobile devices. In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems. ACM, 2156–2164. \ 40. Christian Lander, Sven Gehring, Antonio Krüger, Sebastian Boring, and Andreas Bulling. 2015. Gazeprojector: Accurate gaze estimation and seamless gaze interaction across multiple displays. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology. ACM, 395–404. \ 41. Byungjoo Lee and Hyunwoo Bang. 2013. A kinematic analysis of directional effects on mouse control. Ergonomics 56, 11 (2013), 1754–1765. \ 42. Byungjoo Lee and Hyunwoo Bang. 2015. A mouse with two optical sensors that eliminates coordinate disturbance during skilled strokes. Human–Computer Interaction 30, 2 (2015), 122–155. \ 43. Byungjoo Lee, Mathieu Nancel, and Antti Oulasvirta. 2016. AutoGain: Adapting Gain Functions by Optimizing Submovement Efﬁciency. arXiv preprint arXiv:1611.08154 (2016). \ 44. Byungjoo Lee and Antti Oulasvirta. 2016. Modelling error rates in temporal pointing. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 1857–1868. \ 45. Wing Ho Andy Li and Hongbo Fu. 2012. Augmented reﬂection of reality. In ACM SIGGRAPH 2012 Emerging Technologies. ACM, 3. \ 46. Chengdong Lu and Douglas Frye. 1992. Mastering the machine: A comparison of the mouse and touch screen for children’s use of computers. Computer assisted learning (1992), 417–427. \ 47. Yuexing Luo and Daniel Vogel. 2015. Pin-and-cross: A unimanual multitouch technique combining static touches with crossing selection. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology. ACM, 323–332. \ 48. Päivi Majaranta, I Scott MacKenzie, Anne Aula, and Kari-Jouko Räihä. 2006. Effects of feedback and dwell time on eye typing speed and accuracy. Universal Access in the Information Society 5, 2 (2006), 199–208. \ 49. Päivi Majaranta and Kari-Jouko Räihä. 2002. Twenty years of eye typing: systems and design issues. In Proceedings of the 2002 symposium on Eye tracking research & applications. ACM, 15–22. \ 50. Diego Martinez Plasencia, Florent Berthaut, Abhijit Karnik, and Sriram Subramanian. 2014. Through the combining glass. In Proceedings of the 27th annual ACM symposium on User interface software and technology. ACM, 341–350. \ 51. Luana Micallef, Gregorio Palmas, Antti Oulasvirta, and Tino Weinkauf. 2017. Towards Perceptual Optimization of the Visual Design of Scatterplots. IEEE Transactions on Visualization and Computer Graphics 23, 6 (2017), 1588–1599. \ 52. Carlos H Morimoto and Marcio RM Mimica. 2005. Eye gaze tracking techniques for interactive applications. Computer vision and image understanding 98, 1 (2005), 4–24. \ 53. Jurriaan D Mulder and BR Boscker. 2004. A modular system for collaborative desktop vr/ar with a shared workspace. In Virtual Reality, 2004. Proceedings. IEEE. IEEE, 75–280. \ 54. Mathieu Nancel, Emmanuel Pietriga, Olivier Chapuis, and Michel Beaudouin-Lafon. 2015. Mid-air pointing on ultra-walls. ACM Transactions on Computer-Human Interaction (TOCHI) 22, 5 (2015), 21. \ 55. Antti Oulasvirta. 2017. User interface design with combinatorial optimization. Computer 50, 1 (2017), 40–47. \ 56. Volker Roth, Kai Richter, and Rene Freidinger. 2004. A PIN-entry method resilient against shoulder surﬁng. In Proceedings of the 11th ACM conference on Computer and communications security. ACM, 236–245. \ 57. Hideaki Sato, Itaru Kitahara, and Yuichi Ohta. 2009. MR-mirror: a complex of real and virtual mirrors. In International Conference on Virtual and Mixed Reality. Springer, 482–491. \ 58. Florian Schaub, Ruben Deyhle, and Michael Weber. 2012. Password entry usability and shoulder surﬁng susceptibility on different smartphone platforms. In Proceedings of the 11th international conference on mobile and ubiquitous multimedia. ACM, 13. \ 59. Christopher Schmandt. 1983. Spatial input/display correspondence in a stereoscopic computer graphic work station. In ACM SIGGRAPH Computer Graphics, Vol. 17. ACM, 253–261. \ 60. Ju Shen, SC S Cheung, and Jian Zhao. 2012. Virtual mirror by fusing multiple RGB-D cameras. In Signal & Information Processing Association Annual Summit and Conference (APSIPA ASC), 2012 Asia-Paciﬁc. IEEE, 1–9. \ 61. Ju Shen, Po-Chang Su, Sen-ching Samson Cheung, and Jian Zhao. 2013. Virtual mirror rendering with stationary rgb-d cameras and stored 3-d background. IEEE Transactions on Image Processing 22, 9 (2013), 3433–3448. \ 62. Matthias Straka, Stefan Hauswiesner, Matthias Rüther, and Horst Bischof. 2011. A free-viewpoint virtual mirror with marker-less user interaction. In Scandinavian Conference on Image Analysis. Springer, 635–645. \ 63. Yusuke Sugano, Yasuyuki Matsushita, and Yoichi Sato. 2013. Appearance-based gaze estimation using visual saliency. IEEE transactions on pattern analysis and machine intelligence 35, 2 (2013), 329–341. \ 64. Kashyap Todi, Daryl Weir, and Antti Oulasvirta. 2016. Sketchplore: Sketch and explore with a layout optimiser. In Proceedings of the 2016 ACM Conference on Designing Interactive Systems. ACM, 543–555. \ 65. Keita Ushida, Yu Tanaka, Takeshi Naemura, and Hiroshi Harashima. 2002. i-mirror: An Interaction/Information Environment Based on a Mirror Metaphor Aiming to Install into Our Life Space. In Proceedings of the 12th International Conference on Artiﬁcial Reality and Telexistence (ICAT2002). 113–118. \ 66. Mélodie Vidal, Andreas Bulling, and Hans Gellersen. 2013. Pursuits: spontaneous interaction with displays based on smooth pursuit eye movement and moving targets. In Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing. ACM, 439–448. \ 67. Daniel Vogel and Ravin Balakrishnan. 2004. Interactive public ambient displays: transitioning from implicit to explicit, public to personal, interaction with multiple users. In Proceedings of the 17th annual ACM symposium on User interface software and technology. ACM, 137–146. \ 68. Daniel Vogel and Ravin Balakrishnan. 2005. Distant freehand pointing and clicking on very large, high resolution displays. In Proceedings of the 18th annual ACM symposium on User interface software and technology. ACM, 33–42. \ 69. Emanuel Von Zezschwitz, Alexander De Luca, Bruno Brunkow, and Heinrich Hussmann. 2015. Swipin: Fast and secure pin-entry on smartphones. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 1403–1406. \ 70. Andrew D Wilson. 2004. TouchLight: an imaging touch screen and display for gesture-based interaction. In Proceedings of the 6th international conference on Multimodal interfaces. ACM, 69–76. \ 71. Xuan Zhang and I Scott MacKenzie. 2007. Evaluating eye tracking with ISO 9241-part 9. In International Conference on Human-Computer Interaction. Springer, 779–788. \ ",Direct input; onscreen reflection; distance independence; private pointing; shoulder surfing; touch screen; eye tracking,H.5.2,uistf4922-file1.zip,,uistf4922-file3.mp4,uistf4922-file4.zip,,PDF with marked changes,Reflector is a direct input technique utilizing onscreen reflection. We have solved the problem of conventional direct input methods on the screen by making full use of the two unique features of reflection.,"We would like to thank the AC for giving us an effective comment to improve the quality of our paper. We have made concrete improvements throughout the Introduction, Reflection model, Implementation, Study 1, Study 2 and Limitation as suggested by the rebuttal. Here we briefly describe the changes. All detailed changes are marked in the reflector-final-change-marked.pdf file. And the final copy has undergone professional proofreading. \   \ 1. Limitation (AC1,R1,R2,R3) \   \ —> We removed Study 3 and added about three more paragraphs to Limitation. The new paragraphs discuss the limitations of the Reflector in spontaneous interaction and problems with the brightness of the background image. \    \ 2. Related work (AC1,R3) \   \ —> We have critically cited related works that the reviewers have given us. \   \ 3. Calibration process (R2) \   \ —> In the calibration process of Study 1, we made clear that we used the actual distance information obtained from the depth sensing camera, not the distance between the eyes. The calibration process has also been added to the video. \   \ 4. Techniques should be recalibrated at different distances (R1) \   \ —> We made it clear in Study1's discussion that Reflector's error rate would be much more robust for perturbed distances, even with new calibration each time. \   \ 5. Comparing with gesture input through Kinect (R3) \   \ —> We emphasized in the Limitations and Future work section that a comparison of performance with indirect input using Kinect is also needed. \   \ 6.  Confounding effect from 1€ filter? (R2) \   \ —> In Study 1, we added a discussion of the effect of filtering delay on the trial completion time, pointed out by R2. As rebuttal says, we discussed that the user would not be affected by the delay because the cursor was not visible. \   \ 7. Design of Study 2 (R2) \   \ —> In the discussion of Study 2, we clarified the limitation of the fact that the expert who used Reflector frequently played the role of a true user. We also made it clear that participants (hackers) have always entered password guessing via touch. \   \ 8. Tone down the claim of distance independence (R1) \   \ —> In the Introduction with Figure 2, we have clarified that the distance independence does not mean that the error rate does not change with distance. \   \ 9. Description of implementation (R2) \   \ —> Algorithm 1 is completely rewritten. We also clarified the physical meaning of Equation (2), (4), and (6). In the last paragraph of the related work, we clarified the contribution of how we developed the existing reflection model. \   \ 10. Main contribution of the study (R3) \   \ —> In the last paragraph of Introduction, we have clarified that the main contribution of this paper is a pointing method called Reflector. \  \ 11. Minor comments \   \ —> We referenced the source of the TLX form we used. And we added error bars to all graphs. \  \ 12. Overall flow \  \ —> Finally, our authors tried to make paper as possible as possible by reviewing the paper several times from the beginning to the end. \  \  ",JONG-IN LEE,Byungjoo Lee,FormatComplete,NRF-2017R1C1B2002101,National Research Foundation of Korea (NRF),,,,,Aug 7 15:28,
uistf4706,10/24,8,"Pointing, Pens & Text",9:00:00 AM,10:30:00 AM,4,9:20:00 AM,9:40:00 AM,long,long,uistf4922,2,802,Honorable Mention,,uistf4706,A,DodecaPen: Accurate 6DoF Tracking of a Passive Stylus,Po-Chen,Wu,pcwu@media.ee.ntu.edu.tw,uistf4706-paper.pdf,10,letter,,,"Po-Chen Wu, Robert Wang, Kenrick Kin, Christopher Twigg, Shangchen Han, Ming-Hsuan Yang, Shao-Yi Chien","pcwu@media.ee.ntu.edu.tw, rob.wang@oculus.com, kenrick.kin@oculus.com, chris.twigg@oculus.com, shangchen.han@oculus.com, mhyang@ucmerced.edu, sychien@ntu.edu.tw",71300,Po-Chen,,Wu,pcwu@media.ee.ntu.edu.tw,Graduate Institute of Electronics Engineering,National Taiwan University,Taipei,,Taiwan,Oculus Research,Facebook Inc.,Redmond,Washington,United States,71784,Robert,,Wang,rob.wang@oculus.com,Oculus Research,Facebook Inc.,Redmond,Washington,United States,,,,,,71785,Kenrick,,Kin,kenrick.kin@oculus.com,Oculus Research,Facebook Inc.,Redmond,Washington,United States,,,,,,71786,Christopher,,Twigg,chris.twigg@oculus.com,Oculus Research,Facebook Inc.,Redmond,Washington,United States,,,,,,71787,Shangchen,,Han,shangchen.han@oculus.com,Oculus Research,Facebook Inc.,Redmond,Washington,United States,,,,,,72785,Ming-Hsuan,,Yang,mhyang@ucmerced.edu,"Electrical Engineering and Computer Science, University of California at Merced, Merced, California, United States",University of California at Merced,Merced,California,United States,,,,,,55517,Shao-Yi,,Chien,sychien@ntu.edu.tw,Graduate Institute of Electronics Engineering,National Taiwan University,Taipei,,Taiwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a system for real-time six degrees of freedom (6DoF) tracking of a passive stylus that achieves sub-millimeter accuracy, which is suitable for writing or drawing in mixed reality applications.  Our system is particularly easy to implement, requiring only a monocular camera, a 3D printed dodecahedron, and hand-glued binary square markers. The accuracy and performance we achieve are due to model-based tracking using a calibrated model and a combination of sparse pose estimation and dense alignment.  We demonstrate the system performance in terms of speed and accuracy on a number of synthetic and real datasets, showing that it can be competitive with state-of-the-art multi-camera motion capture systems.  We also demonstrate several applications of the technology ranging from 2D and 3D drawing in VR to general object manipulation and board games.",pcwu@media.ee.ntu.edu.tw,"1. Pierre Alliez, David Cohen-Steiner, Olivier Devillers, Bruno Lévy, and Mathieu Desbrun. 2003. Anisotropic Polygonal Remeshing. In Proceedings of ACM SIGGRAPH. 485–493. \ 2. Anoto. Accessed: 2017-07-17. Anoto. http://www.anoto.com/ \ 3. Simon Baker and Iain Matthews. 2004. Lucas-Kanade 20 Years On: A Unifying Framework. International Journal of Computer Vision 56, 3 (2004), 221–255. \ 4. Paul J Besl and Neil D McKay. 1992. A Method for Registration of 3-D Shapes. IEEE Transactions on Pattern Analysis and Machine Intelligence 14, 2 (1992), 239–256. \ 5. Jean-Yves Bouguet. 2001. Pyramidal Implementation of the Lucas Kanade Feature Tracker: Description of the Algorithm. Intel Corporation 5, 1-10 (2001), 4. \ 6. Vojtech Bubník and Vlastimil Havran. 2015. Light Chisel: 6DOF Pen Tracking. Computer Graphics Forum 34, 2 (2015), 325–336. \ 7. Computer History Museum. Accessed: 2017-07-17. Mapping Sutherland’s Volkswagen. http://www.computerhistory.org/revolution/ computer-graphics-music-and-art/15/206/560 \ 8. Alberto Crivellaro, Pascal Fua, and Vincent Lepetit. 2014. Dense Methods for Image Alignment with an Application to 3D Tracking. Technical Report. \ 9. Mark Fiala. 2005. ARTag, a Fiducial Marker System Using Digital Techniques. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. \ 10. Tinsley A. Galyean and John F. Hughes. 1991. Sculpting: An Interactive Volumetric Modeling Technique. In Proceedings of ACM SIGGRAPH. 267–274. \ 11. Sergio Garrido-Jurado, Rafael Muñoz-Salinas, Francisco José Madrid-Cuevas, and Manuel Jesús Marín-Jiménez. 2014. Automatic Generation and Detection of Highly Reliable Fiducial Markers Under Occlusion. Pattern Recognition 47, 6 (2014), 2280–2292. \ 12. Sergio Garrido-Jurado, Rafael Munoz-Salinas, Francisco José Madrid-Cuevas, and Rafael Medina-Carnicer. 2016. Generation of Fiducial Marker Dictionaries using Mixed Integer Linear Programming. Pattern Recognition 51 (2016), 481–491. \ 13. Tovi Grossman, Ken Hinckley, Patrick Baudisch, Maneesh Agrawala, and Ravin Balakrishnan. 2006. Hover Widgets: Using the Tracking State to Extend the Capabilities of Pen-operated Devices. In Proceedings of ACM SIGCHI. 861–870. \ 14. Taejin Ha and Woontack Woo. 2010. An Empirical Evaluation of Virtual Hand Techniques for 3D Object Manipulation in a Tangible Augmented Reality Environment. In IEEE Symposium on 3D User Interfaces. \ 15. Jaehyun Han, Seongkook Heo, Hyong-Euk Lee, and Geehyuk Lee. 2014. The IrPen: A 6-DOF Pen for Interaction with Tablet Computers. IEEE Computer Graphics and Applications 34, 3 (2014), 22–29. \ 16. Robert Held, Ankit Gupta, Brian Curless, and Maneesh Agrawala. 2012. 3D Puppetry: A Kinect-based Interface for 3D Animation. In Proceedings of ACM Symposium on User Interface Software and Technology. 423–434. \ 17. Seongkook Heo, Jaehyun Han, Sangwon Choi, Seunghwan Lee, Geehyuk Lee, Hyong-Euk Lee, SangHyun Kim, Won-Chul Bang, DoKyoon Kim, and ChangYeong Kim. 2011. IrCube Tracker: An Optical 6DOF Tracker based on LED Directivity. In Proceedings of ACM Symposium on User Interface Software and Technology. \ 18. HTC. Accessed: 2017-07-17. HTC Vive. https://www.vive.com/us/ \ 19. Hiroshi Ishii and Brygg Ullmer. 1997. Tangible Bits: Towards Seamless Interfaces Between People, Bits and Atoms. In Proceedings of ACM SIGCHI. 234–241. \ 20. Bruce D Lucas and Takeo Kanade. 1981. An Iterative Image Registration Technique with an Application to Stereo Vision. In Proceedings of the IJCAI. \ 21. NaturalPoint. Accessed: 2017-07-17. OptiTrack. http://optitrack.com/ \ 22. Richard A Newcombe, Steven J Lovegrove, and Andrew J Davison. 2011. DTAM: Dense Tracking and Mapping in Real-Time. In Proceedings of IEEE International Conference on Computer Vision. \ 23. Jorge Nocedal and Stephen J Wright. 2006. Numerical Optimization. Springer (2006). \ 24. Oculus. Accessed: 2017-07-17. Oculus Touch. https://www.oculus.com/rift/ \ 25. Nobuyuki Otsu. 1975. A Threshold Selection Method from Gray-Level Histograms. Automatica 11, 285-296 (1975), 23–27. \ 26. Karl Pauwels, Leonardo Rubio, Javier Diaz, and Eduardo Ros. 2013. Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. \ 27. Thomas Petersen. 2008. A Comparison of 2D-3D Pose Estimation Methods. Aalborg University (2008). \ 28. Razer. Accessed: 2017-07-17. Razer Hydra. https://www2.razerzone.com/au-en/gaming-controllers/ razer-hydra-portal-2-bundle \ 29. Sony. Accessed: 2017-07-17. PlayStation Move Motion Controller. https://www.playstation.com/en-us/explore/ accessories/playstation-move/ \ 30. Sriram Subramanian, Dzimitry Aliakseyeu, and Andrés Lucero. 2006. Multi-layer Interaction for Digital Tables. In Proceedings of ACM Symposium on User Interface Software and Technology. 269–272. \ 31. James Tompkin, Samuel Muff, James McCann, Hanspeter Pﬁster, Jan Kautz, Marc Alexa, and Wojciech Matusik. 2015. Joint 5D Pen Input for Light Field Displays. In Proceedings of ACM Symposium on User Interface Software and Technology. \ 32. Hung-Yu Tseng, Po-Chen Wu, Ming-Hsuan Yang, and Shao-Yi Chien. 2016. Direct 3D Pose Estimation of a Planar Target. In Proceedings of IEEE Winter Conference on Applications of Computer Vision. \ 33. Wacom. Accessed: 2017-07-17. Wacom. https://www.wacom.com/ \ 34. Robert Xiao, Chris Harrison, Karl DD Willis, Ivan Poupyrev, and Scott E Hudson. 2013. Lumitrack: Low Cost, High Precision, High Speed Tracking with Projected m-Sequences. In Proceedings of ACM Symposium on User Interface Software and Technology. \ ",6DoF pose tracking; binary square markers; mixed reality,H.5.m,uistf4706-file1.zip,uistf4706-file2.jpg,uistf4706-file3.mp4,,The proposed system can track the 6DoF pose of a calibrated DodecaPen from a single camera with submillimeter accuracy.,,The proposed system can track the 6DoF pose of a calibrated DodecaPen from a single camera with submillimeter accuracy.,We have added one more page with more details.,Po-Chen Wu,Kenrick Kin,FormatComplete,,,,,,,Aug 9 22:11,
uistf1172,10/24,8,"Pointing, Pens & Text",9:00:00 AM,10:30:00 AM,4,9:40:00 AM,10:00:00 AM,long,long,uistf4706,3,803,,,uistf1172,A,FlexStylus: Leveraging Bend Input for Pen Interaction,Nicholas,Fellion,nicholas.fellion@carleton.ca,uistf1172-paper.pdf,11,letter,,,"Nicholas Fellion, Thomas Pietrzak, Audrey Girouard","nicholas.fellion@carleton.ca, thomas.pietrzak@univ-lille1.fr, audrey.girouard@carleton.ca",58192,Nicholas,,Fellion,nicholas.fellion@carleton.ca,,Carleton University,Ottawa,Ontario,Canada,,,,,,14172,Thomas,,Pietrzak,thomas.pietrzak@univ-lille1.fr,,Univ. Lille,Lille,,France,,,,,,7465,Audrey,,Girouard,audrey.girouard@carleton.ca,,Carleton University,Ottawa,Ontario,Canada,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"FlexStylus, a flexible stylus, detects deformation of the barrel as a vector with both a rotational and an absolute value, providing two degrees of freedom with the goal of improving the expressivity of digital art using a stylus device. We outline the construction of the prototype and the principles behind the sensing method, which uses a cluster of four fibre-optic based deformation sensors. We propose interaction techniques using the FlexStylus to improve menu navigation and tool selection. Finally, we describe a study comparing users’ ability to match a changing target value using a commercial pressure stylus and the FlexStylus’ absolute deformation. When using the FlexStylus, users had a significantly higher accuracy overall. This suggests that deformation may be a useful input method for future work considering stylus augmentation.",nick.fellion@gmail.com,"1. Teemu T. Ahmaniemi, Johan Kildal, and Merja Haveri. 2014. What is a device bend gesture really good for?. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '14). ACM, New York, NY, USA, 3503-3512. https://doi.org/10.1145/2556288.2557306 \ 2. Ravin Balakrishnan, George Fitzmaurice, Gordon Kurtenbach, and Karan Singh. 1999. Exploring interactive curve and surface manipulation using a bend and twist sensitive input strip. In Proceedings of the 1999 symposium on Interactive 3D graphics (I3D '99). ACM, New York, NY, USA, 111-118. http://dx.doi.org/10.1145/300523.300536 \ 3. Xiaojun Bi, Tomer Moscovich, Gonzalo Ramos, Ravin Balakrishnan, and Ken Hinckley. 2008. An exploration of pen rolling for pen-based interaction. In Proceedings of the 21st annual ACM symposium on User interface software and technology (UIST '08). ACM, New York, NY, USA, 191-200. https://doi.org/10.1145/1449715.1449745 \ 4. Eric A. Bier, Maureen C. Stone, Ken Pier, Ken Fishkin, Thomas Baudel, Matt Conway, William Buxton, and Tony DeRose. 1994. Toolglass and magic lenses: the see-through interface. In Conference Companion on Human Factors in Computing Systems (CHI '94), Catherine Plaisant (Ed.). ACM, New York, NY, USA, 445-446. http://dx.doi.org/10.1145/259963.260447 \ 5. Jesse Burstyn, Amartya Banerjee, and Roel Vertegaal. 2013. FlexView: an evaluation of depth navigation on deformable mobile devices. In Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction (TEI '13). ACM, New York, NY, USA, 193-200. http://dx.doi.org/10.1145/2460625.2460655 \ 6. Jesse Burstyn, Juan Pablo Carrascal, and Roel Vertegaal. 2016. Fitts' Law and the Effects of Input Mapping and Stiffness on Flexible Display Interactions. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16). ACM, New York, NY, USA, 36493658. https://doi.org/10.1145/2858036.2858383 \ 7. Géry Casiez and Daniel Vogel. 2008. The effect of spring stiffness and control gain with an elastic rate control pointing device. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '08). ACM, New York, NY, USA, 17091718. https://doi.org/10.1145/1357054.1357321 \ 8. Gery Casiez, Daniel Vogel, Ravin Balakrishnan, and Andy Cockburn. 2008. The Impact of Control-Display Gain on User Performance in Pointing Tasks. HumanComputer Interaction 23, 3: 215–250. https://doi.org/10.1080/07370020802278163 \ 9. Matthew Ernst and Audrey Girouard. 2016. Bending Blindly: Exploring Bend Gestures for the Blind. In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '16). ACM, New York, NY, USA, 2088-2096. https://doi.org/10.1145/2851581.2892303 \ 10. Nicholas Fellion, Alexander Keith Eady, and Audrey Girouard. 2016. FlexStylus: A Deformable Stylus for Digital Art. In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '16). ACM, New York, NY, USA, 2482-2489. https://doi.org/10.1145/2851581.2892444 \ 11. Audrey Girouard, Jessica Lo, Md Riyadh, Farshad Daliri, Alexander Keith Eady, and Jerome Pasquero. 2015. One-Handed Bend Interactions with Deformable Smartphones. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15). ACM, New York, NY, USA, 15091518. https://doi.org/10.1145/2702123.2702513 \ 12. Khalad Hasan, Xing-Dong Yang, Andrea Bunt, and Pourang Irani. 2012. A-coord input: coordinating auxiliary input streams for augmenting contextual penbased interactions. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '12). ACM, New York, NY, USA, 805814. http://dx.doi.org/10.1145/2207676.2208519 \ 13. Gero Herkenrath, Thorsten Karrer, and Jan Borchers. 2008. Twend: twisting and bending as new interaction gesture in mobile devices. In CHI '08 Extended Abstracts on Human Factors in Computing Systems (CHI EA '08). ACM, New York, NY, USA, 3819-3824. https://doi.org/10.1145/1358628.1358936 \ 14. Ken Hinckley, Xiang 'Anthony' Chen, and Hrvoje Benko. 2013. Motion and context sensing techniques for pen computing. In Proceedings of Graphics Interface 2013 (GI '13). Canadian Information Processing Society, Toronto, Ont., Canada, Canada, 71-78. \ 15. Stéphane Huot, Mathieu Nancel, and Michel Beaudouin-Lafon. 2008. PushMenu: Extending Marking Menus for Pressure-Enabled Input Devices. Inria Technical Report. Retrieved from https://hal.inria.fr/inria-00550597 \ 16. Johan Kildal and Marion Boberg. 2013. Feel the action: dynamic tactile cues in the interaction with deformable uis. In CHI '13 Extended Abstracts on Human Factors in Computing Systems (CHI EA '13). ACM, New York, NY, USA, 1563-1568. https://doi.org/10.1145/2468356.2468636 \ 17. Gordon Kurtenbach and William Buxton. 1994. User learning and performance with marking menus. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '94), Beth Adelson, Susan Dumais, and Judith Olson (Eds.). ACM, New York, NY, USA, 258-264. http://dx.doi.org/10.1145/191666.191759 \ 18. Byron Lahey, Audrey Girouard, Winslow Burleson, and Roel Vertegaal. 2011. PaperPhone: understanding the use of bend gestures in mobile devices with flexible electronic paper displays. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '11). ACM, New York, NY, USA, 13031312. https://doi.org/10.1145/1978942.1979136 \ 19. Gonzalo Ramos and Ravin Balakrishnan. 2005. Zliding: fluid zooming and sliding for high precision parameter manipulation. In Proceedings of the 18th annual ACM symposium on User interface software and technology (UIST '05). ACM, New York, NY, USA, 143-152. http://dx.doi.org/10.1145/1095034.1095059 \ 20. Gonzalo Ramos, Matthew Boulos, and Ravin Balakrishnan. 2004. Pressure widgets. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '04). ACM, New York, NY, USA, 487-494. http://dx.doi.org/10.1145/985692.985754 \ 21. Anne Roudaut, Gilles Bailly, Eric Lecolinet, and Laurence Nigay. 2009. Leaf Menus: Linear Menus with Stroke Shortcuts for Small Handheld Devices. In Proceedings of the 12th IFIP TC 13 International Conference on Human-Computer Interaction: Part I (INTERACT '09). Springer-Verlag, Berlin, Heidelberg, 616-619. http://dx.doi.org/10.1007/978-3642-03655-2_69 \ 22. Anne Roudaut, Abhijit Karnik, Markus Löchtefeld, and Sriram Subramanian. 2013. Morphees: toward high ""shape resolution"" in self-actuated flexible mobile devices. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '13). ACM, New York, NY, USA, 593-602. https://doi.org/10.1145/2470654.2470738 \ 23. Eric Singer. 2003. Sonic banana: a novel bend-sensorbased MIDI controller. In Proceedings of the 2003 conference on New interfaces for musical expression (NIME '03). National University of Singapore, Singapore, Singapore, 220-221. \ 24. Ronit Slyper, Ivan Poupyrev, and Jessica Hodgins. 2010. Sensing through structure: designing soft silicone sensors. In Proceedings of the fifth international conference on Tangible, embedded, and embodied interaction (TEI '11). ACM, New York, NY, USA, 213-220. http://dx.doi.org/10.1145/1935701.1935744 \ 25. Hyunyoung Song, Hrvoje Benko, Francois Guimbretiere, Shahram Izadi, Xiang Cao, and Ken Hinckley. 2011. Grips and gestures on a multi-touch pen. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '11). ACM, New York, NY, USA, 1323-1332. https://doi.org/10.1145/1978942.1979138 \ 26. Paul Strohmeier, Jesse Burstyn, Juan Pablo Carrascal, Vincent Levesque, and Roel Vertegaal. 2016. ReFlex: A Flexible Smartphone with Active Haptic Feedback for Bend Input. In Proceedings of the TEI '16: Tenth International Conference on Tangible, Embedded, and Embodied Interaction(TEI '16). ACM, New York, NY, USA, 185-192. https://doi.org/10.1145/2839462.2839494 \ 27. Yu Suzuki, Kazuo Misue, and Jiro Tanaka. 2010. Interaction technique combining gripping and pen pressures. Lecture Notes in Computer Science 6279. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-15384-6_47 \ 28. Feng Tian, Lishuang Xu, Hongan Wang, Xiaolong Zhang, Yuanyuan Liu, Vidya Setlur, and Guozhong Dai. 2008. Tilt menu: using the 3D orientation information of pen devices to extend the selection capability of pen-based user interfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '08). ACM, New York, NY, USA, 1371-1380. https://doi.org/10.1145/1357054.1357269 \ 29. Giovanni Maria Troiano, Esben Warming Pedersen, and Kasper Hornbæk. 2015. Deformable Interfaces for Performing Music. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15). ACM, New York, NY, USA, 377386. https://doi.org/10.1145/2702123.2702492 \ 30. Nirzaree Vadgama and Jürgen Steimle. 2017. Flexy: Shape-Customizable, Single-Layer, Inkjet Printable Patterns for 1D and 2D Flex Sensing. In Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction (TEI '17). ACM, New York, NY, USA, 153-162. https://doi.org/10.1145/3024969.30249 \ 31. Daniel Vogel and Géry Casiez. 2011. Conté: multimodal input inspired by an artist's crayon. In Proceedings of the 24th annual ACM symposium on User interface software and technology (UIST '11). ACM, New York, NY, USA, 357-366. https://doi.org/10.1145/2047196.2047242 \ 32. Kristen Warren, Jessica Lo, Vaibhav Vadgama, and Audrey Girouard. 2013. Bending the rules: bend gesture classification for flexible displays. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '13). ACM, New York, NY, USA, 607-610. https://doi.org/10.1145/2470654.2470740 \ 33. Jacob O. Wobbrock, Andrew D. Wilson, and Yang Li. 2007. Gestures without libraries, toolkits or training: a $1 recognizer for user interface prototypes. In Proceedings of the 20th annual ACM symposium on User interface software and technology (UIST '07). ACM, New York, NY, USA, 159-168. https://doi.org/10.1145/1294211.1294238 \ 34. Xiaolei Zhou. 2014. Comparative Study on Cursor Position Controlled by Pen Pressure and Pen Tilt. In 2014 Seventh International Symposium on Computational Intelligence and Design. IEEE, Los Alamitos, CA, USA, 375-378. https://doi.org/10.1109/ISCID.2014.151 \ 35. Yizhong Xin, Xiaojun Bi, and Xiangshi Ren. 2011. Acquiring and pointing: an empirical study of pen-tiltbased interaction. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '11). ACM, New York, NY, USA, 849-858. https://doi.org/10.1145/1978942.1979066 \ 36. Shumin Zhai. 1998. User Performance in Relation to 3D Input Device Design. SIGGRAPH Comput. Graph. 32, 4: 50–54. https://doi.org/10.1145/307710.307728 \ ",Pen interaction; input technique; bending; deformation; augmented stylus; HCI,"H.5.2 User Interfaces, Input devices and strategies (e.g., mouse, touchscreen)",uistf1172-file1.docx,uistf1172-file2.jpg,uistf1172-file3.mp4,,"FlexStylus, a flexible stylus, detects deformation of the barrel as a vector with both a rotational and an absolute value.",,"We present FlexStylus, a stylus which uses bend input with the goal of improving expressivity and accuracy in digital art.","Summary of changes: \  \ -General clarification of language and correction of typos. Most important changes to language are: \ -Replacing the term ""flexion"" with more appropriate terms, such as ""bending"" or ""deformation.""  \ -Unified terms ""scroll grip"" and ""in-air grip"" to simply, ""in-air grip."" \  \ -Included a brief discussion of why it was necessary to use fibre optic bend sensors instead of thin-film resistive sensors. (Page 3): ""We used fibre optic flex sensing because, due to their flat shape and lack of elasticity, thin film sensors cannot be flexed perpendicularly to their surface normal, which precludes the possibility of using multiple perpendicular thin film sensors."" \  \ -Cited Leaf Menus [1] for further illustration regarding tool grip menus.  \  \ -Included a discussion (Page 4) of the device's position in a broader taxonomy of input devices: ""The flexible part is slightly elastic—it returns to its initial position when released. This makes the device elastic, with a behaviour close to isotonic devices."" \  \ -Included a brief explanation for the logic of performing a position-control study with an isotonic device: ""While Zhai’s work [38] generalizes rate control as being preferable for isotonic and elastic devices, the specific interaction in which a user alters the width of a brush is a widely-used implementation of pressure input with a pen."" \  \ -Included a concluding sentence responding to reviewer's interest in the sensing method in other contexts: ""We believe that the angular bend detection is a promising and relatively-unexplored research domain for a variety of contexts, and invite other members of the HCI community to consider this input modality for systems requiring fine-grained directional and amplitude control with inherent haptic feedback."" \ ",Nicholas Fellion,Audrey Girouard,FormatComplete,402494-2011,NSERC Discovery Grant,Mitacs Globalink Research Award,IT05609,,,Aug 5 15:29,
TOCHI 0014.R2,10/24,8,"Pointing, Pens & Text",9:00:00 AM,10:30:00 AM,4,10:00:00 AM,10:20:00 AM,long,long,uistf1172,4,804,,,TOCHI 0014.R2,,TOCHI 0014.R2 Beyond Just Text: Semantic Emoji Similarity Modeling to Support Expressive Communication,Pohl,Henning,,,42,,,,"Henning Pohl, Christian Domin, Micheal Leibniz Rohs",,,Henning,,Pohl, Henning@hci.uni-hannover.de,Human Computer Interaction,Universitat Hannover,,,Germany,,,,,,,Christian,,Domin,,Human Computer Interaction,Universitat Hannover,,,Germany,,,,,,,Michael,L.,Rohs,,Human Computer Interaction,,Universitat Hannover,,,Germany,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Abstract: Emoji, a set of pictographic Unicode characters, have seen strong uptake over the last couple of years. All common mobile platforms and many desktop systems now support emoji entry and users have embraced their use. Yet, we currently know very little about what makes for good emoji entry. While soft keyboards for text entry are well optimized, based on language and touch models, no such information exists to guide the design of emoji keyboards. In this article, we investigate of the problem of emoji entry, starting with a study of the current state of the emoji keyboard implementation in Android. To enable moving forward to novel emoji keyboard designs, we then explore a model for emoji similarity that is able to inform such designs. This semantic model is based on data from 21 million collected tweets containing emoji. We compare this model against a solely description-based model of emoji in a crowdsourced study. Our model shows good performance in capturing detailed relationships between emoji.
"," Henning@hci.uni-hannover.de
 ",,"Emoji, mobile text entry, emoticons",,,,,,,,,,,,,,,,,,,"Aug 26, 2017",
uistf1973,10/24,9,3D Sketching,11:00:00 AM,12:10:00 AM,3+1,11:00:00 AM,11:20:00 AM,long,long,none,1,901,,,uistf1973,A,SweepCanvas: Sketch-based 3D Prototyping on an RGB-D Image,Youyi,Zheng,zhengyy@shanghaitech.edu.cn,uistf1973-paper.pdf,13,letter,,,"Yuwei Li, Xi Luo, Youyi Zheng, Pengfei Xu, Hongbo Fu","liyw@shanghaitech.edu.cn, luoxi@shanghaitech.edu.cn, zhengyy@shanghaitech.edu.cn, xupengfei.cg@gmail.com, fuplus@gmail.com",71534,Yuwei,,Li,liyw@shanghaitech.edu.cn,School of Information Science and Technology,ShanghaiTech University,Shanghai,,China,,,,,,71537,Xi,,Luo,luoxi@shanghaitech.edu.cn,School of Information Science and Technology,ShanghaiTech University ,Shanghai,Shanghai,China,,,,,,64433,Youyi,,Zheng,zhengyy@shanghaitech.edu.cn,State Key Lab of CAD&CG,Zhejiang University of Technology,Hangzhou,Zhejiang ,China,,ShanghaiTech University,Shanghai,,China,42896,Pengfei,,Xu,xupengfei.cg@gmail.com,College of Computer Science and Software Engineering,Shenzhen University,Shenzhen,Guangdong,China,,,,,,28683,Hongbo,,Fu,fuplus@gmail.com,School of Creative Media,City University of Hong Kong,Hong Kong,,China,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The creation of 3D contents still remains one of the most crucial problems for the emerging applications such as 3D printing and Augmented Reality. In Augmented Reality, how to create virtual contents that seamlessly overlay with the real environment is a key problem for human-computer interaction and many subsequent applications. In this paper, we present a sketch-based interactive tool, which we term \\emph{SweepCanvas}, for rapid exploratory 3D modeling on top of an RGB-D image. Our aim is to offer end-users a simple yet efficient way to quickly create 3D models on an image. We develop a novel sketch-based modeling interface, which takes a pair of user strokes as input and instantly generates a curved 3D surface by sweeping one stroke along the other. A key enabler of our system is an optimization procedure that extracts pairs of spatial planes from the context to position and sweep the strokes. We demonstrate the effectiveness and power of our modeling system on various RGB-D data sets and validate the use cases via a pilot study.",youyizheng@zju.edu.cn,"WARNING: Reference 41 starts with a non-alphanumeric character.  Please check it. \  \ 1. Bae, S.-H., Balakrishnan, R., and Singh, K. Ilovesketch: As-natural-as-possible sketching system for creating 3d curve models. In UIST ’08 (2008), 151–160. \ 2. Bae, S.-H., Balakrishnan, R., and Singh, K. Everybodylovessketch: 3d sketching for a broader audience. In UIST ’09 (2009), 59–68. \ 3. Canny, J. A computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelligence, 6 (1986), 679–698. \ 4. Casarrubias-Vargas, H., Petrilli-Barcel´o, A., and Bayro-Corrochano, E. Fast edge detection in rgb-d images. In Iberoamerican Congress on Pattern Recognition, Springer (2014), 868–875. \ 5. Chaudhuri, S., and Koltun, V. Data-driven suggestions for creativity support in 3d modeling. ACM Transactions on Graphics (TOG) 29, 6 (2010), 183. \ 6. Chen, T., Zhu, Z., Shamir, A., Hu, S.-M., and Cohen-Or, D. 3-sweep: Extracting editable objects from a single photo. ACM Trans. Graph. 32, 6 (Nov. 2013), 195:1–195:10. \ 7. Chen, X., Kang, S. B., Xu, Y.-Q., Dorsey, J., and Shum, H.-Y. Sketching reality: Realistic interpretation of architectural designs. ACM Trans. Graph. 27, 2 (May 2008), 11:1–11:15. \ 8. Cheng, Y. Mean shift, mode seeking, and clustering. IEEE transactions on pattern analysis and machine intelligence 17, 8 (1995), 790–799. \ 9. Choi, S., Zhou, Q.-Y., Miller, S., and Koltun, V. A large dataset of object scans. arXiv:1602.02481 (2016). \ 10. Cooper, M. Line Drawing Interpretation. Springer-Verlag London, 2008. \ 11. De Paoli, C., and Singh, K. Secondskin: Sketch-based construction of layered 3d models. ACM Trans. Graph. 34, 4 (July 2015), 126:1–126:10. \ 12. Doll´ar, P., and Zitnick, C. L. Fast edge detection using structured forests. IEEE transactions on pattern analysis and machine intelligence 37, 8 (2015), 1558–1570. \ 13. Dorsey, J., Xu, S., Smedresman, G., Rushmeier, H., and McMillan, L. The mental canvas: A tool for conceptual architectural design and analysis. In PG ’07 (2007). \ 14. Eitz, M., Richter, R., Boubekeur, T., Hildebrand, K., and Alexa, M. Sketch-based shape retrieval. ACM Trans. Graph. 31, 4 (July 2012), 31:1–31:10. \ 15. Engel, J., Koltun, V., and Cremers, D. Direct sparse odometry. In arXiv:1607.02565 (July 2016). \ 16. Favreau, J.-D., Lafarge, F., and Bousseau, A. Line drawing interpretation in a multi-view context. In CVPR (2015). \ 17. Gannon, M., Grossman, T., and Fitzmaurice, G. Tactum: a skin-centric approach to digital design and fabrication. In CHI ’15 (2015), 1779–1788. \ 18. Gingold, Y., Igarashi, T., and Zorin, D. Structured annotations for 2D-to-3D modeling. ACM Transactions on Graphics (TOG) 28, 5 (2009), 148. \ 19. Google, Inc. Tilt brush. 2015. \ 20. Holz, D., Holzer, S., Rusu, R. B., and Behnke, S. Real-time plane segmentation using rgb-d cameras. In Robot Soccer World Cup, Springer (2011), 306–317. \ 21. Huo, K., Vinayak, and Ramani, K. Window-shaping: 3d design ideation in mixed reality. In SUI ’16 (2016), 189–189. \ 22. Igarashi, T., and Hughes, J. F. A suggestive interface for 3d drawing. In UIST ’01 (2001), 173–181. \ 23. Igarashi, T., Matsuoka, S., and Tanaka, H. Teddy: A sketching interface for 3d freeform design. In SIGGRAPH ’99 (1999), 409–416. \ 24. Isack, H., and Boykov, Y. Energy-based geometric multi-model ﬁtting. Int. J. Comput. Vision 97, 2 (Apr. 2012), 123–147. \ 25. Izadi, S., Kim, D., Hilliges, O., Molyneaux, D., Newcombe, R., Kohli, P., Shotton, J., Hodges, S., Freeman, D., Davison, A., et al. Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera. In UIST ’11 (2011), 559–568. \ 26. Janoch, A., Karayev, S., Jia, Y., Barron, J. T., Fritz, M., Saenko, K., and Darrell, T. A category-level 3d object dataset: Putting the kinect to work. In Consumer Depth Cameras for Computer Vision. Springer, 2013, 141–165. \ 27. Jones, W., and Sagoo, N. Architects’ Sketchbooks. Thames and Hudson, 2011. \ 28. Kallio, K. 3D6B Editor: Projective 3D Sketching with Line-Based Rendering. In Eurographics Workshop on Sketch-Based Interfaces and Modeling (2005). \ 29. Kim, Y., and Bae, S.-H. Sketchingwithhands: 3d sketching handheld products with ﬁrst-person hand posture. In UIST ’16 (2016), 797–808. \ 30. Kolmogorov, V., and Zabih, R. What energy functions can be minimized via graph cuts? In ECCV ’02 (2002), 65–81. \ 31. Lau, M., Saul, G., Mitani, J., and Igarashi, T. Modeling-in-context: User design of complementary objects with a single photo. In Proc. SBIM (2010), 17–24. \ 32. Lipp, M., Wonka, P., and M¨uller, P. Pushpull++. ACM Transactions on Graphics (TOG) 33, 4 (2014), 130. \ 33. Lipson, H., and Shpitalni, M. Optimization-based reconstruction of a 3d object from a single freehand line drawing. In ACM SIGGRAPH 2007 courses, ACM (2007), 45. \ 34. Liu, F., Shen, C., Lin, G., and Reid, I. Learning depth from single monocular images using deep convolutional neural ﬁelds. IEEE transactions on pattern analysis and machine intelligence 38, 10 (2016), 2024–2039. \ 35. Monszpart, A., Mellado, N., Brostow, G. J., and Mitra, N. J. Rapter: Rebuilding man-made scenes with regular arrangements of planes. ACM Trans. Graph. 34, 4 (July 2015), 103:1–103:12. \ 36. Nathan Silberman, Derek Hoiem, P. K., and Fergus, R. Indoor segmentation and support inference from rgbd images. In ECCV (2012). \ 37. Nealen, A., Igarashi, T., Sorkine, O., and Alexa, M. FiberMesh: Designing freeform surfaces with 3D curves. ACM Transactions on Graphics 26, 3 (2007), article no. 41. \ 38. Nuernberger, B., Ofek, E., Benko, H., and Wilson, A. D. Snaptoreality: Aligning augmented reality to the real world. In CHI ’16 (2016), 1233–1244. \ 39. Olsen, L., Samavati, F. F., Sousa, M. C., and Jorge, J. A. Sketch-based modeling: A survey. Computers & Graphics 33, 1 (2009), 85–103. \ 40. Owada, S., Nielsen, F., Nakazawa, K., and Igarashi, T. A sketching interface for modeling the internal structures of 3d shapes. In ACM SIGGRAPH 2007 courses, ACM (2007), 38. \ 41. ¨Oztireli, A. C., Uyumaz, U., Popa, T., Sheffer, A., and Gross, M. 3d modeling with a symmetric sketch. EG (August 2011). \ 42. Paczkowski, P., Dorsey, J., Rushmeier, H., and Kim, M. H. Paper3d: Bringing casual 3d modeling to a multi-touch interface. In UIST ’14 (2014), 23–32. \ 43. Paczkowski, P., Kim, M. H., Morvan, Y., Dorsey, J., Rushmeier, H., and O’Sullivan, C. Insitu: Sketching architectural designs in context. ACM TOG (SIGGRAPH Asia) 30, 6 (2011), 182:1–10. \ 44. Sachs, E., Roberts, A., and Stoops, D. 3draww: A tool for designing 3d shapes. IEEE Comput. Graph. Appl. 11, 6 (Nov. 1991), 18–26. \ 45. Sadri, B., and Singh, K. Flow-complex-based shape reconstruction from 3d curves. ACM Trans. Graph. 33, 2 (Apr. 2014), 20:1–20:15. \ 46. Schmidt, R., Khan, A., Singh, K., and Kurtenbach, G. Analytic drawing of 3d scaffolds. In ACM TOG (SIGGRAPH Asia), vol. 28 (2009), 149. \ 47. Schnabel, R., Wahl, R., and Klein, R. Efﬁcient ransac for point-cloud shape detection. Computer Graphics Forum 26, 2 (June 2007), 214–226. \ 48. Shao, C., Bousseau, A., Sheffer, A., and Singh, K. Crossshade: Shading concept sketches using cross-section curves. ACM Transactions on Graphics (SIGGRAPH Conference Proceedings) 31, 4 (2012). \ 49. Shao, T., Li, W., Zhou, K., Xu, W., Guo, B., and Mitra, N. J. Interpreting concept sketches. ACM TOG (SIGGRAPH) 32, 4 (2013). \ 50. Shao, T., Monszpart, A., Zheng, Y., Koo, B., Xu, W., Zhou, K., and Mitra, N. J. Imagining the unseen: Stability-based cuboid arrangements for scene understanding. ACM Transactions on Graphics 33, 6 (2014), 209:1–209:11. \ 51. Shao, T., Xu, W., Zhou, K., Wang, J., Li, D., and Guo, B. An interactive approach to semantic modeling of indoor scenes with an rgbd camera. ACM Transactions on Graphics (TOG) 31, 6 (2012), 136. \ 52. Shen, C.-H., Fu, H., Chen, K., and Hu, S.-M. Structure recovery by part assembly. ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH Asia 2012 31, 6 (2012), 180:1–180:11. \ 53. Shesh, A., and Chen, B. Smartpaper: An interactive and user friendly sketching system. In Computer Graphics Forum, vol. 23, Wiley Online Library (2004), 301–310. \ 54. Shtof, A., Agathos, A., Gingold, Y., Shamir, A., and Cohen-Or, D. Geosemantic snapping for sketch-based modeling. Computer Graphics Forum 32, 2 (2013), 245–253. \ 55. Snavely, N., Seitz, S. M., and Szeliski, R. Photo tourism: Exploring photo collections in 3d. In SIGGRAPH Conference Proceedings, ACM Press (New York, NY, USA, 2006), 835–846. \ 56. Song, S., and Xiao, J. Sliding shapes for 3d object detection in depth images. In European Conference on Computer Vision, Springer (2014), 634–651. \ 57. Thorm¨ahlen, T., and Seidel, H.-P. 3d-modeling by ortho-image generation from image sequences. In ACM Transactions on Graphics (TOG), vol. 27, ACM (2008), 86. \ 58. Trimble, Inc. SketchUp. 2017. \ 59. Xie, X., Xu, K., Mitra, N. J., Cohen-Or, D., Gong, W., Su, Q., and Chen, B. Sketch-to-design: Context-based part assembly. Comput. Graph. Forum 32, 8 (2013), 233–245. \ 60. Xu, B., Chang, W., Sheffer, A., Bousseau, A., McCrae, J., and Singh, K. True2form: 3d curve networks from 2d sketches via selective regularization. ACM TOG (SIGGRAPH) 33, 4 (2014). \ 61. Xu, K., Chen, K., Fu, H., Sun, W.-L., and Hu, S.-M. Sketch2scene: Sketch-based co-retrieval and co-placement of 3d models. ACM Transactions on Graphics 32, 4 (2013), 123:1–123:15. \ 62. Zeleznik, R. C., Herndon, K. P., and Hughes, J. F. Sketch: An interface for sketching 3d scenes. In SIGGRAPH ’96 (1996), 163–170. \ 63. Zhang, Y., Xu, W., Tong, Y., and Zhou, K. Online structure analysis for real-time indoor scene reconstruction. ACM Transactions on Graphics (TOG) 34, 5 (2015), 159. \ 64. Zheng, Y., Chen, X., Cheng, M.-M., Zhou, K., Hu, S.-M., and Mitra, N. J. Interactive images: Cuboid proxies for smart image manipulation. ACM Trans. Graph. 31, 4 (July 2012), 99:1–99:11. \ 65. Zheng, Y., Liu, H., Dorsey, J., and Mitra, M. Smart canvas: Context-inferred interpretation of sketches for preparatory design studies. Computer Graphics Forum (Proc. Eurographics) 35, 2 (2016). \ 66. Zollh¨ofer, M., Nießner, M., Izadi, S., Rehmann, C., Zach, C., Fisher, M., Wu, C., Fitzgibbon, A., Loop, C., Theobalt, C., et al. Real-time non-rigid reconstruction using an rgb-d camera. ACM Transactions on Graphics (TOG) 33, 4 (2014), 156. \ ","Sketch-based modeling, modeling in context, swept surfaces",H.5.2;I.3.8,uistf1973-file1.zip,,uistf1973-file3.mp4,,,,An effective system for fast prototyping 3d modeling on rgbd images.,"Summary of changes: \  \ 1. We have added a short discussion on the input being stylus and mouse. \ 2. Comparison with baseline approaches mimicing SketchUp or similar interfaces is being mentioned in a more upfront manner (in Evaluation section).  \ 3. We have added a more detailed comparison on the plane extraction methods (with manually specified ground truth planes). See Figure 12, Table 2, and the two paragraphs in the Experiments Section. \ 4. We have added the missing column in Table 1. \ 5. We have added a separate subsection to discuss the expressiveness of our tool in terms of the modeling capability. \ 6. We have properly rearrange the figures to reduce unnecessary jumps. \ 7. We have toned down the claims on ""lack of 3D contents"" as the main motivation.  \ 8. We have condensed the experiments part. \ 9. We have updated the rendering of bins (in separated bins). \  \ All changes are highlighted.",Yuwei Li,Xi Luo,FormatComplete,61502306; 61602310,National Natural Science Foundation of China,the Research Grants Council of the Hong Kong,"CityU11300615, 11204014, 113513",Shenzhen Innovation Program,JCYJ20170302154106666,Aug 2 5:19,
uistf4637,10/24,9,3D Sketching,11:00:00 AM,12:10:00 AM,3+1,11:20:00 AM,11:40:00 AM,long,long,uistf1973,2,902,,,uistf4637,A,DreamSketch: Early Stage 3D Design Explorations with Sketching and Generative Design,Rubaiat Habib,Kazi,rubaiat.habib@autodesk.com,uistf4637-paper.pdf,14,letter,,,"Rubaiat Habib Kazi, Tovi Grossman, Hyunmin Cheong, Ali B. Hashemi, George Fitzmaurice","rubaiat.habib@autodesk.com, tovi.grossman@autodesk.com, hyunmin.cheong@autodesk.com, ali.hashemi@autodesk.com, George.Fitzmaurice@autodesk.com",17764,Rubaiat Habib,,Kazi,rubaiat.habib@autodesk.com,,Autodesk Research,Toronto,Ontario,Canada,,,,,,1476,Tovi,,Grossman,tovi.grossman@autodesk.com,,Autodesk Research,Toronto,Ontario,Canada,,,,,,46204,Hyunmin,,Cheong,hyunmin.cheong@autodesk.com,Autodesk Research,Autodesk,Toronto,Ontario,Canada,,,,,,71729,Ali,B.,Hashemi,ali.hashemi@autodesk.com,Autodesk Research,Autodesk,Toronto,Ontario,Canada,,,,,,1229,George,,Fitzmaurice,George.Fitzmaurice@autodesk.com,,Autodesk Research,Toronto,Ontario,Canada,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present DreamSketch, a novel 3D design interface that combines the free-form and expressive qualities of sketching with the computational power of generative design algorithms. In DreamSketch, a user coarsely defines the problem by sketching the design context. Then, a generative design algorithm produces multiple solutions that are augmented as 3D objects in the sketched context. The user can interact with the scene to navigate through the generated solutions. The combination of sketching and generative algorithms enables designers to explore multiple ideas and make better informed design decisions during the early stages of design. Design study sessions with designers and mechanical engineers demonstrate the expressive nature and creative possibilities of DreamSketch.",rubaiat.habib@autodesk.com,"1. Allaire, G., Jouve, F., & Toader, A. M. (2002). A levelset method for shape optimization. Comptes Rendus Mathematique, 334(12), 1125-1130. \ 2. Andre, A., & Saito, S. (2011, August). Single-view sketch based modeling. In Proceedings of the Eighth Eurographics Symposium on Sketch-Based Interfaces and Modeling (pp. 133-140). ACM. \ 3. Arora, R., Darolia, I., Namboodiri, V. P., Singh, K., & Bousseau, A. (2016). SketchSoup: Exploratory Ideation using Design Sketches. \ 4. Attar, R., Aish, R., Stam, J., Brinsmead, D., Tessier, A., Glueck, M., & Khan, A. (2009). Physics-based generative design. \ 5. Bae, S. H., Balakrishnan, R., & Singh, K. (2008, October). ILoveSketch: as-natural-as-possible sketching system for creating 3d curve models. In Proceedings of the 21st annual ACM symposium on User interface software and technology (pp. 151-160). ACM. \ 6. Bae, S. H., Balakrishnan, R., & Singh, K. (2009, October). EverybodyLovesSketch: 3D sketching for a broader audience. In Proceedings of the 22nd annual ACM symposium on User interface software and technology (pp. 59-68). ACM. \ 7. Baskinger, M., & Bardel, W. (2013). Drawing Ideas: A Hand-drawn Approach for Better Design. WatsonGuptill. \ 8. Bendsøe, M. P. (1989). Optimal shape design as a material distribution problem. Structural optimization, 1(4), 193-202. \ 9. Bendsoe, M. P., & Sigmund, O. (2013). Topology optimization: theory, methods, and applications. Springer Science & Business Media. \ 10. Buxton, B. (2010). Sketching user experiences: getting the design right and the right design. Morgan Kaufmann. \ 11. Dillon, M. R. (2010). Dynamic design: Cognitive processes in design sketching. Indiana Undergraduate Journal of Cognitive Science, 5, 28-43. \ 12. Do, E. Y. L. (2005). Design sketches and sketch design tools. Knowledge-Based Systems, 18(8), 383-405. \ 13. Dorsey, J., Xu, S., Smedresman, G., Rushmeier, H., & McMillan, L. (2007, October). The mental canvas: A tool for conceptual architectural design and analysis. In Computer Graphics and Applications, 2007. PG'07. 15th Pacific Conference on (pp. 201-210). IEEE. \ 14. Du, T., Schulz, A., Zhu, B., Bickel, B., & Matusik, W. (2016). Computational multicopter design. ACM Transactions on Graphics (TOG), 35(6), 227. \ 15. Goel, V. (1995). Sketches of thought. MIt Press. \ 16. Gross, M. D., & Do, E. Y. L. (1996, November). Ambiguous intentions: a paper-like interface for creative design. In Proceedings of the 9th annual ACM symposium on User interface software and technology (pp. 183-192). ACM. \ 17. Igarashi, T., & Hughes, J. F. (2001, November). A suggestive interface for 3D drawing. In Proceedings of the 14th annual ACM symposium on User interface software and technology (pp. 173-181). ACM. \ 18. Kallio, K. (2005). 3D6B editor: projective 3D sketching with line-based rendering. \ 19. Kazi, R. H., Chevalier, F., Grossman, T., & Fitzmaurice, G. (2014, October). Kitty: sketching dynamic and interactive illustrations. In Proceedings of the 27th annual ACM symposium on User interface software and technology (pp. 395-405). ACM. \ 20. Kazi, R. H., Grossman, T., Mogk, C., Schmidt, R., & Fitzmaurice, G. (2016, May). ChronoFab: Fabricating Motion. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (pp. 908918). ACM. \ 21. Kenwright, B. (2012). Inverse kinematics–cyclic coordinate descent (CCD). Journal of Graphics Tools, 16(4), 177-217. \ 22. Kim, Y., & Bae, S. H. (2016, October). SketchingWithHands: 3D Sketching Handheld Products with First-Person Hand Posture. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (pp. 797-808). ACM. \ 23. Lee, Y. J., Zitnick, C. L., & Cohen, M. F. (2011, August). Shadowdraw: real-time user guidance for freehand drawing. In ACM Transactions on Graphics (TOG) (Vol. 30, No. 4, p. 27). ACM. \ 24. Martínez, J., Dumas, J., Lefebvre, S., & Wei, L. Y. (2015). Structure and appearance optimization for controllable shape design. ACM Transactions on Graphics (TOG), 34(6), 229. \ 25. Murugappan, S., & Ramani, K. (2009, January). Feasy: a sketch-based interface integrating structural analysis in early design. In ASME 2009 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference (pp. 743752). American Society of Mechanical Engineers. \ 26. Nishida, G., Garcia-Dorado, I., Aliaga, D. G., Benes, B., & Bousseau, A. Interactive Sketching of Urban Procedural Models. ACM Transactions on Graphics (TOG), 35(4), 130. \ 27. Olsen, L., Samavati, F. F., Sousa, M. C., & Jorge, J. A. (2009). Sketch-based modeling: A survey. Computers & Graphics, 33(1), 85-103. \ 28. Paczkowski, P., Kim, M. H., Morvan, Y., Dorsey, J., Rushmeier, H. E., & O'Sullivan, C. (2011). Insitu: sketching architectural designs in context. ACM Trans. Graph., 30(6), 182. \ 29. Parish, Y. I., & Müller, P. (2001, August). Procedural modeling of cities. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques (pp. 301-308). ACM. \ 30. Piccolotto, M. A. (1998). Sketchpad+ architectural modeling through perspective sketching on a penbased display (Doctoral dissertation, Cornell University). \ 31. Schmidt, R., Khan, A., Kurtenbach, G., & Singh, K. (2009, August). On expert performance in 3D curvedrawing tasks. In Proceedings of the 6th eurographics symposium on sketch-based interfaces and modeling (pp. 133-140). ACM. \ 32. Shao, C., Bousseau, A., Sheffer, A., & Singh, K. (2012). CrossShade: shading concept sketches using cross-section curves. ACM Transactions on Graphics, 31(4). \ 33. Shao, T., Li, W., Zhou, K., Xu, W., Guo, B., & Mitra, N. J. (2013). Interpreting concept sketches. ACM Transactions on Graphics (TOG), 32(4), 56. \ 34. Suwa, M., & Tversky, B. (1996, April). What architects see in their sketches: Implications for design tools. In Conference Companion on Human Factors in Computing Systems (pp. 191-192). ACM. \ 35. Ulu, N.G, Kara, B.L., Generative interface structure design for supporting existing objects. Journal of Visual Languages and Computing (pp. 171-183). \ 36. Umetani, N., Igarashi, T., & Mitra, N. J. (2012). Guided exploration of physically valid shapes for furniture design. ACM Trans. Graph., 31(4), 86-1. \ 37. Wang, M. Y., Wang, X., & Guo, D. (2003). A level set method for structural topology optimization. Computer methods in applied mechanics and engineering, 192(1), 227-246. \ 38. Wang, Y., Chen, Y., Liu, J., & Tang, X. (2009, June). 3D reconstruction of curved objects from single 2D line drawings. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on (pp. 1834-1841). IEEE. \ 39. Xu, B., Chang, W., Sheffer, A., Bousseau, A., McCrae, J., & Singh, K. (2014). True2Form: 3D curve networks from 2D sketches via selective regularization. ACM Transactions on Graphics, 33(4). \ 40. Zaman, L., Stuerzlinger, W., Neugebauer, C., Woodbury, R., Elkhaldi, M., Shireen, N., & Terry, M. (2015, April). Gem-ni: A system for creating and managing alternatives in generative design. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (pp. 12011210). ACM. \ 41. Zheng, Y., Liu, H., Dorsey, J., & Mitra, N. J. (2016). Ergonomics-inspired reshaping and exploration of collections of models. IEEE Transactions on Visualization and Computer Graphics, 22(6), 17321744.  \ ","Sketching, 3D design, generative design, ambiguity, CAD",H.5.2,uistf4637-file1.docx,uistf4637-file2.jpg,uistf4637-file3.mp4,,DreamSketch is a novel 3D design interface that combines the free-form and expressive qualities of sketching with the computational power of generative design algorithms.,,DreamSketch is a novel 3D design interface that combines the free-form and expressive qualities of sketching with the computational power of generative design algorithms.,"We made the following changed in the revised version. \  \ 1. Clarified the contribution, limitations, and opportunities of the ""merits"" of sketching in SBIM (page 10). \  \ 2. Discussed and elaborated the limitations of our work, in particular the conflict between style & goals, and GD algorithms as a source of conflict (R1), the implications of GD algorithms in iterative workflow, and potential approaches to improve them (R3), and the ability to specify design and aesthetic intents (page 10).  \  \ 3. Clarified the minimum input (R2, R3) that is required by Dreamsketch(page 7). \  \ 4. In addition to these changes, we have also made some minor edits for clarification, as suggested by our reviewers. \  \ Thank you very much for your constructive feedback and comments.",Rubaiat Habib Kazi,Tovi Grossman,FormatComplete,,,,,,,Aug 8 11:18,
uistf3196,10/24,9,3D Sketching,11:00:00 AM,12:10:00 AM,3+1,11:40:00 AM,12:00:00 PM,long,long,uistf4637,3,903,,,uistf3196,A,Interactive Room Capture on 3D-Aware Mobile Devices,Aditya,Sankar,aditya@cs.washington.edu,uistf3196-paper.pdf,12,letter,,,"Aditya Sankar, Steve Seitz","aditya@cs.washington.edu, seitz@cs.washington.edu",11571,Aditya,,Sankar,aditya@cs.washington.edu,,University of Washington,Seattle,Washington,United States,,,,,,5749,Steve,,Seitz,seitz@cs.washington.edu,,University of Washington,Seattle,Washington,United States,,Google Inc.,Seattle,Washington,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a novel interactive system to simplify the process of indoor 3D CAD room modeling. Traditional room modeling methods require users to measure room and furniture dimensions, and manually select models that match the scene from large catalogs. Users then employ a mouse and keyboard interface to construct walls and place the objects in their appropriate locations. In contrast, our system leverages the sensing capabilities of a 3D aware mobile device, recent advances in object recognition, and a novel augmented reality user interface, to capture indoor 3D room models in-situ. With a few taps, a user can mark the surface of an object, take a photo, and the system retrieves and places a matching 3D model into the scene, from a large online database. User studies indicate that this modality is significantly quicker, more accurate, and requires less effort than traditional desktop tools.",aditya@cs.washington.edu,"1. Meta 2. 2017. MetaVision. https://www.metavision.com/. (2017). Accessed: 2017-04-04. \ 2. Sweet Home 3D. 2016. eTeks.  http://www.sweethome3d.com/. (2016). Accessed:  2017-04-04.  \ 3. Planner 5d. 2017. UAB. https://planner5d.com/. (2017). Accessed: 2017-04-04. \ 4. Harshit Agrawal, Udayan Umapathi, Robert Kovacs,  Johannes Frohnhofen, Hsiang-Ting Chen, Stefanie  Mueller, and Patrick Baudisch. 2015. Protopiper:  Physically Sketching Room-Sized Objects at Actual  Scale. In Proceedings of the 28th Annual ACM  Symposium on User Interface Software &  Technology (UIST ’15). ACM, New York, NY, USA,  427–436. DOI: http://dx.doi.org/10.1145/2807442.2807505 \ 5. Zen Phone AR. 2017. ASUS.  https://www.asus.com/Phone/ZenFone-AR-ZS571KL/.  (2017). Accessed: 2017-04-04.  \ 6. Mathieu Aubry, Daniel Maturana, Alexei A. Efros, Bryan C. Russell, and Josef Sivic. 2014. Seeing 3D Chairs: Exemplar Part-Based 2D-3D Alignment Using a Large Dataset of CAD Models. In Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR ’14). IEEE, 3762–3769. DOI: http://dx.doi.org/10.1109/CVPR.2014.487 \ 7. Sean Bell, Kavita Bala, and Noah Snavely. 2014. Intrinsic Images in the Wild. ACM Trans. on Graphics (SIGGRAPH) 33, 4 (2014). \ 8. Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. 2015. ShapeNet: An Information-Rich 3D Model Repository. Technical Report http://arxiv.org/abs/1512.03012 [cs.GR]. \ 9. Sungjoon Choi, Qian-Yi Zhou, and Vladlen Koltun.  2015. Robust reconstruction of indoor scenes. 2015  IEEE Conference on Computer Vision and Pattern  Recognition (CVPR) 00 (2015), 5556–5565. DOI: http://dx.doi.org/doi.ieeecomputersociety.org/10.1109/ CVPR.2015.7299195 \ 10. AR Home Designer. 2017. Elementals. http://www. elementalsweb.com/home_augmented_reality_designer/. (2017). Accessed: 2017-04-04. \ 11. Hao Du, Peter Henry, Xiaofeng Ren, Marvin Cheng, Dan B. Goldman, Steven M. Seitz, and Dieter Fox. 2011. Interactive 3D Modeling of Indoor Environments with a Consumer Depth Camera. In Proceedings of the 13th International Conference on Ubiquitous Computing (UbiComp ’11). ACM, New York, NY, USA, 75–84. DOI:http://dx.doi.org/10.1145/2030112.2030123 \ 12. Martin A. Fischler and Robert C. Bolles. 1981. Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography. Commun. ACM 24, 6 (June 1981), 381–395. DOI:http://dx.doi.org/10.1145/358669.358692 \ 13. Robert B. Fisher. 2001. Projective ICP and Stabilizing Architectural Augmented Reality Overlays. In Proc. Int. Symp. on Virtual and Augmented Architecture (VAA01. 69–80. \ 14. Yasutaka Furukawa, Brian Curless, Steven M. Seitz, and Richard Szeliski. 2009. Reconstructing building interiors from images. In In Proc. of the International Conference on Computer Vision (ICCV ’09). DOI: http://dx.doi.org/10.1109/ICCV.2009.5459145 \ 15. HoloLens. 2015. Microsoft Corp. https://www.microsoft.com/microsoft-hololens/en-us. (2015). Accessed: 2016-05-27. \ 16. Homestyler. 2013. Easyhome (Formerly Autodesk). http://www.homestyler.com. (2013). Accessed: 2017-07-13. \ 17. Qixing Huang, Hai Wang, and Vladlen Koltun. 2015. Single-view Reconstruction via Joint Analysis of Image and Shape Collections. ACM Trans. Graph. 34, 4, Article 87 (July 2015), 10 pages. DOI: http://dx.doi.org/10.1145/2766890 \ 18. Tomoya Ishikawa, Kalaivani Thangamani, Masakatsu Kourogi, AndrewP. Gee, Walterio Mayol-Cuevas, Keechul Jung, and Takeshi Kurata. 2009. In-Situ 3D Indoor Modeler with a Camera and Self-contained Sensors. In Virtual and Mixed Reality, Randall Shumaker (Ed.). Lecture Notes in Computer Science, Vol. 5622. Springer Berlin Heidelberg, 454–464. DOI: http://dx.doi.org/10.1007/978-3-642-02771-0_51 \ 19. Vision Innovation Labs. 2017. Lowes. http://www.lowesinnovationlabs.com/tango/. (2017). Accessed: 2017-04-04. \ 20. Tobias Langlotz, Stefan Mooslechner, Stefanie Zollmann, Claus Degendorfer, Gerhard Reitmayr, and Dieter Schmalstieg. 2012. Sketching Up the World: In Situ Authoring for Mobile Augmented Reality. Personal Ubiquitous Comput. 16, 6 (Aug. 2012), 623–630. DOI: http://dx.doi.org/10.1007/s00779-011-0430-0 \ 21. Manfred Lau, Masaki Hirose, Akira Ohgawara, Jun Mitani, and Takeo Igarashi. 2012. Situated Modeling: A Shape-stamping Interface with Tangible Primitives. In Proceedings of the Sixth International Conference on Tangible, Embedded and Embodied Interaction (TEI ’12). ACM, New York, NY, USA, 275–282. DOI: http://dx.doi.org/10.1145/2148131.2148190 \ 22. Yangyan Li, Hao Su, Charles Ruizhongtai Qi, Noa Fish, Daniel Cohen-Or, and Leonidas J. Guibas. 2015. Joint Embeddings of Shapes and Images via CNN Image Puriﬁcation. ACM Trans. Graph. 34, 6, Article 234 (Oct. 2015), 12 pages. DOI: http://dx.doi.org/10.1145/2816795.2818071 \ 23. Joseph J. Lim, Hamed Pirsiavash, and Antonio Torralba. 2013. Parsing IKEA Objects: Fine Pose Estimation. In Proceedings of the 2013 IEEE International Conference on Computer Vision (ICCV ’13). IEEE, 2992–2999. DOI:http://dx.doi.org/10.1109/ICCV.2013.372 \ 24. MagicPlan. 2011. Sensopia Inc. https://www.metavision.com/. (2011). Accessed: 2017-04-04. \ 25. Richard A. Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim, Andrew J. Davison, Pushmeet Kohli, Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon. 2011. KinectFusion: Real-time Dense Surface Mapping and Tracking. In Proceedings of the 2011 10th IEEE International Symposium on Mixed and Augmented Reality (ISMAR ’11). IEEE, 127–136. DOI:http://dx.doi.org/10.1109/ISMAR.2011.6092378 \ 26. Benjamin Nuernberger, Eyal Ofek, Hrvoje Benko, and Andrew D. Wilson. 2016. SnapToReality: Aligning Augmented Reality to the Real World. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 1233–1244. DOI: http://dx.doi.org/10.1145/2858036.2858250 \ 27. Patrick Paczkowski, Min H. Kim, Yann Morvan, Julie Dorsey, Holly Rushmeier, and Carol O’Sullivan. 2011. Insitu: Sketching Architectural Designs in Context. ACM Trans. Graph. 30, 6, Article 182 (Dec. 2011), 10 pages. DOI:http://dx.doi.org/10.1145/2070781.2024216 \ 28. Floor Planner. 2016. http://www.floorplanner.com. (2016). Accessed: 2017-04-04. \ 29. Planoplan. 2017. http://planoplan.com/en/. (2017). Accessed: 2017-04-04. \ 30. Phab 2 Pro. 2017. Lenovo. http://shop.lenovo.com/us/en/tango/. (2017). Accessed: 2017-04-04. \ 31. Project Tango. 2014. Google Inc., ATAP. https://www.google.com/atap/projecttango/. (2014). Accessed: 2016-05-27. \ 32. Home Planner. 2013. IKEA. http://www.ikea.com/ms/en_ JP/rooms_ideas/splashplanners.html. (2013). Accessed: 2016-05-27. \ 33. Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. 2004. ""GrabCut"": Interactive Foreground Extraction Using Iterated Graph Cuts. ACM Trans. Graph. 23, 3 (Aug. 2004), 309–314. DOI: http://dx.doi.org/10.1145/1015706.1015720 \ 34. Renato F. Salas-Moreno, Richard A. Newcombe, Hauke Strasdat, Paul H.J. Kelly, and Andrew J. Davison. 2013. SLAM++: Simultaneous Localisation and Mapping at the Level of Objects. 2013 IEEE Conference on Computer Vision and Pattern Recognition (2013), 1352–1359. DOI: http://dx.doi.org/10.1109/CVPR.2013.178 \ 35. Aditya Sankar and Steven M. Seitz. 2012. Capturing Indoor Scenes with Smartphones. In Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology (UIST ’12). ACM, 403–412. DOI:http://dx.doi.org/10.1145/2380116.2380168 \ 36. Aditya Sankar and Steven M. Seitz. 2016. In Situ CAD Capture. In Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI ’16). ACM, New York, NY, USA, 233–243. DOI: http://dx.doi.org/10.1145/2935334.2935337 \ 37. T. Schöps, T. Sattler, C. Häne, and M. Pollefeys. 2015. 3D Modeling on the Go: Interactive 3D Reconstruction of Large-Scale Scenes on Mobile Devices. In 2015 International Conference on 3D Vision. 291–299. DOI: http://dx.doi.org/10.1109/3DV.2015.40 \ 38. Michael Ian Shamos. 1978. Computational Geometry. (1978). \ 39. Tianjia Shao, Weiwei Xu, Kun Zhou, Jingdong Wang, Dongping Li, and Baining Guo. 2012. An Interactive Approach to Semantic Modeling of Indoor Scenes with an RGBD Camera. ACM Trans. Graph. 31, 6, Article 136 (Nov. 2012), 11 pages. DOI: http://dx.doi.org/10.1145/2366145.2366155 \ 40. Sketchup. 2016. Trimble Navigation Limited. http://www.sketchup.com/. (2016). Accessed: 2016-05-27. \ 41. Noah Snavely, Steven M. Seitz, and Richard Szeliski. 2006. Photo Tourism: Exploring Photo Collections in 3D. ACM Trans. Graph. 25, 3 (July 2006), 835–846. DOI:http://dx.doi.org/10.1145/1141911.1141964 \ 42. TLX. 2009. NASA. http://www.nasatlx.com/. (2009). Accessed: 2016-05-27. \ 43. Julien Valentin, Vibhav Vineet, Ming-Ming Cheng, David Kim, Jamie Shotton, Pushmeet Kohli, Matthias Nießner, Antonio Criminisi, Shahram Izadi, and Philip Torr. 2015. SemanticPaint: Interactive 3D Labeling and Learning at Your Fingertips. ACM Trans. Graph. 34, 5, Article 154 (Nov. 2015), 17 pages. DOI: http://dx.doi.org/10.1145/2751556 \ 44. Michael Waechter, Mate Beljan, Simon Fuhrmann, Nils Moehrle, Johannes Kopf, and Michael Goesele. 2017. Virtual Rephotography: Novel View Prediction Error for 3D Reconstruction. ACM Trans. Graph. 36, 1, Article 8 (Jan. 2017), 11 pages. DOI: http://dx.doi.org/10.1145/2999533 \ 45. Sketchup 3D Warehouse. 2016. Trimble Navigation Limited. https://3dwarehouse.sketchup.com/. (2016). Accessed: 2017-04-04. \ 46. Brandon Yee, Yuan Ning, and Hod Lipson. 2009. Augmented Reality In-Situ 3D Sketching of Physical Objects. (2009). \ 47. Edward Zhang, Michael F. Cohen, and Brian Curless. 2016. Emptying, Refurnishing, and Relighting Indoor Spaces. ACM Trans. Graph. 35, 6, Article 174 (Nov. 2016), 14 pages. DOI: http://dx.doi.org/10.1145/2980179.2982432 \ ",Design; Human Factors; 3D Modeling; CAD; Interactive Modeling; Mobile; Augmented Reality; User Study;,H.5.2; I.3.6,uistf3196-file1.zip,uistf3196-file2.jpg,uistf3196-file3.mp4,uistf3196-file4.zip,"Our system lets casual users generate a representative 3D CAD model of a room, in a few minutes, on a '3D-aware' mobile device. ","Auxiliary material contains a PDF document (Auxiliary_Material.pdf) with detailed results from quantitative and qualitative analysis of our user study. This includes finer-grained numerical analysis, graphs and raw quotes from users collected during the study. Summaries of this information are included in the main paper.","We present a system with which a casual user can generate a representative 3D CAD model of a room, in a few minutes, on a '3D-aware' mobile device. ","Final Camera ready revisions after acceptance: \ - PDF made accessible with tags and alt text \ - All content de-anonymized \ - PDF proofread and minor typos corrected \ - Auxiliary material updated (removed changelog and markup pdf) \  \ Requested Revisions (Highlighted in marked-up version in aux material): \ - Added a section “Design Goals and Observations” (Page 3), which succinctly defines the system goals and consolidates observations and assumptions that were previously scattered in the paper. \ - Removed scattered observations and assumptions which were consolidated above \ - Reduced length of User Study section by moving Quant data and raw quotes to Auxiliary  \ - Evaluated initial assumptions in ‘Discussion and Limitations’ section \  \ Changes based on rebuttal: \ - Added relevant references suggested by R2 (Page 2, In-Situ 3D Modeling, Line 6) \ - Added further distinction to 3D scanning techniques and reference to - \ SemanticPaint(as requested by R2) (Fully automatic 3D reconstruction, Page 3) \ - Added info about workaround to capturing glass surfaces (R1) (Discussions and limitations, Page) \ - Added info about limitation of 3D model retrieval (R2) (Discussions and limitations, Page) \ - Added analysis of initial assumptions (Discussions and limitations, Page)  \ Changed ‘Captured model’ to ‘generated model’ as suggested by R2 \  \ Minor changes: \ - Folded future work into the discussion section \ - Moved visual results figure to earlier in the text \ - Added CAD to Author Keywords (Page 1) \ - Added a figure that illustrates 3D model sampling, alignment and scoring ( Page 6, Figure 4). \ - Removed figure for metric chamfer distance error \ - Fixed formatting of Sections (that were previously marked as ‘subsections’ in error \ - Fixed typos and made minor changes in text to improve readability \ - Added Author names and de-anonymized paper and video",Aditya Sankar,Steve Seitz,FormatComplete,IIS-1250793,National Science Foundation,University of Washington Animation Research Labs,,Google,,Aug 9 18:47,
uistf1384,10/24,9,3D Sketching,11:00:00 AM,12:10:00 AM,3+1,12:00:00 PM,12:10:00 PM,short,short,uistf3196,4,904,,,uistf1384,A,SceneCtrl: Mixed Reality Enhancement via Efficient Scene Editing,Ya-Ting,Yue,ytyue@cs.hku.hk,uistf1384-paper.pdf,6,letter,,,"Ya-Ting Yue, Yongliang Yang, Gang Ren, Wenping Wang","ytyue2305@gmail.com, strongyang@gmail.com, rengang@xmut.edu.cn, wenping@cs.hku.hk",63058,Ya-Ting,,Yue,ytyue2305@gmail.com,Department of Computer Science,The University of Hong Kong,Hong Kong,,China,,,,,,63404,Yongliang,,Yang,strongyang@gmail.com,Computer Science Department,University of Bath,Bath,Avon,United Kingdom,,,,,,40060,Gang,,Ren,rengang@xmut.edu.cn,School of Digital Art,Xiamen University of Technology,Xiamen,Fujian,China,,,,,,63778,Wenping,,Wang,wenping@cs.hku.hk,,The University of Hong Kong,Hong Kong,,Hong Kong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Due to the development of 3D sensing and modeling techniques, the state-of-the-art mixed reality devices such as Microsoft Hololens have the ability of digitalizing the physical world. \ This unique feature bridges the gap between virtuality and reality and largely elevates the user experience. \ Unfortunately, the current solution only performs well if the virtual contents complement the real scene. \ It can easily cause visual artifacts when the reality needs to be modified due to the virtuality (e.g., remove real objects to offer more space for virtual objects), a common scenario in mixed reality applications such as room redecoration and environment design. \ We present a novel system, called \\emph{SceneCtrl}, that allows the user to interactively edit the real scene sensed by Hololens, such that the reality can be adapted to suit virtuality. \ Our proof-of-concept prototype employs scene reconstruction and understanding to enable efficient editing such as deleting, moving, and copying real objects in the scene. \ We also demonstrate \\emph{SceneCtrl} on a number of example scenarios in mixed reality, verifying the enhanced experience by resolving conflicts between virtuality and reality.",ytyue2305@gmail.com,"1. Santiago Arroyave-Tobón, Gilberto Osorio-Gómez, and Juan F Cardona-McCormick. 2015. Air-modelling: a tool for gesture-based solid modelling in context during early design stages in AR environments. Computers in Industry 66 (2015), 73–81. \ 2. Hrvoje Benko, Andrew D Wilson, and Federico Zannier. 2014. Dyadic projected spatial augmented reality. In Proceedings of the 27th annual ACM symposium on User interface software and technology. ACM, 645–655. \ 3. Chris Buehler, Michael Bosse, Leonard McMillan, Steven Gortler, and Michael Cohen. 2001. Unstructured Lumigraph Rendering. In Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH ’01). ACM, New York, NY, USA, 425–432. \ 4. Kang Chen, Yu-Kun Lai, and Shi-Min Hu. 2015. 3D indoor scene modeling from RGB-D data: a survey. Computational Visual Media 1, 4 (2015), 267–278. \ 5. Kai-Yin Cheng, Yu-Hsiang Lin, Yu-Hsin Lin, Bing-Yu Chen, and Takeo Igarashi. 2011. Grab-carry-release: manipulating physical objects in a real scene through a smart phone. In SIGGRAPH Asia 2011 Emerging Technologies. ACM, 13. \ 6. Michael D Grossberg, Harish Peri, Shree K Nayar, and Peter N Belhumeur. 2004. Making one object look like another: Controlling appearance using a projector-camera system. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, Vol. 1. IEEE, I–I. \ 7. Anselm Grundhofer and Oliver Bimber. 2008. Real-time adaptive radiometric compensation. IEEE transactions on visualization and computer graphics 14, 1 (2008), 97–108. \ 8. Saurabh Gupta, Pablo Arbeláez, Ross Girshick, and Jitendra Malik. 2015. Indoor Scene Understanding with RGB-D Images: Bottom-up Segmentation, Object Detection and Semantic Segmentation. Int. J. Comput. Vision 112, 2 (2015), 133–149. \ 9. S. Gupta, P. ArbelÂ´lÂ´cez, and J. Malik. 2013. Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images. In 2013 IEEE Conference on Computer Vision and Pattern Recognition. 564–571. \ 10. Anuruddha Hettiarachchi and Daniel Wigdor. 2016. Annexing Reality: Enabling Opportunistic Use of Everyday Objects As Tangible Proxies in Augmented Reality. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). 1957–1967. \ 11. Hololens. 2017. http://www.hololens.com. (2017). \ 12. HoloToolkit. 2017. https://github.com/Microsoft/HoloToolkit-Unity. (2017). \ 13. Fu-Jen Hsiao, Chih-Jen Teng, Chung-Wei Lin, An-Chun Luo, and Jinn-Cherng Yang. 2010. Dream Home: a multiview stereoscopic interior design system. In IS&T/SPIE Electronic Imaging. International Society for Optics and Photonics, 75250J–75250J. \ 14. Shahram Izadi, David Kim, Otmar Hilliges, David Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie Shotton, Steve Hodges, Dustin Freeman, Andrew Davison, and Andrew Fitzgibbon. 2011. KinectFusion: Real-time 3D Reconstruction and Interaction Using a Moving Depth Camera. In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (UIST ’11). 559–568. \ 15. Brett Jones, Rajinder Sodhi, Michael Murdock, Ravish Mehra, Hrvoje Benko, Andrew Wilson, Eyal Ofek, Blair MacIntyre, Nikunj Raghuvanshi, and Lior Shapira. 2014. RoomAlive: Magical Experiences Enabled by Scalable, Adaptive Projector-camera Units. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST ’14). 637–644. \ 16. Brett R. Jones, Hrvoje Benko, Eyal Ofek, and Andrew D. Wilson. 2013. IllumiRoom: Peripheral Projected Illusions for Interactive Experiences. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’13). 869–878. \ 17. Robert S Kennedy, Norman E Lane, Kevin S Berbaum, and Michael G Lilienthal. 1993. Simulator sickness questionnaire: An enhanced method for quantifying simulator sickness. The international journal of aviation psychology 3, 3 (1993), 203–220. \ 18. Jarrod Knibbe, Hrvoje Benko, and Andrew D Wilson. 2015. Juggling the effects of latency: Motion prediction approaches to reducing latency in dynamic projector-camera systems. (2015). \ 19. Tiina Kymäläinen and Sanni Siltanen. 2013. Co-Designing Novel Interior Design Services that Utilise Augmented Reality: A Case Study. Advanced Research and Trends in New Technologies, Software, Human-Computer Interaction, and Communicability (2013), 269. \ 20. P. Musialski, P. Wonka, D. G. Aliaga, M. Wimmer, L. Gool, and W. Purgathofer. 2013. A Survey of Urban Reconstruction. Comput. Graph. Forum 32, 6 (2013), 146–177. \ 21. Shree K. Nayar, Harish Peri, Michael D. Grossberg, and Peter N. Belhumeur. 2003. A projection system with radiometric compensation for screen imperfections. \ 22. Benjamin Nuernberger, Eyal Ofek, Hrvoje Benko, and Andrew D. Wilson. 2016. SnapToReality: Aligning Augmented Reality to the Real World. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). 1233–1244. \ 23. Sergio Orts-Escolano, Christoph Rhemann, Sean Fanello, Wayne Chang, Adarsh Kowdle, Yury Degtyarev, David Kim, Philip L Davidson, Sameh Khamis, Mingsong Dou, and others. 2016. Holoportation: Virtual 3D Teleportation in Real-time. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology. ACM, 741–754. \ 24. Patrick Pérez, Michel Gangnet, and Andrew Blake. 2003. Poisson Image Editing. ACM Trans. Graph. 22, 3 (July 2003), 313–318. \ 25. Viet Toan Phan and Seung Yeon Choo. 2010. Interior design in augmented reality environment. International Journal of Computer Applications 5, 5 (2010). \ 26. Christina Pollalis, Whitney Fahnbulleh, Jordan Tynes, and Orit Shaer. 2017. HoloMuse: Enhancing Engagement with Archaeological Artifacts through Gesture-Based Interaction with Holograms. In Proceedings of the Tenth International Conference on Tangible, Embedded, and Embodied Interaction. ACM, 565–570. \ 27. Renato F Salas-Moreno, Ben Glocken, Paul HJ Kelly, and Andrew J Davison. 2014. Dense planar SLAM. In Mixed and Augmented Reality (ISMAR), 2014 IEEE International Symposium on. IEEE, 157–164. \ 28. Renato F Salas-Moreno, Richard A Newcombe, Hauke Strasdat, Paul HJ Kelly, and Andrew J Davison. 2013. Slam++: Simultaneous localisation and mapping at the level of objects. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1352–1359. \ 29. Jonas Schild, Joseph LaViola, and Maic Masuch. 2012. Understanding User Experience in Stereoscopic 3D Games. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’12). ACM, New York, NY, USA, 89–98. \ 30. R. Schnabel, R. Wahl, and R. Klein. 2007. Efﬁcient RANSAC for Point-Cloud Shape Detection. Computer Graphics Forum 26, 2 (2007), 214–226. \ 31. Carsten Schwede and Thomas Hermann. 2015. HoloR: Interactive mixed-reality rooms. In Cognitive Infocommunications (CogInfoCom), 2015 6th IEEE International Conference on. IEEE, 517–522. \ 32. S. Song and J. Xiao. 2016. Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 808–816. \ 33. SpatialMapping. 2017. https://developer.microsoft.com/en-us/windows/mixedreality/spatial_mapping. (2017). \ 34. Julien Valentin, Vibhav Vineet, Ming-Ming Cheng, David Kim, Jamie Shotton, Pushmeet Kohli, Matthias Nießner, Antonio Criminisi, Shahram Izadi, and Philip Torr. 2015. Semanticpaint: Interactive 3d labeling and learning at your ﬁngertips. ACM Transactions on Graphics (TOG) 34, 5 (2015), 154. \ 35. Xiangyu Wang, Mi Jeong Kim, Peter ED Love, and Shih-Chung Kang. 2013. Augmented Reality in built environment: Classiﬁcation and implications for future research. Automation in Construction 32 (2013), 1–13. \ 36. Gordon Wetzstein and Oliver Bimber. 2007. Radiometric compensation through inverse light transport. (2007). \ 37. Andrew Wilson, Hrvoje Benko, Shahram Izadi, and Otmar Hilliges. 2012. Steerable augmented reality with the beamatron. In Proceedings of the 25th annual ACM symposium on User interface software and technology. ACM, 413–422. \ 38. Edward Zhang, Michael F. Cohen, and Brian Curless. 2016. Emptying, Refurnishing, and Relighting Indoor Spaces. ACM Trans. Graph. 35, 6 (2016), 174:1–174:14. \ 39. Qian-Yi Zhou and Vladlen Koltun. 2014. Color Map Optimization for 3D Reconstruction with Consumer Depth Cameras. ACM Trans. Graph. 33, 4 (2014), 155:1–155:10. \ 40. Qian-Yi Zhou and Ulrich Neumann. 2013. Complete Residential Urban Area Reconstruction from Dense Aerial LiDAR Point Clouds. Graph. Models 75, 3 (2013), 118–125. \ ",Mixed reality; enhanced experience; scene editing,H.5.1,uistf1384-file1.zip,,,,,,"We propose a proof-of-concept system that allows efficient scene editing in MR, which provides novel editing tools for efficiently editing (delete, copy and move) real objects in the scene.","Many thanks for all reviewers’ comments and suggestions. They are really helpful to improve the quality of the paper. We have carefully addressed all the reviewers’ comments. The revised parts are highlighted in blue and summarized as follows. Please let us know if further improvements are required. Thanks very much! \  \ ** System evaluation** \ We have added a user study to further evaluate our system. 10 volunteers were recruited and 2 example scenarios were used for the evaluation. We have tested the level of comfort and satisfaction of using our prototype system, and received positive feedback. (see updates in “User Evaluation” section).  \  \ ** Optical see-through discussion** \ We have investigated how to capture video from “optical see-through” but it turns out to be very difficult. So instead, we have noted and discussed the difference between “optical see-through” and “video see through” based on user evaluation, and also addressed it in the limitation section. (see updates “Noted that Hololens is an optical see-through device…”, discussions of “Optical See-through Experience”, and limitation about “the user’s visual experience can be affected by the optical see-through…”) \  \ ** Clarification on algorithm selection ** \ We have added clarification on algorithm selection for texture blending and scene understanding. (see updates “Note that we have tried advanced texture mapping and blending algorithms…”, and “On the other hand, high-level scene understanding can…”) \  \ ** 3D geometry of real object ** \ We have clarified that we got 3D geometry of real object from the initial rough 3D mesh patches provided by Hololens. (see several updates in the first paragraph of “Scene Reconstruction” section) \  \ ** Missing references ** \ We have added all the missing references pointed out by the reviewers (see “[21, 6, 36, 7] for radiometric compensation”, [33] for spatial mapping of Hololens, [24] for advanced texture blending, and [28][34] for high-level scene understanding). \  \ ** Our contribution ** \ We have clarified one of the contributions of our paper: we employ straightforward yet effective algorithms to realize efficient scene editing tools solely on Hololens (see updates “Compared with traditional MR solutions …” and “A set of novel scene editing tools based on…” in the “Introduction” section) \  \  \ ",Ya-Ting Yue,Yongliang Yang,FormatComplete,EP/M023281/1,EPSRC,Fujian Provincial Social Science Project,FJ2016C095,Fujian Provinci al Natural Science Project,2017J01784,Aug 8 21:53,
uistf4937,10/24,10,Fabrication,11:00:00 AM,12:10:00 AM,3+1,11:00:00 AM,11:20:00 AM,long,long,none,1,1001,,,uistf4937,A,FoamSense: Design of three dimensional soft sensors with porous materials,Satoshi,Nakamaru,s.nakamaru@keio.jp,uistf4937-paper.pdf,11,letter,,,"Satoshi Nakamaru, Ryosuke Nakayama, Ryuma Niiyama, Yasuaki Kakehi","s.nakamaru@keio.jp, wayne.marine47@gmail.com, niiyama@isi.imi.i.u-tokyo.ac.jp, ykakehi@sfc.keio.ac.jp",71876,Satoshi,,Nakamaru,s.nakamaru@keio.jp,,Keio University,Kanagawa,,Japan,,,,,,71887,Ryosuke,,Nakayama,wayne.marine47@gmail.com,,Keio University,Kanagawa,,Japan,,,,,,35547,Ryuma,,Niiyama,niiyama@isi.imi.i.u-tokyo.ac.jp,,The University of Tokyo,Tokyo,,Japan,,,,,,19312,Yasuaki,,Kakehi,ykakehi@sfc.keio.ac.jp,,Keio University,Kanagawa,,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Here we report the new soft sensor “FoamSense” that can measure the deformation state of a volumetric soft object such as compressed, bent, twisted and sheared (Figure 1). This sensor is made by impregnating a porous soft object with conductive ink. The design process of FoamSense is explained. We then summarized the features and basic characteristics of some porous materials for designing these sensors appropriately. We also proposed the potential of using digital fabrication for controlling the carrier structure of FoamSense. Proposed porous structure showed an anisotropic sensor characteristic. We discussed the potential and limitation of this approach. Three possible applications are proposed by using FoamSense. FoamSense supports a richer interaction between the user and soft objects.",s.nakamaru@keio.jp,"1. Chun, S., Hong, A., Choi, Y., Ha, C. and Park, W., 2016, A tactile sensor using a conductive graphenesponge composite, Nanoscale, 8, 9185. \ 2. Dunne, L.E., Brady, S., Smyth, B., and Diamond, D. 2005. Initial development and testing of a novel foambased pressure sensor for wearable sensing. Journal of NeuroEngineering and Rehabilitation. 2:4 \ 3. Dunne, L. E., Brady, S., Tynan, R., Lau, K., Smyth, B., and O'Hare, G. M. P. 2006. ç. In Proceedings of the7th Australian User interface conference(AUIC '06), 50. \ 4. Hammock, M.L., Chortos, A., Tee, B.C.K., Tok, J.B.H. and Bao, Z. 2013. 25th anniversary article: The evolution of electronic skin (E-Skin): A brief history, design considerations, and recent progress. Advanced Materials. 25, 42: 5997–6038. \ 5. Holman, D., Girouard, A., Benko, H., and Vertegaal, R. 2013. The design of organic user interfaces: Shape, sketching and hypercontext. Interacting with Computers 25, 2: 133–142. \ 6. Jentoft, L.P., Tenzer, Y., Vogt, D., Wood, R.J. and Howe, R.D. 2013. Flexible, stretchable tactile arrays from MEMS barometers. 2013 16th International Conference on Advanced Robotics (ICAR). (2013), 1– 6. \ 7. Johnson, M.P., Wilson, A., Blumberg, B., Kline, C. and Bobick, A. 1999. Sympathetic Interfaces: Using a Plush Toy to Direct Synthetic Characters. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '99), 152–158. \ 8. Kong, X., Duan, X., Zhao, H. and Wang, Y. 2012. An active medical supporting manipulator and experiments for vascular interventional robot. 2012 IEEE International Conference on Mechatronics and Automation ( ICMA 2012), 617–622. \ 9. Li, Y., Zhu, W., Yu, X., Huang, P., Fu, S. and Hu, N. 2016. Multifunctional Wearable Device Based on Flexible and Conductive Carbon Figure 21. Stick shape toy prototype (Left: system diagram, Right: appearance) Sponge/Polydimethylsiloxane Composite. Appl. Mater. Interfaces. 8, 48: 33189-33196 \ 10. Liang, J., Li, L., Tong, K., Ren, Z., Hu, W., Niu, X., Chen, Y., Pei, Q., 2014. Silver Nanowire Percolation Network Soldered with Graphene Oxide at Room Temperature and Its Application for Fully Stretchable Polymer Light-emitting diodes. ACS Nano 8, 2: 1590– 1600. \ 11. Liu. W., Chen, Z., Zhou, G., Yongming Sun, Hye Ryoung Lee, and Chong Liu. 2016. 3D Porous Sponge-Inspired Electrode for Stretchable LithiumIon Batteries. Adv. Mater. 28, 18: 3578-3583. \ 12. Lopes, A.J., MacDonald, E. and Wicker, R.B. 2012. Integrating stereolithography and direct print technologies for 3D structural electronics fabrication. Rapid Prototyping Journal. 18, 2: 129-143. \ 13. Makino, Y., Sugiura, Y., Ogata, M. and Inami, M. 2013. Tangential Force Sensing System on Forearm, In Proceedings of the 4th Augmented Human International Conference (AH'13), 29–34. \ 14. Marti, S. and Schmandt, C. 2005. Physical Embodiments for Mobile Communication Agents. In Proceedings of the 18th annual ACM symposium on User interface software and technology (UIST '05), 231-240. \ 15. Matsuhisa, N., Kaltenbrunner, M., Yokota, T., Jinno, H., Kuribara, K., Sekitani, T. and Someya, T. 2015. Printable elastic conductors with a high conductivity for electronic textile applications. Nature Communications. 6, 7461. \ 16. Menguc, Y., Park, Y., Martinez-villalpando, E., Aubin, P., Zisook, M., Stirling, L., Wood, R.J. and Walsh, C.J. 2013. Soft Wearable Motion Sensing Suit for Lower Limb. In Proccedigns of IEEE International conference on Robotics and Automation (ICRA '13), 5289–5296. \ 17. Ogata, M., Sugiura, Y., Makino, Y., Inami, M. and Imai, M. Augmenting a Wearable Display with Skin Surface as an Expanded Input Area. In Proceedings of Interanational Conference of Design, User Experience, and Usability (DUXU '14), 606-614 \ 18. Schkarbanenko, H. A. 2013. 2D ELECTRICAL IMPEDANCE TOMOGRAPHY., Retived March 24, 2017 from http://www.math.kit.edu/iag1 \ 19. Sekitani, T., Zschieschang, U., Klauk, H. and Someya, T. 2010. Flexible organic transistors and circuits with extreme bending stability. Nature materials. 9, 12: 1015–1022. \ 20. Shaw, M., Ziglioli, F., Combi, C. and Baldo, L. 2008. Package design of pressure sensors for high volume consumer applications. 2008 58th Electronic Components and Technology Conference. C, (2008), 834–840. \ 21. Sugiura, Y. and Igarashi, T. 2012. A Thin Stretchable Interface for Tangential Force Measurement. In Proceedings of the 25th annual ACM symposium on User interface software and technology (UIST '12), 529–535. \ 22. Sugiura, Y., Kakehi, G., Withana, A., Lee, C., Sakamoto, D., Sugimoto, M., Inami, M. and Igarashi, T. 2011. Detecting Shape Deformation of Soft Objects Using Directional Photoreflectivity Measurement. In Proceedings of the 24th annual ACM symposium on User interface and software technology (UIST '11), 509–516. \ 23. Sugiura, Y., Lee, C., Ogata, M., Withana, A., Makino, Y., Sakamoto, D., Inami, M. and Igarashi, T. 2012. PINOKY : A Ring That Animates Your Plush Toys. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '12), 725-734. \ 24. Trimmer, B., Lewis, J.A., Shepherd, R.F. and Lipson, H. 2015. 3D Printing Soft Materials: What Is Possible?, Soft Robotics. 2, 1: 3–6. \ 25. Vogt, D., Park, Y.L. and Wood, R.J. 2012. A soft multi-axis force sensor. In Proceedings of IEEE Sensors. (2012), 0–3. \ 26. Wang, Y., Hua, T. and Zhu, B. 2015. Novel highly sensitive and wearable pressure sensors from conductive three- dimensional fabric structures. Smart Mater. Struct. 24, 125022. \ 27. Wessely, M., Orsay, F.- and Mackay, W.E. 2016. Stretchis : Fabricating Highly Stretchable User Interfaces. In Proceedings of the 29rh Annual Symposium on User Interface Software and Technology (UIST '16), 697-704. \ 28. Xu, D., Tairych, A., and Anderson, A.I. 2016. Stretch not flex: programmable rubber keyboard. Smart Materials and Structures 25, 1: 15012. \ 29. Yao, H., Ge, J., Wang, C., Wang, X., Hu, W. and Zheng, Z. 2013. A Flexible and Highly PressureSensitive Graphene – Polyurethane Sponge Based on Fractured Microstructure Design. Adv. Mater. 25, 46: 6692–6698. \ ",Soft sensor; Design Process; Tangible User Interface ,"H.5.m. Information interfaces and presentation (e.g., HCI): Miscellaneous",uistf4937-file1.docx,uistf4937-file2.jpg,uistf4937-file3.mp4,,FoamSense is a volumeric soft sensor made by the soft porous material and conductive ink.,,"FoamSense is a kind of volumetric soft sensor made of soft porous material and conductive inks that can measure various deformations such as compression, twist, shear.",Update the points that our shepherd pointed out. \ Author name and Acknowledgement was added,Satoshi Nakamaru,Ryosuke Nakayama,FormatComplete,JPMJER1501,JST ERATO Grant,,,,,Aug 8 17:35,
uistf3497,10/24,10,Fabrication,11:00:00 AM,12:10:00 AM,3+1,11:20:00 AM,11:40:00 AM,long,long,uistf4937,2,1002,Best Paper,,uistf3497,A,AirCode: Unobtrusive Physical Tags for Digital Fabrication,Dingzeyu,Li,dli@cs.columbia.edu,uistf3497-paper.pdf,12,letter,,,"Dingzeyu Li, Avinash S Nair, Shree K Nayar, Changxi Zheng","dli@cs.columbia.edu, asn2129@columbia.edu, nayar@cs.columbia.edu, cxz@cs.columbia.edu",71676,Dingzeyu,,Li,dli@cs.columbia.edu,Computer Science,Columbia University,New York,New York,United States,,,,,,71678,Avinash,S,Nair,asn2129@columbia.edu,Computer Science,Columbia University,New York,New York,United States,,,,,,35498,Shree,K,Nayar,nayar@cs.columbia.edu,,Columbia University,New York City,New York,United States,,,,,,71677,Changxi,,Zheng,cxz@cs.columbia.edu,Computer Science,Columbia University,New York,New York,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present AirCode, a technique that allows the user to tag physically fabricated objects with given information. An AirCode tag consists of a group of carefully designed air pockets placed beneath the object surface. These air pockets are easily produced during the fabrication process of the object, without any additional material or postprocessing. Meanwhile, the air pockets affect only the scattering light transport under the surface, and thus are hard to notice to our naked eyes. But, by using a computational imaging method, the tags become detectable. We present a tool that automates the design of air pockets for the user to encode information. AirCode system also allows the user to retrieve the information from captured images via a robust decoding algorithm. We demonstrate our tagging technique with applications for metadata embedding, robotic grasping, as well as conveying object affordances. \ ",dli@cs.columbia.edu,"1. Harshit Agrawal, Udayan Umapathi, Robert Kovacs, Johannes Frohnhofen, Hsiang-Ting Chen, Stefanie Müller, and Patrick Baudisch. 2015. Protopiper: Physically Sketching Room-Sized Objects at Actual Scale. In UIST 2015. DOI: http://dx.doi.org/10.1145/2807442.2807505 \ 2. P Bijl, JJ Koenderink, and A Toet. 1989. Visibility of blobs with a Gaussian luminance proﬁle. Vision research 29, 4 (1989), 447–456. \ 3. Craig F Bohren and Donald R Huﬀman. 2008. Absorption and scattering of light by small particles. John Wiley & Sons. \ 4. Varun Perumal C and Daniel J. Wigdor. 2016. Foldem: Heterogeneous Object Fabrication via Selective Ablation of Multi-Material Sheets. In CHI 2016. DOI: http://dx.doi.org/10.1145/2858036.2858135 \ 5. Li-Wei Chan, Yi-Ling Chen, Chi-Hao Hsieh, Rong-Hao Liang, and Bing-Yu Chen. 2015. CyclopsRing: Enabling Whole-Hand and Context-Aware Interactions Through a Fisheye Ring. In UIST 2015. DOI: http://dx.doi.org/10.1145/2807442.2807450 \ 6. Tongbo Chen, Hendrik P. A. Lensch, Christian Fuchs, and Hans-Peter Seidel. 2007. Polarization and Phase-Shifting for 3D Scanning of Translucent Objects. In CVPR 2007. DOI: http://dx.doi.org/10.1109/CVPR.2007.383209 \ 7. Xiang Chen, Changxi Zheng, and Kun Zhou. 2016. Example-Based Subspace Stress Analysis for Interactive Shape Design. IEEE Trans Vis Comput Graph. (TVCG) (2016). \ 8. Frédéric Cortat. 2004. The Kubelka-Munk theory, applications and modiﬁcations. Presentation for the graduate course on Optical properties of Paper, Linkoping University (2004). \ 9. Craig Donner and Henrik Wann Jensen. 2005. Light diﬀusion in multi-layered translucent materials. ACM Trans. Graph. (2005). DOI: http://dx.doi.org/10.1145/1073204.1073308 \ 10. David Espalin, Danny W Muse, Eric MacDonald, and Ryan B Wicker. 2014. 3D Printing multifunctionality: structures with electronics. The International Journal of Advanced Manufacturing Technology 72, 5-8 (2014), 963–978. \ 11. Mark Fiala. 2005. ARTag, a Fiducial Marker System Using Digital Techniques. In CVPR 2005. DOI: http://dx.doi.org/10.1109/CVPR.2005.74 \ 12. Martin A Fischler and Robert C Bolles. 1981. Random sample consensus: a paradigm for model ﬁtting with applications to image analysis and automated cartography. Commun. ACM 24, 6 (1981), 381–395. \ 13. Neil Gershenfeld. 2008. Fab: the coming revolution on your desktop–from personal computers to personal fabrication. Basic Books. \ 14. James J Gibson. 2014. The ecological approach to visual perception: classic edition. Psychology Press. \ 15. Jane Greenberg. 2005. Understanding metadata and metadata schemes. Cataloging & classiﬁcation quarterly 40, 3-4 (2005), 17–36. \ 16. Chris Harrison, Robert Xiao, and Scott E. Hudson. 2012. Acoustic barcodes: passive, durable and inexpensive notched identiﬁcation tags. In UIST 2012. DOI:http://dx.doi.org/10.1145/2380116.2380187 \ 17. Milo˘s Ha˘san, Martin Fuchs, Wojciech Matusik, Hanspeter Pﬁster, and Szymon Rusinkiewicz. 2010. Physical reproduction of materials with speciﬁed subsurface scattering. ACM Trans. Graph. (2010). DOI: http://dx.doi.org/10.1145/1833351.1778798 \ 18. Haibo Hu, Jian Tang, Hao Zhong, Zheng Xi, Changle Chen, and Qianwang Chen. 2013. Invisible photonic printing: computer designing graphics, UV printing and shown by a magnetic ﬁeld. Scientiﬁc reports 3 (2013). \ 19. Alexandra Ion, Johannes Frohnhofen, Ludwig Wall, Robert Kovacs, Mirela Alistar, Jack Lindsay, Pedro Lopes, Hsiang-Ting Chen, and Patrick Baudisch. 2016. Metamaterial Mechanisms. In UIST 2016. DOI: http://dx.doi.org/10.1145/2984511.2984540 \ 20. Henrik Wann Jensen, Stephen R Marschner, Marc Levoy, and Pat Hanrahan. 2001. A practical model for subsurface light transport. In SIGGRAPH 2001. DOI: http://dx.doi.org/10.1145/383259.383319 \ 21. Rubaiat Habib Kazi, Tovi Grossman, Cory Mogk, Ryan M. Schmidt, and George W. Fitzmaurice. 2016. ChronoFab: Fabricating Motion. In CHI 2016. DOI: http://dx.doi.org/10.1145/2858036.2858138 \ 22. Gierad Laput, Eric Brockmeyer, Scott E. Hudson, and Chris Harrison. 2015a. Acoustruments: Passive, Acoustically-Driven, Interactive Controls for Handheld Devices. In CHI 2015. DOI: http://dx.doi.org/10.1145/2702123.2702414 \ 23. Gierad Laput, Xiang ‘Anthony’ Chen, and Chris Harrison. 2015b. 3D Printed Hair: Fused Deposition Modeling of Soft Strands, Fibers, and Bristles. In UIST 2015. \ 24. Dingzeyu Li, David I.W. Levin, Wojciech Matusik, and Changxi Zheng. 2016. Acoustic Voxels: Computational Optimization of Modular Acoustic Filters. ACM Trans. Graph. (2016). DOI: http://dx.doi.org/10.1145/2897824.2925960 \ 25. Shu Lin and Daniel J Costello. 2004. Error control coding. Pearson Education India. \ 26. Pedro Lopes, Patrik Jonell, and Patrick Baudisch. 2015. Aﬀordance++: Allowing Objects to Communicate Dynamic Use. In CHI 2015. DOI: http://dx.doi.org/10.1145/2702123.2702128 \ 27. James McCrae, Nobuyuki Umetani, and Karan Singh. 2014. FlatFitFab: interactive modeling with planar sections. In UIST 2014. DOI: http://dx.doi.org/10.1145/2642918.2647388 \ 28. Stefanie Mueller, Tobias Mohr, Kerstin Guenther, Johannes Frohnhofen, and Patrick Baudisch. 2014. faBrickation: fast 3D printing of functional objects by integrating construction kit building blocks. In CHI 2014. DOI:http://dx.doi.org/10.1145/2556288.2557005 \ 29. Shree K. Nayar, Gurunandan Krishnan, Michael D. Grossberg, and Ramesh Raskar. 2006. Fast separation of direct and global components of a scene using high frequency illumination. ACM Trans. Graph. (2006). DOI:http://dx.doi.org/10.1145/1141911.1141977 \ 30. Jean-Nicolas Ouellet and Patrick Hébert. 2009. Precise ellipse estimation without contour point extraction. Mach. Vis. Appl. 21, 1 (2009), 59–67. \ 31. Marios Papas, Christian Regg, Wojciech Jarosz, Bernd Bickel, Philip Jackson, Wojciech Matusik, Steve Marschner, and Markus H. Gross. 2013. Fabricating translucent materials using continuous pigment mixtures. ACM Trans. Graph. (2013). DOI: http://dx.doi.org/10.1145/2461912.2461974 \ 32. Huaishu Peng, Rundong Wu, Steve Marschner, and François Guimbretière. 2016. On-The-Fly Print: Incremental Printing While Modelling. In CHI 2016. DOI:http://dx.doi.org/10.1145/2858036.2858106 \ 33. Valkyrie Savage, Andrew Head, Björn Hartmann, Dan B. Goldman, Gautham J. Mysore, and Wilmot Li. 2015. Lamello: Passive Acoustic Sensing for Tangible Input Components. In CHI 2015. DOI: http://dx.doi.org/10.1145/2702123.2702207 \ 34. Ying Song, Xin Tong, Fabio Pellacini, and Pieter Peers. 2009. SubEdit: a representation for editing measured heterogeneous subsurface scattering. ACM Trans. Graph. (2009). DOI: http://dx.doi.org/10.1145/1531326.1531337 \ 35. Andrew Spielberg, Alanson P. Sample, Scott E. Hudson, Jennifer Mankoﬀ, and James McCann. 2016. RapID: A Framework for Fabricating Low-Latency Interactive Objects with RFID Tags. In CHI 2016. DOI: http://dx.doi.org/10.1145/2858036.2858243 \ 36. Richard Szeliski. 2010. Computer vision: algorithms and applications. Springer Science & Business Media. \ 37. Alexander Teibrich, Stefanie Müller, François Guimbretière, Robert Kovacs, Stefan Neubert, and Patrick Baudisch. 2015. Patching Physical Objects. In UIST 2015. DOI: http://dx.doi.org/10.1145/2807442.2807467 \ 38. Tatyana Vasilevitsky and Amit Zoran. 2016. Steel-Sense: Integrating Machine Elements with Sensors by Additive Manufacturing. In CHI 2016. DOI: http://dx.doi.org/10.1145/2858036.2858309 \ 39. RJ Watt and MJ Morgan. 1983. The recognition and representation of edge blur: evidence for spatial primitives in human vision. Vision research 23, 12 (1983), 1465–1477. \ 40. Karl D. D. Willis and Andrew D. Wilson. 2013. InfraStructs: fabricating information inside physical objects for imaging in the terahertz region. ACM Trans. Graph. (2013). DOI: http://dx.doi.org/10.1145/2461912.2461936 \ 41. L. B. Wolﬀ. 1989. Using polarization to separate reﬂection components. In CVPR 1989. DOI: http://dx.doi.org/10.1109/CVPR.1989.37873 \ 42. Shengdong Zhao, Koichi Nakamura, Kentaro Ishii, and Takeo Igarashi. 2009. Magic cards: a paper tag interface for implicit robot control. In CHI 2009. DOI: http://dx.doi.org/10.1145/1518701.1518730 \ ",digital fabrication; 3D printing; unobtrusive tags; air pockets; sensing,I.2.10; J.6; H.5.m.,uistf3497-file1.zip,uistf3497-file2.jpg,uistf3497-file3.mp4,,"An AirCode tag is embedded in the statue, remaining invisible under regular lighting. Using our imaging system that separates out the global scattering effects, the user detects the embedded tag and retrieve the data.",,"We develop AirCode, a tool to use plastic material's transparency to tag 3D printed objects automatically in the printing process. ","Dear reviewers, \  \ We incorporated the suggestions from the review into our revision. These are the major updates: \ - Added discussion with invisible ink and RFID approaches.  \ - Tuned down our claim on consumer level cameras. \ - Discussed fabrication details and extension to different materials. \ - Added limitation on opaque surface paints and etc. \  \ In the auxiliary zip file, in addition to the appendix file, we also included our main pdf file with major changes marked in red. Please let us know if more changes are required. Thank you very much. \  \ Sincerely, \ Authors",Dingzeyu Li,Changxi Zheng,FormatComplete,1453101,NSF CAREER,Adobe ,Adobe Research Fellowship,,,Aug 7 18:24,
uistf2860,10/24,10,Fabrication,11:00:00 AM,12:10:00 AM,3+1,11:40:00 AM,12:00:00 PM,long,long,uistf3497,3,1003,Honorable Mention,,uistf2860,A,BlowFab: Rapid Prototyping for Rigid and Reusable Objects using Inflation of Laser-cut Surfaces,JUNICHI,YAMAOKA,yamajun@sfc.keio.ac.jp,uistf2860-paper.pdf,10,letter,,,"Junichi Yamaoka, Ryuma Niiyama, Yasuaki Kakehi","yamajun@sfc.keio.ac.jp, niiyama@isi.imi.i.u-tokyo.ac.jp, ykakehi@sfc.keio.ac.jp",35583,Junichi,,Yamaoka,yamajun@sfc.keio.ac.jp,,Keio University,Kanagawa,,Japan,,,,,,35547,Ryuma,,Niiyama,niiyama@isi.imi.i.u-tokyo.ac.jp,,The University of Tokyo,Tokyo,,Japan,,,,,,19312,Yasuaki,,Kakehi,ykakehi@sfc.keio.ac.jp,,Keio University,Kanagawa,,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This study proposes BlowFab, a prototyping method used to create a 2.5-dimensional prototype in a short time by combining laser cutting and blow molding techniques. The user creates adhesive areas and inflatable areas by engraving and cutting multilayered plastic sheets using a laser cutter. These adhesive areas are fused automatically by overlapping two crafted sheets and softening them with a heater. The user can then create hard prototypes by injecting air into the sheets. \  \ Objects can be bent in any direction by cutting incisions or engraving a resistant resin. The user can create uneven textures by engraving a pattern with a heat-resistant film. These techniques can be used for prototyping various strong inflatable objects. The finished prototype is strong and can be collapsed readily for storage when not required.  \  \ In this study, the design process is described using the proposed method.  \ The study also evaluates possible bending mechanisms and texture expression methods along with various usage scenarios and discusses the resolution, strength, and reusability of the prototype developed.",yamajun@sfc.keio.ac.jp,"1. Harshit Agrawal, Udayan Umapathi, Robert Kovacs, Johannes Frohnhofen, Hsiang-Ting Chen, Stefanie Mueller, and Patrick Baudisch. 2015. Protopiper: Physically Sketching Room-Sized Objects at Actual Scale. UIST 2015. 427-436. \ 2. Marcelo Coelho, Hiroshi Ishii, and Pattie Maes. 2008. Surflex: a programmable surface for the design of tangible interfaces. CHI EA 2008. ACM, New York, NY, USA, 3429-3434. \ 3. Sean Follmer, Daniel Leithinger, Alex Olwal, Nadia Cheng and Hiroshi Ishii. 2012. Jamming user interfaces: programmable particle stiffness and sensing for malleable and shape-changing devices. UIST 2012, 519-528. \ 4. George Fereday, Blow Moulding Gun; http://www.georgefereday.com/index.php?/projects/extrusion- gun/ \ 5. Filip Ilievski, Aaron D. Mazzeo, Robert F. Shepherd, Xin Chen, and George M. Whitesides. 2011. Soft robotics for chemists. Angew. Chem.Int. Ed., 50, 8, 1890-1895. \ 6. E. Hawkesa, B. Anb, N. M. Benbernoub, H. Tanakaa, S. Kimc, E. D. Demaineb, D. Rusb, and R. J. Wooda. 2010. Programmable matter by folding. PNAS 107, 28,12441-12445. \ 7. Theo Jansen. 2008. Once a physicist: Theo Jansen. Physics World, 21(10), 58-58 . \ 8. Naoya Koizumi, Kentaro Yasu, Angela Liu, Maki Sugimoto, and Masahiko Inami. 2010. Animated paper: A toolkit for building moving toys. Comput. Entertain. 8, 2, Article 7 , 16. \ 9. Josua Krause, Adam Perer, and Kenney Ng. 2016. Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models. CHI 2016. ACM, New York, NY, USA, 5686-5697. \ 10. Yuki Mori and Takeo Igarashi. 2007. Plushie: an interactive design system for plush toys. ACM Trans. Graph. 26, 3, Article 45. \ 11. Stefanie Mueller, Bastian Kruck, and Patrick Baudisch. 2013. LaserOrigami:Laser-Cutting 3D Objects. Proc. CHI ’13, 2585-2592. \ 12. Ryuma Niiyama, Xu Sun, Lining Yao, Hiroshi Ishii, Daniela Rus, and Sangbae Kim. 2015. Sticky Actuator: Free-Form Planar Actuators for Animated Objects. TEI 2015. ACM, New York, NY, USA, 77-84. \ 13. Jifei Ou, Melina Skouras, Nikolaos Vlavianos, Felix Heibeck, Chin-Yi Cheng, Jannik Peters, and Hiroshi Ishii. 2016. aeroMorph - Heat-sealing Inflatable Shape-change Materials for Interaction Design. UIST 2016. ACM, New York, NY, USA, 121-132. \ 14. Huaishu Peng, Jennifer Mankoff, Scott E. Hudson, and James McCann. 2015. A Layered Fabric 3D Printer for Soft Interactive Objects. CHI 2015. ACM, New York, NY, USA, 1789-1798. \ 15. Greg Saul, Manfred Lau, Jun Mitani, and Takeo Igarashi. 2010. SketchChair: an all-in-one chair design system for end users. TEI 2011. ACM, New York, NY, USA, 73-80. \ 16. Robert F. Shepherda, Filip Ilievskia, Wonjae Choia, Stephen A. Morina, Adam A. Stokesa, Aaron D. Mazzeoa, Xin Chena, Michael Wanga, and George M. Whitesidesa.2011.Multigaitsoftrobot.InPNAS ’11. 108, 51, 20400-3. \ 17. Melina Skouras, Bernhard Thomaszewski, Peter Kaufmann, Akash Garg, Bernd Bickel, Eitan Grinspun, and Markus Gross. 2014. Designing inflatable structures. ACM Trans. Graph. 33, 4, Article 63), 10 pages. \ 18. Harpreet Sareen, Udayan Umapathi, Patrick Shin, Yasuaki Kakehi, Jifei Ou, Hiroshi Ishii, and Pattie Maes. 2017. Printflatables: Printing Human-Scale, Functional and Dynamic Inflatable Objects. CHI 2017. ACM, New York, NY, USA, 3669-3680. \ 19. Udayan Umapathi, Hsiang-Ting Chen, Stefanie Mueller, Ludwig Wall, Anna Seufert, and Patrick Baudisch. 2015. LaserStacker: Fabricating 3D Objects by Laser Cutting and Welding. UIST 2015. ACM, New York, NY, USA, 575-582. \ 20. Beth Weinstein. 2013. Performing Architectures: Closed and open logics of mutable scenes, Performing Research: A Journal of the Performing arts, 18:3, 161-168. \ 21. Lining Yao, Ryuma Niiyama, Jifei Ou, Sean Follmer, Clark Della Silva, and Hiroshi Ishii. 2013. PneUI: pneumatically actuated soft composite materials for shape changing interfaces. In UIST ’13, 13-22. \ 22. Bahramzadeh Yousef and Shahinpoor Mohsen. 2013. A Review of Ionic Polymeric Soft Actuators and Sensors. Soft Robotics, 1, 38-52. \ 23. Hironori Yoshida, Takeo Igarashi, Yusuke Obuchi, Yosuke Takami, Jun Sato, Mika Araki, Masaaki Miki, Kosuke Nagata, Kazuhide Sakai, and Syunsuke Igarashi. 2015. Architecture-scale human-assisted additive manufacturing. ACM Trans. Graph. 34, 4, Article 88, 8. \ 24. Kentaro Yasu. 2015. MOR4R: microwave oven recipes for resins. In SIGGRAPH 2015: Studio, Article 4 , 1. \ 25. Mataerial , http://www.mataerial.com/ \ 26. Hyperform, Marcelo Coelho Studio, http://www.cmarcelo.com/hyperform/",Fabrication; Prototyping/Implementation; Creativity Support; Smart materials.,H.5.2. Information Interfaces and Presentation; User Inter- faces.,uistf2860-file1.zip,,uistf2860-file3.m4v,,,,BlowFab is a novel fabrication technique which users can a 2.5-dimensional prototype in a short time by combining laser cutting and blow molding techniques. ,"As pointed out by R5 and R3, we added Printflatables (2nd paragraph of Inflatable Prototyping Methods using Soft Material) and Foldem (2nd paragraph of Rapid Prototyping Techniques for 3D Objects) to appropriate places as similar researches in the section on Related work. \ As pointed out by R1, in the 2nd paragraph of Fabrication Process chapter, We described the state of the two-layer plastic sheet in detail, and corrected Fig 2. \ Fig 5 was also difficult to understand by readers, so we added annotations. \ In 2nd paragraph of Designing Pattern, as indicated by R1 and 2, we added the maximum power and speed of the laser cutter and expressed parameter value as a percentage. \  \ Regarding Horizontal Bending Mechanism and Vertical Bending Mechanism, as indicated by R1, we added standard deviation to the average value and for showing no unevenness. \ We also modified formulas and figures (Figures 13 and 16). \ In 4th paragraph of Horizontal Bending Mechanism,  as indicated by R4, we added about how to make long objects. \ In addition to the indication of Reviewer 1, we explained the explanation of the bonding between polycarbonate and PET in 1st paragraph of Vertical Bending Mechanism. \  \ In1st paragraph of Large Scale Prototyping of  APPLICATIONS, we added about heating and inflating objects of large size as indicated by R3. In addition, in response to the point of R2, we measured the load carrying capacity with a simple shape by experiment. \ In DISCUSSION, as indicated by R3, we mentioned the possibility with other materials such as a PVC, and we added Material subsection. \  \ We also fixed about notation of references, grammar and layout.",Junichi Yamaoka,Yasuaki Kakehi,FormatComplete,JPMJER1501,JST ERATO,,,,,Jul 27 21:20,
uistf3775,10/24,10,Fabrication,11:00:00 AM,12:10:00 AM,3+1,12:00:00 PM,12:10:00 PM,short,short,uistf2860,4,1004,,,uistf3775,A,"StrutModeling: A Low-Fidelity Construction Kit to Iteratively Model, Test, and Adapt 3D Objects",Raf,Ramakers,raf.ramakers@gmail.com,uistf3775-paper.pdf,9,letter,,,"Danny Leen, Raf Ramakers, Kris Luyten","danny@fablabgenk.be, raf.ramakers@gmail.com, kris.luyten@uhasselt.be",50064,Danny,,Leen,danny@fablabgenk.be,"LUCA, School of Arts",KULeuven,Leuven,,Belgium,,,,,,27274,Raf,,Ramakers,raf.ramakers@gmail.com,Expertise Centre for Digital Media,UHasselt-tUL-imec,Hasselt,,Belgium,,,,,,2807,Kris,,Luyten,kris.luyten@uhasselt.be,Expertise Centre for Digital Media,UHasselt-tUL-imec,Diepenbeek,,Belgium,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present StrutModeling, a computationally enhanced construction kit that enables users without a 3D modeling background to prototype 3D models by assembling struts and hub primitives in physical space. Physical 3D models are immediately captured in software and result in readily available models for 3D printing. Given the concrete physical format of StrutModels, modeled objects can be tested and fine tuned in the presence of existing objects and specific needs of users. StrutModeling avoids puzzling with pieces by contributing an adjustable strut and universal hub design. Struts can be adjusted in length and snap to magnetic hubs in any configuration. As such, arbitrarily complex models can be modeled, tested, and adjusted during the design phase. In addition, the embedded sensing capabilities allow struts to be used as measuring devices for lengths and angles, and tune physical mesh models according to existing physical objects.",Raf Ramakers," \ WARNING: These references are very long: 2, 17.  Please verify that they were extracted correctly. \  \ 1. Harshit Agrawal, Udayan Umapathi, Robert Kovacs, Johannes Frohnhofen, Hsiang-Ting Chen, Stefanie Mueller, and Patrick Baudisch. Protopiper: Physically Sketching Room-Sized Objects at Actual Scale. In Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology (UIST ’15). ACM, New York, NY, USA, 427–436. DOI: http://dx.doi.org/10.1145/2807442.2807505 \ 2. David Anderson, James L. Frankel, Joe Marks, Aseem Agarwala, Paul Beardsley, Jessica Hodgins, Darren Leigh, Kathy Ryall, Eddie Sullivan, and Jonathan S. Yedidia. Tangible Interaction + Graphical Interpretation: A New Approach to 3D Modeling. In Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH ’00). ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 393–402. DOI: http://dx.doi.org/10.1145/344779.344960 \ 3. Dustin Beyer, Seraﬁma Gurevich, Stefanie Mueller, Hsiang-Ting Chen, and Patrick Baudisch. Platener: Low-Fidelity Fabrication of 3D Objects by Substituting 3D Print with Laser-Cut Plates. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15). ACM, New York, NY, USA, 1799–1806. DOI: http://dx.doi.org/10.1145/2702123.2702225 \ 4. Xiang ’Anthony’ Chen, Jeeeun Kim, Jennifer Mankoff, Tovi Grossman, Stelian Coros, and Scott E. Hudson. Reprise: A Design Tool for Specifying, Generating, and Customizing 3D Printable Adaptations on Everyday Objects. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 29–39. DOI: http://dx.doi.org/10.1145/2984511.2984512 \ 5. Chin-yu Chien, Rong-Hao Liang, Long-Fei Lin, Liwei Chan, and Bing-Yu Chen. FlexiBend: Enabling Interactivity of Multi-Part, Deformable Fabrications Using Single Shape-Sensing Strip. In Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology (UIST ’15). ACM, New York, NY, USA, 659–663. DOI: http://dx.doi.org/10.1145/2807442.2807456 \ 6. Markus Eng, Ken Camarata, Ellen Yi-Luen Do, and Mark D Gross. 2006. Flexm: Designing a physical construction kit for 3d modeling. International Journal of Architectural Computing 4, 2 (2006), 27–47. \ 7. Sean Follmer and Hiroshi Ishii. KidCAD: Digitally Remixing Toys Through Tangible Tools. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’12). ACM, New York, NY, USA, 2401–2410. DOI: http://dx.doi.org/10.1145/2207676.2208403 \ 8. Oliver Glauser, Benedek Vartok, Wan-Chun Ma, Daniele Panozzo, Alec Jacobson, Cédric Pradalier, Otmar Hilliges, and Olga Sorkine-Hornung. 2017. Rig Animation with a Tangible and Modular Input Device. interactions 24, 2 (Feb. 2017), 16–17. DOI: http://dx.doi.org/10.1145/3041959 \ 9. Matthew G. Gorbet, Maggie Orth, and Hiroshi Ishii. Triangles: Tangible Interface for Manipulation and Exploration of Digital Information Topography. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’98). ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 49–56. DOI: http://dx.doi.org/10.1145/274644.274652 \ 10. Tovi Grossman, Ravin Balakrishnan, and Karan Singh. An Interface for Creating and Manipulating Curves Using a High Degree-of-freedom Curve Input Device. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’03). ACM, New York, NY, USA, 185–192. DOI: http://dx.doi.org/10.1145/642611.642645 \ 11. Ankit Gupta, Dieter Fox, Brian Curless, and Michael Cohen. DuploTrack: A Real-time System for Authoring and Guiding Duplo Block Assembly. In Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology (UIST ’12). ACM, New York, NY, USA, 389–402. DOI: http://dx.doi.org/10.1145/2380116.2380167 \ 12. Yingdan Huang and Michael Eisenberg. Easigami: Virtual Creation by Physical Folding. In Proceedings of the Sixth International Conference on Tangible, Embedded and Embodied Interaction (TEI ’12). ACM, New York, NY, USA, 41–48. DOI: http://dx.doi.org/10.1145/2148131.2148143 \ 13. Takeo Igarashi, Satoshi Matsuoka, and Hidehiko Tanaka. Teddy: A Sketching Interface for 3D Freeform Design. In Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH ’99). ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 409–416. DOI: http://dx.doi.org/10.1145/311535.311602 \ 14. Koshi Ikegawa, Masaya Tsuruta, Tetsuya Abe, Arika Yoshida, Buntarou Shizuki, and Shin Takahashi. Lightweight Capacitance-based Block System for 3D Space Interaction. In Proceedings of the 2016 ACM on Interactive Surfaces and Spaces (ISS ’16). ACM, New York, NY, USA, 307–312. DOI: http://dx.doi.org/10.1145/2992154.2996772 \ 15. Alec Jacobson, Daniele Panozzo, Oliver Glauser, Cédric Pradalier, Otmar Hilliges, and Olga Sorkine-Hornung. Tangible and Modular Input Device for Character Articulation. In ACM SIGGRAPH 2014 Emerging Technologies (SIGGRAPH ’14). ACM, New York, NY, USA, Article 24, 1 pages. DOI: http://dx.doi.org/10.1145/2614066.2614072 \ 16. Yoshifumi Kitamura, Yuichi Itoh, and Fumio Kishino. Real-time 3D Interaction with ActiveCube. In CHI ’01 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’01). ACM, New York, NY, USA, 355–356. DOI:http://dx.doi.org/10.1145/634067.634277 \ 17. Robert Kovacs, Anna Seufert, Ludwig Wall, Hsiang-Ting Chen, Florian Meinel, Willi Müller, Sijing You, Maximilian Brehm, Jonathan Striebel, Yannis Kommana, Alexander Popiak, Thomas Bläsius, and Patrick Baudisch. TrussFab: Fabricating Sturdy Large-Scale Structures on Desktop 3D Printers. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’17). ACM, New York, NY, USA, 2606–2616. DOI: http://dx.doi.org/10.1145/3025453.3026016 \ 18. Vincent LeClerc, Amanda Parkes, and Hiroshi Ishii. Senspectra: A Computationally Augmented Physical Modeling Toolkit for Sensing and Visualization of Structural Strain. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’07). ACM, New York, NY, USA, 801–804. DOI: http://dx.doi.org/10.1145/1240624.1240744 \ 19. David Ledo, Fraser Anderson, Ryan Schmidt, Lora Oehlberg, Saul Greenberg, and Tovi Grossman. Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’17). ACM, New York, NY, USA, 2583–2593. DOI: http://dx.doi.org/10.1145/3025453.3025652 \ 20. Andrew Miller, Brandyn White, Emiko Charbonneau, Zach Kanzler, and Joseph J LaViola Jr. 2012. Interactive 3D model acquisition and tracking of building block structures. IEEE transactions on visualization and computer graphics 18, 4 (2012), 651–659. DOI: http://dx.doi.org/10.1109/TVCG.2012.48 \ 21. Stefanie Mueller, Sangha Im, Seraﬁma Gurevich, Alexander Teibrich, Lisa Pﬁsterer, François Guimbretière, and Patrick Baudisch. WirePrint: 3D Printed Previews for Fast Prototyping. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST ’14). ACM, New York, NY, USA, 273–280. DOI: http://dx.doi.org/10.1145/2642918.2647359 \ 22. Hayes Solos Rafﬂe, Amanda J. Parkes, and Hiroshi Ishii. Topobo: A Constructive Assembly System with Kinetic Memory. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’04). ACM, New York, NY, USA, 647–654. DOI: http://dx.doi.org/10.1145/985692.985774 \ 23. Raf Ramakers, Fraser Anderson, Tovi Grossman, and George Fitzmaurice. RetroFab: A Design Tool for Retroﬁtting Physical Interfaces Using Actuators, Sensors and 3D Printing. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 409–419. DOI: http://dx.doi.org/10.1145/2858036.2858485 \ 24. Robert L. Read. 2016. The Gluss Project. Article. (16 September 2016). Retrieved March 27, 2016 from https://hackaday.io/project/13277-the-gluss-project. \ 25. Ben Roth. 1981. Rigid and ﬂexible frameworks. The American Mathematical Monthly 88, 1 (1981), 6–21. DOI:http://dx.doi.org/10.2307/2320705 \ 26. Sutphen S., Sharlin E., Watson B., and Frazer J. Tangible Computer Interfaces and the Revival of the Segal Model. In In Western Computer Graphics Symposium (WCGS ’00). \ 27. Greg Saul, Manfred Lau, Jun Mitani, and Takeo Igarashi. 2011. SketchChair: An All-in-one Chair Design System for End Users. In Proceedings of the Fifth International Conference on Tangible, Embedded, and Embodied Interaction (TEI ’11). ACM, New York, NY, USA, 73–80. DOI:http://dx.doi.org/10.1145/1935701.1935717 \ 28. Valkyrie Savage, Sean Follmer, Jingyi Li, and Björn Hartmann. Makers’ Marks: Physical Markup for Designing and Fabricating Functional Objects. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST ’15). ACM, New York, NY, USA, 103–108. DOI: http://dx.doi.org/10.1145/2807442.2807508 \ 29. Ryan Schmidt and Karan Singh. Meshmixer: An Interface for Rapid Mesh Composition. In ACM SIGGRAPH 2010 Talks (SIGGRAPH ’10). ACM, New York, NY, USA, Article 6, 1 pages. DOI: http://dx.doi.org/10.1145/1837026.1837034 \ 30. Hyunyoung Song, François Guimbretière, Chang Hu, and Hod Lipson. ModelCraft: Capturing Freehand Annotations and Edits on Physical 3D Models. In Proceedings of the 19th Annual ACM Symposium on User Interface Software and Technology (UIST ’06). ACM, New York, NY, USA, 13–22. DOI: http://dx.doi.org/10.1145/1166253.1166258 \ 31. Christian Weichel, Jason Alexander, Abhijit Karnik, and Hans Gellersen. SPATA: Spatio-Tangible Tools for Fabrication-Aware Design. In Proceedings of the Ninth International Conference on Tangible, Embedded, and Embodied Interaction (TEI ’15). ACM, New York, NY, USA, 189–196. DOI: http://dx.doi.org/10.1145/2677199.2680576 \ 32. Christian Weichel, John Hardy, Jason Alexander, and Hans Gellersen. ReForm: Integrating Physical and Digital Design Through Bidirectional Fabrication. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST ’15). ACM, New York, NY, USA, 93–102. DOI: http://dx.doi.org/10.1145/2807442.2807451 \ 33. Christian Weichel, Manfred Lau, David Kim, Nicolas Villar, and Hans W. Gellersen. MixFab: A Mixed-reality Environment for Personal Fabrication. In Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI ’14). ACM, New York, NY, USA, 3855–3864. DOI: http://dx.doi.org/10.1145/2556288.2557090 \ 34. Michael Philetus Weller, Ellen Yi-Luen Do, and Mark D Gross. Posey: Instrumenting a Poseable Hub and Strut Construction Toy. In Proceedings of the 2Nd International Conference on Tangible and Embedded Interaction (TEI ’08). ACM, New York, NY, USA, 39–46. DOI:http://dx.doi.org/10.1145/1347390.1347402 \ 35. Robert C. Zeleznik, Kenneth P. Herndon, and John F. Hughes. 1996. SKETCH: An Interface for Sketching 3D Scenes. In Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH ’96). ACM, New York, NY, USA, 163–170. DOI:http://dx.doi.org/10.1145/237170.237238 \ ",Tangible Modeling; Construction Kit; 3D Modeling; Personal/Digital Fabrication,H.5.2,uistf3775-file1.zip,uistf3775-file2.jpg,uistf3775-file3.mp4,,"StrutModeling: A Low-Fidelity Construction Kit to Iteratively Model, Test, and Adapt 3D Objects",,"StrutModeling is a computationally construction kit for non-experts to prototype 3D models. Arbitrarily complex models are modeled, tested, and adjusted during the design phase in the presence of existing objects.","-Please find the annotated version, highlighting all the changes, in the auxiliary material  \  \ -We sharpened the contribution with respect to previous approaches as discussed in the rebuttal. \ The comparison with traditional LEGO building bricks is clarified in the introduction. \ -We include details on the max number of struts (780) supported by our current implementation. \ -Features to analyze structural rigidity of StrutModels in future versions are discussed in the future work section. \ -We included details on the type of files that are generated for the 3 export options. \ -We clarified that for adjusting existing 3D models with StrutModeling, users start from a parametric design, modelled by experts. \ -We clarified that our rendering environment runs Meshmixer/OpenSCAD in the background. \ -Details on the shear strength of the magnets are added to the paper. \  \ -We updated the video and showed the interface export options as requested by R2. \ ",Danny Leen,Raf Ramakers,FormatComplete,G0E7317N,Research Foundation - Flanders (FWO),,,,,Aug 7 11:11,
uistf4607,10/24,11,Alt.Modalities,2:00:00 PM,3:30:00 PM,4+1,2:00:00 PM,2:20:00 PM,long,long,none,1,1101,,,uistf4607,A,Designing and Evaluating Livefonts,Danielle,Bragg,dkbragg@cs.washington.edu,uistf4607-paper.pdf,12,letter,,,"Danielle Bragg, Shiri Azenkot, Kevin Larson, Ann Bessemans, Adam Kalai","dkbragg@cs.washington.edu, shiri.azenkot@cornell.edu, kevlar@microsoft.com, ann.bessemans@uhasselt.be, adam.kalai@microsoft.com",44151,Danielle,,Bragg,dkbragg@cs.washington.edu,Computer Science & Engineering,University of Washington,Seattle,WA,USA,,,,,,18183,Shiri,,Azenkot,shiri.azenkot@cornell.edu,Jacobs Technion-Cornell Institute,Cornell Tech,New York,New York,United States,,,,,,1115,Kevin,,Larson,kevlar@microsoft.com,,Microsoft,Redmond,Washington,United States,,,,,,71875,Ann,,Bessemans,ann.bessemans@uhasselt.be,,Hasselt University,Hasselt,,Belgium,READSEARCH,PXL-MAD,Hasselt,,Belgium,55481,Adam,,Kalai,adam.kalai@microsoft.com,,Microsoft Research,Cambridge,Massachusetts,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The emergence of personal computing devices offers both a challenge and opportunity for displaying text: small screens can be hard to read, but also support higher resolution. To fit content on a small screen, text must be small. This small text size can make computing devices unusable, in particular to low-vision users, whose vision is not correctable with glasses. Usability is also decreased for sighted users straining to read the small letters, especially without glasses at hand. We propose animated scripts called livefonts for displaying English with improved legibility for all users. Because paper does not support animation, traditional text is static. However, modern screens support animation, and livefonts capitalize on this capability. We evaluate our livefont variations' legibility through a controlled lab study with low-vision and sighted participants, and find our animated scripts to be legible across vision types at approximately half the size (area) of traditional letters, while previous smartfonts (static alternate scripts) did not show a significant legibility advantage for low-vision users. We evaluate the learnability of our livefont with low-vision and sighted participants, and find it to be comparably learnable to static smartfonts after two thousand practice sentences.",dkbragg@cs.washington.edu,"1. Aries Arditi. 2004. Adjustable typography: an approach to enhancing low vision text accessibility. Ergonomics 47, 5 (2004), 469–482. \ 2. Rudolf Arnheim. 1956. Art and visual perception: A psychology of the creative eye. Univ of California Press. \ 3. Ann Bessemans. 2016. Matilda: a typeface for children with low vision. Digital Fonts and Reading 1 (2016), 19. \ 4. Ann Bessemans and others. 2012. Letterontwerp voor kinderen met een visuele functiebeperking. Ph.D.  Dissertation. Leiden University.  \ 5. Danielle Bragg, Shiri Azenkot, and Adam Tauman Kalai. 2016. Reading and Learning Smartfonts. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology. ACM, 391–402. \ 6. Barbara Brownie. 2014. Transforming Type: New Directions in Kinetic Typography. Bloomsbury  Publishing.  \ 7. M Davis and P Edberg. 2016. Unicode emoji-unicode technical report# 51. Technical Report. Technical Report 51 (3). \ 8. Donald C Fletcher, Ronald A Schuchard, and Gale Watson. 1999. Relative locations of macular scotomas near the PRL: effect on low vision reading. Journal of Rehabilitation Research and Development 36, 4 (1999), 356–364. \ 9. American Printing House for the Blind Inc. 2004. APHontTM: A Font for Low Vision. http://www.aph.org/products/aphont/. (2004). (Accessed 2017-03-12). \ 10. Jodi Forlizzi, Johnny Lee, and Scott Hudson. 2003. The kinedit system: affective messages using dynamic texts. In Proceedings of the SIGCHI conference on Human factors in computing systems. ACM, 377–384. \ 11. Wilson Geisler and Richard Murray. 2003. Cognitive neuroscience: Practice doesn’t make perfect. Nature 423, 6941 (2003), 696–697. \ 12. John Gill and Sylvie Perera. 2003. Accessible universal design of interactive digital television. In Proceedings of the 1st European conference on interactive television: from viewers to actors. Citeseer, 83–89. \ 13. Paul Green-Armytage. 2010. A colour alphabet and the limits of colour coding. JAIC-Journal of the International Colour Association 5 (2010). \ 14. Jun Kato, Tomoyasu Nakano, and Masataka Goto. 2015. TextAlive: Integrated design environment for kinetic typography. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 3403–3412. \ 15. Ryan Kelly and Leon Watts. 2015. Characterising the inventive appropriation of emoji as relationally meaningful in mediated close personal relationships. Experiences of Technology Appropriation: Unanticipated Users, Usage, Circumstances, and Design (2015). \ 16. Joseph S Lappin, Duje Tadin, Jeffrey B Nyquist, and Anne L Corn. 2009. Spatial and temporal limits of motion perception across variations in speed, eccentricity, and low vision. Journal of Vision 9, 1 (2009), 30–30. \ 17. Johnny C Lee, Jodi Forlizzi, and Scott E Hudson. 2002. The kinetic typography engine: an extensible system for animating expressive text. In Proceedings of the 15th annual ACM symposium on User interface software and technology. ACM, 81–90. \ 18. Gordon E Legge. 2007. Psychophysics of reading in normal and low vision. In OSA Noninvasive Assessment of the Visual System, 1993, Monterey; Portions of this research (MNREAD acuity charts) were presented at the aforementioned conference. Lawrence Erlbaum Associates Publishers. \ 19. Gordon E Legge, J Stephen Mansﬁeld, and Susana TL Chung. 2001. Psychophysics of reading: XX. Linking letter recognition to reading speed in central and peripheral vision. Vision research 41, 6 (2001), 725–743. \ 20. Gordon E Legge, Denis G Pelli, Gar S Rubin, and Mary M Schleske. 1985. Psychophysics of reading - I. Normal vision. Vision research 25, 2 (1985), 239–252. \ 21. Gordon E Legge, Julie A Ross, Lisa M Isenberg, and James M Lamay. 1992. Psychophysics of reading. Clinical predictors of low-vision reading speed. Investigative Ophthalmology & Visual Science 33, 3 (1992), 677–687. \ 22. Gordon E Legge and Gary S Rubin. 1986. Psychophysics of reading. IV. Wavelength effects in normal and low vision. JOSA A 3, 1 (1986), 40–51. \ 23. Gordon E Legge, Gary S Rubin, and Andrew Luebker. 1987. Psychophysics of reading-V. The role of contrast in normal vision. Vision research 27, 7 (1987), 1165–1177. \ 24. Jason E Lewis and Alex Weyers. 1999. ActiveText: a method for creating dynamic and interactive texts. In Proceedings of the 12th annual ACM symposium on User interface software and technology. ACM, 131–140. \ 25. JS Mansﬁeld, SJ Ahn, GE Legge, and A Luebker. 1993. A new reading-acuity chart for normal and low vision. Ophthalmic and Visual Optics/Noninvasive Assessment of the Visual System Technical Digest 3 (1993), 232–235. \ 26. Chris McKinstry, Rick Dale, and Michael J Spivey. 2008. Action dynamics reveal parallel competition in decision making. Psychological Science 19, 1 (2008), 22–24. \ 27. Mayumi Negishi. 2014. Meet Shigetaka Kurita, the Father of Emoji. Wall Street Journal (2014). \ 28. Denis G Pelli, Catherine W Burns, Bart Farell, and Deborah C Moore-Page. 2006. Feature detection and letter identiﬁcation. Vision research 46, 28 (2006), 4646–4674. \ 29. Louise L Sloan. 1977. Reading aids for the partially sighted. American Journal of Optometry and Physiological Optics 54, 9 (1977), 646. \ 30. Ai Squared. 2017. ZoomText. http://www.aisquared.com/products/zoomtext/. (2017). (Accessed 2017-03-13). \ 31. Sarit Szpiro, Yuhang Zhao, and Shiri Azenkot. 2016b. Finding a store, searching for a product: a study of daily challenges of low vision people. In Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing. ACM, 61–72. \ 32. Sarit Felicia Anais Szpiro, Shafeka Hashash, Yuhang Zhao, and Shiri Azenkot. 2016a. How People with Low Vision Access Computing Devices: Understanding Challenges and Opportunities. In Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility. ACM, 171–180. \ 33. Duje Tadin, Jeffrey B Nyquist, Kelly E Lusk, Anne L Corn, and Joseph S Lappin. 2012. Peripheral Vision of Youths with Low Vision: Motion Perception, Crowding, and Visual Search Peripheral Function in Youths with Low Vision. Investigative ophthalmology & visual science 53, 9 (2012), 5860–5868. \ 34. Stephen G Whittaker and JAN Lovie-Kitchin. 1993. Visual requirements for reading. Optometry & Vision Science 70, 1 (1993), 54–65. \ 35. Arnold J Wilkins, Nirmal Sihra, and Andrew Myers. 2005. Increasing reading speed by using colours: issues concerning reliability and speciﬁcity, and their theoretical and practical implications. Perception 34, 1 (2005), 109–120. \ 36. Jonathan Winawer, Nathan Witthoft, Michael C Frank, Lisa Wu, Alex R Wade, and Lera Boroditsky. 2007. Russian blues reveal effects of language on color discrimination. Proceedings of the National Academy of Sciences 104, 19 (2007), 7780–7785. \ 37. Yuhang Zhao, Sarit Szpiro, and Shiri Azenkot. 2015. Foresee: A customizable head-mounted vision enhancement system for people with low vision. In Proceedings of the 17th International ACM SIGACCESS Conference on Computers & Accessibility. ACM, 239–249. \ 38. Yuhang Zhao, Sarit Szpiro, Jonathan Knighten, and Shiri Azenkot. 2016. CueSee: exploring visual cues for people with low vision to facilitate a visual search task. In Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing. ACM, 73–84. \ ",Fonts; Reading; Learning; Scripts; Low-vision; Accessibility.,H.5.2,uistf4607-file1.zip,uistf4607-file2.jpg,uistf4607-file3.mp4,,"Livefonts are animated scripts that enrich reading and can enhance legibility. The word “livefonts” in: a standard font, a font for low vision, a smartfont (stationary script), and a livefont (animated script).",,"We propose livefonts, animated scripts that improve legibility for low-vision readers. They use animation itself to define and distinguish characters, animating them individually and systematically. We present initial designs, and studies of legibility and readability.","We clarified the distinction between livefonts and kinetic typography by adding a related work section on kinetic typography and by clarifying the initial presentation of livefonts in the introduction, differentiating livefonts from animated text in video.  \  \ We toned down our claims by largely rewriting our results section, highlighting that follow-up studies are needed to confirm our results due to small sample size and noise, and redoing our statistical analyses to replace unpaired t-tests with one-way ANOVAs with repeated measures. We added post-hoc paired t-tests with a Bonferroni correction, to account for multiple tests and place stricter constraints on results. We also highlight limitations of our experiment designs. \  \ We expanded our discussion section to address the livefont design space more thoroughly, highlight the limitations of our designs, and present related areas for future work. \  \ Please find a PDF version with major revision highlighted in yellow in the auxiliary material field for convenience.",Danielle Bragg,Adam Kalai,FormatComplete,,,,,,,Aug 4 21:24,
uistf4174,10/24,11,Alt.Modalities,2:00:00 PM,3:30:00 PM,4+1,2:20:00 PM,2:40:00 PM,long,long,uistf4607,2,1102,,,uistf4174,A,Markit and Talkit: A Low-Barrier Toolkit to Augment 3D Printed Models with Audio Annotations,Lei,Shi,ls776@cornell.edu,uistf4174-paper.pdf,14,letter,Times-Roman,,"Lei Shi, Yuhang Zhao, Shiri Azenkot","ls776@cornell.edu, yz769@cornell.edu, shiri.azenkot@cornell.edu",51224,Lei,,Shi,ls776@cornell.edu,Jacobs Technion-Cornell Institute,Cornell Tech,New York,New York,United States,,,,,,71885,Yuhang,,Zhao,yz769@cornell.edu,Jacobs Technion-Cornell Institute,Cornell Tech,New York,New York,United States,,,,,,18183,Shiri,,Azenkot,shiri.azenkot@cornell.edu,Jacobs Technion-Cornell Institute,Cornell Tech,New York,New York,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"As three-dimensional printers become more available, 3D printed models can serve as important learning materials, especially for blind people who perceive the models tactilely. Such models can be much more powerful when augmented with audio annotations that describe the model and their elements. We present Markit and Talkit, a low-barrier toolkit for creating and interacting with 3D models with audio annotations. Makers (e.g., hobbyists, teachers, and friends of blind people) can use Markit to mark model elements and associate then with text annotations. A blind user can then print the augmented model, launch the Talkit application, and access the annotations by touching the model and following Talkit’s verbal cues. Talkit uses an RGB camera and a microphone to sense users’ inputs so it can run on a variety of devices. We evaluated Markit with eight sighted “makers” and Talkit with eight blind people. On average, non-experts added two annotations to a model in 275 seconds (SD=70) with Markit. Meanwhile, with Talkit, blind people found a specified annotation on a model in an average of 7 seconds (SD=8).",ls776@cornell.edu,"1. Eric Akaoka, Tim Ginn, and Roel Vertegaal. 2010. DisplayObjects: Prototyping Functional Physical Interfaces on 3D Styrofoam, Paper or Cardboard Models. In Proceedings of the fourth international conference on Tangible, embedded, and embodied interaction (TEI ’10), 49–56. https://doi.org/10.1145/1709886.1709897 \ 2. American Printing House. Product: DNA Twist. Retrieved April 2, 2017 from https://shop.aph.org/webapp/wcs/stores/servlet/Product _DNA Twist_1-08978-00P_10001_11051 \ 3. American Printing House. Product: Globe: Tactile and Visual. Retrieved April 2, 2017 from https://shop.aph.org/webapp/wcs/stores/servlet/Product _Globe: Tactile and Visual_1-0155100P_10001_11051 \ 4. Catherine M. Baker, Lauren R. Milne, Jeffrey Scofield, Cynthia L. Bennett, and Richard E. Ladner. 2014. Tactile graphics with a voice. In Proceedings of the 16th international ACM SIGACCESS conference on Computers & accessibility (ASSETS ’14), 75–82. https://doi.org/10.1145/2661334.2661366 \ 5. Connelly Barnes, David E. Jacobs, Jason Sanders, Dan B Goldman, Szymon Rusinkiewicz, Adam Finkelstein, and Maneesh Agrawala. 2008. Video Puppetry: A Performative Interface for Cutout Animation. ACM Transactions on Graphics 27, 5: 1–9. https://doi.org/10.1145/1409060.1409077 \ 6. bld. Textured Earth - Thingiverse. Retrieved April 4, 2017 from http://www.thingiverse.com/thing:17336 \ 7. Blender. Free and Open 3D Creation Software. Retrieved April 4, 2017 from https://www.blender.org/ \ 8. Quentin Bonnard, Severin Lemaignan, Guillaume Zufferey, Andrea Mazzei, Sebastien Cuendet, Nan Li, Ayberk Ozgur, and Pierre Dillenbourg. 2013. Chilitags 2: Robust Fiducial Markers for Augmented Reality and Robotics. \ 9. Gary R. Bradski and Adrian. Kaehler. 2008. Learning OpenCV : computer vision with the OpenCV library. O’Reilly. \ 10. John Brooke. 1996. SUS-A quick and dirty usability scale. Usability evaluation in industry 189, 194: 4–7. \ 11. Craig Brown and Amy Hurst. 2012. VizTouch: automatically generated tactile visualizations of coordinate spaces. In Proceedings of the Sixth International Conference on Tangible, Embedded and Embodied Interaction (TEI ’12), 131–138.  https://doi.org/10.1145/2148131.2148160  \ 12. Emeline Brule, Gilles Bailly, Anke Brock, Frederic Valentin, Grégoire Denis, and Christophe Jouffrais. 2016. MapSense: Multi-Sensory Interactive Maps for Children Living with Visual Impairments. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16), 445–457. https://doi.org/10.1145/2858036.2858375 \ 13. Erin Buehler, Stacy Branham, Abdullah Ali, Jeremy J Chang, Megan Kelly Hofmann, Amy Hurst, and Shaun K Kane. 2015. Sharing is Caring: Assistive Technology Designs on Thingiverse. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15), 525–534. https://doi.org/10.1145/2702123.2702525 \ 14. Erin Buehler, Niara Comrie, Megan Hofmann, Samantha McDonald, and Amy Hurst. 2016. Investigating the Implications of 3D Printing in Special Education. ACM Transactions on Accessible Computing 8, 3: 1–28. https://doi.org/10.1145/2870640 \ 15. Erin Buehler, Amy Hurst, and Megan Hofmann. 2014. Coming to grips: 3D printing for accessibility. In Proceedings of the 16th international ACM SIGACCESS conference on Computers & accessibility (ASSETS ’14), 291–292. https://doi.org/10.1145/2661334.2661345 \ 16. Erin Buehler, Shaun K. Kane, and Amy Hurst. 2014. ABC and 3D: opportunities and obstacles to 3D printing in special education environments. In Proceedings of the 16th international ACM SIGACCESS conference on Computers & accessibility (ASSETS ’14), 107–114. https://doi.org/10.1145/2661334.2661365 \ 17. Giovanni Fusco and Valerie S. Morash. 2015. The Tactile Graphics Helper : Providing Audio Clarification for Tactile Graphics Using Machine Vision Categories and Subject Descriptors. Proceedings of the 17th International ACM SIGACCESS Conference on Computers & Accessibility (ASSETS ’15): 491–500. https://doi.org/10.1145/2700648.2809868 \ 18. Timo Götzelmann. 2016. LucentMaps: 3D Printed Audiovisual Tactile Maps for Blind and Visually Impaired People. In Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS ’16), 81–90. https://doi.org/10.1145/2982142.2982163 \ 19. Anhong Guo, Xiang “Anthony” Chen, Haoran Qi, Samuel White, Suman Ghosh, Chieko Asakawa, and Jeffrey P. Bigham. 2016. VizLens : A Robust and Interactive Screen Reader for Interfaces in the Real World. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16), 651–664. https://doi.org/10.1145/2984511.2984518 \ 20. Anhong Guo, Jeeeun Kim, Xiang ’Anthony Chen, Tom Yeh, Scott E Hudson, Jennifer Mankoff, and Jeffrey P Bigham. 2017. Facade: Auto-generating Tactile Interfaces to Appliances. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17), 5826–5838. https://doi.org/10.1145/3025453.3025845 \ 21. Ankit Gupta, Dieter Fox, Brian Curless, and Michael Cohen. 2012. DuploTrack: A Real-time System for Authoring and Guiding Duplo Block Assembly. In Proceedings of the 25th annual ACM symposium on User interface software and technology (UIST ’12), 389–402. https://doi.org/10.1145/2380116.2380167 \ 22. Mark Hatch. 2013. The Maker Movement Manifesto: Rules for Innovation in the New World of Crafters, Hackers, and Tinkerers. McGraw Hill Professional. https://doi.org/10.1036/9780071821131 \ 23. Megan Hofmann, Jeffrey Harris, Scott E. Hudson, and Jennifer Mankoff. 2016. Helping Hands: Requirements for a Prototyping Methodology for Upper-limb Prosthetics Users. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16), 1769–1780. https://doi.org/10.1145/2858036.2858340 \ 24. David Huggins-daines, Mohit Kumar, Arthur Chan, Alan W Black, Mosur Ravishankar, and Alex I. Rudnicky. 2006. PocketSphinx: A free, real-time continuous speech recognition system for hand-held devices. In Proceedings of ICASSP. https://doi.org/10.1.1.67.2374 \ 25. Amy Hurst and Jasmine Tobias. 2011. Empowering individuals with do-it-yourself assistive technology. The proceedings of the 13th international ACM SIGACCESS conference on Computers and accessibility (ASSETS ’11): 11–18. https://doi.org/10.1145/2049536.2049541 \ 26. jaqtikkun. Cutaway Earth with Braille - Thingiverse. Retrieved April 1, 2017 from http://www.thingiverse.com/thing:510654 \ 27. Shaun K. Kane and Jeffrey P. Bigham. 2014. Tracking @stemxcomet. In Proceedings of the 45th ACM technical symposium on Computer science education (SIGCSE ’14), 247–252. https://doi.org/10.1145/2538862.2538975 \ 28. Shaun K. Kane, Brian Frey, and Jacob O. Wobbrock. 2013. Access Lens: A Gesture-Based Screen Reader for Real-World Documents. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’13), 347–350. https://doi.org/10.1145/2470654.2470704 \ 29. Jeeeun Kim and Tom Yeh. 2015. Toward 3D-Printed Movable Tactile Pictures for Children al Impairments with Visual Impairments. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15), 2815–2824. https://doi.org/10.1145/2702123.2702144 \ 30. Michael A. Kolitsky. Making Tactile 3D Prints Talk. Retrieved April 2, 2017 from http://www.instructables.com/id/Making-Tactile-3DPrints-Talk/?ALLSTEPS \ 31. Michael A. Kolitsky. 2015. 3D printing makes Visible Human cadaver sections accessible for blind students. Retrieved April 2, 2017 from http://www.nextgenemedia.com/NLM3Dprints/NLM3 Dprints.html \ 32. Gierad Laput, Eric Brockmeyer, Scott E. Hudson, and Chris Harrison. 2015. Acoustruments: Passive, Acoustically-Driven, Interactive Controls for Handheld Devices. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15), 2161–2170. https://doi.org/10.1145/2702123.2702414 \ 33. Make: Assistive technology News, Reviews and More. Retrieved April 4, 2017 from http://makezine.com/tag/assistive-technology/ \ 34. Assistive Technology Makers. Helping Makers Help Others. Retrieved April 4, 2017 from http://atmakers.org/ \ 35. Samantha McDonald, Joshua Dutterer, Ali Abdolrahmani, Shaun K. Kane, and Amy Hurst. 2014. Tactile aids for visually impaired graphical design education. In Proceedings of the 16th international ACM SIGACCESS conference on Computers & accessibility (ASSETS ’14), 275–276. https://doi.org/10.1145/2661334.2661392 \ 36. Roderick Murray-Smith, John Williamson, Stephen Hughes, and Torben Quaade. 2008. Stane: Synthesized Surfaces for Tactile Input. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’08): 1299–1302. https://doi.org/10.1145/1357054.1357257 \ 37. National Federation of the Blind Jernigan Institute. 2009. The Braille Literacy Crisis in America. National Federation of the Blind. \ 38. OSM Buildings. OSM Buildings. Retrieved April 4, 2017 from https://osmbuildings.org/ \ 39. PaLEoS. Animal Cell - Thingiverse. Retrieved April 4, 2017 from http://www.thingiverse.com/thing:689381 \ 40. Huaishu Peng, François Guimbretière, James Mccann, and Scott E Hudson. 2016. A 3D Printer for Interactive   Electromagnetic Devices. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16), 553–562. https://doi.org/10.1145/2984511.2984523 \ 41. Huaishu Peng, Jennifer Mankoff, Scott E Hudson, and James McCann. 2015. A Layered Fabric 3D Printer for Soft Interactive Objects. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI’15), 1789–1798. https://doi.org/10.1145/2702123.2702327 \ 42. Thomas Poon and Ronit Ovadia. 2008. Using Tactile Learning Aids for Students with Visual Impairments in a First-Semester Organic Chemistry Course. Journal of Chemical Education 85, 2: 240. https://doi.org/10.1021/ed085p240 \ 43. Andreas Reichinger, Anton Fuhrmann, Stefan Maierhofer, and Werner Purgathofer. 2016. GestureBased Interactive Audio Guide on Tactile Reliefs. In Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS ’16), 91–100. https://doi.org/10.1145/2982142.2982176 \ 44. roman_hegglin. Customizable Atom Deluxe (every element preconfigured) - Thingiverse. Retrieved April 1, 2017 from http://www.thingiverse.com/thing:114247 \ 45. Audrey C. Rule. 2011. Tactile Earth and Space Science Materials for Students with Visual Impairments: Contours, Craters, Asteroids, and Features of Mars. Journal of Geoscience Education 59, 4: 205–218. https://doi.org/10.5408/1.3651404 \ 46. Audrey C. Rule, Greg P. Stefanich, Robert M. Boody, and Belinda Peiffer. 2011. Impact of Adaptive Materials on Teachers and their Students with Visual Impairments in Secondary Science and Mathematics Classes. International Journal of Science Education 33, 6: 865–887. https://doi.org/10.1080/09500693.2010.506619 \ 47. Valkyrie Savage, Colin Chang, and Björn Hartmann. 2013. Sauron: embedded single-camera sensing of printed physical user interfaces. In Proceedings of the 26th annual ACM symposium on User interface software and technology (UIST ’13), 447–456. https://doi.org/10.1145/2501988.2501992 \ 48. Valkyrie Savage, Sean Follmer, Jingyi Li, and Björn Hartmann. 2015. Makers’ Marks: Physical Markup for Designing and Fabricating Functional Objects. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST ’15), 103–108. https://doi.org/10.1145/2807442.2807508 \ 49. Valkyrie Savage, Andrew Head, Björn Hartmann, Dan B. Goldman, Gautham Mysore, and Wilmot Li. 2015.  Lamello: Passive Acoustic Sensing for Tangible Input  Components. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15), 1277–1280. https://doi.org/10.1145/2702123.2702207 \ 50. Valkyrie Savage, Ryan Schmidt, Tovi Grossman, George Fitzmaurice, and Björn Hartmann. 2014. A series of tubes: adding interactivity to 3D prints using internal pipes. In Proceedings of the 27th annual ACM symposium on User interface software and technology (UIST ’14), 3–12. https://doi.org/10.1145/2642918.2647374 \ 51. Martin Schmitz, Mohammadreza Khalilbeigi, Matthias Balwierz, Roman Lissermann, Max Mühlhäuser, and Jürgen Steimle. 2015. Capricate: A Fabrication Pipeline to Design and 3D Print Capacitive Touch Sensors for Interactive Objects. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST ’15), 253–258. https://doi.org/10.1145/2807442.2807503 \ 52. Huiying Shen, Owen Edwards, Joshua Miele, and James M. Coughlan. 2013. CamIO. In Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS ’13), 1–2. https://doi.org/10.1145/2513383.2513423 \ 53. Lei Shi. 2015. Talkabel: A Labeling Method for 3D Printed Models. In Proceedings of the 17th International ACM SIGACCESS Conference on Computers & Accessibility (ASSETS ’15), 361–362. https://doi.org/10.1145/2700648.2811327 \ 54. Lei Shi, Ross McLachlan, Yuhang Zhao, and Shiri Azenkot. 2016. Magic Touch : Interacting with 3D Printed Graphics. In ASSETS ’16, 329–330. \ 55. Lei Shi, Idan Zelzer, Catherine Feng, and Shiri Azenkot. 2016. Tickers and Talker: An Accessible Labeling Toolkit for 3D Printed Models. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16), 4896–4907. https://doi.org/10.1145/2858036.2858507 \ 56. Lei Shi, Yuhang Zhao, and Shiri Azenkot. 2017. Designing Interactions for 3D Printed Models with Blind People. In In Proceedings of the 17th International ACM SIGACCESS Conference on Computers & Accessibility (ASSETS ’17). \ 57. Roy Shilkrot, Jochen Huber, Wong Meng Ee, Pattie Maes, and Suranga Chandima Nanayakkara. 2015. FingerReader: A Wearable Device to Explore Printed Text on the Go. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15), 2363–2372. https://doi.org/10.1145/2702123.2702421 \ 58. Hyunyoung Song, François Guimbretière, Chang Hu, and Hod Lipson. 2006. ModelCraft: capturing freehand   annotations and edits on physical 3D models. In Proceedings of the 19th annual ACM symposium on User interface software and technology (UIST ’06), 1– 10. https://doi.org/10.1145/1166253.1166258 \ 59. Abigale Stangl, Jeeeun Kim, and Tom Yeh. 2014. 3D printed tactile picture books for children with visual impairments. In Proceedings of the 2014 conference on Interaction design and children (IDC ’14), 321–324. https://doi.org/10.1145/2593968.2610482 \ 60. Lee Stearns, Ruofei Du, Uran Oh, Catherine Jou, Leah Findlater, David A. Ross, and Jon E. Froehlich. 2016. Evaluating Haptic and Auditory Directional Guidance to Assist Blind People in Reading Printed Text Using Finger-Mounted Cameras. ACM Transactions on Accessible Computing 9, 1: 1–38. https://doi.org/10.1145/2914793 \ 61. Saiganesh Swaminathan, Thijs Roumen, Robert Kovacs, David Stangl, Stefanie Mueller, and Patrick Baudisch. 2016. Linespace: A Sensemaking Platform for the Blind. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16): 2175–2185. https://doi.org/10.1145/2858036.2858245 \ 62. Brandon Taylor, Anind Dey, Dan Siewiorek, and Asim Smailagic. 2016. Customizable 3D Printed Tactile Maps as Interactive Overlays. In Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS ’16), 71–79. https://doi.org/10.1145/2982142.2982167 \ 63. Jolene K. Teske, Phyllis Gray, Mason A. Kuhn, Courtney K. Clausen, Latisha L. Smith, Sukainah A. Alsubia, Maryam Ghayoorad, Audrey C. Rule, and Jean Suchsland Schneider. 2014. Teacher-made Tactile Science Materials with Critical and Creative Thinking Activities for Learners including those with Visual Impairments. Online Submission. \ 64. TOM. TOM: Tikkun Olam Makers. Retrieved April 4, 2017 from http://tomglobal.org/ \ 65. Marynel Vázquez, Eric Brockmeyer, Ruta Desai, Chris Harrison, and Scott E. Hudson. 2015. 3D Printing Pneumatic Device Controls with Variable Activation Force Capabilities. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15), 1295–1304. https://doi.org/10.1145/2702123.2702569 \ 66. Karl Willis, Eric Brockmeyer, Scott Hudson, and Ivan Poupyrev. 2012. Printed optics: 3D printing of embedded optical elements for interactive devices. In Proceedings of the 25th annual ACM symposium on User interface software and technology (UIST ’12), 589–598. https://doi.org/10.1145/2380116.2380190 \ 67. xehpuk. Simplified Tiger Tank - Thingiverse. Retrieved April 4, 2017 from http://www.thingiverse.com/thing:90265 \ 68. Hansong Zhang and Kenneth E. Hoff. 1997. Fast backface culling using normal masks. In Proceedings of the 1997 symposium on Interactive 3D graphics (SI3D ’97), 103-. https://doi.org/10.1145/253284.253314 \ 69. Yang Zhang, Gierad Laput, and Chris Harrison. 2017. Electrick : Low - Cost Touch Sensing Using Electric Field Tomography. In In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17), 1–14. https://doi.org/10.1145/3025453.3025842 \ ",Visual impairments; 3D models; computer vision ,H.5.2,uistf4174-file1.docx,,uistf4174-file3.m4v,,,,Three-dimensional printed models are important learning tools for blind people. We created a low-barrier toolkit that uses computer-vision technology to make these models more accessible and powerful by adding audio annotations.  \  \ ,"We included necessary references, corrected writing issues, and added technical details as reviewers suggested. We also checked the accessibility setting of the PDF document.",Lei Shi,Shiri Azenkot,FormatComplete,,,,,,,Aug 9 0:58,
uistf3835,10/24,11,Alt.Modalities,2:00:00 PM,3:30:00 PM,4+1,2:40:00 PM,3:00:00 PM,long,long,uistf4174,3,1103,,,uistf3835,A,INVISO: A Cross-platform User Interface for Creating Virtual Sonic Environments,Anil,Camci,anilcamci@gmail.com,uistf3835-paper.pdf,12,letter,,,"Anil Camci, Kristine Lee, Cody J Roberts, Angus G Forbes","anilcamci@gmail.com, khlee2@uic.edu, codyjroberts@gmail.com, aforbes@uic.edu",57417,Anil,,Camci,anilcamci@gmail.com,Department of Performing Arts Technology,University of Michigan,Ann Arbor,Michigan,United States,,,,,,60559,Kristine,,Lee,khlee2@uic.edu,,University of Illinois at Chicago,Chicago,Illinois,United States,,,,,,71447,Cody,J,Roberts,codyjroberts@gmail.com,Department of Computer Science,University of Illinois at Chicago,Chicago,Illinois,United States,,,,,,27438,Angus,G,Forbes,aforbes@uic.edu,Department of Computer Science,University of Illinois at Chicago,Chicago,Illinois,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The predominant interaction paradigm of current audio spatialization tools, which are primarily geared towards expert users, imposes a design process in which users are characterized as stationary, limiting the application domain of these tools. Navigable 3D sonic virtual realities, on the other hand, can support many applications ranging from soundscape prototyping to spatial data representation. Although modern game engines provide a limited set of audio features to create such sonic environments, the interaction methods are inherited from the graphical design features of such systems, and are not specific to the auditory modality. To address such limitations, we introduce INVISO, a novel web-based user interface for designing and experiencing rich and dynamic sonic virtual realities. Our interface enables both novice and expert users to construct complex immersive sonic environments with 3D dynamic sound components. INVISO is platform-independent and facilitates a variety of mixed reality applications, such as those where users can simultaneously experience and manipulate a virtual sonic environment. In this paper, we detail the interface design considerations for our audio-specific VR tool. To evaluate the usability of INVISO, we conduct two user studies: The first demonstrates that our visual interface effectively facilitates the generation of creative audio environments; the second demonstrates that both expert and non-expert users are able to use our software to accurately recreate complex 3D audio scenes. ",anilcamci@gmail.com,"1. Paul Adenot and Chris Wilson. 2015. Web Audio API. (2015). Available: http://webaudio.github.io/web-audio-api/. \ 2. Michel Beaudouin-Lafon and William W Gaver. 1994. ENO: synthesizing structured sound spaces. In Proceedings of the 7th Annual ACM Symposium on User Interface Software and Technology (UIST). 49–57. \ 3. Benjamin B. Bederson, James D. Hollan, Ken Perlin, Jonatham Meyer, David Bacon, and George Furnas. 1995. PAD++: A Zoomable Graphical Sketchpad for Exploring Alternate Interface Physics. Journal of Visual Languages and Computing 7 (1995), 3–31. \ 4. Doug A. Bowman, Chris North, Jian Chen, Nicholas F. Polys, Pardha S. Pyla, and Umur Yilmaz. 2003. Information-rich Virtual Environments: Theory, Tools, and Research Agenda. In Proceedings of the ACM Symposium on Virtual Reality Software and Technology (VRST). 81–90. \ 5. Thibaut Carpentier. 2015. Binaural synthesis with the Web Audio API. In Proceedings of the 1st Web Audio Conference. \ 6. Anıl Çamcı, Paul Murray, and Angus G. Forbes. 2016a. Designing and Controlling Virtual Sonic Environments Using a Browser-based 3DUI. In Proceedings of the IEEE Symposium on 3D User Interfaces (3DUI). 235–236. \ 7. Anıl Çamcı, Paul Murray, and Angus G. Forbes. 2016b. A Web-based System for Designing Interactive Virtual Soundscapes. In Proceedings of the 42nd International Computer Music Conference (ICMC). 579–584. \ 8. Anıl Çamcı, Zeynep Özcan, and Damla Pehlevan. 2015. Interactive Virtual Soundscapes: a Research Report. In Proceedings of the 41st International Computer Music Conference (ICMC). 163–169. \ 9. Thomas Funkhouser, Jean Marc Jot, and Nicolas Tsingos. 2002. “Sounds good to me!”–Computational sound for graphics, virtual reality, and interactive systems. ACM SIGGRAPH Course Notes (2002), 1–43. \ 10. Bo Gehring. 1988. Attitude Indicator. (27 September 1988). US Patent #4,774,515. \ 11. Matthias Geier and Sascha Spors. 2012. Spatial Audio with the SoundScape Renderer. In 27th Tonmeistertagung–VDT International Convention. \ 12. Morton L. Heilig. 1960. Stereoscopic-television apparatus for individual use. (4 October 1960). US Patent #2,955,156. \ 13. Jyri Huopaniemi, Lauri Savioja, and Tapio Takala. 1996. DIVA virtual audio reality system. In Proceedings of International Conference on Auditory Display (ICAD). \ 14. Jacek Jankowski and Stefan Decker. 2012. A Dual-mode User Interface for Accessing 3D Content on the World Wide Web. In Proceedings of the 21st International Conference on World Wide Web (WWW). 1047–1056. \ 15. Mikko-Ville Laitinen, Tapani Pihlajamäki, Cumhur Erkut, and Ville Pulkki. 2012. Parametric time-frequency representation of spatial sound in virtual worlds. ACM Transactions on Applied Perception (TAP) 9, 2 (2012), 8:1–20. \ 16. Kyla McMullen. 2014. The potentials for spatial audio to convey information in virtual environments. In IEEE VR Workshop on Sonic Interaction in Virtual Environments (SIVE). 31–34. \ 17. Ravish Mehra, Atul Rungta, Abhinav Golas, Ming Lin, and Dinesh Manocha. 2015. WAVE: Interactive Wave-based Sound Propagation for Virtual Environments. IEEE Transactions on Visualization and Computer Graphics 21, 4 (2015), 434–442. \ 18. Elizabeth D Mynatt, Maribeth Back, Roy Want, and Ron Frederick. 1997. Audio Aura: Light-weight audio augmented reality. In Proceedings of the 10th Annual ACM Symposium on User interface Software and Technology (UIST). 211–212. \ 19. Ji-Young Oh and Wolfgang Stuerzlinger. 2005. Moving Objects with 2D Input Devices in CAD Systems and Desktop Virtual Environments. In Proceedings of Graphics Interface. 195–202. \ 20. Rui Penha and J Oliveira. 2013. Spatium, tools for sound spatialization. In Proceedings of the Sound and Music Computing Conference. 660–667. \ 21. Chris Pike, Peter Taylour, and Frank Melchior. 2015. Delivering Object-Based 3D Audio Using The Web Audio API And The Audio Deﬁnition Model. In Proceedings of the 1st Web Audio Conference. \ 22. Khairi Reda, Alessandro Febretti, Aaron Knoll, Jillian Aurisano, Jason Leigh, Andrew Johnson, Michael E Papka, and Mark Hereld. 2013. Visualizing large, heterogeneous data in hybrid-reality environments. IEEE Computer Graphics and Applications 4 (2013), 38–48. \ 23. Mathias Rossignol, Gregoire Lafay, Mathieu Lagrange, and Nicolas Misdarris. 2015. SimScene: A web-based acoustic scenes simulator. In Proceedings of the 1st Web Audio Conference. \ 24. Joseph Rozier, Karrie Karahalios, and Judith Donath. 2000. Hear&There: An Augmented Reality System of Linked Audio. In Proceedings of International Conference on Auditory Display (ICAD). \ 25. Lauri Savioja, Jyri Huopaniemi, Tapio Lokki, and Ritta Väänänen. 1999. Creating Interactive Virtual Acoustic Environments. J. Audio Eng. Soc 47, 9 (1999), 675–705. \ 26. Chris Schmandt. 1998. Audio hallway: A virtual acoustic environment for browsing. In Proceedings of the 11th Annual ACM Symposium on User Interface Software and Technology (UIST). 163–170. \ 27. B Shinn-Cunningham. 1998. Applications of virtual auditory displays. In Proceedings of the International Conference of the IEEE Engineering in Biology and Medicine Society, Vol. 3. 1105–1108. \ 28. Ben Shneiderman. 2007. Creativity support tools: Accelerating discovery and innovation. Commun. ACM 50, 12 (2007), 20–32. \ 29. Ken Shoemake. 1992. ARCBALL: A User Interface for Specifying Three-dimensional Orientation Using a Mouse. In Proceedings of the Conference on Graphics Interface. 151–156. \ 30. Tapio Takala and James Hahn. 1992. Sound rendering. Computer Graphics 26, 2 (1992), 211–220. \ 31. Tan Yiyu, Yasushi Inoguchi, Eiko Sugawara, Makoto Otani, Yukio Iwaya, Yukinori Sato, Hiroshi Matsuoka, and Takao Tsuchiya. 2011. A real-time sound ﬁeld renderer based on digital Huygens’ model. Journal of Sound and Vibration 330, 17 (2011), 4302–4312. \ 32. Shengkui Zhao, Ryan Rogowski, Reece Johnson, and Douglas L Jones. 2012. 3D binaural audio capture and reproduction using a miniature microphone array. In Proceedings of the 15th International Conference on Digital Audio Effects (DAFx). 151–154. \ ",Virtual Sonic Environments; 3D Audio; Virtual Reality; Design Environment; Browser-based Interface,H.5.1; H.5.2; H.5.5,uistf3835-file1.zip,,uistf3835-file3.mp4,,,,"INVISO is a novel cross-platform UI for creating rich, dynamic and navigable 3D virtual sonic environments in the browser.","We thank the reviewers for their helpful and detailed feedback. We very much appreciate the positive responses to our paper from the reviewers, and we are grateful for the clearly outlined recommendations to improve it. We have submitted a revised version of our paper “INVISO: A Cross-platform User Interface for Creating Virtual Sonic Environments”. A summary of all improvements to the paper is provided below. \  \ •	Per R3 and R5’s suggestion, we have revised Figure 5. We have replaced the two pie charts with a bubble-graph that clearly indicates the distribution of experience levels across platforms and users in a way that reveals, for instance, we had 1 user who had no experience with either platform.  \ •	We have included the literature suggested by R2 in our Related Work section. \ •	We have addressed the issue regarding the inconsistency between a ‘simply connected’ and an ‘arbitrary shape’. We have also removed the related reference per R2’s suggestion. \ •	We have corrected the issues in Figure 9.  \ •	We have fixed the typos and errors explicitly stated by the reviewers. We have also passed the text through several editing cycles to address phrasing issues and other errors. \ •	We referred to inspectors as a broader term used for the kind of attribute windows used in our system. \ •	Per R4’s suggestion, we summed up the last two contributions in our introduction into one. \ •	Upon R5’s comment about INVISO’s adherence to Shneiderman’s design principles for creativity support tools, we have added a brief reference to this literature since we believe it perfectly frames some of our goals in creating INVISO. We also believe this partially addresses R1’s question regarding where the system’s novelty lies. \ •	We have made minor editions to Applications and Future Work sections to incorporate some of the suggestions by the reviewers (e.g. consumer applications, GPS integration). \ •	We have reworded parts of our analysis of the user study to address R3’s concern about the paper being too strong about its conclusions.",Anil Camci,Angus Forbes,FormatComplete,,,,,,,Aug 9 10:46,
uistf4499,10/24,11,Alt.Modalities,2:00:00 PM,3:30:00 PM,4+1,3:00:00 PM,3:20:00 PM,long,long,uistf3835,4,1104,,,uistf4499,A,Neuroanatomical Correlates of Perceived Usability,Kasper,Hornbæk,kash@diku.dk,uistf4499-paper.pdf,14,letter,,,"Chi Thanh Vi, Kasper Hornbæk, Sriram Subramanian","vichithanh@gmail.com, kash@diku.dk, sriram@sussex.ac.uk",18175,Chi,,Thanh Vi,vichithanh@gmail.com,"SCHI lab, Creative Technology Group, School of Engineering and Informatics",University of Sussex,Brighton,,United Kingdom,,,,,,1245,Kasper,,Hornbæk,kash@diku.dk,Department of Computer Science,University of Copenhagen,Copenhagen,,Denmark,,,,,,2491,Sriram,,Subramanian,sriram@sussex.ac.uk,Interact Lab,University of Sussex,Brighton,,United Kingdom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Usability has a distinct subjective component, yet surprisingly little is known about its neural basis and relation to the neuroanatomy of aesthetics. To begin closing this gap, we conducted two functional magnetic resonance imaging studies in which participants were shown static webpages (in the first study) and videos of interaction with webpages (in the second study). The webpages were controlled so as to exhibit high and low levels of perceived usability and perceived aesthetics. Our results show unique links between perceived usability and brain areas involved in functions such as emotional processing (left fusiform gyrus, superior frontal gyrus), anticipation of physical interaction (precentral gyrus), task intention (anterior cingulate cortex), and linguistic processing (medial and bilateral superior frontal gyri). We use these findings to discuss the brain correlates of perceived usability and the use of fMRI for usability evaluation and for generating new user experiences. \ ",kash@diku.dk,"1. Ergonomics of human system interaction - Part 210: Human-centred design for interactive systems, in ISO DIS 9241-210. 2008, International Organization for Standardization (ISO). \ 2. Aharon, I., N. Etcoff, D. Ariely, C.F. Chabris, E. O'Connor, and H.C. Breiter, Beautiful faces have variable reward value: fMRI and behavioral evidence. Neuron, 2001. 32(3): p. 537-51. \ 3. Al-Shamaileh, O. and A. Sutcliffe, The effect of website interactivity and repeated exposure on user experience, in Proceedings of the 4th Mexican Conference on Human-Computer Interaction. 2012, ACM: Mexico City, Mexico. p. 1-8. \ 4. Amting, J.M., S.G. Greening, and D.G.V. Mitchell, Multiple Mechanisms of Consciousness: The Neural Correlates of Emotional Awareness. The Journal of Neuroscience, 2010. 30(30): p. 9. \ 5. Anderson, B.B., C.B. Kirwan, J.L. Jenkins, D. Eargle, S. Howard, and A. Vance, How Polymorphic Warnings Reduce Habituation in the Brain: Insights from an fMRI Study, in CHI. 2015, ACM: Seoul, Republic of Korea. p. 2883-2892. \ 6. Anderson, E.W., K.C. Potter, L.E. Matzen, J.F. Shepherd, G.A. Preston, and C.T. Silva, A user study of visualization effectiveness using EEG and cognitive load, in EuroVis. 2011, Eurographics Association: Bergen, Norway. p. 791-800. \ 7. Angeli, A.D., A. Sutcliffe, and J. Hartmann, Interaction, usability and aesthetics: what influences users' preferences?, in Proceedings of the 6th conference on Designing Interactive systems. 2006, ACM: University Park, PA, USA. p. 271-280. \ 8. Bargas-Avila, J.A. and K. Hornbæk, Old wine in new bottles or novel challenges: a critical analysis of empirical studies of user experience, in CHI. 2011, ACM: Vancouver, BC, Canada. p. 2689-2698. \ 9. Baumgartner, T., D. Speck, D. Wettstein, O. Masnari, G. Beeli, and L. Jancke, Feeling present in arousing virtual reality worlds: prefrontal brain regions differentially orchestrate presence experience in adults and children. Front Hum Neurosci, 2008. 2: p. 8. \ 10. Beauregard, M., J. Levesque, and P. Bourgouin, Neural correlates of conscious self-regulation of emotion. J Neurosci, 2001. 21(18): p. Rc165. \ 11. Bias, R.G. and D.J. Mayhew, Cost-Justifying Usability: An Update for the Internet Age. 2005: Morgan Kaufmann Publishers Inc. \ 12. Blakemore, S.-J., D. Bristow, G. Bird, C. Frith, and J. Ward, Somatosensory activations during the observation of touch and a case of vision–touch synaesthesia. Brain, 2005. 128(7): p. 1571-1583. \ 13. Bohrn, I.C., U. Altmann, O. Lubrich, W. Menninghaus, and A.M. Jacobs, When we like what we know--a parametric fMRI analysis of beauty and familiarity. Brain Lang, 2013. 124(1): p. 1-8. \ 14. Brainard, D.H., The Psychophysics Toolbox. Spat Vis, 1997. 10(4): p. 433-6. \ 15. Bush, G., B.A. Vogt, J. Holmes, A.M. Dale, D. Greve, M.A. Jenike, and B.R. Rosen, Dorsal anterior cingulate cortex: A role in reward-based decision making. Proceedings of the National Academy of Sciences, 2002. 99(1): p. 523-528. \ 16. Carter, C.S., T.S. Braver, D.M. Barch, M.M. Botvinick, D. Noll, and J.D. Cohen, Anterior cingulate cortex, error detection, and the online monitoring of performance. Science, 1998. 280(5364): p. 747-9. \ 17. Casey, B.J., K.M. Thomas, T.F. Welsh, R.D. Badgaiyan, C.H. Eccard, J.R. Jennings, and E.A. Crone, Dissociation of response conflict, attentional selection, and expectancy with functional magnetic resonance imaging. Proc Natl Acad Sci U S A, 2000. 97(15): p. 8728-33. \ 18. Cela-Conde, C.J., G. Marty, F. Maestú, T. Ortiz, E. Munar, A. Fernández, M. Roca, J. Rosselló, and F. Quesney, Activation of the prefrontal cortex in the human visual aesthetic perception. PNAS, 2004. 101(16): p. 6321-6325. \ 19. Chouinard, P.A. and T. Paus, The primary motor and premotor areas of the human cerebral cortex. Neuroscientist, 2006. 12(2): p. 143-52. \ 20. Cinzia, D.D. and G. Vittorio, Neuroaesthetics: a review. Curr Opin Neurobiol, 2009. 19(6): p. 682-7. \ 21. Clemente, M., B. Rey, A. Rodríguez-Pujadas, A. Barros-Loscertales, R.M. Baños, C. Botella, M. Alcañiz, and C. Ávila, An fMRI Study to Analyze Neural Correlates of Presence during Virtual Reality Experiences. Interacting with Computers, 2013. \ 22. CrowdFlower: AI for your business. https://www.crowdflower.com/ \ 23. Dimoka, A., How to conduct a functional magnetic resonance (fMRI) study in social science research. MIS Q., 2012. 36(3): p. 811-840. \ 24. Ebisch, S.J., M.G. Perrucci, A. Ferretti, C. Del Gratta, G.L. Romani, and V. Gallese, The sense of touch: embodied simulation in a visuotactile mirroring mechanism for observed animate or inanimate touch. J Cogn Neurosci, 2008. 20(9): p. 1611-23. \ 25. Elliott, R., K.J. Friston, and R.J. Dolan, Dissociable neural responses in human reward systems. J Neurosci, 2000. 20(16): p. 6159-65. \ 26. Foley, E., G. Rippon, N.J. Thai, O. Longe, and C. Senior, Dynamic Facial Expressions Evoke Distinct Activation in the Face Perception Network: A Connectivity Analysis Study. Journal of Cognitive Neuroscience, 2011. 24(2): p. 507-520. \ 27. Frøkjær, E., M. Hertzum, and K. Hornbæk, Measuring usability: are effectiveness, efficiency, and satisfaction really correlated?, in CHI. 2000, ACM: The Hague, The Netherlands. p. 345-352. \ 28. Gallagher, H.L., F. Happé, N. Brunswick, P.C. Fletcher, U. Frith, and C.D. Frith, Reading the mind in cartoons and stories: an fMRI study of ‘theory of mind’ in verbal and nonverbal tasks. Neuropsychologia, 2000. 38(1): p. 11-21. \ 29. Ge, J., G. Peng, B. Lyu, Y. Wang, Y. Zhuo, Z. Niu, L.H. Tan, A.P. Leff, and J.-H. Gao, Cross-language differences in the brain network subserving intelligible speech. PNAS, 2015. 112(10): p. 2972-2977. \ 30. Glocker, M.L., D.D. Langleben, K. Ruparel, J.W. Loughead, J.N. Valdez, M.D. Griffin, N. Sachser, and R.C. Gur, Baby schema modulates the brain reward system in nulliparous women. PNAS, 2009. 106(22): p. 9115-9119. \ 31. Goldberg, I.I., M. Harel, and R. Malach, When the Brain Loses Its Self: Prefrontal Inactivation during Sensorimotor Processing. Neuron, 2006. 50(2): p. 329339. \ 32. Grecucci, A., C. Giorgetta, N. Bonini, and A.G. Sanfey, Reappraising social emotions: the role of inferior frontal gyrus, temporo-parietal junction and insula in interpersonal emotion regulation. Frontiers in Human Neuroscience, 2013. 7: p. 523. \ 33. Greene, J.D., R.B. Sommerville, L.E. Nystrom, J.M. Darley, and J.D. Cohen, An fMRI Investigation of Emotional Engagement in Moral Judgment. Science, 2001. 293(5537): p. 2105-2108. \ 34. Grezes, J., M. Tucker, J. Armony, R. Ellis, and R.E. Passingham, Objects automatically potentiate action: an fMRI study of implicit processing. European Journal of Neuroscience, 2003: p. 6. \ 35. Hamann, S. and H. Mao, Positive and negative emotional verbal stimuli elicit activity in the left amygdala. Neuroreport, 2002. 13(1): p. 15-9. \ 36. Hassenzahl, M., The interplay of beauty, goodness, and usability in interactive products. Hum.-Comput. Interact., 2008. 19(4): p. 319-349. \ 37. Hassenzahl, M., M. Burmester, and F. Koller, AttrakDiff: Ein Fragebogen zur Messung wahrgenommener hedonischer und pragmatischer Qualität, in Mensch & Computer 2003, G. Szwillus and J. Ziegler, Editors. 2003, Vieweg+Teubner Verlag. p. 187-196. \ 38. Hassenzahl, M. and N. Tractinsky, User experience-a research agenda. Behaviour & Information Technology, 2006. 25(2): p. 91-97. \ 39. Haynes, J.-D., K. Sakai, G. Rees, S. Gilbert, C. Frith, and R.E. Passingham, Reading Hidden Intentions in the Human Brain. Current Biology, 2007. 17(4): p. 323328. \ 40. Hertzum, M., Images of Usability. International Journal of Human-Computer Interaction, 2010. 26(6): p. 567-600. \ 41. Hill, A.P. and C.J. Bohil, Applications of Optical Neuroimaging in Usability Research. Ergonomics in design : the magazine of human factors applications, 2016. 24(2): p. 4-9. \ 42. Hirshfield, L.M., E.T. Solovey, A. Girouard, J. Kebinger, R.J.K. Jacob, A. Sassaroli, and S. Fantini, Brain measurement for usability testing and adaptive interfaces: an example of uncovering syntactic workload with functional near infrared spectroscopy, in ACM CHI Conference on Human Factors in Computing Systems. 2009, ACM: Boston, MA, USA. p. 2185-2194. \ 43. Hornbæk, K., Current practice in measuring usability: Challenges to usability studies and research. International Journal of Human-Computer Studies, 2006. 64(2): p. 79-102. \ 44. Huettel, S.A., A.W. Song, and G. McCarthy, Functional Magnetic Resonance Imaging. 2009: Sinauer Associates, Incorporated. \ 45. Jacobs, R.H.A.H., R. Renken, and F.W. Cornelissen, Neural Correlates of Visual Aesthetics – Beauty as the Coalescence of Stimulus and Internal State. PLoS ONE, 2012. 7(2): p. e31248. \ 46. Jacobsen, T., R.I. Schubotz, L. Hofel, and D.Y. Cramon, Brain correlates of aesthetic judgment of beauty. Neuroimage, 2006. 29(1): p. 276-85. \ 47. Kampe, K.K., C.D. Frith, R.J. Dolan, and U. Frith, Reward value of attractiveness and gaze. Nature, 2001. 413(6856): p. 589. \ 48. Karapanos, E., User Experience Over Time, in Modeling Users' Experiences with Interactive Systems. 2013, Springer Berlin Heidelberg: Berlin, Heidelberg. p. 57-83. \ 49. Kawabata, H. and S. Zeki, Neural correlates of beauty. J Neurophysiol, 2004. 91(4): p. 1699-705. \ 50. Kim, H., R. Adolphs, J.P. O'Doherty, and S. Shimojo, Temporal isolation of neural processes underlying face preference decisions. PNAS, 2007. 104(46): p. 1825318258. \ 51. Kühn, S. and J. Gallinat, The neural correlates of subjective pleasantness. NeuroImage, 2012. 61(1): p. 289-294. \ 52. Kurth, R., K. Villringer, G. Curio, K.J. Wolf, T. Krause, J. Repenthin, J. Schwiemann, M. Deuchert, and A. Villringer, fMRI shows multiple somatotopic digit representations in human primary somatosensory cortex. Neuroreport, 2000. 11(7): p. 1487-91. \ 53. Landauer, T.K., The Trouble with Computers: Usefulness, Usability, and Productivity. 1996: MIT Press. \ 54. Laugwitz, B., T. Held, and M. Schrepp, Construction and Evaluation of a User Experience Questionnaire, in HCI and Usability for Education and Work, A. Holzinger, Editor. 2008, Springer Berlin Heidelberg. p. 63-76. \ 55. Law, E.L.-C., V. Roto, M. Hassenzahl, A.P.O.S. Vermeeren, and J. Kort, Understanding, scoping and defining user experience: a survey approach, in CHI. 2009, ACM: Boston, MA, USA. p. 719-728. \ 56. Lewis, J.R., Usability: Lessons Learned … and Yet to Be Learned. International Journal of Human-Computer Interaction, 2014. 30(9): p. 663-684. \ 57. Lindgaard, G. and C. Dudek, What is this evasive beast we call user satisfaction? Interacting with Computers, 2003. 15(3): p. 429-452. \ 58. Lindgaard, G., C. Dudek, D. Sen, L. Sumegi, and P. Noonan, An exploration of relations between visual appeal, trustworthiness and perceived usability of homepages. ACM Trans. Comput.-Hum. Interact., 2011. 18(1): p. 1-30. \ 59. Lindgaard, G., G. Fernandes, C. Dudek, and J. Brown, Attention web designers: you have 50 milliseconds to make a good first impression! \ 60. Lukanov, K., H.A. Maior, and M.L. Wilson, Using fNIRS in Usability Testing: Understanding the Effect of Web Form Layout on Mental Workload, in Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. 2016, ACM: Santa Clara, California, USA. p. 4011-4016. \ 61. McCandliss, B.D., L. Cohen, and S. Dehaene, The visual word form area: expertise for reading in the fusiform gyrus. Trends in Cognitive Sciences, 2003. 7(7): p. 293-299. \ 62. Mecklinger, A., C. Gruenewald, N. Weiskopf, and C.F. Doeller, Motor affordance and its role for visual working memory: evidence from fMRI studies. Exp Psychol, 2004. 51(4): p. 258-69. \ 63. Miller, E.K., D.J. Freedman, and J.D. Wallis, The prefrontal cortex: categories, concepts and cognition. Philosophical Transactions of the Royal Society B: Biological Sciences, 2002. 357(1424): p. 1123-1136. \ 64. Molenberghs, P., R. Cunnington, and J.B. Mattingley, Is the mirror neuron system involved in imitation? A short review and meta-analysis. Neuroscience & Biobehavioral Reviews, 2009. 33(7): p. 975-980. \ 65. Moll, J., P.J. Eslinger, and R. Oliveira-Souza, Frontopolar and anterior temporal cortex activation in a moral judgment task: preliminary functional MRI results in normal subjects. Arq Neuropsiquiatr, 2001. 59(3-b): p. 657-64. \ 66. Monti, M.M., L.M. Parsons, and D.N. Osherson, The boundaries of language and thought in deductive inference. PNAS, 2009. 106(30): p. 12554-12559. \ 67. Moshagen, M. and M.T. Thielsch, Facets of visual aesthetics. International Journal of Human-Computer Studies, 2010. 68(10): p. 689-709. \ 68. Nielsen, J., Usability Engineering. 1994: AP Professional. \ 69. Paulus, M.P. and L.R. Frank, Ventromedial prefrontal cortex activation is critical for preference judgments. Neuroreport, 2003. 14(10): p. 1311-5. \ 70. Pedersen, E.W., S. Subramanian, and K. Hornbæk. Is my Phone Alive? A Large-Scale Study of Shape Change in Handheld Devices Using Videos. CHI. 2014. ACM. \ 71. Peirce, C.S. and J. Buchler, Philosophical Writings of Peirce. 2012: Dover Publications. \ 72. Phillips, M.L., L. Williams, C. Senior, E.T. Bullmore, M.J. Brammer, C. Andrew, S.C.R. Williams, and A.S. David, A differential neural response to threatening and non-threatening negative facial expressions in paranoid and non-paranoid schizophrenics. Psychiatry Research: Neuroimaging, 1999. 92(1): p. 11-31. \ 73. Pine, D.S., J. Grun, E.A. Maguire, N. Burgess, E. Zarahn, V. Koda, A. Fyer, P.R. Szeszko, and R.M. Bilder, Neurodevelopmental aspects of spatial navigation: a virtual reality fMRI study. Neuroimage, 2002. 15(2): p. 396-406. \ 74. Pochon, J.B., R. Levy, P. Fossati, S. Lehericy, J.B. Poline, B. Pillon, D. Le Bihan, and B. Dubois, The neural system that bridges reward and cognition in humans: An fMRI study. PNAS, 2002. 99(8): p. 56695674. \ 75. Poldrack, R.A., Can cognitive processes be inferred from neuroimaging data? Trends Cogn Sci, 2006. 10(2): p. 59-63. \ 76. Pólya, G., Mathematics and Plausible Reasoning: Induction and analogy in mathematics. 1990: Princeton University Press. \ 77. Purves, D., Neuroscience. 2012: Sinauer Associates. \ 78. Rizzolatti, G. and L. Craighero, The mirror-neuron system. Annu Rev Neurosci, 2004. 27: p. 169-92. \ 79. Sauro, J. and J.R. Lewis, Correlations among prototypical usability metrics: evidence for the construct of usability, in CHI. 2009, ACM: Boston, MA, USA. p. 1609-1618. \ 80. Sjölie, D., K. Bodin, E. Elgh, J. Eriksson, L.-E. Janlert, and L. Nyberg, Effects of interactivity and 3D-motion on mental rotation brain activity in an immersive virtual environment, in CHI. 2010, ACM: Atlanta, Georgia, USA. p. 869-878. \ 81. Tractinsky, N., A. Cokhavi, M. Kirschenbaum, and T. Sharfi, Evaluating the consistency of immediate aesthetic perceptions of web pages. IJHCS, 2006. 64(11): p. 1071-1083. \ 82. Tuch, A.N., S.P. Roth, K. Hornbæk, K. Opwis, and J.A. Bargas-Avila, Is beautiful really usable? Toward understanding the relation between usability, aesthetics, and affect in HCI. Computers in Human Behavior, 2012. 28(5): p. 1596-1607. \ 83. Vartanian, O. and V. Goel, Neuroanatomical correlates of aesthetic preference for paintings. Neuroreport, 2004. 15(5): p. 893-7. \ 84. Vartanian, O., G. Navarrete, A. Chatterjee, L.B. Fich, H. Leder, C. Modroño, M. Nadal, N. Rostrup, and M. Skov, Impact of contour on aesthetic judgments and approach-avoidance decisions in architecture. PNAS, 2013. 110(Supplement 2): p. 10446-10453. \ 85. Vi, C., K. Takashima, H. Yokoyama, G. Liu, Y. Itoh, S. Subramanian, and Y. Kitamura, D-FLIP: Dynamic and Flexible Interactive PhotoShow, in Advances in Computer Entertainment, D. Reidsma, H. Katayose, and A. Nijholt, Editors. 2013, Springer. p. 415-427. \ 86. Yamamoto, S., I.E. Monosov, M. Yasuda, and O. Hikosaka, What and where information in the caudate tail guides saccades to visual objects. J Neurosci, 2012. 32(32): p. 11005-16. \ ","User Experience, Aesthetics, Usability, fMRI",H.5.2 [Information interfaces and presentation]: User Interfaces - Graphical user interfaces,uistf4499-file1.docx,,uistf4499-file3.mp4,uistf4499-file4.zip,,,Uses fMRI to study neural activity when looking at web pages. Identifies distinct brain areas related to usability. ,"List of changes \ 1.	Insert authors’ names, affiliations, and emails \ 2.	Correct overly strong statements about the originality of the work, by stating that “no prior work has used fMRI to investigate the neural basis of usability” on p1 (right, para 3). \ 3.	Include references of previous investigations of usability in HCI using functional near infrared spectroscopy (fNIRS) on p2 (left, last paragraph). \ 4.	Highlight that the fMRI has advantage in higher spatial resolution and accurate activation localization (compared to EEG, fNIRS), on p2, section “Functional magnetic resonance imaging (fMRI)”, first paragraph, last sentence. \ 5.	Tone down the claims of significances by presenting the results and stating the differences and activations only: \ a.	p5, right column, section “Behaviour results”, first paragraph \ b.	p6, left column, section “Brain correlates of Perceived Usability” \ c.	p8, right column, section “Behavioural” \ 6.	Add clarifications about brain regions, on p2, section  “Functional magnetic resonance imaging (fMRI)” with the content: \ “The human cerebral cortex is mapped by divisions into different functional areas known as Brodmann’s areas. A Brodmann Area (BA) is a region of the cerebral cortex in the human brain, defined by its histological structure and organization of cells. Each BA can correlate with more than one cortical function and vice versa. For example, BA 1/ 2/ 3 are the primary somatosensory cortex which contains tactile representation of our body ([52]). In contrast, BA 10 occupies parts of superior frontal gyrus and the middle frontal gyrus. Therefore, reporting neural activations using fMRI should include information of both systems (cortical function area and BA number).” \ 7.	Make suggestion of future research which treat data like correlational (e.g., correlating ratings and brain activity) (p10, right, paragraph 3). \ 8.	Elaborate on the limitation of fMRI by discussing what other HCI researchers who might be interested in using fMRI for their research should know about the technique (p10, right, paragraph 3). \ 9.	Add explanation of the stimuli exposure time (5 seconds) on p5, left, first paragraph. \ 10.	Add participants’ background on p8, right, paragraph 2 as: “They were all right-handed (by self-report) and did not participant in the previous experiment.” \ 11.	Add “BA40” to “inferior parietal lobule” (p7, section “Discussion of Experiment 1”, paragraph 1). \ 12.	Fix spacing in References. \ 13.	Remove the sentence stating NASA-TLX is an example of subjective measure of usability (p2, section “Perceived Usability”, 3rd paragraph). \ 14.	Move (www.crowdflower.com) to References. \ ",Chi Thanh Vi,Sriram Subramanian,FormatComplete,,,,,,,Jul 17 9:35,
uistf4623,10/24,11,Alt.Modalities,2:00:00 PM,3:30:00 PM,4+1,3:20:00 PM,3:30:00 PM,short,short,uistf4499,5,1105,,,uistf4623,A,AutoDub: Automatic Redubbing for Voiceover Editing,Shrikant,Venkataramani,svnktrm2@illinois.edu,uistf4623-paper.pdf,6,letter,Helvetica Times-Roman Times-Italic Times-Roman Times-Roman Helvetica Times-Roman Helvetica,,"Shrikant Venkataramani, Paris Smaragdis, Gautham Mysore","svnktrm2@illinois.edu, paris@illinois.edu, gmysore@adobe.com",64207,Shrikant,,Venkataramani,svnktrm2@illinois.edu,Electrical and Computer Engineering,University of Illinois at Urbana Champaign,Champaign,Illinois,United States,,,,,,71743,Paris,,Smaragdis,paris@illinois.edu,Computer science,University of Illinois at Urbana Champaign,Champaign,Illinois,United States,Adobe Research,Adobe,"San Francisco,",California,United States,24668,Gautham,,Mysore,gmysore@adobe.com,,Adobe Research,San Francisco,California,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Redubbing is an extensively used technique to correct errors in voiceover recordings. It involves re-recording a part of a voiceover, identifying the corresponding section of audio in the original recording that needs to be replaced, and using low level audio tools to replace the audio. Although this sequence of steps can be performed using traditional audio editing tools, the process can be tedious when dealing with long voiceover recordings and prohibitively difficult for users not familiar with such tools. To address this issue, we present AutoDub, a novel system for redubbing voiceover recordings. Using our system, a user simply needs to re-record the part of the voiceover that needs to be replaced. Our system automatically locates the corresponding part in the original recording and performs the low level audio processing to replace it. The system can be easily incorporated in any existing sophisticated audio editor or can be employed as a functionality in an audio-guided user interface. User studies involving participation from novice, knowledgeable and expert users indicate that our tool is preferred to a traditional audio editor based redubbing approach by all categories of users due to its faster and easier redubbing capabilities.",svnktrm2@illinois.edu,"1. Francois G. Germain, Gautham J. Mysore, and Takako Fujioka. 2016. Equalization matching of speech recordings in real-world environments. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 609–613. \ 2. Meinard Muller. 2007. Dynamic Time Warping. Springer Berlin Heidelberg, 69–84. \ 3. Gautham J Mysore. 2015. Can we automatically transform speech recorded on common consumer devices in real-world environments into professional production quality speech?–a dataset, insights, and challenges. IEEE Signal Processing Letters 22, 8 (2015), 1006–1010. \ 4. Lawrence R. Rabiner and Biing-Hwang Juang. 1993. Fundamentals of Speech Recognition. Prentice-Hall, Inc. \ 5. Lawrence R. Rabiner and Ronald W. Schafer. 1978. Digital processing of speech signals. N.J. Prentice-Hall. \ 6. Steve Rubin, Floraine Berthouzoz, Gautham J. Mysore, and Maneesh Agrawala. 2015. Capture-Time Feedback for Recording Scripted Narration.. In UIST, Celine Latulipe, Bjoern Hartmann, and Tovi Grossman (Eds.). ACM, 191–199. \ 7. Steve Rubin, Floraine Berthouzoz, Gautham J. Mysore, Wilmot Li, and Maneesh Agrawala. 2013. Content-based tools for editing audio stories.. In UIST, Shahram Izadi, Aaron J. Quigley, Ivan Poupyrev, and Takeo Igarashi (Eds.). ACM, 113–122. \ 8. Francis Rumsey. 2008. Digital Audio Recording Formats and Editing Principles. Springer New York, 703–729. \ 9. Pascal Scalart and Jose V. Filho. 1996. Speech enhancement based on a priori signal to noise estimation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 629–632. \ 10. K. Vertanen and P. O. Kristensson. 2009. Automatic selection of recognition errors by respeaking the intended text. In 2009 IEEE Workshop on Automatic Speech Recognition Understanding. 130–135. \ 11. K. Vertanen and P. O. Kristensson. 2010. Getting it right the second time: Recognition of spoken corrections. In 2010 IEEE Spoken Language Technology Workshop. 289–294. \ ",Redubbing; Voiceover; Overdub; Dynamic time warping,H.5.5; I.5.2; I.5.4,uistf4623-file1.zip,,uistf4623-file3.mp4,,,,"Editing a narration track for videos can be frustrating. We demonstrate an alternative workflow where a user will only need to record small snippets with the correct content, and it will automatically replace the errors.",- Re-writing evaluation and user study section to clearly articulate the conditions and limitations of the user study and evaluations. \ - Included references suggested by 1AC related to error detection using spoken text and context. \ - Clarified GUI caption \ - Brief comment about dealing with multiple matches in DTW,Shrikant Venkataramani,Paris Smaragdis,FormatComplete,1451380,National Science Foundation,,,,,Aug 7 23:31,
uistf1910,10/24,12,Phones & Watches,2:00:00 PM,3:30:00 PM,4+1,2:00:00 PM,2:20:00 PM,long,long,none,1,1201,,,uistf1910,A,RetroShape: Leveraging Rear-Surface Shape Displays for 2.5D Interaction on Smartwatches,Xing-Dong,Yang,xing-dong.yang@dartmouth.edu,uistf1910-paper.pdf,13,letter,,,"Da-Yuan Huang, Ruizhen Guo, Jun Gong, Jingxian Wang, John M Graham, De-Nian Yang, Xing-Dong Yang","dayuan.huang.tw@gmail.com, ruizhen.guo.gr@dartmouth.edu, Jun.Gong.GR@dartmouth.edu, jingxian.wang@acm.org, Jack.m.graham.iii.gr@dartmouth.edu, dnyang@iis.sinica.edu.tw, xing-dong.yang@dartmouth.edu",27289,Da-Yuan,,Huang,dayuan.huang.tw@gmail.com,,National Taiwan University of Science and Technology,Taipei,,Taiwan,,,,,,46386,Ruizhen,,Guo,ruizhen.guo.gr@dartmouth.edu,Computer Science,Dartmouth College,Hanover,NH,United States,,,,,,60341,Jun,,Gong,Jun.Gong.GR@dartmouth.edu,Compute Science,Dartmouth College,Hanover,New Hampshire,United States,,,,,,71282,Jingxian,,Wang,jingxian.wang@acm.org,Electrical & Computer Engineering,Carnegie Mellon University,Pittsburgh,Pennsylvania,United States,,,,,,71749,John,M,Graham,Jack.m.graham.iii.gr@dartmouth.edu,Computer Science,Dartmouth College,Hanover,New Hampshire,United States,,,,,,17016,De-Nian,,Yang,dnyang@iis.sinica.edu.tw,,Academia Sinica,Taipei,,Taiwan,,,,,,10966,Xing-Dong,,Yang,xing-dong.yang@dartmouth.edu,Department of Computer Science,Dartmouth College,Hanover,New Hampshire,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The small screen size of a smartwatch limits user experience when watching or interacting with media. We propose a supplementary tactile feedback system to enhance the user experience with a method unique to the smartwatch form factor. Our system has a deformable surface on the back of the watch face, allowing the visual scene on screen to extend into 2.5D physical space. This allows the user to watch and feel virtual objects, such as experiencing a ball bouncing against the wrist. We devised two controlled experiments to analyze the influence of tactile display resolution on the illusion of virtual object presence. Our first study revealed that on average, a taxel can render virtual objects between 70% and 138% of its own size without shattering the illusion. From the second study, we found visual and haptic feedback can be separated by 4.5mm to 16.2mm for the tested taxels. Based on the results, we developed a prototype (called RetroShape) with 4×4 10mm taxels using micro servo motors, and demonstrated its unique capability through a set of tactile-enhanced games and videos. A preliminary user evaluation showed that participants welcome RetroShape as a useful addition to existing smartwatch output. ",dayuan.huang@csie.ntust.edu.tw,"1. Rochelle Ackerley, Ida Carlsson, Henric Wester, Håkan Olausson and Helena Backlund Wasling. 2014. Touch perceptions across skin sites: differences between sensitivity, direction discrimination and pleasantness. Frontiers in Behavioral Neuroscience, 8. 54. DOI=http://journal.frontiersin.org/article/10.3389/fnbeh. 2014.00054 \ 2. Jessalyn Alvina, Shengdong Zhao, Simon T. Perrault, Maryam Azh, Thijs Roumen and Morten Fjeld. 2015. OmniVib: Towards Cross-body Spatiotemporal Vibrotactile Notifications for Mobile Phones. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15), 24872496. DOI= http://doi.acm.org/10.1145/2702123.2702341 \ 3. Olivier Bau, Ivan Poupyrev, Ali Israr and Chris Harrison. 2010. TeslaTouch: electrovibration for touch surfaces. In Proceedings of the 23nd annual ACM symposium on User interface software and technology (UIST '10), 283-292. DOI= http://doi.acm.org/10.1145/1866029.1866074 \ 4. Mohamed Benali-Khoudja, Moustapha Hafez, JeanMarc Alexandre and Abderrahmane Kheddar. 2004. Tactile interfaces: a state-of-the-art survey. In Int Symposium on Robotics. 721-726. \ 5. Hrvoje Benko, Christian Holz, Mike Sinclair and Eyal Ofek. 2016. NormalTouch and TextureTouch: Highfidelity 3D Haptic Shape Rendering on Handheld Virtual Reality Controllers. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16), 717-728. http://doi.acm.org/10.1145/2984511.2984526 \ 6. Ian Summers Craig, Craig M Chanter, Anna L Southall and Alan C Brady. 2001. Results from a Tactile Array on the Fingertip. In Proceedings of Eurohaptics, 2001 ( Eurohaptics '01). 26-28. \ 7. Stephen Brewster and Lorna M. Brown. 2004. Tactons: structured tactile messages for non-visual information display. In Proceedings of the fifth conference on Australasian User Snterface (AUIC '04), 15-23. \ 8. Tom Carter, Sue Ann Seah, Benjamin Long, Bruce Drinkwater and Sriram Subramanian. 2013. UltraHaptics: multi-point mid-air haptic feedback for touch surfaces. In Proceedings of the 26th annual ACM symposium on User interface software and technology (UIST '13), 505-514. DOI= http://doi.acm.org/10.1145/2501988.2502018 \ 9. Sean Follmer, Daniel Leithinger, Alex Olwal, Akimitsu Hogge and Hiroshi Ishii. 2013. inFORM: dynamic physical affordances and constraints through shape and object actuation. In Proceedings of the 26th annual ACM symposium on User interface software and technology (UIST '13), 417-426. DOI= http://doi.acm.org/10.1145/2501988.2502032 \ 10. Alexandra Ion, Edward Jay Wang and Patrick Baudisch. 2015. Skin Drag Displays: Dragging a Physical Tactor across the User's Skin Produces a Stronger Tactile Stimulus than Vibrotactile. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15), 2501-2504. DOI= http://doi.acm.org/10.1145/2702123.2702459 \ 11. Ali Israr and Ivan Poupyrev. 2011. Tactile brush: drawing on skin with a tactile grid display. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '11), 2019-2028. DOI= http://doi.acm.org/10.1145/1978942.1979235 \ 12. Hiroo Iwata, Hiroaki Yano, Fumitaka Nakaizumi and Ryo Kawamura. 2001. Project FEELEX: adding haptic surface to graphics. In Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH '01), 469-476. DOI= http://doi.acm.org/10.1145/383259.383314 \ 13. Sungjune Jang, Lawrence H. Kim, Kesler Tanner, Hiroshi Ishii and Sean Follmer. 2016. Haptic Edge Display for Mobile Tactile Interaction. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16), 3706-3716. DOI= http://doi.acm.org/10.1145/2858036.2858264 \ 14. Hwan Kim, Minhwan Kim and Woohun Lee. 2016. HapThimble: A Wearable Haptic Device towards Usable Virtual Touch Screen. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16), 3694-3705. DOI= http://doi.acm.org/10.1145/2858036.2858196 \ 15. J.F. Kramer. 1993. Force feedback and textures simulating interface device, U.S. Patents, No 5,184,319. \ 16. A. Kron and G. Schmidt. 2003. Multi-fingered tactile feedback from virtual and remote environments. In Proceedings of 11th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems, 2003 (HAPTICS '03), 16-23. DOI= http://dx.doi.org/10.1109/HAPTIC.2003.1191219 \ 17. Shinobu Kuroki, Hiroyuki Kajimoto, Hideaki Nii, Naoki Kawakami and Susumu Tachi. 2007. Proposal for tactile sense presentation that combines electrical and mechanical stimulus. In Second Joint EuroHaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems (WHC'07), 121-126. DOI= http://dx.doi.org/10.1109/WHC.2007.92 \ 18. Martin Kuschel, Franziska Freyberger, Berthold Färber and Martin Buss. 2008. Visual–haptic perception of compliant objects in artificially generated environments. The Visual Computer, 24 (10). 923-931. DOI= http://dx.doi.org/10.1007/s00371-008-0289-x \ 19. Jaeyeon Lee, Jaehyun Han and Geehyuk Lee. 2015. Investigating the Information Transfer Efficiency of a 3x3 Watch-back Tactile Display. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15), 1229-1232. DOI= http://doi.acm.org/10.1145/2702123.2702530 \ 20. Seungyon ""Claire"" Lee and Thad Starner. 2010. BuzzWear: alert perception in wearable tactile displays on the wrist. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '10), 433-442. DOI= http://doi.acm.org/10.1145/1753326.1753392 \ 21. Seungyon Claire Lee and Thad Starner. 2009. Mobile gesture interaction using wearable tactile displays. In CHI '09 Extended Abstracts on Human Factors in Computing Systems (CHI EA '09), 3437-3442. DOI= http://doi.acm.org/10.1145/1520340.1520499 \ 22. Sang-won Leigh and Pattie Maes. 2016. Body Integrated Programmable Joints Interface. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16), 6053-6057. DOI= http://doi.acm.org/10.1145/2858036.2858538 \ 23. Daniel Leithinger, Sean Follmer, Alex Olwal and Hiroshi Ishii. 2014. Physical telepresence: shape capture and display for embodied, computer-mediated remote collaboration. In Proceedings of the 27th annual ACM symposium on User interface software and technology (UIST '14), 461-470. DOI= http://doi.acm.org/10.1145/2642918.2647377 \ 24. H. Levitt. 1971. Transformed Up-Down Methods in Psychoacoustics. The Journal of the Acoustical Society of America, 49 (2B). DOI= http://dx.doi.org/10.1121/1.1912375 \ 25. Yi-Chi Liao, Yi-Ling Chen, Jo-Yu Lo, Rong-Hao Liang, Liwei Chan and Bing-Yu Chen. 2016. EdgeVib: Effective Alphanumeric Character Output Using a Wrist-Worn Tactile Display. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16), 595-601. DOI= http://doi.acm.org/10.1145/2984511.2984522 \ 26. Yi-Chi Liao, Shun-Yao Yang, Rong-Hao Liang, Liwei Chan and Bing-Yu Chen. 2015. ThirdHand: wearing a robotic arm to experience rich force feedback. In SIGGRAPH Asia 2015 Emerging Technologies (SA '15), 1-1. DOI= http://doi.acm.org/10.1145/2818466.2818487 \ 27. Jeff Lieberman and Cynthia Breazeal. 2007. TIKL: Development of a Wearable Vibrotactile Feedback Suit for Improved Human Motor Learning. IEEE Transactions on Robotics, 23 (5). 919-926. DOI= http://dx.doi.org/10.1109/TRO.2007.907481 \ 28. Joseph Luk, Jerome Pasquero, Shannon Little, Karon MacLean, Vincent Levesque and Vincent Hayward. 2006. A role for haptics in mobile interaction: initial design using a handheld tactile display prototype. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '06), 171-180. DOI= http://doi.acm.org/10.1145/1124772.1124800 \ 29. Michael Matscheko, Alois Ferscha, Andreas Riener and Manuel Lehner. 2010. Tactor placement in wrist worn wearables. In International Symposium on Wearable Computers (ISWC '10), 1-8. DOI= http://dx.doi.org/10.1109/ISWC.2010.5665867 \ 30. Troy McDaniel, Morris Goldberg, Daniel Villanueva, Lakshmie Narayan Viswanathan and Sethuraman Panchanathan. 2011. Motor learning using a kinematicvibrotactile mapping targeting fundamental movements. In Proceedings of the 19th ACM International Conference on Multimedia (MM '11), 543-552. DOI= http://doi.acm.org/10.1145/2072298.2072369 \ 31. mDrawBot. 2016. http://www.makeblock.com/mdrawbot-kit/. \ 32. Ken Nakagaki, Sean Follmer and Hiroshi Ishii. 2015. LineFORM: Actuated Curve Interfaces for Display, Interaction, and Constraint. In Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology (UIST '15), 333-339. DOI= http://doi.acm.org/10.1145/2807442.2807452 \ 33. Ken Nakagaki, Luke Vink, Jared Counts, Daniel Windham, Daniel Leithinger, Sean Follmer and Hiroshi Ishii. 2016. Materiable: Rendering Dynamic Material Properties in Response to Direct Physical Touch with Shape Changing Interfaces. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16), 2764-2772. DOI= http://doi.acm.org/10.1145/2858036.2858104 \ 34. Akio Nakamura, Sou Tabata, Tomoya Ueda, Shinichiro Kiyofuji and Yoshinori Kuno. 2005. Multimodal presentation method for a dance training system. In CHI '05 Extended Abstracts on Human Factors in Computing Systems (CHI EA '05), 1685-1688. DOI= http://doi.acm.org/10.1145/1056808.1056997 \ 35. Jerome Pasquero, Scott J. Stobbe and Noel Stonehouse. 2011. A haptic wristwatch for eyes-free interactions. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '11), 3257-3266. DOI= http://doi.acm.org/10.1145/1978942.1979425 \ 36. Ivan Poupyrev, Tatsushi Nashida and Makoto Okabe. 2007. Actuation and tangible user interfaces: the Vaucanson duck, robots, and shape displays. In Proceedings of the 1st international conference on Tangible and embedded interaction (TEI '07), 205-212. DOI= http://doi.acm.org/10.1145/1226969.1227012 \ 37. Wang Qi and Vincent Hayward. 2009. Biomechanically Optimized Distributed Tactile Transducer Based on Lateral Skin Deformation. The International Journal of Robotics Research, 29 (4). 323335. DOI= http://dx.doi.org/10.1177/0278364909345289 \ 38. Vincent Hayward, Oliver R. Astley, Manuel CruzHernandez, Manuel Cruz-Hernandez, Danny Grant and Gabriel Robles-de-la-Torre. 2004. Haptic interfaces and devices. Sensor Review. 24(1), 16-29. DOI=https://doi.org/10.1108/02602280410515770 \ 39. Christian Schonauer, Kenichiro Fukushi, Alex Olwal, Hannes Kaufmann and Ramesh Raskar. 2012. Multimodal motion guidance: techniques for adaptive and dynamic feedback. In Proceedings of the 14th ACM international conference on Multimodal interaction (ICMI '12), 133-140. DOI= http://doi.acm.org/10.1145/2388676.2388706 \ 40. Seungwoo Je, Brendan Rooney, Liwei Chan and Andrea Bianchi. 2017. tactoRing: A Skin-Drag Discrete Display. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17), 31063114. DOI= http://doi.acm.org/10.1145/3025453.3025703 \ 41. Rajinder Sodhi, Matthew Glisson and Ivan Poupyrev. 2013. AIREAL: tactile gaming experiences in free air. In ACM SIGGRAPH 2013 Emerging Technologies (SIGGRAPH '13), 1-1. DOI= http://doi.acm.org/10.1145/2503368.2503370 \ 42. Daniel Spelmezan, Mareike Jacobs, Anke Hilgers and Jan Borchers. 2009. Tactile motion instructions for physical activities. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '09), 2243-2252. http://doi.acm.org/10.1145/1518701.1519044 \ 43. Ramiro Velazquez, Edwige E. Pissaloux, Moustapha Hafez, and JÉrÔme Szewczyk. 2008. Tactile Rendering With Shape-Memory-Alloy Pin-Matrix. IEEE Transactions on Instrumentation and Measurement, 57 (5). 1051-1057. DOI= http://dx.doi.org/10.1109/TIM.2007.913768 \ 44. Ramiro Velázquez. 2010. Wearable Assistive Devices for the Blind. Wearable and Autonomous Biomedical Devices and Systems for Smart Environment: Issues and Characterization, 331-349. DOI= http://dx.doi.org/10.1007/978-3-642-15687-8 \ 45. SAMSUNG GEAR VR. www.samsung.com/us/explore/gear-vr. \ 46. Lining Yao, Ryuma Niiyama, Jifei Ou, Sean Follmer, Clark Della Silva and Hiroshi Ishii. 2013. PneUI: pneumatically actuated soft composite materials for shape changing interfaces. In Proceedings of the 26th annual ACM symposium on User interface software and technology (UIST '13), 13-22. DOI= http://doi.acm.org/10.1145/2501988.2502037 \ 47. Vibol Yem, Ryuta Okazaki and Hiroyuki Kajimoto. 2016. FinGAR: combination of electrical and mechanical stimulation for high-fidelity tactile presentation. In ACM SIGGRAPH 2016 Emerging Technologies (SIGGRAPH '16), ACM, Anaheim, California, 1-2. DOI= http://doi.acm.org/10.1145/2929464.2929474 \ 48. M. J. Zigler and Kathryn M. Northup. 1926. The Tactual Perception of Form. The American Journal of Psychology, 37 (3). 391-397. DOI=http://dx.doi.org/10.2307/1413625 \ ",Mobile haptics; Shape-changing display; Taxel; Smartwatch,H.5.2.,uistf1910-file1.docx,uistf1910-file2.jpg,uistf1910-file3.mp4,,"Through RetroShape, a user can feel a visual scene haptically from a deformable display on the rear surface of a smartwatch.  \  ",,"Presents RetroShpae, a smartwatch that can extend visual scene to the 2.5D physical space on its rear surface using a deformable display. ","Based on the valuable suggestions from all reviewers, we have revised our paper as follows: \  \ STUDY1 \ a.    [Title] We modify the title of STUDY1 to “ACCEPTANCE RANGE OF SIZE.” \ b.    [Subsection “Design”, 2nd paragraph] As suggested by all reviewers, we explain why we modified the standard JND design.  \ c.    [Subsection “Design”, 1st paragraph] As suggested by R1, we fix the terminology by replacing “testing stimuli” to “probes” in the 11th line.  \ d.    [Subsection “Participants”] As R3 suggested, we add the handedness of participants. \ e.    Based our rebuttal, all “discrimination thresholds” are replaced by “acceptance threshold” \  \ STUDY2 \ a.    [Subsection “Participants”] As R3 suggested, we add the handedness of participants. \ b.    [Subsection “Procedure and Experimental Design”, 1st paragraph, 3rd to 8th line] As suggested by R1, we fix the terminology by replacing “testing stimuli” to “probes.  \  \ PROTOTYPE DESIGN, AUTHORING, AND TESTING \ a.	[Subsection “RetroShape Prototype Design”, 2nd paragraph, 3th to 5th line] As suggested by R2 and R3, we explain why we adopt square taxels instead of circular taxels.  \ b.	[Subsection “RetroShape Prototype Design”, 2nd paragraph, 15th  to 19th line] We append more details of the hardware configuration of RetroShape prototype.  \ c.	[Subsection “RetroShape User Evaluation Study”, subsection “Experimental Design and Procedure”] As suggested by R2, R3, and 2AC, we add the several details of the study design, including (1) the types and specs of vibration motors (paragraph 1, line 12 to 21), (2) the counter-balance design between technique conditions (paragraph 2), and (3) the details about the ratings (paragraph 2). \  \ LIMITATION AND FUTURE WORK \ a.  [2nd paragraph] As suggested by R2 and R3, we explain how we connect our study results to RetroShape prototypes (line 1 to 8). In addition, we claim the comparison results between RetroShape and vibration motors are for the preliminary exploration (line 9 to 12).  \  \ OTHERS     \ a.	We fix several typos and citation formats in our revision.  \ b.	Figure positions are rearranged. ",Da-Yuan Huang,Jun Gong,FormatComplete,,,,,,,Aug 6 10:09,
uistf2235,10/24,12,Phones & Watches,2:00:00 PM,3:30:00 PM,4+1,2:20:00 PM,2:40:00 PM,long,long,uistf1910,2,1202,,,uistf2235,A,Pyro: Thumb-Tip Gesture Recognition Using Pyroelectric Infrared Sensing,Jun,Gong,Jun.Gong.GR@dartmouth.edu,uistf2235-paper.pdf,11,letter,,,"Jun Gong, Yang Zhang, Xia Zhou, Xing-Dong Yang","Jun.Gong.GR@dartmouth.edu, yang.zhang@cs.cmu.edu, xia@cs.dartmouth.edu, xing-dong.yang@dartmouth.edu",60341,Jun,,Gong,Jun.Gong.GR@dartmouth.edu,Compute Science,Dartmouth College,Hanover,New Hampshire,United States,,,,,,51674,Yang,,Zhang,yang.zhang@cs.cmu.edu,Human-Computer Interaction Institute,Carnegie Mellon University,Pittsburgh,Pennsylvania,United States,,,,,,60060,Xia,,Zhou,xia@cs.dartmouth.edu,Computer Science,Dartmouth College,Hanover,NH,United States,,,,,,10966,Xing-Dong,,Yang,xing-dong.yang@dartmouth.edu,Department of Computer Science,Dartmouth College,Hanover,New Hampshire,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present Pyro, a micro thumb-tip gesture recognition technique based on thermal infrared signals radiating from the fingers. Pyro uses a compact, low-power passive sensor, making it suitable for wearable and mobile applications. To demonstrate the feasibility of Pyro, we developed a self-contained prototype consisting of the infrared pyroelectric sensor, a custom sensing circuit, and software for signal processing and machine learning. A ten-participant user study yielded a 93.9% cross-validation accuracy and 84.9% leave-one-session-out accuracy on six thumb-tip gestures. Subsequent lab studies demonstrated Pyro’s robustness to varying light conditions, hand temperatures, and background motion. We conclude by discussing the insights we gained from this work and future research questions.",Jun.Gong.GR@dartmouth.edu,"1. AFB. Facts and Figures on American Adults with Vision Loss. 2017. Retrieved July 15, 2017 from http://www.afb.org/info/blindnessstatistics/adults/facts-and-figures/235 \ 2. Aria Wearable. 2017. Retrieved July 15, 2017 from http://www.flicktek.com/ \ 3. Freescale Microcontroller Datasheet. 2017. Retrieved July 4, 2017 from http://cache.freescale.com/files/32bit/doc/data_sheet/K 20P64M72SF1.pdf \ 4. JAWS. Screen reader from Freedom Scientific. 2017. Retrieved July 5, 2017 from http://www.freedomscientific.com/Products/Blindness/ JAWS \ 5. PJRC Electronic Projects - Teensy 3.2 & 3.1 s. 2017. Retrieved July 5, 2017 from https://www.pjrc.com/teensy/teensy31.html \ 6. Texas Instruments. 2017. Retrieved July 4, 2017 from http://www.ti.com/lit/ds/snosc16d/snosc16d.pdf \ 7. tsfresh Toolkit. 2017. Retrieved July 4, 2017 from https://tsfresh.readthedocs.io/en/latest/text/introduction. html \ 8. VoiceOver, Screen reader from Apple. 2017. Retrieved July 4, 2017 from https://www.apple.com/accessibility/mac/vision/ \ 9. Window-Eyes, Screen Reader GW Micro. WindowEyes, Screen Reader GW Micro. 2017 \ 10. Ali Abdolrahmani, Ravi Kuber and Amy Hurst. 2016. An empirical investigation of the situationally-induced impairments experienced by blind mobile device users. In Proceedings of the 13th Web for All Conference (W4A '16), Article 21, 8 pages. DOI=https://doi.org/10.1145/2899475.2899482 \ 11. Troy Allman, Rupinder K Dhillon, Molly AE Landau and Sri H Kurniawan. 2009. Rock Vibe: Rock Band® computer games for people with no or limited vision. In Proceedings of the 11th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '09), 51-58. \ 12. Brian Amento, Will Hill and Loren Terveen. 2002. The sound of one hand: a wrist-mounted bio-acoustic fingertip gesture interface. In CHI '02 Extended Abstracts on Human Factors in Computing Systems (CHI EA '02), 724-725. DOI=http://dx.doi.org/10.1145/506443.506566 \ 13. Chieko Asakawa, Hironobu Takagi, Shuichi Ino and Tohru Ifukube. 2002. Auditory and tactile interfaces for representing the visual effects on the web. In Proceedings of the Fifth International ACM SIGACCESS Conference on Assistive Technologies (ASSETS '02), 65-72. DOI=http://dx.doi.org/10.1145/638249.638263 \ 14. Shiri Azenkot and Emily Fortuna. 2010. Improving public transit usability for blind and deaf-blind people by connecting a braille display to a smartphone. In Proceedings of the 12th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '10), 317-318. DOI=http://dx.doi.org/10.1145/1878803.1878890 \ 15. Shiri Azenkot, Richard E Ladner and Jacob O Wobbrock. 2011. Smartphone haptic feedback for nonvisual wayfinding. In proceedings of the 13th international ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '11), 281-282. DOI=http://dx.doi.org/10.1145/2049536.2049607 \ 16. Shiri Azenkot and Nicole B Lee. 2013. Exploring the use of speech input by blind people on mobile devices. In Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '13), Article 11, 8 pages. DOI= http://dx.doi.org/10.1145/2513383.2513440 \ 17. Stephen Brewster and Lorna M Brown. 2004. Tactons: structured tactile messages for non-visual information display. In Proceedings of the Fifth Conference on Australasian User Interface-Volume 28 (AUIC '04), 15-23. \ 18. Liwei Chan, Yi-Ling Chen, Chi-Hao Hsieh, Rong-Hao Liang and Bing-Yu Chen. 2015. CyclopsRing: Enabling Whole-Hand and Context-Aware Interactions Through a Fisheye Ring. In Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology (UIST '15), 549-556. DOI=https://doi.org/10.1145/2807442.2807450 \ 19. Ke-Yu Chen, Kent Lyons, Sean White and Shwetak Patel. 2013. uTrack: 3D input using two magnetic sensors. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST '13), 237-244. DOI=http://dx.doi.org/10.1145/2501988.2502035 \ 20. Xiang'Anthony' Chen, Julia Schwarz, Chris Harrison, Jennifer Mankoff and Scott E Hudson. 2014. Air+ touch: interweaving touch & in-air gestures. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST '14), 519-525. DOI=https://doi.org/10.1145/2642918.2647392 \ 21. Andrea Colaço, Ahmed Kirmani, Hye Soo Yang, NanWei Gong, Chris Schmandt and Vivek K Goyal. 2013. Mime: compact, low power 3D gesture sensing for interaction with head mounted displays. In Proceedings of the 26th Annual ACM Symposium on User interface Software and Technology (UIST '13), 227-236. DOI=http://dx.doi.org/10.1145/2501988.2502042 \ 22. Artem Dementyev and Joseph A. Paradiso. 2014. WristFlex: low-power gesture input with wrist-worn pressure sensors. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST '14), 161-166. DOI=https://doi.org/10.1145/2642918.2647396 \ 23. Yasmine N El-Glaly, Francis Quek, Tonya SmithJackson and Gurjot Dhillon. 2013. Touch-screens are not tangible: Fusing tangible interaction with touch glass in readers for the blind. In Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction (TEI '13), 245-253. DOI=http://dx.doi.org/10.1145/2460625.2460665 \ 24. Rui Fukui, Masahiko Watanabe, Tomoaki Gyota, Masamichi Shimosaka and Tomomasa Sato. 2011. Hand shape classification with a wrist contour sensor: development of a prototype device. In Proceedings of the 13th International Conference on Ubiquitous Computing (UbiComp '11), 311-314. DOI=https://doi.org/10.1145/2030112.2030154 \ 25. Jun Gong, Lan Li, Daniel Vogel and Xing-Dong Yang. 2017. Cito: An Actuated Smartwatch for Extended Interactions. In Proceedings of the 35th Annual ACM Conference on Human Factors in Computing Systems (CHI '17), 5331-5345. DOI=https://doi.org/10.1145/3025453.3025568 \ 26. Timo Götzelmann. 2016. LucentMaps: 3D Printed Audiovisual Tactile Maps for Blind and Visually Impaired People. In Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '16), 81-90. DOI=https://doi.org/10.1145/2982142.2982163 \ 27. Sidhant Gupta, Tim Campbell, Jeffrey R Hightower and Shwetak N Patel. 2010. SqueezeBlock: using virtual springs in mobile devices for eyes-free interaction. In Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Tchnology (UIST '10), 101-104. DOI=https://doi.org/10.1145/1866029.1866046 \ 28. Edward Twitchell Hall. 1966. The hidden dimension. \ 29. Chris Harrison, Desney S. Tan and Dan Morris. 2010. Skinput: appropriating the body as an input surface. In 28th Annual ACM Conference on Human Factors in Computing Systems (CHI '10), 453-462. DOI=https://doi.org/10.1145/1753326.1753394 \ 30. Victoria E Hribar, Laura G Deal and Dianne TV Pawluk. 2012. Displaying braille and graphics with a tactile mouse. In Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '11), 251-252. DOI=http://dx.doi.org/10.1145/2049536.2049584 \ 31. Rabia Jafri, Asmaa Mohammed Aljuhani and Syed Abid Ali. 2015. A tangible interface-based application for teaching tactual shape perception and spatial awareness sub-concepts to visually impaired children. Procedia Manufacturing, 3. 5562-5569. \ 32. Shaun K Kane, Meredith Ringel Morris and Jacob O Wobbrock. 2013. Touchplates: low-cost tactile overlays for visually impaired touch screen users. In Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '13), 22. DOI=http://dx.doi.org/10.1145/2513383.2513442 \ 33. David Kim, Otmar Hilliges, Shahram Izadi, Alex D. Butler, Jiawen Chen, Iason Oikonomidis and Patrick Olivier. 2012. Digits: freehand 3D interactions anywhere using a wrist-worn gloveless sensor. In Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology (UIST '12), 167-176. DOI=https://doi.org/10.1145/2380116.2380139 \ 34. Jaime Lien, Nicholas Gillian, M. Emre Karagozler, Patrick Amihood, Carsten Schwesig, Erik Olson, Hakim Raja and Ivan Poupyrev. 2016. Soli: Ubiquitous Gesture Sensing with Millimeter Wave Radar. In Proceedings of ACM SIGGRAPH (SIGGRAPH '16), 142. DOI=https://doi.org/10.1145/2897824.2925953 \ 35. Christian Loclair, Sean Gustafson and Patrick Baudisch. 2010. PinchWatch: a wearable device for one-handed microinteractions. In proceedings of the 12th International Conference on Human-Computer Interaction with Mobile Devices and Servies (MobileHCI '10). DOI=https://doi.org/10.1.1.453.6052 \ 36. Muhanad S Manshad, Enrico Pontelli and Shakir J Manshad. 2011. MICOO (multimodal interactive cubes for object orientation): a tangible user interface for the blind and visually impaired. In proceedings of the 13th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '11), 261-262. DOI=http://dx.doi.org/10.1145/2049536.2049597 \ 37. David McGookin, Euan Robertson and Stephen Brewster. 2010. Clutching at straws: using tangible interaction to provide non-visual access to graphs. In 28th Annual ACM Conference on Human Factors in Computing Systems (CHI '10), 1715-1724. DOI=https://doi.org/10.1145/1753326.1753583 \ 38. Tony Morelli, John Foley and Eelke Folmer. 2010. Vibowling: a tactile spatial exergame for individuals with visual impairments. In Proceedings of the 12th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '10), 179-186. DOI=http://dx.doi.org/10.1145/1878803.1878836 \ 39. Rajalakshmi Nandakumar, Vikram Iyer, Desney Tan and Shyamnath Gollakota. 2016. Fingerio: Using active sonar for fine-grained finger tracking. In 34th Annual ACM Conference on Human Factors in Computing Systems (CHI '16), 1515-1525. DOI=https://doi.org/10.1145/2858036.2858580 \ 40. Santiago Ortega-Avila, Bogdana Rakova, Sajid Sadi and Pranav Mistry. 2015. Non-invasive optical detection of hand gestures. In Proceedings of the 6th Augmented Human International Conference (AH '15), 179-180. DOI=http://dx.doi.org/10.1145/2735711.2735801 \ 41. Denise Prescher, Oliver Nadig and Gerhard Weber. 2010. Reading braille and tactile ink-print on a planar tactile display. Computers Helping People with Special Needs. 482-489. \ 42. Denise Prescher, Gerhard Weber and Martin Spindler. 2010. A tactile windowing system for blind users. In Proceedings of the 12th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '10), 91-98. DOI=http://dx.doi.org/10.1145/1878803.1878821 \ 43. Jun Rekimoto. 2001. GestureWrist and GesturePad: Unobtrusive Wearable Interaction Devices. In Proceedings of the 5th IEEE International Symposium on Wearable Computers, 21. DOI=10.1109/ISWC.2001.962092 \ 44. T. Scott Saponas, Desney S. Tan, Dan Morris, Ravin Balakrishnan, Jim Turner and James A. Landay. 2009. Enabling always-available input with muscle-computer interfaces. In Proceedings of the 22nd Annual ACM Symposium on User Interface Software and Technology (UIST '09), 167-176. DOI=https://doi.org/10.1145/1622176.1622208 \ 45. Toby Sharp, Cem Keskin, Duncan Robertson, Jonathan Taylor, Jamie Shotton, David Kim, Christoph Rhemann, Ido Leichter, Alon Vinnikov, Yichen Wei, Daniel Freedman, Pushmeet Kohli, Eyal Krupka, Andrew Fitzgibbon and Shahram Izadi. 2015. Accurate, Robust, and Flexible Real-time Hand Tracking. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15), 3633-3642. DOI=https://doi.org/10.1145/2702123.2702179 \ 46. Jie Song, Gabor Soros, Fabrizio Pece, Sean Ryan Fanello, Shahram Izadi, Cem Keskin and Otmar Hilliges. 2014. In-air gestures around unmodified mobile devices. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST '14), 319-329. DOI=https://doi.org/10.1145/2642918.2647373 \ 47. Andrii Soviak, Anatoliy Borodin, Vikas Ashok, Yevgen Borodin, Yury Puzis and IV Ramakrishnan. 2016. Tactile Accessibility: Does Anyone Need a Haptic Glove? In Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '16), 101-109. DOI=https://doi.org/10.1145/2982142.2982175 \ 48. Thad Starner. 2001. The challenges of wearable computing: Part 1. Ieee Micro, 21 (4). 44-52. \ 49. Burkay Sucu and Eelke Folmer. 2014. The blind driver challenge: steering using haptic cues. In Proceedings of the 16th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '14), 3-10. DOI=http://dx.doi.org/10.1145/2661334.2661357 \ 50. Li Sun, Souvik Sen, Dimitrios Koutsonikolas and KyuHan Kim. 2015. WiDraw: Enabling Hands-free Drawing in the Air on Commodity WiFi Devices. In Proceedings of the 21st Annual International Conference on Mobile Computing and Networking (MobiCom '15), 7-89. DOI=http://dx.doi.org/10.1145/2789168.2790129 \ 51. Shuai Tao, Mineichi Kudo, Hidetoshi Nonaka and Jun Toyama. 2011. Person authentication and activities analysis in an office environment using a sensor network. In International Joint Conference on Ambient Intelligence, Springer, 119-127. \ 52. Christiane Taras and Thomas Ertl. 2009. Interaction with Colored Graphical Representations on Braille Devices. Universal Access in Human-Computer Interaction. Addressing Diversity. 164-173. DOI= https://doi.org/10.1007/978-3-642-31479-7_19 \ 53. Marlon Twyman, Joe Mullenbach, Craig Shultz, J Edward Colgate and Anne Marie Piper. 2015. Designing wearable haptic information displays for people with vision impairments. In Proceedings of the Ninth International Conference on Tangible, Embedded, and Embodied Interaction (TEI '15). DOI=https://doi.org/10.1145/2677199.2680578 \ 54. Fernando Vidal-Verdú and Moustapha Hafez. 2007. Graphical tactile displays for visually-impaired people. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 15 (1). 119-130. DOI=10.1109/TNSRE.2007.891375 \ 55. Anita Vogl, Patrick Parzer, Teo Babic, Joanne Leong, Alex Olwal and Michael Haller. 2017. StretchEBand: Enabling Fabric-based Interactions through Rapid Fabrication of Textile Stretch Sensors. In Proceedings of the 35th Annual ACM Conference on Human Factors in Computing Systems (CHI '17), 2617-2627. DOI=https://doi.org/10.1145/3025453.3025938 \ 56. Thorsten Völkel, Gerhard Weber and Ulrich Baumann. 2008. Tactile graphics revised: the novel brailledis 9000 pin-matrix device with multitouch input. In Proceedings of the 11th International Conference on Computers Helping People with Special Needs (ICCHP '08). 835-842. DOI=http://dx.doi.org/10.1007/978-3-540-705406_124 \ 57. Robert Y. Wang, Sylvain Paris and Jovan Popovic. 2011. 6D hands: markerless hand-tracking for computer aided design. In Proceedings of the 24th annual ACM Symposium on User Interface Software and Technology (UIST '11), 549-558. DOI=https://doi.org/10.1145/2047196.2047269 \ 58. Wei Wang, Alex X. Liu and Ke Sun. 2016. Device-free gesture tracking using acoustic signals. In Proceedings of the 22nd Annual International Conference on Mobile Computing and Networking (MobiCom '16). DOI=https://doi.org/10.1145/2973750.2987385 \ 59. Jacob O. Wobbrock, Meredith Ringel Morris and Andrew D. Wilson. 2009. User-defined gestures for surface computing. In Proceedings of the 27th Annual ACM Conference on Human Factors in Computing Systems (CHI '09), 1083-1092. DOI=https://doi.org/10.1145/1518701.1518866 \ 60. Jacob Wobbrock, Andy Wilson and Yang Li. 2007. Gestures without libraries, toolkits or Training: a $1.00 Recognizer for User Interface Prototypes. In Proceedings of the 20th annual ACM Symposium on User Interface Software and Technology (UIST '07), 159-168. \ 61. Piotr Wojtczuk, Alistair Armitage, T David Binnie and Tim Chamberlain. 2012. Recognition of simple gestures using a PIR sensor array. Sensors and Transducers, 14 (1). 83. \ 62. Hanlu Ye, Meethu Malu, Uran Oh and Leah Findlater. 2014. Current and future mobile and wearable device use by people with visual impairments. In Proceedings of the 32th Annual ACM Conference on Human Factors in Computing Systems (CHI '14), 3123-3132. DOI=https://doi.org/10.1145/2556288.2557085 \ 63. Bei Yuan and Eelke Folmer. 2008. Blind hero: enabling guitar hero for the visually impaired. In Proceedings of the 10th international ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '08), 169-176. DOI=http://dx.doi.org/10.1145/1414471.1414503 \ 64. Ouyang Zhang and Kannan Srinivasan. 2016. Mudra: User-friendly Fine-grained Gesture Recognition using WiFi Signals. In Proceedings of the 12th International on Conference on Emerging Networking Experiments and Technologies (CoNEXT '16), 83-96. DOI=https://doi.org/10.1145/2999572.2999582 \ 65. Yang Zhang and Chris Harrison. 2015. Tomo: Wearable, Low-Cost Electrical Impedance Tomography for Hand Gesture Recognition. In Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology (UIST '15), 167-173. DOI=https://doi.org/10.1145/2807442.2807480 \ ",Micro thumb-tip gesture; PIR; pyroelectric; wearable,H.5.2. [User interfaces] – Input devices and strategies.,uistf2235-file1.docx,uistf2235-file2.jpg,uistf2235-file3.mp4,,Pyro is a micro thumb-tip gesture sensing and recognition technique based on thermal infrared signals radiated from our fingers.,,Our primary contributions include: 1) an approach to detect micro thumb-tip gestures using pyroelectric infrared sensing; 2) development of a prototype; and 3) initial validation of this approach.,"1. We removed the claims related to battery life on a smartwatch and toned down the claims on power consumption and system robustness (e.g. abstract, introduction, conclusion). We also removed health concern in the introduction. \  \ 2. We reorganized related work section, summarizing the previous research based on sensing techniques. We included recommendations and comparisons with prior works recommended by R1 and R2.  \  \ 3. We added all missing implementation details in PYRO IMPLEMENTATION and showed a new figure of FOV visualization with respect to the watch (figure 4).  \  \ 4. We added pictures of Fresnel Lens and PIR sensor in figure 5. \  \ 5. We modified the figure 6, adding the raw signals and axis labels. \  \ 6. We added more details in Feature Extraction. \  \ 7. We removed the user study 1 and 4 (in the previous version) and made study 2 as a supplementary experiment followed by USER EVALUATION (study 3). We toned down the claims in the supplementary study and added temperature details. We also corrected the conclusion in hand temperature study and added explanations in nearby hand movement study. We added a power session in discussion and limitations. \  \ 8. We admitted the limitations of the supplementary study in discussion and added more future work directions (e.g., try already developed near-range, passive sensor).",Jun Gong,Xing-Dong Yang,FormatComplete,,,,,,,Jul 28 21:47,
uistf4289,10/24,12,Phones & Watches,2:00:00 PM,3:30:00 PM,4+1,2:40:00 PM,3:00:00 PM,long,long,uistf2235,3,1203,,,uistf4289,A,"SmartSleeve: Real-time Sensing of Surface and Deformation Gestures on Flexible, Interactive Textiles, using a Hybrid Gesture Detection Pipeline",Patrick,Parzer,patrick.parzer@fh-hagenberg.at,uistf4289-paper.pdf,13,letter,,,"Patrick Parzer, Adwait Sharma, Anita Vogl, Jürgen Steimle, Alex Olwal, Michael Haller","patrick.parzer@fh-hagenberg.at, adwait.sharma@fh-hagenberg.at, Anita.Vogl@fh-hagenberg.at, jsteimle@mpi-inf.mpg.de, olwal@google.com, haller@fh-hagenberg.at",43327,Patrick,,Parzer,patrick.parzer@fh-hagenberg.at,Media Interaction Lab,University of Applied Sciences Upper Austria,Hagenberg,,Austria,,,,,,73553,Adwait,,Sharma,adwait.sharma@fh-hagenberg.at,Media Interaction Lab,University of Applied Sciences Upper Austria,Hagenberg,,Austria,,,,,,43412,Anita,,Vogl,Anita.Vogl@fh-hagenberg.at,Media Interaction Lab,University of Applied Sciences Upper Austria,Hagenberg,,Austria,,,,,,9521,Jürgen,,Steimle,jsteimle@mpi-inf.mpg.de,,Saarland University,Saarbrücken,,Germany,,,,,,4018,Alex,,Olwal,olwal@google.com,,"Google, Inc.",Mountain View,California,United States,,,,,,3937,Michael,,Haller,haller@fh-hagenberg.at,Media Interaction Lab,University of Applied Sciences Upper Austria,Hagenberg,,Austria,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Over the last decades, there have been numerous efforts in wearable computing research to enable interactive textiles. Most work focus, however, on integrating sensors for planar touch gestures, and thus do not fully take advantage of the flexible, deformable and tangible material properties of textile. In this work, we introduce SmartSleeve, a deformable textile sensor, which can sense both surface and deformation gestures in real-time. It expands the gesture vocabulary with a range of expressive interaction techniques, and we explore new opportunities using advanced deformation gestures, such as, Twirl, Twist, Fold, Push and Stretch. We describe our sensor design, hardware implementation and its novel non-rigid connector architecture. We provide a detailed description of our hybrid gesture detection pipeline that uses learning-based algorithms and heuristics to enable real-time gesture detection and tracking. Its modular architecture allows us to derive new gestures through the combination with continuous properties like pressure, location, and direction. Finally, we report on the promising results from our evaluations which demonstrate real-time classification. ",patrick.parzer@fh-hagenberg.at,"1. Daniel Ashbrook. 2010. Enabling Mobile Microinteractions. Dissertation. Georgia Institute of Technology. http: //smartech.gatech.edu/handle/1853/33986 \ 2. Patrick Baudisch and Gerry Chu. 2009. Back-of-device interaction allows creating very small touch devices. In CHI. ACM Press, New York, New York, USA, 1923. DOI: http://dx.doi.org/10.1145/1518701.1518995 \ 3. Alan Br¨anzel, Christian Holz, Daniel Hoffmann, Dominik Schmidt, Marius Knaust, Patrick L¨uhne, Ren´e Meusel, Stephan Richter, and Patrick Baudisch. 2013. GravitySpace: tracking users and their poses in a smart room using a pressure-sensing ﬂoor. In CHI. ACM Press, New York, New York, USA, 725. DOI: http://dx.doi.org/10.1145/2470654.2470757 \ 4. Leah Buechley. 2008. SensorMania - Notes from e-Fashion day. (2008). http: //www.mediamatic.net/36637/en/sensormania \ 5. Leah Buechley and Michael Eisenberg. 2009. Fabric PCBs, electronic sequins, and socket buttons: Techniques for e-textile craft. Personal and Ubiquitous Computing 13, 2 (feb 2009), 133–150. DOI:http: //dx.doi.org/10.1007/s00779-007-0181-0 \ 6. Leah. Buechley, Kylie A. Peppler, Michael Eisenberg, and Yasmin B. Kafai. 2013. Textile messages : dispatches from the world of e-textiles and education (2 ed.). Peter Lang. 246 pages. http: //dx.doi.org/10.3726/978-1-4539-0941-6 \ 7. Lina M Castano and Alison B Flatau. 2014. Smart fabric sensors and e-textile technologies: a review. Smart Materials and Structures 23, 5 (may 2014), 053001. DOI:http: //dx.doi.org/10.1088/0964-1726/23/5/053001 \ 8. Jared Cechanowicz, Pourang Irani, and Sriram Subramanian. 2007. Augmenting the mouse with pressure sensitive input. In CHI. ACM Press, New York, New York, USA, 1385. DOI: http://dx.doi.org/10.1145/1240624.1240835 \ 9. Fu Chang and Chun Jen Chen. 2003. A component-labeling algorithm using contour tracing technique. In ICDAR, Vol. 2003. IEEE Comput. Soc, 741–745. DOI:http: //dx.doi.org/10.1109/ICDAR.2003.1227760 \ 10. Tommaso D’Alessio. 1999. Measurement errors in the scanning of piezoresistive sensors arrays. Sensors and Actuators A: Physical 72, 1 (1999), 71–76. DOI: http://dx.doi.org/10.1016/S0924-4247(98) 00204-0 \ 11. Lucy E. Dunne, Kaila Bibeau, Lucie Mulligan, Ashton Frith, and Cory Simon. 2012. Multi-layer e-textile circuits. In UbiComp. 649. DOI: http://dx.doi.org/10.1145/2370216.2370348 \ 12. Sean Follmer, Micah Johnson, Edward Adelson, and Hiroshi Ishii. 2011. deForm: an interactive malleable surface for capturing 2.5D arbitrary objects, tools and touch. In UIST. ACM Press, New York, New York, USA, 527. DOI: http://dx.doi.org/10.1145/2047196.2047265 \ 13. Markus Funk, Alireza Sahami, Niels Henze, and Albrecht Schmidt. 2014. Using a touch-sensitive wristband for text entry on smart watches. In CHI EA. ACM Press, New York, New York, USA, 2305–2310. DOI: http://dx.doi.org/10.1145/2559206.2581143 \ 14. Guido Gioberto, James Coughlin, Kaila Bibeau, and Lucy E Dunne. 2013. Detecting Bends and Fabric Folds using Stitched Sensors. In ISWC. New York, New York, USA, 53–56. DOI: http://dx.doi.org/10.1145/2493988.2494355 \ 15. Antonio Gomes, Andrea Nesbitt, and Roel Vertegaal. 2013. MorePhone: an actuated shape changing ﬂexible smartphone. In CHI. ACM Press, New York, New York, USA, 583. DOI: http://dx.doi.org/10.1145/2470654.2470737 \ 16. Nan-Wei Gong, J¨urgen Steimle, Simon Olberding, Steve Hodges, Nicholas Edward Gillian, Yoshihiro Kawahara, and Joseph A. Paradiso. 2014. PrintSense: a versatile sensing technique to support multimodal ﬂexible surface interaction. In CHI. ACM Press, New York, New York, USA, 1407–1410. DOI: http://dx.doi.org/10.1145/2556288.2557173 \ 17. Nur Al-huda Hamdan, Jeffrey R Blum, Florian Heller, Ravi Kanth Kosuru, and Jan Borchers. 2016. Grabbing at an angle: menu selection for fabric interfaces. In ISWC. ACM Press, New York, New York, USA, 1–7. DOI: http://dx.doi.org/10.1145/2971763.2971786 \ 18. Chris Harrison, Hrvoje Benko, and Andrew D. Wilson. 2011. OmniTouch: wearable multitouch interaction everywhere. In UIST. ACM Press, New York, New York, USA, 441. DOI: http://dx.doi.org/10.1145/2047196.2047255 \ 19. Chris Harrison and Haakon Faste. 2014. Implications of location and touch for on-body projected interfaces. In DIS. ACM Press, New York, New York, USA, 543–552. DOI: http://dx.doi.org/10.1145/2598510.2598587 \ 20. Chris Harrison, Brian Y. Lim, Aubrey Shick, and Scott E. Hudson. 2009. Where to locate wearable displays?. In CHI. ACM Press, New York, New York, USA, 941. DOI: http://dx.doi.org/10.1145/1518701.1518845 \ 21. Chris Harrison, Shilpa Ramamurthy, and Scott E. Hudson. 2012. On-body interaction: armed and dangerous. In TEI. ACM Press, New York, New York, USA, 69. DOI: http://dx.doi.org/10.1145/2148131.2148148 \ 22. Chris Harrison, Desney Tan, and Dan Morris. 2010. Skinput: appropriating the skin as an interactive canvas. In CHI. ACM Press, New York, New York, USA, 453. DOI: http://dx.doi.org/10.1145/1753326.1753394 \ 23. Florian Heller, Stefan Ivanov, Chat Wacharamanotham, and Jan Borchers. 2014. FabriTouch: exploring ﬂexible touch input on textiles. In ISWC. ACM Press, New York, New York, USA, 59–62. DOI: http://dx.doi.org/10.1145/2634317.2634345 \ 24. Christopher F Herot and Guy Weinzapfel. 1978. One-point touch input of vector information for computer displays. ACM SIGGRAPH Computer Graphics 12, 3 (1978), 210–216. DOI: http://dx.doi.org/10.1145/965139.807392 \ 25. David Holman and Roel Vertegaal. 2008. Organic user interfaces: designing computers in any way, shape, or form. Commun. ACM 51, 6 (2008), 48. DOI: http://dx.doi.org/10.1145/1349026.1349037 \ 26. Christian Holz, Tovi Grossman, George Fitzmaurice, and Anne Agur. 2012. Implanted user interfaces. In CHI. ACM Press, New York, New York, USA, 503. DOI: http://dx.doi.org/10.1145/2207676.2207745 \ 27. Jonathan Hook, Stuart Taylor, Alex Butler, Nicolas Villar, and Shahram Izadi. 2009. A reconﬁgurable ferromagnetic input device. In UIST. ACM Press, New York, New York, USA, 51–54. DOI: http://dx.doi.org/10.1145/1622176.1622186 \ 28. Sungjune Jang, Lawrence H Kim, Kesler Tanner, Hiroshi Ishii, and Sean Follmer. 2016. Haptic Edge Display for Mobile Tactile Interaction. In CHI. 3706–3716. DOI: http://dx.doi.org/10.1145/2858036.2858264 \ 29. Thorsten Karrer, Moritz Wittenhagen, Leonhard Lichtschlag, Florian Heller, and Jan Borchers. 2011. Pinstripe: eyes-free continuous input anywhere on interactive clothing. In CHI. ACM Press, New York, New York, USA, 1313. DOI: http://dx.doi.org/10.1145/1978942.1979137 \ 30. Byron Lahey, Audrey Girouard, Winslow Burleson, and Roel Vertegaal. 2011. PaperPhone: understanding the use of bend gestures in mobile devices with ﬂexible electronic paper displays. In CHI. ACM Press, New York, New York, USA, 1303. DOI: http://dx.doi.org/10.1145/1978942.1979136 \ 31. Sang-Su Lee, Sohyun Kim, Bipil Jin, Eunji Choi, Boa Kim, Xu Jia, Daeeop Kim, and Kun-pyo Lee. 2010. How users manipulate deformable displays as input devices. In CHI. ACM Press, New York, New York, USA, 1647. DOI: http://dx.doi.org/10.1145/1753326.1753572 \ 32. Joanne Leong, Patrick Parzer, Florian Perteneder, Teo Babic, Christian Rendl, Anita Vogl, Hubert Egger, Alex Olwal, and Michael Haller. 2016. proCover: Sensory Augmentation of Prosthetic Limbs Using Smart Textile Covers. In UIST. 335–346. DOI: http://dx.doi.org/10.1145/2984511.2984572 \ 33. Dinesh Mandalapu and Sriram Subramanian. 2011. Exploring Pressure as an Alternative to Multi-Touch Based Interaction. IndiaHCI (2011), 88. DOI: http://dx.doi.org/10.1145/2407796.2407810 \ 34. David C. McCallum, Edward Mak, Pourang Irani, and Sriram Subramanian. 2009. PressureText: pressure input for mobile phone text entry. CHI (2009), 4519. DOI: http://dx.doi.org/10.1145/1520340.1520693 \ 35. Sachi Mizobuchi, Shinya Terasaki, Turo Keski-Jaskari, Jari Nousiainen, Matti Ryynanen, and Miika Silfverberg. 2005. Making an impression: force-controlled pen input for handheld devices. CHI EA (2005), 1661. DOI: http://dx.doi.org/10.1145/1056808.1056991 \ 36. Simon Olberding, Kian Peen Yeo, Suranga Nanayakkara, and Jurgen Steimle. 2013. AugmentedForearm: exploring the design space of a display-enhanced forearm. In AH. ACM Press, New York, New York, USA, 9–12. DOI: http://dx.doi.org/10.1145/2459236.2459239 \ 37. Patrick Parzer, Kathrin Probst, Teo Babic, Christian Rendl, Anita Vogl, Alex Olwal, and Michael Haller. 2016. FlexTiles: A Flexible, Stretchable, Formable, Pressure-Sensitive, Tactile Input Sensor. In CHI EA. ACM Press, New York, New York, USA, 3754–3757. DOI: http://dx.doi.org/10.1145/2851581.2890253 \ 38. Jerome Pasquero, Scott J. Stobbe, and Noel Stonehouse. 2011. A haptic wristwatch for eyes-free interactions. In CHI. ACM Press, New York, New York, USA, 3257. DOI: http://dx.doi.org/10.1145/1978942.1979425 \ 39. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830. \ 40. Ivan Poupyrev, Nan-Wei Gong, Shiho Fukuhara, Mustafa Emre Karagozler, Carsten Schwesig, and Karen E. Robinson. 2016. Project Jacquard: Interactive Digital Textiles at Scale. In CHI. ACM Press, New York, New York, USA, 4216–4227. DOI: http://dx.doi.org/10.1145/2858036.2858176 \ 41. Philip Quinn and Andy Cockburn. 2009. Zooﬁng!: faster list selections with pressure-zoom-ﬂick-scrolling. In OzCHI. 185. DOI: http://dx.doi.org/10.1145/1738826.1738856 \ 42. Gonzalo Ramos and Ravin Balakrishnan. 2005. Zliding: ﬂuid zooming and sliding for high precision parameter manipulation. In UIST. 143. DOI: http://dx.doi.org/10.1145/1095034.1095059 \ 43. Gonzalo Ramos, Matthew Boulos, and Ravin Balakrishnan. 2004. Pressure widgets. In CHI, Vol. 6. 487–494. DOI: http://dx.doi.org/10.1145/985692.985754 \ 44. Christian Rendl, Patrick Greindl, Kathrin Probst, Martin Behrens, and Michael Haller. 2014a. Presstures: exploring pressure-sensitive multi-touch gestures on trackpads. In CHI. 431–434. DOI: http://dx.doi.org/10.1145/2556288.2557146 \ 45. Christian Rendl, David Kim, Sean Fanello, Patrick Parzer, Christoph Rhemann, Jonathan Taylor, Martin Zirkl, Gregor Scheipl, Thomas Rothl¨ander, Michael Haller, and Shahram Izadi. 2014b. FlexSense: a transparent self-sensing deformable surface. In UIST. 129–138. DOI: http://dx.doi.org/10.1145/2642918.2647405 \ 46. Ilya Rosenberg and Ken Perlin. 2009. The UnMousePad: the future of touch sensing. In ACM Transactions on Graphics, Vol. 28. ACM Press, New York, New York, USA, 1. DOI: http://dx.doi.org/10.1145/1531326.1531371 \ 47. T. Scott Saponas, Chris Harrison, and Hrvoje Benko. 2011. PocketTouch: through-fabric capacitive touch input. In UIST. ACM Press, New York, New York, USA, 303. DOI: http://dx.doi.org/10.1145/2047196.2047235 \ 48. T. Scott Saponas, Desney S. Tan, Dan Morris, Ravin Balakrishnan, Jim Turner, and James A. Landay. 2009. Enabling always-available input with muscle-computer interfaces. In UIST. ACM Press, New York, New York, USA, 167. DOI: http://dx.doi.org/10.1145/1622176.1622208 \ 49. Toshiki Sato, Haruko Mamiya, Hideki Koike, and Kentaro Fukuchi. 2009. PhotoelasticTouch: transparent rubbery tangible interface using an LCD and photoelasticity. In UIST. ACM Press, New York, New York, USA, 43. DOI: http://dx.doi.org/10.1145/1622176.1622185 \ 50. Mika Satomi and Hannah Perner-Wilson. 2016. How To Get What You Want. (2016). http://www.kobakant.at/DIY/?cat=24 \ 51. R. S. Saxena, R. K. Bhan, and Anita Aggrawal. 2009. A new discrete circuit for readout of resistive sensor arrays. Sensors and Actuators, A: Physical 149, 1 (2009), 93–99. DOI:http: //dx.doi.org/10.1016/j.sna.2008.10.013 \ 52. Raghvendra Sahai Saxena, R. K. Bhan, Navneet Kaur Saini, and R. Muralidharan. 2011a. Virtual ground technique for crosstalk suppression in networked resistive sensors. IEEE Sensors Journal 11, 2 (feb 2011), 432–433. DOI:http: //dx.doi.org/10.1109/JSEN.2010.2060186 \ 53. Raghvendra Sahai Saxena, Navneet Kaur Saini, and R. K. Bhan. 2011b. Analysis of crosstalk in networked arrays of resistive sensors. IEEE Sensors Journal 11, 4 (2011), 920–924. DOI:http: //dx.doi.org/10.1109/JSEN.2010.2063699 \ 54. Stefan Schneegass and Alexandra Voit. 2016. GestureSleeve: using touch sensitive fabrics for gestural input on the forearm for controlling smartwatches. In ISWC. ACM Press, New York, New York, USA, 108–115. DOI: http://dx.doi.org/10.1145/2971763.2971797 \ 55. Carsten Schwesig, Ivan Poupyrev, and Eijiro Mori. 2003. Gummi: user interface for deformable computers. In CHI EA. 954. DOI: http://dx.doi.org/10.1145/765891.766091 \ 56. M. Shimojo, M. Ishikawa, and K. Kanaya. 1991. A ﬂexible high resolution tactile imager with video signal output. In ICRA. 384–391. DOI:http: //dx.doi.org/10.1109/ROBOT.1991.131607 \ 57. Lin Shu, Xiaoming Tao, and David Dagan Feng. 2015. A New Approach for Readout of Resistive Sensor Arrays for Wearable Electronic Applications. IEEE Sensors Journal 15, 1 (2015), 442–452. DOI:http: //dx.doi.org/10.1109/JSEN.2014.2333518 \ 58. Craig Stewart, Michael Rohs, Sven Kratz, and Georg Essl. 2010. Characteristics of Pressure-Based Input for Mobile Devices. In CHI. ACM Press, New York, New York, USA, 801–810. DOI: http://dx.doi.org/10.1145/1753326.1753444 \ 59. Mathias Sundholm, Jingyuan Cheng, Bo Zhou, Akash Sethi, and Paul Lukowicz. 2014. Smart-mat: recognizing and counting gym exercises with low-cost resistive pressure sensing matrix. In UbiComp. ACM Press, New York, New York, USA, 373–382. DOI: http://dx.doi.org/10.1145/2632048.2636088 \ 60. Stuart Taylor, Cem Keskin, Otmar Hilliges, Shahram Izadi, and John Helmes. 2014. Type-hover-swipe in 96 bytes: a motion sensing mechanical keyboard. CHI (2014), 1695–1704. DOI: http://dx.doi.org/10.1145/2556288.2557030 \ 61. Martijn ten Bh¨omer and Pauline van Dongen. 2014. Vigour. Interactions 21, 5 (Sep 2014), 12–13. DOI: http://dx.doi.org/10.1145/2641396 \ 62. Giovanni Maria Troiano, Esben Warming Pedersen, and Kasper Hornbæk. 2014. User-deﬁned gestures for elastic, deformable displays. AVI (2014), 1–8. DOI: http://dx.doi.org/10.1145/2598153.2598184 \ 63. Nirzaree Vadgama and J¨urgen Steimle. 2017. Flexy: Shape-Customizable, Single-Layer, Inkjet Printable Patterns for 1D and 2D Flex Sensing. In TEI. ACM Press, New York, New York, USA, 153–162. DOI: http://dx.doi.org/10.1145/3024969.3024989 \ 64. Martin Weigel, Tong Lu, Gilles Bailly, Antti Oulasvirta, Carmel Majidi, and J¨urgen Steimle. 2015. iSkin: Flexible, Stretchable and Visually Customizable On-Body Touch Sensors for Mobile Computing. In CHI. ACM Press, New York, New York, USA, 2991–3000. DOI: http://dx.doi.org/10.1145/2702123.2702391 \ 65. Martin Weigel, Vikram Mehta, and J¨urgen Steimle. 2014. More than touch: understanding how people use skin as an input surface for mobile computing. In CHI. ACM Press, New York, New York, USA, 179–188. DOI: http://dx.doi.org/10.1145/2556288.2557239 \ 66. Mark Weiser. 1995. Human-computer Interaction. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, Chapter The Computer for the 21st Century, 933–940. http: //dl.acm.org/citation.cfm?id=212925.213017 \ 67. G Wilson. 2013. Using Pressure Input and Thermal Feedback to Broaden Haptic Interaction with Mobile Devices. In PhD Thesis. 1–273. http://encore.lib. gla.ac.uk/iii/encore/record/C__Rb2982123 \ 68. Jacob O. Wobbrock, Meredith Ringel Morris, and Andrew D. Wilson. 2009. User-deﬁned gestures for surface computing. In CHI. ACM Press, New York, New York, USA, 1083. DOI: http://dx.doi.org/10.1145/1518701.1518866 \ 69. Bo Zhou, Harald Koerger, Markus Wirth, Constantin Zwick, Christine Martindale, Heber Cruz, Bjoern Eskoﬁer, and Paul Lukowicz. 2016a. Smart soccer shoe: monitoring foot-ball interaction with shoe integrated textile pressure sensor matrix. In ISWC. ACM Press, New York, New York, USA, 64–71. DOI: http://dx.doi.org/10.1145/2971763.2971784 \ 70. Bo Zhou, Mathias Sundholm, Jingyuan Cheng, Heber Cruz, and Paul Lukowicz. 2016b. Never skip leg day: A novel wearable approach to monitoring gym leg exercises. In PerCom. DOI:http: //dx.doi.org/10.1109/PERCOM.2016.7456520 \ ","Smart Textile, Deformation Gestures, Surface Gestures",H.5.2,uistf4289-file1.zip,uistf4289-file2.jpg,uistf4289-file3.mp4,,"SmartSleeve is a wearable textile that can recognize 22 gestures including surface, and deformation gestures. It expands the gesture vocabulary with a range of expressive interaction techniques on wearables.",,"SmartSleeve is a wearable textile that can recognize 22 gestures including surface, and deformation gestures. It expands the gesture vocabulary with a range of expressive interaction techniques on wearables.",We revised all figures for easier comprehension. Added information of the systematic gesture design. Clarify the the evaluation of the algorithm and added a chapter about False Positives in the discussion. \ ,Patrick Parzer,Adwait Sharma,FormatComplete,,,,,,,Aug 9 4:21,
uistf2014,10/24,12,Phones & Watches,2:00:00 PM,3:30:00 PM,4+1,3:00:00 PM,3:20:00 PM,long,long,uistf4289,4,1204,,,uistf2014,A,SoundCraft: Enabling Spatial Interactions on Smartwatches using Hand Generated Acoustics,Teng,Han,hanteng1021@gmail.com,uistf2014-paper.pdf,13,letter,,,"Teng Han, Khalad Hasan, Keisuke Nakamura, Randy Gomez, Pourang Irani","hanteng1021@gmail.com, khalad@cs.umanitoba.ca, nakamura@hri.jp, r.gomez@jp.honda-ri.com, irani@cs.umanitoba.ca",22861,Teng,,Han,hanteng1021@gmail.com,Department of Computer Science,University of Manitoba,Winnipeg,Manitoba,Canada,,,,,,18067,Khalad,,Hasan,khalad@cs.umanitoba.ca,Department of Computer Science,University of Manitoba,Winnipeg,Manitoba,Canada,,,,,,56563,Keisuke,,Nakamura,nakamura@hri.jp,,Honda Research Institute,Wako,Saitama,Japan,,,,,,39200,Randy,,Gomez,r.gomez@jp.honda-ri.com,Honda Research Institute,Honda Research Institute Japan,Wako,Saitama,Japan,,,,,,2567,Pourang,,Irani,irani@cs.umanitoba.ca,Department of Computer Science,University of Manitoba,Winnipeg,Manitoba,Canada,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present SoundCraft, a smartwatch prototype embedded with a microphone array, that localizes angularly, in azimuth and elevation, acoustic signatures: non-vocal acoustics that are produced using our hands. Acoustic signatures are common in our daily lives, such as when snapping or rubbing our fingers, tapping on objects or even when using an auxiliary object to generate the sound. We demonstrate that we can capture and leverage the spatial location of such naturally occurring acoustics using our prototype. We describe our algorithm, which we adopt from the MUltiple SIgnal Classification (MUSIC) technique [31], that enables robust localization and classification of the acoustics when the microphones are required to be placed at close proximity. SoundCraft enables a rich set of spatial interaction techniques, including quick access to smartwatch content, rapid command invocation, in-situ sketching, and also multi-user around device interaction. Via a series of user studies, we validate SoundCraft’s localization and classification capabilities in non-noisy and noisy environments.",hanteng1021@gmail.com,"1. Brian Amento, Will Hill, and Loren Terveen. 2002. The sound of one hand: a wrist-mounted bio-acoustic fingertip gesture interface. In CHI '02 Extended Abstracts on Human Factors in Computing Systems (CHI EA '02). ACM, New York, NY, USA, 724-725. http://dx.doi.org.uml.idm.oclc.org/10.1145/506443.5065 \ 2. Daniel L. Ashbrook. 2010. Enabling Mobile Microinteractions. Ph.D. Dissertation. Georgia Institute of Technology, Atlanta, GA, USA. AAI3414437. \ 3. Wei-Hung Chen. 2015. Blowatch: Blowable and Handsfree Interaction for Smartwatches. In Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '15). 103-108. http://dx.doi.org.uml.idm.oclc.org/10.1145/2702613.272 \ 4. Jackson Feijó Filho, Wilson Prata, and Thiago Valle. 2015. Advances on Breathing Based Text Input for Mobile Devices. In International Conference on Universal Access in Human-Computer Interaction. 279– 287. Springer International Publishing, 2015. DOI: 10.1007/978-3-319-20678-3_27 \ 5. Mathieu Le Goc, Stuart Taylor, Shahram Izadi, and Cem Keskin. 2014. A low-cost transparent electric field sensor for 3d interaction on mobile devices. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '14). 3167-3170. http://dx.doi.org.uml.idm.oclc.org/10.1145/2556288.255 \ 6. Randy Gomez, Tatsuya Kawahara and Kazuhrio Nakadai. 2015. Optimized wavelet-domain filtering under noisy and reverberant conditions. APSIPA Transactions on Signal and Information Processing, 4, e3 doi:10.1017/ATSIP.2015.5 \ 7. Susumu Harada, James A. Landay, Jonathan Malkin, Xiao Li, and Jeff A. Bilmes. 2006. The vocal joystick:: evaluation of voice-based cursor control techniques. In Proceedings of the 8th international ACM SIGACCESS conference on Computers and accessibility (Assets '06). ACM, New York, NY, USA, 197-204. http://dx.doi.org/10.1145/1168987.1169021 \ 8. Chris Harrison and Scott E. Hudson. 2008. Scratch input: creating large, inexpensive, unpowered and mobile finger input surfaces. In Proceedings of the 21st annual ACM symposium on User interface software and technology (UIST '08). 205-208. http://dx.doi.org.uml.idm.oclc.org/10.1145/1449715.144 \ 9. Chris Harrison and Scott E. Hudson. 2009. Abracadabra: wireless, high-precision, and unpowered finger input for very small mobile devices. In Proceedings of the 22nd annual ACM symposium on User interface software and technology (UIST '09). 121124. http://dx.doi.org.uml.idm.oclc.org/10.1145/1622176.162 \ 10. Chris Harrison, Desney Tan, and Dan Morris. 2010. Skinput: appropriating the body as an input surface. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '10). 453-462. http://dx.doi.org.uml.idm.oclc.org/10.1145/1753326.175 \ 11. Chris Harrison, Robert Xiao, and Scott Hudson. 2012. Acoustic barcodes: passive, durable and inexpensive notched identification tags. In Proceedings of the 25th annual ACM symposium on User interface software and technology (UIST '12). 563-568. http://dx.doi.org.uml.idm.oclc.org/10.1145/2380116.238 0187 \ 12. Khalad Hasan, David Ahlström, and Pourang Irani. 2015. SAMMI: A Spatially-Aware Multi-Mobile Interface for Analytic Map Navigation Tasks. In Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI '15). ACM, New York, NY, USA, 36-45. http://dx.doi.org.uml.idm.oclc.org/10.1145/2785830.278 \ 13. Takeo Igarashi and John F. Hughes. 2001. Voice as sound: using non-verbal voice input for interactive control. In Proceedings of the 14th annual ACM symposium on User interface software and technology (UIST '01). ACM, New York, NY, USA, 155-156. http://dx.doi.org.uml.idm.oclc.org/10.1145/502348.5023 \ 14. Hiroshi Ishii, Craig Wisneski, Julian Orbanes, Ben Chun, and Joe Paradiso. 1999. PingPongPlus: design of an athletic-tangible interface for computer-supported cooperative play. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems (CHI '99). 394-401. http://dx.doi.org.uml.idm.oclc.org/10.1145/302979.3031 \ 15. Hamed Ketabdar, Mehran Roshandel, and Kamer Ali Yüksel. 2010. MagiWrite: towards touchless digit entry using 3D space around mobile devices. In Proceedings of the 12th international conference on Human computer interaction with mobile devices and services (MobileHCI '10). ACM, New York, NY, USA, 443-446. http://doi.acm.org.uml.idm.oclc.org/10.1145/1851600.1 \ 16. David Kim, Otmar Hilliges, Shahram Izadi, Alex D. Butler, Jiawen Chen, Iason Oikonomidis, and Patrick Olivier. 2012. Digits: freehand 3D interactions anywhere using a wrist-worn gloveless sensor. In Proceedings of the 25th annual ACM symposium on User interface software and technology (UIST '12). 167-176. http://dx.doi.org.uml.idm.oclc.org/10.1145/2380116.238 0139 \ 17. Jungsoo Kim, Jiasheng He, Kent Lyons, and Thad Starner. 2007. The Gesture Watch: A Wireless Contactfree Gesture based Wrist Interface. In Proceedings of the 2007 11th IEEE International Symposium on Wearable Computers (ISWC '07). IEEE Computer Society. 1-8. http://dx.doi.org/10.1109/ISWC.2007.4373770 \ 18. Sven Kratz and Michael Rohs. 2009. Hoverflow: exploring around-device interaction with IR distance sensors. In Proceedings of the 11th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI '09). Article 42, 4 pages. http://dx.doi.org.uml.idm.oclc.org/10.1145/1613858.161 \ 19. MindMeld Launches Voice Assistant 2.0, Says Voice Search Growing ramatically. 2015. Retrieved August 25, 2016 from http://searchengineland.com/mindmeldlaunches-voice-assistant-2-0-says-voice-searchgrowing-dramatically-238130 \ 20. Gierad Laput, Robert Xiao, and Chris Harrison. 2016. ViBand: High-Fidelity Bio-Acoustic Sensing Using Commodity Smartwatch Accelerometers. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). ACM, New York, NY, USA, 321-333. https://doi.org/10.1145/2984511.2984582 \ 21. Pedro Lopes, Ricardo Jota, and Joaquim A. Jorge. 2011. Augmenting touch interaction through acoustic sensing. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces (ITS '11). ACM, New York, NY, USA, 53-56. https://doiorg.uml.idm.oclc.org/10.1145/2076354.2076364 \ 22. Keisuke Nakamura, Kazuhiro Nakadai, Futoshi Asano, Yuji Hasegawa, and Hiroshi Tsujino. 2009. Intelligent sound source localization for dynamic environments. IEEE/RSJ International Conference on Intelligent Robots and Systems, St. Louis, MO, pp. 664-669. doi: 10.1109/IROS.2009.5354419 \ 23. OMG! Mobile voice survey reveals teens love to talk. 2014. Retrieved August 25, 2016 from https://googleblog.blogspot.ca/2014/10/omg-mobilevoice-survey-reveals-teens.html \ 24. Joseph A. Paradiso and Che King Leo. 2005. Tracking and characterizing knocks atop large interactive displays. Sensor Review, Vol. 25 Iss: 2, pp.134-143. http://dx.doi.org/10.1108/02602280510585727 \ 25. Shwetak N. Patel and Gregory D. Abowd. 2007. Blui: low-cost localized blowable user interfaces. In Proceedings of the 20th annual ACM symposium on User interface software and technology (UIST '07). 217220. http://dx.doi.org.uml.idm.oclc.org/10.1145/1294211.129 \ 26. D. T. Pham, Ze Ji, Ming Yang, Zuobin Wang, and Mostafa Al-Kutubi. 2007. A novel human-computer interface based on passive acoustic localization. In Proceedings of the 12th international conference on Human-computer interaction: interaction platforms and techniques (HCI'07), Julie A. Jacko (Ed.). SpringerVerlag, Berlin, Heidelberg, 901-909. \ 27. Nissanka B. Priyantha, Anit Chakraborty, and Hari Balakrishnan. 2000. The Cricket location-support system. In Proceedings of the 6th annual international conference on Mobile computing and networking (MobiCom '00). ACM, New York, NY, USA, 32-43. http://dx.doi.org/10.1145/345910.345917 \ 28. Jun Rekimoto. 2001. Gesturewrist and gesturepad: Unobtrusive wearable interaction devices. In Proceedings of Fifth International Symposium on Wearable Computers, 21-27. IEEE. \ 29. Gabriel Reyes, Dingtian Zhang, Sarthak Ghosh, Pratik Shah, Jason Wu, Aman Parnami, Bailey Bercik, Thad Starner, Gregory D. Abowd, and W. Keith Edwards. 2016. Whoosh: non-voice acoustics for low-cost, handsfree, and rapid input on smartwatches. In Proceedings of the 2016 ACM International Symposium on Wearable Computers (ISWC '16). ACM, New York, NY, USA, 120-127. http://dx.doi.org/10.1145/2971763.2971765 \ 30. Daisuke Sakamoto, Takanori Komatsu, and Takeo Igarashi. 2013. Voice augmented manipulation: using paralinguistic information to manipulate mobile devices. In Proceedings of the 15th international conference on Human-computer interaction with mobile devices and services (MobileHCI '13). 69-78. http://dx.doi.org.uml.idm.oclc.org/10.1145/2493190.249 \ 31. R. Schmidt. 1986. Multiple emitter location and signal parameter estimation. In IEEE Transactions on Antennas and Propagation, vol. 34, no. 3, pp. 276-280. doi: 10.1109/TAP.1986.1143830 \ 32. Project Soli. 2016. Retrieved August 25, 2016 from http://atap.google.com/soli. \ 33. Jie Song, Gábor Sörös, Fabrizio Pece, Sean Ryan Fanello, Shahram Izadi, Cem Keskin, and Otmar Hilliges. 2014. In-air gestures around unmodified mobile devices. In Proceedings of the 27th annual ACM symposium on User interface software and technology (UIST '14). 319-329. http://doi.acm.org.uml.idm.oclc.org/10.1145/2642918.2 \ 34. Adam J. Sporka, Sri H. Kurniawan, Murni Mahmud, and Pavel Slavík. 2006. Non-speech input and speech recognition for real-time control of computer games. In Proceedings of the 8th international ACM SIGACCESS conference on Computers and accessibility (Assets '06). 213-220. http://dx.doi.org.uml.idm.oclc.org/10.1145/1168987.116 \ 35. T. Scott Saponas, Desney S. Tan, Dan Morris, Ravin Balakrishnan, Jim Turner, and James A. Landay. 2009. Enabling always-available input with muscle-computer interfaces. In Proceedings of the 22nd annual ACM symposium on User interface software and technology (UIST '09). 167-176. http://dx.doi.org.uml.idm.oclc.org/10.1145/1622176.162 \ 36. Wouter Van Vlaenderen, Jens Brulmans, Jo Vermeulen, and Johannes Schöning. 2015. WatchMe: A Novel Input Method Combining a Smartwatch and Bimanual Interaction. In Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '15). 2091-2095. http://dx.doi.org.uml.idm.oclc.org/10.1145/2702613.273 \ 37. Cheng-Yao Wang, Min-Chieh Hsiu, Po-Tsung Chiu, Chiao-Hui Chang, Liwei Chan, Bing-Yu Chen, and Mike Y. Chen. 2015. PalmGesture: Using Palms as Gesture Interfaces for Eyes-free Input. In Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI '15). 217-226. http://dx.doi.org.uml.idm.oclc.org/10.1145/2785830.278 \ 38. Hongyi Wen, Julian Ramos Rojas, and Anind K. Dey. 2016. Serendipity: Finger Gesture Recognition using an Off-the-Shelf Smartwatch. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16). 3847-3851. http://dx.doi.org.uml.idm.oclc.org/10.1145/2858036.285 \ 39. Anusha Withana, Roshan Peiris, Nipuna Samarasekara, and Suranga Nanayakkara. 2015. zSense: Enabling Shallow Depth Gesture Recognition for Greater Input Expressivity on Smart Wearables. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15). 3661-3670. http://dx.doi.org.uml.idm.oclc.org/10.1145/2702123.270 \ 40. B. D. Van Veen and K. M. Buckley. 1988. Beamforming: a versatile approach to spatial filtering. In IEEE ASSP Magazine, vol. 5, no. 2, pp. 4-24. doi: 10.1109/53.665 \ 41. Robert Xiao, Greg Lew, James Marsanico, Divya Hariharan, Scott Hudson, and Chris Harrison. 2014. Toffee: enabling ad hoc, around-device interaction with acoustic time-of-arrival correlation. In Proceedings of the 16th international conference on Human-computer interaction with mobile devices & services (MobileHCI '14). 67-76. http://dx.doi.org.uml.idm.oclc.org/10.1145/2628363.262 \ 42. Cheng Zhang, Qiuyue Xue, Anandghan Waghmare, Sumeet Jain, Yiming Pu, Sinan Hersek, Kent Lyons, Kenneth A. Cunefare, Omer T. Inan, and Gregory D. Abowd. 2017. SoundTrak: Continuous 3D Tracking of a Finger Using Active Acoustics. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1, 2, Article 30 (June 2017), 25 pages. https://doiorg.uml.idm.oclc.org/10.1145/3090095 \ 43. Yang Zhang and Chris Harrison. 2015. Tomo: Wearable, Low-Cost Electrical Impedance Tomography for Hand Gesture Recognition. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST '15). 167-173. http://dx.doi.org.uml.idm.oclc.org/10.1145/2807442.280 \ 44. Yang Zhang, Junhan Zhou, Gierad Laput, and Chris Harrison. 2016. SkinTrack: Using the Body as an Electrical Waveguide for Continuous Finger Tracking on the Skin. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16). 1491-1503. http://dx.doi.org.uml.idm.oclc.org/10.1145/2858036.285 \ ",Microphone Array Processing; Smartwatch Spatial Input; Acoustic Signatures; Classification and Localization,H.5.2,uistf2014-file1.docx,uistf2014-file2.jpg,uistf2014-file3.mp4,,The spatial position of a finger snap is detected around a smartwatch to trigger a command.,,Implemented a prototype with proximal microphones on a smartwatch to facilitate a broad range of spatial input using localized acoustic signatures.,"Paper #2014 \  \ We thank again the ACs and the PC for giving us the chance to revise the paper. We modified all elements as we had outlined in our rebuttal. We believe we have a stronger version with the suggested revisions. Below is the list of changes for the AC to follow up when reviewing the paper: \  \ Page 1. ABSTRACT \  \ We revised our abstract to emphasize (i) the key contributions, and; (ii) the novelty of our work. We replaced in the abstract, and throughout the paper, the terms “3D localization”, “3D interaction” with “angular localization, in azimuth and elevation”, or “spatial interaction”. This offers more precise terms and is less misleading than the one previously used. \  \ As reviewers were also not pleased with the term “gestural acoustics”, we replaced it with a more precise term “acoustic signatures”. \  \ Pages 1 & 2. INTRODUCTION \  \ We now revised the introduction, as suggested to emphasize: (i) the key technical challenges; (ii) how we addressed these, including our technical solution; and (iii) then the key capabilities. This provides a better flow as had been suggested. \  \ We modified our contribution statement to be as follows: \  \ “The contributions of this paper include: (i) the use of acoustic signatures for spatial input on smartwatches; (ii) our SoundCraft prototype, which includes the design and implementation of suitable algorithms that allow angular localization of acoustic signatures with microphones at very close proximity; and (iii) a set of applications showing the unique capabilities of SoundCraft over current smartwatch input techniques.” \  \ Page 2. RELATED WORK \  \ We made three significant changes in the related work, minor edits and revisions.  \  \ (i)	We emphasized the difference between SoundCraft and Viband. We highlighted those elements that are critically different between the techniques. \  \ (ii)	We explained why we did not use TDOA techniques, and instead relied on other approaches. \  \ (iii)	We added the missing references as suggested by reviewers. \  \ Page 3. GESTURAL ACOUSTIC CATALOG now called CATALOG OF ACOUSTIC SIGNATURES \  \ As suggested, we shortened this section significantly. This offered more room to the other sections in which we added additional clarifications. \  \ Page 4. HARDWARE \ We added all the missing details concerning the hardware. We included 3 footnotes, referring to specific units we purchased for our hardware. We also described in further details the assembly of our prototype.  \  \ We unified our measurement units to inch throughout the paper. \  \ Page 4-6. ACOUSTIC PROCESSING \  \ We further detailed the challenges and the novelty of our solutions. \  \ We also elaborated on the diagram in Figure 4, and described the various block components in more detail. \  \ At the beginning of the subsections, in a step-wise manner, described the purpose/function of the corresponding block and how it is related to the other blocks. This should help readers connect the subsections with the diagram block and understand the flow of the algorithm. \  \ We further clarified and reviewed the equations to ensure all the variable definitions are included (M & K, for example as per our rebuttal). \  \ We provide an extended justification on the use of a trigger mechanism to initiate a signature. \  \ Page 6. RECOGNIZING MULTIPLE GESTURAL ACOUSTICS \  \ We clarified the sentence “…separate overlapped sound sources”. \  \ Page 9. APPLICATIONS \  \ We provided details as needed, and in places where there was too much detail, or if the scenario seemed less exciting, we reduced those in size. We explained that in the game scenario we use directional information to differentiate among users. \  \ We clarified the use of sound (i.e., white sound) used by the phone. \  \ Page 10. DISCUSSION AND FUTURE WORK \  \ We clarified the sentence “…define an operation zone based on direction information” to remove and contradiction and confusion in it. \  \ We highlighted the limitations and further added our concerns with issues around “adding other noisy conditions” and “computing power consumption”. \  \ Page  11. CONCLUSION \  \ We added a statement concerning the technical contribution and novelty of our work.  \  \ All typos are now fixed throughout the paper, and we also included minor edits where needed. \  \ We would be glad to revise any further as see you necessary to improve the quality of our paper. \ ",Teng Han,Khalad Hasan,FormatComplete,,Honda Research Institute,NSERC,,Mitacs,,Aug 3 9:32,
uistf1660,10/24,12,Phones & Watches,2:00:00 PM,3:30:00 PM,4+1,3:20:00 PM,3:30:00 PM,short,short,uistf2014,5,1205,,,uistf1660,A,SensIR: Detecting Hand Gestures with a Wearable Bracelet using Infrared Transmission and Reflection,Jess,McIntosh,jm0152@my.bristol.ac.uk,uistf1660-paper.pdf,5,letter,,,"Jess McIntosh, Asier Marzo, Mike Fraser","jm0152@my.bristol.ac.uk, asier.marzo@unavarra.es, mike.fraser@bristol.ac.uk",45962,Jess,,McIntosh,jm0152@my.bristol.ac.uk,Bristol Interaction Group,University of Bristol,Bristol,,United Kingdom,,,,,,23125,Asier,,Marzo,asier.marzo@unavarra.es,Mechanical Engineering,University of Bristol,Bristol,Bristol,United Kingdom,,,,,,34870,Mike,,Fraser,mike.fraser@bristol.ac.uk,Bristol Interaction Group,University of Bristol,Bristol,,United Kingdom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Gestures have become an important tool for natural interaction with computers and thus several wearables have been developed to detect hand gestures. However, many existing solutions are unsuitable for practical use due to low accuracy, high cost or poor ergonomics. We present SensIR, a bracelet that uses near-infrared sensing to infer hand gestures. The bracelet is composed of pairs of infrared emitters and receivers that are used to measure both the transmission and reflection of light through/off the wrist. SensIR improves the accuracy of existing infrared gesture sensing systems through the key idea of taking measurements with all possible combinations of emitters and receivers. Our study shows that SensIR is capable of detecting 12 discrete gestures with 93.3% accuracy. SensIR has several advantages compared to other systems such as high accuracy, low cost, robustness against bad skin coupling and thin form-factor.",jm0152@my.bristol.ac.uk,"1. Christoph Amma, Thomas Krings, Jonas Böer, and Tanja Schultz. 2015. Advancing Muscle-Computer Interfaces with High-Density Electromyography. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 929–938. \ 2. Jingyuan Cheng, Oliver Amft, Gernot Bahle, and Paul Lukowicz. 2013. Designing sensitive wearable capacitive sensors for activity recognition. Sensors Journal, IEEE 13, 10 (2013), 3935–3947. \ 3. Artem Dementyev and Joseph A Paradiso. 2014. WristFlex: Low-power gesture input with wrist-worn pressure sensors. In Proceedings of the 27th annual ACM symposium on User interface software and technology. ACM, 161–166. \ 4. Rui Fukui, Masahiko Watanabe, Tomoaki Gyota, Masamichi Shimosaka, and Tomomasa Sato. 2011. Hand shape classiﬁcation with a wrist contour sensor: development of a prototype device. In Proceedings of the 13th international conference on Ubiquitous computing. ACM, 311–314. \ 5. Jun Gong, Xing-Dong Yang, and Pourang Irani. 2016. WristWhirl: One-handed Continuous Smartwatch Input using Wrist Gestures. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology. ACM, 861–872. \ 6. Hamed Hamid Muhammed and Jammalamadaka Raghavendra. 2015. Optomyography (OMG): A Novel Technique for the Detection of Muscle Surface Displacement Using Photoelectric Sensors. In 10th International Conference on Bioelectromagnetism, ISBEM 2015, Vol. 10. \ 7. Frederic Kerber, Markus Löchtefeld, Antonio Krüger, Jess McIntosh, Charlie McNeill, and Mike Fraser. 2016. Understanding Same-Side Interactions with Wrist-Worn Devices. In Proceedings of the 9th Nordic Conference on Human-Computer Interaction (NordiCHI ’16). ACM, New York, NY, USA, Article 28, 10 pages. DOI: http://dx.doi.org/10.1145/2971485.2971519 \ 8. David Kim, Otmar Hilliges, Shahram Izadi, Alex D Butler, Jiawen Chen, Iason Oikonomidis, and Patrick Olivier. 2012. Digits: freehand 3D interactions anywhere using a wrist-worn gloveless sensor. In Proceedings of the 25th annual ACM symposium on User interface software and technology. ACM, 167–176. \ 9. Gierad Laput, Robert Xiao, and Chris Harrison. 2016. ViBand: High-Fidelity Bio-Acoustic Sensing Using Commodity Smartwatch Accelerometers. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology. ACM, 321–333. \ 10. Jess McIntosh, Asier Marzo, Mike Fraser, and Carol Phillips. 2017. EchoFlex: Hand Gesture Recognition using Ultrasound Imaging. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 1923–1934. \ 11. Masa Ogata and Michita Imai. 2015. SkinWatch: skin gesture interaction for smart watch. In Proceedings of the 6th Augmented Human International Conference. ACM, 21–24. \ 12. Siddharth S Rautaray and Anupam Agrawal. 2015. Vision based hand gesture recognition for human computer interaction: a survey. Artiﬁcial Intelligence Review 43, 1 (2015), 1–54. \ 13. Jun Rekimoto. 2001. Gesturewrist and gesturepad: Unobtrusive wearable interaction devices. In Wearable Computers, 2001. Proceedings. Fifth International Symposium on. IEEE, 21–27. \ 14. T Scott Saponas, Desney S Tan, Dan Morris, and Ravin Balakrishnan. 2008. Demonstrating the feasibility of using forearm electromyography for muscle-computer interfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 515–524. \ 15. T Scott Saponas, Desney S Tan, Dan Morris, Ravin Balakrishnan, Jim Turner, and James A Landay. 2009. Enabling always-available input with muscle-computer interfaces. In Proceedings of the 22nd annual ACM symposium on User interface software and technology. ACM, 167–176. \ 16. Sujit Sikdar, Huzefa Rangwala, Emily B Eastlake, Ira A Hunt, Andrew J Nelson, Jayanth Devanathan, Andrew Shin, and Joseph J Pancrazio. 2014. Novel method for predicting dexterous individual ﬁnger movements by imaging muscle activity using a wearable ultrasonic system. Neural Systems and Rehabilitation Engineering, IEEE Transactions on 22, 1 (2014), 69–76. \ 17. Hongyi Wen, Julian Ramos Rojas, and Anind K Dey. 2016. Serendipity: Finger gesture recognition using an off-the-shelf smartwatch. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 3847–3851. \ 18. Yang Zhang and Chris Harrison. 2015. Tomo: Wearable, low-cost electrical impedance tomography for hand gesture recognition. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology. ACM, 167–173. \ 19. Yang Zhang, Robert Xiao, and Chris Harrison. 2016. Advancing Hand Gesture Recognition with High Resolution Electrical Impedance Tomography. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology. ACM, 843–850. \ 20. Huijuan Zhao, Feng Gao, Yukari Tanikawa, Kazuhiro Homma, and Yukio Yamada. 2005. Time-resolved diffuse optical tomographic imaging for the provision of both anatomical and functional information about biological tissue. Applied optics 44, 10 (2005), 1905–1916. \ ",Gesture recognition; Wearables; Infrared,H.5.m,uistf1660-file1.zip,uistf1660-file2.jpg,uistf1660-file3.mp4,uistf1660-file4.zip,"The SensIR wirstband, as shown worn around the wrist of the wearer. The band is comprised of segments of sensors to emit and receive IR.","In this directory there are design files for the pcb, the laser cut rubber bracelet, 3d printed buckle, and the acquisition software and online classifier.","We present SensIR, a bracelet that uses infra-red sensing to detect hand gestures. This work extends the capabilities of previous systems by utilising the combination of all emitters and receivers.","We have applied the changes suggested by the reviewers and summarised by the AC. \  \ *** Is this an approach that uses tomography? \  \ We have replaced the references to tomography since the word implies some sort of imaging and we directly use the features to classify the gestures. The title is now ""SensIR: Detecting Hand Gestures with a Wearable Bracelet using Transmissive and Reflective Infrared Sensing"". \  \ *** Robustness across users & how is the training done \  \ It has been clarified that all the studies were within user and that the training was made for each individual user. Like most of the previous papers in gesture recognition, cross-participant is left for future research since it would require a large number of representative participants and a reduced gesture set tailored for a specific application. \  \ *** Is the system session-independent, can the device be put back on without re-training? \     \ A detailed cross-session study is still needed but a preliminary feasibility study for correcting shifts between sessions was conducted. More explicit discussion has been added about it: \  \ ""Good cross-session performance is an important requirement often ignored. As found in similar techniques, the main difficulty is sensor misalignment between different sessions. An algorithm to detect and correct placement shift would improve this significantly. Given the accuracy of the calibration shown in the last study, it would be feasible to estimate the orientation of the device and rotate the measurements accordingly."" \  \ *** Underlying principles why the approach works so well \  \ Across the paper there are several mentions to the differences between SensIR and previous IR systems \  \ ""However, these [previous] systems only take a single receiver measurement per emitter. We propose to take measures between all the possibles combinations of emitters and receivers, capturing not only the reflected light but also the amount of light transmitted through the wrist."" \  \ ""Previous IR approaches only emit and receive with the same sensor. We propose to emit and receive with all possible combinations of transmitters and receivers as light passes diffusely through human tissue, obtaining exponentially more features to analyse."" \  \ And in the discussion we have added why it works better: \  \ ""Our results indicate that using all the combinations of emitters and receivers  outperforms previous configurations where only one measurement is taken per emitter/receiver pair. The additional data provided by this new configuration provides extra reflective measurements when the emitter/receiver pair are close, and transmission measurements which indicate orientation and distances between distant emitters and receivers."" \  \  \ *** Clarify some potential issues regarding the study methodology \  \ [clarification of study procedure and reason for within-user] \  \ + ""The study and analysis of the data was performed within-user, and training of the classifier is user dependent due to anatomical differences between users."" \  \ [""As authors provided in the rebuttal, they should clarify what the study is not, so that a reader is clear about the extent of the validation of this technique.""] \  \ ""SensIR seems comparable in accuracy and gesture sets to EMG or electrical impedance tomography with the added advantage of robustness against bad coupling. However, there are many factors such as the machine learning algorithm, number of sensors and study differences that affect the classification outcome. We only claim that SensIR is an improvement over previous IR methods. However, more research should still be conducted with this approach such as cross-session and cross-participant studies."" \  \  \ *** We have added the suggestion of R3 for eliminating background noise \  \ -""A solution would be to put photodiodes on both sides of the bracelet and use each pair as a differential measure to cancel IR noise from the environment."" \ + ""A solution would be to take a measurement of the receivers without emitting, and then use the differential to cancel out any background IR."" \  \ *** We have removed some text that was not completely relevant or added significantly to this short submission. \  \ - ""The system presented here is a prototype designed to test a novel method. All the components are inexpensive and available in surface mounted (SMD) versions, allowing for future designs to have more sensors in the wristband. """,Jess McIntosh,Asier Marzo,FormatComplete,EP/M507994/1,EPSRC Doctoral Training Funding,,,,,Jul 31 13:03,
uistf1723,10/25,13,Displays,10:40:00 AM,12:00:00 PM,4,10:40:00 AM,11:00:00 AM,long,long,none,1,1301,,,uistf1723,A,Qoom: An Interactive Omnidirectional Ball Display,Shio,Miyafuji,miyafuji.s.aa@m.titech.ac.jp,uistf1723-paper.pdf,11,letter,,,"Shio Miyafuji, Zhengqing Li, Toshiki Sato, Hideki Koike","miyafuji.s.aa@m.titech.ac.jp, li.z.ah@m.titech.ac.jp, dendenkamushi@gmail.com, koike@c.titech.ac.jp",60128,Shio,,Miyafuji,miyafuji.s.aa@m.titech.ac.jp,,Tokyo Institute of Technology,Tokyo,,Japan,,,,,,60485,Zhengqing,,Li,li.z.ah@m.titech.ac.jp,,Tokyo Institute of Technology,Tokyo,,Japan,,,,,,12892,Toshiki,,Sato,dendenkamushi@gmail.com,,Tokyo Institute of Technology,Tokyo,,Japan,,,,,,3234,Hideki,,Koike,koike@c.titech.ac.jp,,Tokyo Institute of Technology,Tokyo,,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present a sphere-shaped interactive display system, named Qoom, as a new input and output device. Unlike existing sphere-shaped displays, Qoom is a perfectly spherical ball that can be rotated, thrown, or even kicked.  \ First, we discuss how spherical displays can be used in daily life and describe how users interact with spheres.  \ Then, we show how we developed the Qoom prototype that uses touch and rotation detection, real-time object tracking, and spherical projection mapping.  \ We implemented actions including touching, rotating, bouncing and throwing as controls.  \ We also developed applications for Qoom that utilize the unique advantages of ball displays.",miyafuji.s.aa@m.titech.ac.jp,"1. Eric Akaoka, Tim Ginn, and Roel Vertegaal. 2010. DisplayObjects: Prototyping Functional Physical Interfaces on 3D Styrofoam, Paper or Cardboard Models. In Proceedings of the Fourth International Conference on Tangible, Embedded, and Embodied Interaction (TEI ’10). ACM, New York, NY, USA, 49–56. DOI: http://dx.doi.org/10.1145/1709886.1709897 \ 2. J. Batlle, E. Mouaddib, and J. Salvi. 1998. Recent progress in coded structured light as a technique to solve the correspondence problem: a survey. Pattern Recognition 31, 7 (1998), 963 – 982. DOI: http://dx.doi.org/10.1016/S0031-3203(97)00074-5 \ 3. Hrvoje Benko and Andrew D. Wilson. 2010. Multi-point Interactions with Immersive Omnidirectional Visualizations in a Dome. In ACM International Conference on Interactive Tabletops and Surfaces (ITS ’10). ACM, New York, NY, USA, 19–28. DOI: http://dx.doi.org/10.1145/1936652.1936657 \ 4. Hrvoje Benko, Andrew D. Wilson, and Ravin Balakrishnan. 2008. Sphere: Multi-touch Interactions on a Spherical Display. In Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology (UIST ’08). ACM, New York, NY, USA, 77–86. DOI:http://dx.doi.org/10.1145/1449715.1449729 \ 5. Hrvoje Benko, Andrew D. Wilson, and Federico Zannier. 2014. Dyadic Projected Spatial Augmented Reality. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST ’14). ACM, New York, NY, USA, 645–655. DOI: http://dx.doi.org/10.1145/2642918.2647402 \ 6. John Bolton, Kibum Kim, and Roel Vertegaal. 2011. Privacy and Sharing Information on Spherical and Large Flat Displays. In Proceedings of the ACM 2011 Conference on Computer Supported Cooperative Work (CSCW ’11). ACM, New York, NY, USA, 573–574. DOI: http://dx.doi.org/10.1145/1958824.1958912 \ 7. Li-Wei Chan, Yi-Fan Chuang, Meng-Chieh Yu, Yi-Liu Chao, Ming-Sui Lee, Yi-Ping Hung, and Jane Hsu. 2007. Gesture-based Interaction for a Magic Crystal Ball. In Proceedings of the 2007 ACM Symposium on Virtual Reality Software and Technology (VRST ’07). ACM, New York, NY, USA, 157–164. DOI: http://dx.doi.org/10.1145/1315184.1315214 \ 8. F. Ferreira, M. Cabral, O. Belloc, G. Miller, C. Kurashima, R. de Deus Lopes, I. Stavness, J. Anacleto, M. Zuffo, and S. Fels. 2014. Spheree: A 3D Perspective-corrected Interactive Spherical Scalable Display. In ACM SIGGRAPH 2014 Posters (SIGGRAPH ’14). ACM, New York, NY, USA, Article 86, 1 pages. DOI:http://dx.doi.org/10.1145/2614217.2630585 \ 9. Athanasios Gaitatzes, Georgios Papaioannou, Dimitrios Christopoulos, and Gjergji Zyba. 2006. Media 5https://www.laval-virtual.org/prix-competitions/revolution/palmarevo-16.html Productions for a Dome Display System. In Proceedings of the ACM Symposium on Virtual Reality Software and Technology (VRST ’06). ACM, New York, NY, USA, 261–264. DOI: http://dx.doi.org/10.1145/1180495.1180548 \ 10. David Holman and Roel Vertegaal. 2008. Organic User Interfaces: Designing Computers in Any Way, Shape, or Form. Commun. ACM 51, 6 (June 2008), 48–55. DOI: http://dx.doi.org/10.1145/1349026.1349037 \ 11. David Holman, Roel Vertegaal, Mark Altosaar, Nikolaus Troje, and Derek Johns. 2005. Paper Windows: Interaction Techniques for Digital Paper. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’05). ACM, New York, NY, USA, 591–599. DOI: http://dx.doi.org/10.1145/1054972.1055054 \ 12. T. J. Jankun-Kelly and Kwan-Liu Ma. 2003. MoireGraphs: radial focus+context visualization and interaction for graphs with visual nodes. In IEEE Symposium on Information Visualization 2003 (IEEE Cat. No.03TH8714). 59–66. DOI: http://dx.doi.org/10.1109/INFVIS.2003.1249009 \ 13. Brett Jones, Rajinder Sodhi, Michael Murdock, Ravish Mehra, Hrvoje Benko, Andrew Wilson, Eyal Ofek, Blair MacIntyre, Nikunj Raghuvanshi, and Lior Shapira. 2014. RoomAlive: Magical Experiences Enabled by Scalable, Adaptive Projector-camera Units. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST ’14). ACM, New York, NY, USA, 637–644. DOI: http://dx.doi.org/10.1145/2642918.2647383 \ 14. Brett R. Jones, Hrvoje Benko, Eyal Ofek, and Andrew D. Wilson. 2013. IllumiRoom: Peripheral Projected Illusions for Interactive Experiences. In ACM SIGGRAPH 2013 Emerging Technologies (SIGGRAPH ’13). ACM, New York, NY, USA, Article 7, 1 pages. DOI: http://dx.doi.org/10.1145/2503368.2503375 \ 15. Abhijit Karnik, Archie Henderson, Andrew Dean, Howard Pang, Thomas Campbell, Satoshi Sakurai, Guido Herrmann, Shahram Izadi, Yoshifumi Kitamura, and Sriram Subramanian. 2011. VORTEX: Design and Implementation of an Interactive Volumetric Display. In CHI ’11 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’11). ACM, New York, NY, USA, 2017–2022. DOI: http://dx.doi.org/10.1145/1979742.1979870 \ 16. S. Kettner, C. Madden, and R. Ziegler. 2004. Direct Rotational Interaction With a Spherical Projection. Creativity & Cognition Symposium on Interaction: Systems, Plactice and Theory. (2004). \ 17. Jarrod Knibbe, Hrvoje Benko, and Andrew D. Wilson. 2015. Juggling the Effects of Latency: Software Approaches to Minimizing Latency in Dynamic Projector-Camera Systems. In Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST ’15 Adjunct). ACM, New York, NY, USA, 93–94. DOI: http://dx.doi.org/10.1145/2815585.2815735 \ 18. Hideki Koike and Hiroaki Yamaguchi. 2015. LumoSpheres: Real-time Tracking of Flying Objects and Image Projection for a Volumetric Display. In Proceedings of the 6th Augmented Human International Conference (AH ’15). ACM, New York, NY, USA, 93–96. DOI:http://dx.doi.org/10.1145/2735711.2735824 \ 19. E. Kraft. 2003. A quaternion-based unscented Kalman ﬁlter for orientation tracking. In Sixth International Conference of Information Fusion, 2003. Proceedings of the, Vol. 1. 47–54. DOI: http://dx.doi.org/10.1109/ICIF.2003.177425 \ 20. Zhengqing Li, Shio Miyafuji, Toshiki Sato, and Hideki Koike. 2016. OmniEyeball: Spherical Display Embedded With Omnidirectional Camera Using Dynamic Spherical Mapping. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16 Adjunct). ACM, New York, NY, USA, 193–194. DOI: http://dx.doi.org/10.1145/2984751.2984765 \ 21. Tamotsu Machida. 2002. GEO-COSMOS: World’s First Spherical Display. In ACM SIGGRAPH 2002 Conference Abstracts and Applications (SIGGRAPH ’02). ACM, New York, NY, USA, 189–189. DOI: http://dx.doi.org/10.1145/1242073.1242202 \ 22. Jock D. Mackinlay, George G. Robertson, and Stuart K. Card. 1991. The Perspective Wall: Detail and Context Smoothly Integrated. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’91). ACM, New York, NY, USA, 173–176. DOI: http://dx.doi.org/10.1145/108844.108870 \ 23. Shio Miyafuji, Masato Sugasaki, and Hideki Koike. 2016. Ballumiere: Real-Time Tracking and Spherical Projection for High-Speed Moving Balls. In Proceedings of the 2016 ACM on Interactive Surfaces and Spaces (ISS ’16). ACM, New York, NY, USA, 33–37. DOI: http://dx.doi.org/10.1145/2992154.2992181 \ 24. T. Munzner. 1997. H3: Laying out Large Directed Graphs in 3D Hyperbolic Space. In Proceedings of the 1997 IEEE Symposium on Information Visualization (InfoVis ’97) (INFOVIS ’97). IEEE Computer Society, Washington, DC, USA, 2–. http://dl.acm.org/citation.cfm?id=857188.857627 \ 25. Kohei Okumura, Hiromasa Oku, and Masatoshi Ishikawa. 2012. Lumipen: Projection-Based Mixed Reality for Dynamic Objects. In Proceedings of the 2012 IEEE International Conference on Multimedia and Expo (ICME ’12). IEEE Computer Society, Washington, DC, USA, 699–704. DOI: http://dx.doi.org/10.1109/ICME.2012.34 \ 26. Oyewole Oyekoya, William Steptoe, and Anthony Steed. 2012. SphereAvatar: A Situated Display to Represent a Remote Collaborator. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’12). ACM, New York, NY, USA, 2551–2560. DOI: http://dx.doi.org/10.1145/2207676.2208642 \ 27. Y. Pan and A. Steed. 2012. Preserving gaze direction in teleconferencing using a camera array and a spherical display. In 2012 3DTV-Conference: The True Vision Capture, Transmission and Display of 3D Video (3DTV-CON). 1–4. DOI: http://dx.doi.org/10.1109/3DTV.2012.6365433 \ 28. Ye Pan, William Steptoe, and Anthony Steed. 2014. Comparing Flat and Spherical Displays in a Trust Scenario in Avatar-mediated Interaction. In Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI ’14). ACM, New York, NY, USA, 1397–1406. DOI: http://dx.doi.org/10.1145/2556288.2557276 \ 29. Tomislav Pejsa, Julian Kantor, Hrvoje Benko, Eyal Ofek, and Andrew Wilson. 2016. Room2Room: Enabling Life-Size Telepresence in a Projected Augmented Reality Environment. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing (CSCW ’16). ACM, New York, NY, USA, 1716–1725. DOI: http://dx.doi.org/10.1145/2818048.2819965 \ 30. Ramesh Raskar, Greg Welch, Kok-Lim Low, and Deepak Bandyopadhyay. 2001. Shader Lamps: Animating Real Objects With Image-Based Illumination. In Proceedings of the 12th Eurographics Workshop on Rendering Techniques. Springer-Verlag, London, UK, UK, 89–102. http://dl.acm.org/citation.cfm?id=647653.732300 \ 31. George G. Robertson and Jock D. Mackinlay. 1993. The Document Lens. In Proceedings of the 6th Annual ACM Symposium on User Interface Software and Technology (UIST ’93). ACM, New York, NY, USA, 101–108. DOI: http://dx.doi.org/10.1145/168642.168652 \ 32. S. Sabatelli, F. Sechi, L. Fanucci, and A. Rocchi. 2011. A sensor fusion algorithm for an integrated angular position estimation with inertial measurement units. In 2011 Design, Automation Test in Europe. 1–4. DOI: http://dx.doi.org/10.1109/DATE.2011.5763273 \ 33. Dominik Schmidt, Raf Ramakers, Esben W. Pedersen, Johannes Jasper, Sven Köhler, Aileen Pohl, Hannes Rantzsch, Andreas Rau, Patrick Schmidt, Christoph Sterz, Yanina Yurchenko, and Patrick Baudisch. 2014. Kickables: Tangibles for Feet. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’14). ACM, New York, NY, USA, 3143–3152. DOI: http://dx.doi.org/10.1145/2556288.2557016 \ 34. Chihiro Suga and Itiro Siio. 2011. Anamorphicons: An Extended Display with a Cylindrical Mirror. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces (ITS ’11). ACM, New York, NY, USA, 242–243. DOI: http://dx.doi.org/10.1145/2076354.2076396 \ 35. Julie R. Williamson, John Williamson, Daniel Sundén, and Jay Bradley. 2015. Multi-Player Gaming on Spherical Displays. In Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA ’15). ACM, New York, NY, USA, 355–358. DOI: http://dx.doi.org/10.1145/2702613.2725447 \ ",Interactive display; Spherical display; Ball display; Entertainment,"H.5.m., H.5.1, H.5.2",uistf1723-file1.zip,uistf1723-file2.jpg,uistf1723-file3.mp4,,"Qoom is a sphere-shaped interactive display system. \ Unlike existing sphere-shaped displays, Qoom is a perfectly spherical ball that can be rotated, thrown, or even kicked. ",,"We present a sphere-shaped interactive display system, named Qoom, as a new input and output device. Unlike existing sphere-shaped displays, Qoom is a perfectly spherical ball that can be rotated, thrown, or even kicked. ","As we wrote in the rebuttal, we changed the following points: \ ・included the missing related work \ ・added an explanation about the usefulness of the applications \ ・added and changed a technical explanation of our system \ ・added an explanation of interaction designs and implementations \ ・added the limitation of this system  \ ・revised the paper with native speakers (removed some ambiguous sentences) \  \ Because of the revisions mentioned above, the length of the paper increases by half a page.",Shio Miyafuji,Hideki Koike,FormatComplete,,,,,,,Aug 7 2:02,
uistf2588,10/25,13,Displays,10:40:00 AM,12:00:00 PM,4,11:00:00 AM,11:20:00 AM,long,long,uistf1723,2,1302,,,uistf2588,A,HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior,Andreas,Fender,andreasfender@cs.au.dk,uistf2588-paper.pdf,11,letter,,,"Andreas Fender, David Lindlbauer, Philipp Herholz, Marc Alexa, Joerg Mueller","andreasfender@cs.au.dk, david.lindlbauer@tu-berlin.de, Philipp.Herholz@tu-berlin.de, marc.alexa@tu-berlin.de, joerg.mueller@acm.org",41759,Andreas,,Fender,andreasfender@cs.au.dk,Department of Computer Science,Aarhus University,Aarhus,,Denmark,,,,,,23098,David,,Lindlbauer,david.lindlbauer@tu-berlin.de,,TU Berlin,Berlin,,Germany,,,,,,71716,Philipp,,Herholz,Philipp.Herholz@tu-berlin.de,Computer Graphics,TU Berlin,Berlin,Berlin,Germany,,,,,,18690,Marc,,Alexa,marc.alexa@tu-berlin.de,,TU Berlin,Berlin,,Germany,,,,,,13650,Joerg,,Mueller,joerg.mueller@acm.org,Department of Computer Science,Aarhus University,Aarhus,,Denmark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.",andreasfender@cs.au.dk,"1. Blaine Bell, Steven Feiner, and Tobias Höllerer. 2001. View Management for Virtual and Augmented Reality. In Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology (UIST ’01). ACM, New York, NY, USA, 101–110. DOI: http://dx.doi.org/10.1145/502348.502363 \ 2. Michael L Benedikt. 1979. To take hold of space: isovists and isovist ﬁelds. Environment and Planning B: Planning and design 6, 1 (1979), 47–65. \ 3. Oliver Bimber, Daisuke Iwai, Gordon Wetzstein, and Anselm Grundhöfer. 2008. The Visual Computing of Projector-camera Systems. In ACM SIGGRAPH 2008 Classes (SIGGRAPH ’08). ACM, New York, NY, USA, Article 84, 25 pages. DOI:http://dx.doi.org/10.1145/1401132.1401239 \ 4. Jeremy Birnholtz, Lindsay Reynolds, Eli Luxenberg, Carl Gutwin, and Maryam Mustafa. 2010. Awareness Beyond the Desktop: Exploring Attention and Distraction with a Projected Peripheral-vision Display. In Proceedings of Graphics Interface 2010 (GI ’10). Canadian Information Processing Society, Toronto, Ont., Canada, Canada, 55–62. http://dl.acm.org/citation.cfm?id=1839214.1839225 \ 5. Carolina Cruz-Neira, Daniel J. Sandin, and Thomas A. DeFanti. 1993. Surround-screen Projection-based Virtual Reality: The Design and Implementation of the CAVE. In Proceedings of the 20th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH ’93). ACM, New York, NY, USA, 135–142. DOI: http://dx.doi.org/10.1145/166117.166134 \ 6. Jakub Dostal, Per Ola Kristensson, and Aaron Quigley. 2013. Subtle Gaze-dependent Techniques for Visualising Display Changes in Multi-display Environments. In Proceedings of the 2013 International Conference on Intelligent User Interfaces (IUI ’13). ACM, New York, NY, USA, 137–148. DOI: http://dx.doi.org/10.1145/2449396.2449416 \ 7. Bernard Ghanem, Yuanhao Cao, and Peter Wonka. 2015. Designing Camera Networks by Convex Quadratic Programming. Comput. Graph. Forum 34, 2 (May 2015), 69–80. DOI: http://dx.doi.org/10.1111/cgf.12542 \ 8. Christopher G. Healey and Amit P. Sawant. 2012. On the Limits of Resolution and Visual Angle in Visualization. ACM Trans. Appl. Percept. 9, 4, Article 20 (Oct. 2012), 21 pages. DOI: http://dx.doi.org/10.1145/2355598.2355603 \ 9. B. Johanson, A. Fox, and T. Winograd. 2002. The Interactive Workspaces project: experiences with ubiquitous computing rooms. IEEE Pervasive Computing 1, 2 (April 2002), 67–74. DOI: http://dx.doi.org/10.1109/MPRV.2002.1012339 \ 10. Brett Jones, Rajinder Sodhi, Michael Murdock, Ravish Mehra, Hrvoje Benko, Andrew Wilson, Eyal Ofek, Blair MacIntyre, Nikunj Raghuvanshi, and Lior Shapira. 2014. RoomAlive: Magical Experiences Enabled by Scalable, Adaptive Projector-camera Units. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST ’14). ACM, New York, NY, USA, 637–644. http://doi.acm.org/10.1145/2642918.2647383 \ 11. Christian Lander, Sven Gehring, Antonio Krüger, Sebastian Boring, and Andreas Bulling. 2015. GazeProjector: Accurate Gaze Estimation and Seamless Gaze Interaction Across Multiple Displays. In Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology (UIST ’15). ACM, New York, NY, USA, 395–404. DOI: http://dx.doi.org/10.1145/2807442.2807479 \ 12. Lars Lischke, Sven Mayer, Katrin Wolf, Niels Henze, Harald Reiterer, and Albrecht Schmidt. 2016. Screen Arrangements and Interaction Areas for Large Display Work Places. In Proceedings of the 5th ACM International Symposium on Pervasive Displays (PerDis ’16). ACM, New York, NY, USA, 228–234. DOI: http://dx.doi.org/10.1145/2914920.2915027 \ 13. Aaron Mavrinac and Xiang Chen. 2013. Modeling Coverage in Camera Networks: A Survey. International Journal of Computer Vision 101, 1 (2013), 205–226. DOI: http://dx.doi.org/10.1007/s11263-012-0587-7 \ 14. Lori McCay-Peet, Mounia Lalmas, and Vidhya Navalpakkam. 2012. On Saliency, Aﬀect and Focused Attention. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’12). ACM, New York, NY, USA, 541–550. DOI: http://dx.doi.org/10.1145/2207676.2207751 \ 15. Joseph O’Rourke. 1987. Art Gallery Theorems and Algorithms. Oxford University Press, Inc., New York, NY, USA. \ 16. Thies Pfeiﬀer. 2012. Measuring and Visualizing Attention in Space with 3D Attention Volumes. In Proceedings of the Symposium on Eye Tracking Research and Applications (ETRA ’12). ACM, New York, NY, USA, 29–36. DOI: http://dx.doi.org/10.1145/2168556.2168560 \ 17. Miguel A. Nacenta PhD, Carl Gutwin, Dzmitry Aliakseyeu, and Sriram Subramanian. 2009. There and Back Again: Cross-Display Object Movement in Multi-Display Environments. Human–Computer Interaction 24, 1-2 (2009), 170–229. DOI: http://dx.doi.org/10.1080/07370020902819882 \ 18. Claudio S. Pinhanez. 2001. The Everywhere Displays Projector: A Device to Create Ubiquitous Graphical Interfaces. In Proceedings of the 3rd International Conference on Ubiquitous Computing (UbiComp ’01). Springer-Verlag, London, UK, UK, 315–331. http://dl.acm.org/citation.cfm?id=647987.741324 \ 19. Marc Pomplun, Helge Ritter, and Boris Velichkovsky. 1996. Disambiguating Complex Visual Information: Towards Communication of Personal Views of a Scene. Perception 25, 8 (1996), 931–948. DOI:http://dx.doi.org/10.1068/p250931 PMID: 8938007. \ 20. Umar Rashid, Miguel A. Nacenta, and Aaron Quigley. 2012. Factors Inﬂuencing Visual Attention Switch in Multi-display User Interfaces: A Survey. In Proceedings of the 2012 International Symposium on Pervasive Displays (PerDis ’12). ACM, New York, NY, USA, Article 1, 6 pages. DOI:http://dx.doi.org/10.1145/2307798.2307799 \ 21. George Robertson, Mary Czerwinski, Patrick Baudisch, Brian Meyers, Daniel Robbins, Greg Smith, and Desney Tan. 2005. The Large-Display User Experience. IEEE Comput. Graph. Appl. 25, 4 (July 2005), 44–51. DOI: http://dx.doi.org/10.1109/MCG.2005.88 \ 22. Brian A. Smith, Qi Yin, Steven K. Feiner, and Shree K. Nayar. 2013. Gaze Locking: Passive Eye Contact Detection for Human-object Interaction. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST ’13). ACM, New York, NY, USA, 271–280. DOI: http://dx.doi.org/10.1145/2501988.2501994 \ 23. Sophie Stellmach, Lennart Nacke, and Raimund Dachselt. 2010. 3D Attentional Maps: Aggregated Gaze Visualizations in Three-dimensional Virtual Environments. In Proceedings of the International Conference on Advanced Visual Interfaces (AVI ’10). ACM, New York, NY, USA, 345–348. DOI: http://dx.doi.org/10.1145/1842993.1843058 \ 24. Norbert A. Streitz, Jörg Geißler, Torsten Holmer, Shin’ichi Konomi, Christian Müller-Tomfelde, Wolfgang Reischl, Petra Rexroth, Peter Seitz, and Ralf Steinmetz. 1999. i-LAND: An Interactive Landscape for Creativity and Innovation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’99). ACM, New York, NY, USA, 120–127. DOI: http://dx.doi.org/10.1145/302979.303010 \ 25. Yusuke Sugano, Xucong Zhang, and Andreas Bulling. 2016. AggreGaze: Collective Estimation of Audience Attention on Public Displays. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 821–831. DOI: http://dx.doi.org/10.1145/2984511.2984536 \ 26. Tasos Varoudis and Sophia Psarra. 2014. Beyond two dimensions: architecture through three dimensional visibility graph analysis. The Journal of Space Syntax 5, 1 (2014), 91–108. \ 27. James R. Wallace, Stacey D. Scott, and Carolyn G. MacGregor. 2013. Collaborative Sensemaking on a Digital Tabletop and Personal Tablets: Prioritization, Comparisons, and Tableaux. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’13). ACM, New York, NY, USA, 3345–3354. DOI: http://dx.doi.org/10.1145/2470654.2466458 \ 28. Robert Walter, Andreas Bulling, David Lindlbauer, Martin Schuessler, and Jörg Müller. 2015. Analyzing Visual Attention During Whole Body Interaction with Public Displays. In Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp ’15). ACM, New York, NY, USA, 1263–1267. DOI: http://dx.doi.org/10.1145/2750858.2804255 \ 29. David S. Wooding. 2002. Fixation Maps: Quantifying Eye-movement Traces. In Proceedings of the 2002 Symposium on Eye Tracking Research & Applications (ETRA ’02). ACM, New York, NY, USA, 31–36. DOI: http://dx.doi.org/10.1145/507072.507078 \ 30. Martina Zieﬂe. 1998. Eﬀects of Display Resolution on Visual Performance. Human Factors 40, 4 (1998), 554–568. DOI: http://dx.doi.org/10.1518/001872098779649355 \ ",Multi-display environments; display placement; analyzing user behavior,H.5.m,uistf2588-file1.zip,,uistf2588-file3.wmv,,,,We present a novel approach of analyzing user viewing behavior to optimize display placement.,"Major changes are highlighted in the attached auxiliary material. \ EDIT: the highlighted PDF was removed for camera ready. \  \ Changes include: \ -Extended limitations discussion \ -    ""The same [as roomalive] would be achievable with CAVE systems"" \  -->""CAVE systems have similar capabilities"" \ -""... related to the geometry of a space"" --> ""... related to the geometry within a voxel"" \ -""We calculate for each voxel"" --> ""For each voxel we calculate"" \ -""samples that does not"" --> ""samples, which do not"" \ -removed ""position paintings in an art gallery"" \ -merged Discussion and Future work \     -removed ""Discussion->Voxel representation"" \ -removed from ""Heatmap generation->Occlusion"" \     ""Note that we perform raycasting and intersections directly with M_i with back-face culling enabled. \      This avoids self-occlusion from voxels that currently represent the user's head."" \ -removed from ""Heatmap generation->Occlusion"": \     ""(i.e., the voxel is occluded)"" \ -changed parts of the ""Editor for installers"" sub section \ -rewrote parts of the descriptions of the example scenarios \  \ Minor changes like corrections of typos and rearrangements are not documented.",Andreas Fender,David Lindlbauer,FormatComplete,3067-00001B,Innovation Fund Denmark,,,,,Aug 8 18:48,
uistf1491,10/25,13,Displays,10:40:00 AM,12:00:00 PM,4,11:20:00 AM,11:40:00 AM,long,long,uistf2588,3,1303,,,uistf1491,A,Pepper's Cone: An Inexpensive Do-It-Yourself 3D Display,Xuan,Luo,xuanluo@cs.washington.edu,uistf1491-paper.pdf,11,letter,,,"Xuan Luo, Jason Lawrence, Steven M. Seitz","xuanluo@cs.washington.edu, jdlaw@google.com, seitz@cs.washington.edu",71315,Xuan,,Luo,xuanluo@cs.washington.edu,,University of Washington,Seattle,Washington,United States,,,,,,73749,Jason,,Lawrence,jdlaw@google.com,"Google Inc., Seattle, Washington, United States",Google Inc.,Seattle,Washington,United States,,,,,,5749,Steven,M.,Seitz,seitz@cs.washington.edu,,University of Washington,Seattle,Washington,United States,,Google Inc.,Seattle,Washington,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper describes a simple 3D display that can be built from a tablet computer and a plastic sheet folded into a cone. This display allows naturally viewing a three-dimensional object from any direction over a 360-degree path of travel without the use of a head mount or special glasses. Inspired by the classic Pepper's Ghost illusion, our approach uses a curved transparent surface to reflect the image displayed on a 2D display. By properly pre-distorting the displayed image our system can produce a perspective-correct image to the viewer that appears to be suspended inside the reflector. We use the gyroscope integrated into modern tablet computers to adjust the rendered image based on the relative orientation of the viewer. The end result is a natural and intuitive interface for inspecting a 3D object.  Our choice of a cone reflector is obtained by analyzing optical performance and stereo-compatibility over  rotationally-symmetric conic reflector shapes. \ We also present the prototypes we built and measure the performance of our display through side-by-side comparisons with reference images.",xuanluo@cs.washington.edu,"1. Arena3D. 2015. Arena3D Industrial Illusion. http://arena3d.com/. (2015). \ 2. Tibor Balogh, Péter Tamás Kovács, and Zoltán Megyesi. 2007. Holovizio 3D display system. In Proceedings of the First International Conference on Immersive Telecommunications. ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering), 19. \ 3. Peter C. Barnum, Srinivasa G. Narasimhan, and Takeo Kanade. 2010. A Multi-layered Display with Water Drops. ACM Transactions on Graphics 29, 4 (2010), 76:1–76:7. \ 4. Oliver Bimber, Bernd Fröhlich, Dieter Schmalstieg, and L Miguel Encarnação. 2006. The virtual showcase. In ACM SIGGRAPH 2006 Courses. ACM, 9. \ 5. P-A Blanche, A Bablumian, R Voorakaranam, C Christenson, W Lin, T Gu, D Flores, P Wang, W-Y Hsieh, M Kathaperumal, and others. 2010. Holographic three-dimensional telepresence using large-area photorefractive polymer. Nature 468, 7320 (2010), 80–83. \ 6. Jeremy Brooker. 2007. The Polytechnic Ghost. Early Popular Visual Culture 5, 2 (2007), 189–206. \ 7. Grigore Burdea and Philippe Coiffet. 2003. Virtual reality technology. Presence: Teleoperators and virtual environments 12, 6 (2003), 663–664. \ 8. Dimensional Studioson going. Dimensional Studios. http://www.eyeliner3d.com/. (on going). \ 9. Neil A. Dodgson. 2005. Autostereoscopic 3D Displays. Computer 38, 8 (2005), 31–36. \ 10. Garry A Einicke and Langford B White. 1999. Robust extended Kalman ﬁltering. IEEE Transactions on Signal Processing 47, 9 (1999), 2596–2599. \ 11. Googleon going. Google Cardboard SDK for Unity. https://developers.google.com/cardboard/unity/. (on going). \ 12. Hologram USAon going. Hologram USA. http://www.hologramusa.com/. (on going). \ 13. Holus. 2015. Holus. http://hplustech.com/. (2015). \ 14. Michael Huebschman, Bala Munjuluri, and Harold Garner. 2003. Dynamic holographic 3-D image projection. Optics Express 11, 5 (2003), 437–445. \ 15. Haruo Isono, Minoru Yasuda, and Hideaki Sasazawa. 1993. Autostereoscopic 3-D display using LCD-generated parallax barrier. Electronics and Communications in Japan (Part II: Electronics) 76, 7 (1993), 77–84. \ 16. Andrew Jones, Ian McDowall, Hideshi Yamada, Mark Bolas, and Paul Debevec. 2007. An Interactive 360&Deg; Light Field Display. In ACM SIGGRAPH 2007 Emerging Technologies. \ 17. Andrew Jones, Jonas Unger, Koki Nagano, Jay Busch, Xueming Yu, Hsuan-Yueh Peng, Oleg Alexander, and Paul Debevec. 2014. Creating a Life-sized Automultiscopic Morgan Spurlock for CNNs ""Inside Man"". In ACM SIGGRAPH 2014 Talks. \ 18. Yoshihiro Kajiki, Hiroshi Yoshikawa, and Toshio Honda. 1996. 3D display with focused light array. Proceedings of SPIE 2652 (1996), 106–116. \ 19. Douglas Lanman, Gordon Wetzstein, Matthew Hirsch, Wolfgang Heidrich, and Ramesh Raskar. 2011. Polarization Fields: Dynamic Light Field Display Using Multi-layer LCDs. ACM Transactions on Graphics 30, 6 (2011), 186:1–186:10. \ 20. Andrew Lippman. 1980. Movie-maps: An application of the optical videodisc to computer graphics. In Acm Siggraph Computer Graphics, Vol. 14. ACM, 32–42. \ 21. Keiichi Maeno, Naoki Fukaya, Osamu Nishikawa, Koki Sato, and Toshio Honda. 1996. Electro-holographic display using 15mega pixels LCD. Proceedings of SPIE 2652 (1996), 15–23. \ 22. Wojciech Matusik and Hanspeter Pﬁster. 2004. 3D TV: A Scalable System for Real-time Acquisition, Transmission, and Autostereoscopic Display of Dynamic Scenes. ACM Transactions on Graphics 23, 3 (2004), 814–824. \ 23. Musion Das Hologram Ltd. since 1996. Musion. http://musion.com/. (since 1996). \ 24. Susumu Nakajima, Kozo Nakamura, Ken Masamune, Ichiro Sakuma, and Takeyoshi Dohi. 2001. Three-dimensional medical imaging display with computer-generated integral photography. Computerized Medical Imaging and Graphics 25, 3 (2001), 235–241. \ 25. S. K. Nayar and V. N. Anand. 2007. 3D Display Using Passive Optical Scatterers. Computer 40, 7 (2007), 54–63. \ 26. Damin Qin, Mamoru Takamatsu, and Yoshio Nakashima. 2004. Measurement for the Panum’s fusional area in retinal fovea using a three-dimension display device. Journal of Light & Visual Environment 28, 3 (2004), 126–131. \ 27. Eric G Rawson. 1969. Vibrating varifocal mirrors for 3-D imaging. IEEE Spectrum 6, 9 (1969), 37–43. \ 28. L. Sawalha, M. P. Tull, M. B. Gately, J. J. Sluss, M. Yeary, and R. D. Barnes. 2012. A Large 3D Swept-Volume Video Display. Journal of Display Technology 8, 5 (2012), 256–268. \ 29. Maurice Stanley, Patrick B Conway, Stuart D Coomber, John C Jones, Dave C Scattergood, Christopher W Slinger, Robert W Bannister, Carl V Brown, William A Crossland, and Adrian RL Travis. 2000. Novel electro-optic modulator system for the production of dynamic images from giga-pixel computer-generated holograms. In Electronic Imaging. International Society for Optics and Photonics, 13–22. \ 30. Chihiro Suga and Itiro Siio. 2011. Anamorphicons: An extended display with a cylindrical mirror. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces. ACM, 242–243. \ 31. Ivan E. Sutherland. 1968. A Head-mounted Three Dimensional Display. In Proceedings of the December 9-11, 1968, Fall Joint Computer Conference, Part I (AFIPS ’68 (Fall, part I)). ACM, New York, NY, USA, 757–764. DOI: http://dx.doi.org/10.1145/1476589.1476686 \ 32. Rahul Swaminathan, Michael D Grossberg, and Shree K Nayar. 2006. Non-single viewpoint catadioptric cameras: Geometry and analysis. International Journal of Computer Vision 66, 3 (2006), 211–229. \ 33. Sava¸s Tay, P-A Blanche, R Voorakaranam, AV Tunc, W Lin, S Rokutanda, T Gu, D Flores, P Wang, G Li, and others. 2008. An updatable holographic three-dimensional display. Nature 451, 7179 (2008), 694–698. \ 34. The Smithsonian Institute2014. Smithsonian X 3D. http://3d.si.edu/. (2014). \ 35. G. Wetzstein, D. Lanman, W. Heidrich, and R. Raskar. 2011. Layered 3D: Tomographic Image Synthesis for Attenuation-based Light Field and High Dynamic Range Displays. ACM Trans. Graph. 30, 4 (2011). \ 36. Gordon Wetzstein, Douglas Lanman, Matthew Hirsch, and Ramesh Raskar. 2012. Tensor Displays: Compressive Light Field Synthesis Using Multilayer Displays with Directional Backlighting. ACM Transactions on Graphics 31, 4 (2012), 80:1–80:11. \ 37. Xinxing Xia, Xu Liu, Haifeng Li, Zhenrong Zheng, Han Wang, Yifan Peng, and Weidong Shen. 2013. A 360-degree ﬂoating 3D display based on light ﬁeld regeneration. Optics Express 21, 9 (2013), 11237–11247. \ 38. Douglas E Zongker, Dawn M Werner, Brian Curless, and David H Salesin. 1999. Environment matting and compositing. In Proceedings of ACM SIGGRAPH. 205–214. \ ",3D Displays; Pepper's Ghost; Computer Graphics; Augmented Reality,I.3.1; I.3.7; H.5.1,uistf1491-file1.zip,,uistf1491-file3.mp4,uistf1491-file4.zip,,The prototype with binocular correction is shown in high resolution. Please wear red-cyan anaglyph glasses to view these images.,Fold a piece of plastic sheet into a cone. Together with your tablet you can build the Pepper’s Cone to observe your 3D scene in a fun and compelling way.,"We changed the title to be more expressive \ We added the comparison with the two references that the committee raised. \ The texts regards direct manipulation has been changed. \ As to usability/sensitivity to non-ideal eye location, we added texts and images captured at non-ideal view positions in Section Conclusion and Future Work. We also added video captured with camera sliding over different axes.",Xuan Luo,Jason Lawrence,FormatComplete,IIS-1538618,National Science Foundation,Google,,UW Animation Research Labs,,Aug 9 0:03,
uistf3335,10/25,13,Displays,10:40:00 AM,12:00:00 PM,4,11:40:00 AM,12:00:00 PM,long,short,uistf1491,4,1304,,"only 7 pages -- do we want to give it a full talk slot?; has to be separated from 2895 in time (parallel session, same authors)

keep long? -KZG",uistf3335,A,iSphere: Self-Luminous Spherical Drone Display,Wataru,Yamada,wataruyamada@acm.org,uistf3335-paper.pdf,9,letter,,,"Wataru Yamada, Kazuhiro Yamada, Hiroyuki Manabe, Daizo Ikeda","wataruyamada@acm.org, yamadakazu@nttdocomo.com, manabehiroyuki@acm.org, ikedad@nttdocomo.com",45181,Wataru,,Yamada,wataruyamada@acm.org,Research Labs,NTT DOCOMO,Yokosuka,Kanagawa,Japan,,,,,,71782,Kazuhiro,,Yamada,yamadakazu@nttdocomo.com,,NTT DOCOMO,Chiyoda,Tokyo,Japan,,,,,,36743,Hiroyuki,,Manabe,manabehiroyuki@acm.org,Research Labs,NTT DOCOMO,Yokosuka,Kanagawa,Japan,,,,,,71783,Daizo,,Ikeda,ikedad@nttdocomo.com,Research Labs,NTT DOCOMO,Yokosuka,Kanagawa,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present iSphere, a flying spherical display that can display high resolution and bright images in all directions from anywhere in 3D space. Our goal is to build a new platform which can physically and directly emerge arbitrary bodies in the real world. iSphere flies by itself using a built-in drone and creates a spherical display by rotating arcuate multi light-emitting diode (LED) tapes around the drone. As a result of the persistence of human vision, we see it as a spherical display flying in the sky.  The proposed method yields large display surfaces, high resolution, drone mobility, high visibility and 360° field of view. Previous approaches fail to match these characteristics, because of problems with aerodynamics and payload. We construct a prototype and validate the proposed method. The unique characteristics and benefits of flying spherical display surfaces are discussed and we describe application scenarios based on iSphere such as guidance, signage and telepresence.",wataruyamada@acm.org,"1. Ronald T. Azuma. 1997. A Survey of Augmented Reality. Presence: Teleoperators and Virtual Environments 6, 7 (1997), 355–385. \ 2. Barnum, Peter C., Srinivasa G. Narasimhan, and Takeo Kanade. 2010. A multi-layered display with water drops. In ACM Transactions on Graphics (TOG’10), Vol. 29. Article No. 76. \ 3. Jessica R. Cauchard, Jane L. E, Kevin Y. Zhai, and James A. Landay. 2015. Drone & me: an exploration into natural human-drone interaction. In Proceedings of the ACM International Joint Conference on Pervasive and Ubiquitous Computing (UBICOMP’15). 361–365. \ 4. Wilson Andrew D. 2004. TouchLight: An Imaging Touch Screen and Display for Gesture-Based Interaction. In Proceedings of the international conference on Multimodal interfaces (ICMI’04). 69–76. \ 5. Seth C. Goldstein and Todd C. Mowry. 2004. Claytronics: A Scalable Basis For Future Robots. In Proceedings of RoboSphere. \ 6. Antonio Gomes, Calvin Rubens, Sean Braley, and Roel Vertegaal. 2016. BitDrones: Towards Using 3D Nanocopter Displays as Interactive Self-Levitating Programmable Matter. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI’16). 770–780. \ 7. Norihisa Hashimoto, Yoshitsugu Yanagi, and Yoshiharu Deguchi. 2012. Panorama Ball Vision : A Small Spherical Display with Rotary LED Arrays. In Transactions of the Virtual Reality Society of Japan (VRSJ’12), Vol. 17. 151–160. \ 8. Nozaki Hiroki. 2014. Flying display: a movable display pairing projector and screen in the air. In CHI’14 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’14), Vol. 909-914. \ 9. David Holman and Roel Vertegaal. 2008. Organic user interfaces: designing computers in any way, shape, or form. Commun. ACM 51, 6 (2008), 48–55. \ 10. David Holman, Roel Vertegaal, Mark Altosaar, Nico Troje, and Derek Johns. 2005. PaperWindows: Interaction techniques for digital paper. In Proceedings of the SIGCHI conference on Human factors in computing systems (CHI’05). 591–599. \ 11. Benko Hrvoje, Andrew D. Wilson, and Ravin Balakrishnan. 2008. Sphere: multi-touch interactions on a spherical display. In Proceedings of the ACM symposium on User interface software and technology (UIST’08). 77–86. \ 12. Intel. 2016. Intel-based Drone Technology Pushes Boundaries. (2016). Retrieved March 19, 2017 from http://www.intel.com/content/www/us/en/ technology-innovation/aerial-technology-overview.html. \ 13. Hiroshi Ishii, David Lakatos, Leonard Bonanni, and Jean-Baptiste Labrune. 2012. Radical atoms: beyond tangible bits, toward transformable materials. interactions 19, 1 (2012), 38–51. \ 14. Satoshi Iwaki, Hiroshi Morimasa, Toshiro Noritsugu, and Minoru Kobayashi. 2011. Contactless manipulation of an object on a plane surface using multiple air jets. In Proceedings of IEEE International Conference on Robotics and Automation (ICRA’11). 3257–3262. \ 15. Rekimoto Jun. 1997. NaviCam: A magnifying glass approach to augmented reality. Presence: Teleoperators and Virtual Environments 6, 4 (1997), 399–412. \ 16. Mustafa Karagozler, Brian Kirby, Wei Jie Lee, Eugene Marinelli, and Tze Chang Ng. 2006. Ultralight modular robotic building blocks for the rapid deployment of planetary outposts. In Revolutionary Aerospace Systems Concepts Academic Linkage Forum (RASC-AL’06). \ 17. Pfeil Kevin, Seng Lee Koh, and Joseph LaViola. 2013. Exploring 3D gesture metaphors for interaction with unmanned aerial vehicles. In Proceedings of the international conference on Intelligent user interfaces (IUI’13). 257–266. \ 18. Hidei Kimura, Taro Uchiyama, and Hiroyuki Yoshikawa. 2006. Laser produced 3D display in the air. In ACM SIGGRAPH 2006 Emerging technologies. 20. \ 19. MIT Senseable City Lab. 2010. Flyﬁre. (2010). Retrieved March 19, 2017 from http://senseable.mit.edu/flyfire/. \ 20. Jinha Lee, Rehmi Post, and Hiroshi Ishii. 2011. ZeroN: mid-air tangible interaction enabled by computer controlled magnetic levitation. In Proceedings of the ACM symposium on User interface software and technology (UIST’11). 327–336. \ 21. MicroAd. 2016. SKY MAGIC. (2016). Retrieved March 19, 2017 from https://magic.microad.co.jp/skymagic/. \ 22. MicroSoft. 2015. Hololens. (2015). Retrieved March 19, 2017 from https://www.microsoft.com/microsoft-hololens/. \ 23. Miraikan. 2001. Geo Cosmos. (2001). Retrieved March 19, 2017 from https://www.miraikan.jst.go.jp/ exhibition/tsunagari/geo-cosmos.html. \ 24. Kei Nitta, Keita Higuchi, and Jun Rekimoto. 2014. HoverBall: Augmented sports with a ﬂying ball. In Proceedings of the Augmented Human International Conference (AH’14). Article No.13. \ 25. Yoichi Ochiai, Takayuki Hoshi, and Jun Rekimoto. 2014. Pixie Dust: Graphics Generated by Levitated and Animated Objects in Computational Acoustic-Potential Field. In ACM Transactions on Graphics (TOG’14), Vol. 33. 85. \ 26. Yoichi Ochiai, Kota Kumagai, Takayuki Hoshi, Jun Rekimoto, Satoshi Hasegawa, and Yoshio Hayasaki. 2016. Fairy Lights in Femtoseconds: Aerial and Volumetric Graphics Rendered by Focused Femtosecond Laser Combined with Computational Holographic Fields. In ACM Transactions on Graphics (TOG’16), Vol. 35. 17. \ 27. Caudell Thomas P. and David W. Mizell. 1992. Augmented reality: an application of heads-up display technology to manual manufacturing processes. In Proceedings of the Hawaii International Conference on System Science (HICSS’92), Vol. 2. 659–669. \ 28. Parrot. 2012. ARdrone 2.0. (2012). Retrieved March 19, 2017 from https://www.parrot.com/fr/drones/ parrot-ardrone-20-elite-édition. \ 29. Ben Piper, Carlo Ratti, and Hiroshi Ishii. 2002. Illuminating clay: a 3-D tangible interface for landscape analysis. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI’02). 355–362. \ 30. Ismo Rakkolainen, Stephen DiVerdi, Alex Olwal, Nicola Candussi, Tobias Hüllerer, Markku Laitinen, Mika Piirto, and Karri Palovuori. 2005. The interactive FogScreen. In ACM SIGGRAPH 2005 Emerging technologies. 8. \ 31. Ramesh Raskar, Greg Welch, Kok-Lim Low, and Deepak Bandyopadhyay. 2001. Shader Lamps: Animating real objects with image-based illumination. In Proceedings of the Eurographics Workshop on Rendering Techniques (EGWR’01). 89–102. \ 32. Kettner S., C. Madden, and Ziegler R. 2004. Direct Rotational Interaction with a Spherical Projection. In Creativity & Cognition Symposium on Interaction: Systems, Practice and Theory. \ 33. Jürgen Scheible and Markus Funk. 2016. In-situ-displaydrone: facilitating co-located interactive experiences via a ﬂying screen. In Proceedings of the ACM International Symposium on Pervasive Displays (PERDIS’16). 251–252. \ 34. Jürgen Scheible, Achim Hoth, Julian Saal, and Haifeng Su. 2013. Displaydrone: a ﬂying robot based interactive display. In Proceedings of the ACM International Symposium on Pervasive Displays (PERDIS’13). 49–54. \ 35. Ivan Sutherland. 1965. The Ultimate Display. In Proceedings of the IFIP Congress. 506–508. \ 36. Hiroaki Tobita. 2014. Aero-screen: blimp-based ubiquitous screen for novel digital signage and information visualization. In Proceedings of the ACM Symposium on Applied Computing (SAC’14). 976–980. \ 37. Toffoli Tommaso and Norman Margolus. 1991. Programmable matter: Concepts and realization. Physica D: Nonlinear Phenomena 47, 1 (1991), 263–272. \ ",Tangible User Interfaces; Drone; Persistence of Vision; Spherical Display,H.5.2; ,uistf3335-file1.zip,uistf3335-file2.jpg,uistf3335-file3.mp4,uistf3335-file4.zip,"iSphere, a flying spherical display that can display high resolution and bright images in all directions from anywhere in 3D space.",We marked modified texts in the paper with yellow.,"We present iSphere, a flying spherical display that can display high resolution and bright images in all directions from anywhere in 3D space.","We greatly appreciate the reviewers' comments. \ The major points that we have changed are as follow: \  \ 1) We corrected the extent of our contribution. Moreover, we referred to additional papers such as ""Organic User Interface"" and rethink section names, definitions, and the papers mentioned.  \ 2) Technical details in the paper are appended for the followers to replicate it. We clarified the details such as size, weight, material and electric modules (e.g. motor, electric speed controller and battery).  \ 3) We conducted an experiment of power consumption.  \ 4) We discussed the disadvantages of our system. Besides, we compared it with other systems such as balloons and blimps.",Wataru Yamada,Hiroyuki Manabe,FormatComplete,,,,,,,Aug 9 22:45,
uistf2895,10/25,14,Flexible Sensing,10:40:00 AM,12:00:00 PM,4,10:40:00 AM,11:00:00 AM,long,long,none,1,1401,,"Has to be separated from 3335 in time (parallel session, same authors)",uistf2895,A,A Capacitive Touch Sensing Technique with Series-connected Sensing Electrodes,Hiroyuki,Manabe,manabehiroyuki@acm.org,uistf2895-paper.pdf,10,letter,,,"Hiroyuki Manabe, Wataru Yamada","manabehiroyuki@acm.org, wataruyamada@acm.org",36743,Hiroyuki,,Manabe,manabehiroyuki@acm.org,Research Labs,NTT DOCOMO,Yokosuka,Kanagawa,Japan,,,,,,45181,Wataru,,Yamada,wataruyamada@acm.org,Research Labs,NTT DOCOMO,Yokosuka,Kanagawa,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Touch sensing with multiple electrodes allows expressive touch interactions. \ The adaptability and flexibility of the sensor are important in efficiently prototyping touch based systems. The proposed technique uses capacitive touch sensing and simplifies the connections as the electrodes are connected in series via capacitors and the interface circuit is connected to the electrode array by just two wires. The touched electrode is recognized by measuring the capacitance changes while switching the polarity of the signal. We show that the technique is capable of detecting different touches through simulations and actual measurements. User tests show that ten electrodes are successfully recognized after user calibration. They also show the proposal's other novel capabilities of multi-touch (2-touch) and `capacitor-free' design. Various forms of electrodes and applications are examined to elucidate the application range.,manabehiroyuki@acm.org,"WARNING: Reference 8 starts with a non-alphanumeric character.  Please check it. \  \ 1. Bruce Bohn. 2009. Microchip CTMU for Capacitive Touch Applications. Technical Report. Microchip Technology Inc, Application Notes, AN1250. \ 2. Artem Dementyev, Hsin-Liu (Cindy) Kao, and Joseph A. Paradiso. 2015. SensorTape: Modular and Programmable 3D-Aware Dense Sensor Network on a Tape. In Proc. UIST ’15. 649–658. \ 3. Paul Dietz and Darren Leigh. 2001. DiamondTouch: a multi-user touch technology. In Proc. UIST ’01. 219–226. \ 4. Jefferson Y. Han. 2005. Low-cost Multi-touch Sensing Through Frustrated Total Internal Reﬂection. In Proc. UIST ’05. 115–118. \ 5. David Holman, Nicholas Fellion, and Roel Vertegaal. 2014. Sensing Touch Using Resistive Graphs. In Proc. DIS ’14. 195–198. \ 6. David Holman and Roel Vertegaal. 2011. TactileTape: low-cost touch sensing on curved surfaces. In Adjunct Proc. UIST ’11. 17–18. \ 7. Dana Hughes, Halley Proﬁta, and Nikolaus Correll. 2014. SwitchBack: An On-body RF-based Gesture Input Device. In Proc. ISWC ’14. 63–66. \ 8. Ça˘gda¸s Karata¸s and Marco Gruteser. 2015. Printing Multi-key Touch Interfaces. In Proc. UbiComp ’15. 169–179. \ 9. Kunihiro Kato and Homei Miyashita. 2014. Extension Sticker: A Method for Transferring External Touch Input Using a Striped Pattern Sticker. In Adjunct Proc. UIST ’14. 59–60. \ 10. Yoshihiro Kawahara, Steve Hodges, Benjamin S. Cook, Cheng Zhang, and Gregory D. Abowd. 2013. Instant Inkjet Circuits: Lab-based Inkjet Printing to Support Rapid Prototyping of UbiComp Devices. In Proc. UbiComp ’13. 363–372. \ 11. P. T. Krein and R. D. Meadows. 1990. The electroquasistatics of the capacitive touch panel. IEEE Trans. Industry Applications 26, 3 (1990), 529–534. \ 12. Hiroyuki Manabe and Hiroshi Inamura. 2014. Single Capacitive Touch Sensor That Detects Multi-touch Gestures. In Proc. ISWC ’14. 137–138. \ 13. Simon Olberding, Nan-Wei Gong, John Tiab, Joseph A. Paradiso, and Jürgen Steimle. 2013. A Cuttable Multi-touch Sensor. In Proc. UIST ’13. 245–254. \ 14. Makoto Ono, Buntarou Shizuki, and Jiro Tanaka. 2013. Touch & Activate: adding interactivity to existing objects using active acoustic sensing. In Proc. UIST ’13. 31–40. \ 15. Jie Qi and Leah Buechley. 2010. Electronic Popables: Exploring Paper-based Computing Through an Interactive Pop-up Book. In Proc. TEI ’10. 121–128. \ 16. Jun Rekimoto. 2002. SmartSkin: an infrastructure for freehand manipulation on interactive surfaces. In Proc. CHI ’02. 113–120. \ 17. Ilya Rosenberg and Ken Perlin. 2009. The UnMousePad: an interpolating multi-touch force-sensing input pad. ACM Trans. Graph. 28, 3 (2009), 65:1–65:9. \ 18. Munehiko Sato, Ivan Poupyrev, and Chris Harrison. 2012. Touché: enhancing touch interaction on humans, screens, liquids, and everyday objects. In Proc. CHI ’12. 483–492. \ 19. Valkyrie Savage, Xiaohan Zhang, and Björn Hartmann. 2012. Midas: Fabricating Custom Capacitive Touch Sensors to Prototype Interactive Objects. In Proc. UIST ’12. 579–588. \ 20. Jay Silver and David Shaw. 2012. Makey Makey: Improvising Tangible and Nature-based User Interfaces. In Proc. TEI ’12. 367–370. \ 21. Ryosuke Takada, Buntarou Shizuki, and Jiro Tanaka. 2016. MonoTouch: Single Capacitive Touch Sensor That Differentiates Touch Gestures. In CHI EA ’16. 2736–2743. \ 22. Masaya Tsuruta, Shuta Nakamae, and Buntarou Shizuki. 2016. RootCap: Touch Detection on Multi-electrodes Using Single-line Connected Capacitive Sensing. In Proc. ISS ’16. 23–32. \ 23. Karl Willis, Eric Brockmeyer, Scott Hudson, and Ivan Poupyrev. 2012. Printed Optics: 3D Printing of Embedded Optical Elements for Interactive Devices. In Proc. UIST ’12. 589–598. \ 24. Raphael Wimmer and Patrick Baudisch. 2011. Modular and deformable touch-sensitive surfaces based on time domain reﬂectometry. In Proc. UIST ’11. 517–526. \ ",Touch sensing; capacitive; series-connected; multi-touch; wire; paper; prototyping,H.5.2.,uistf2895-file1.tex,,,,,,"Our techniuqe is based on capacitive touch sensing; only 2 wires for connection, capacitor-free design, and muti-touch capability.",We changed the author's information and the permissions information for the camera ready.,Hiroyuki Manabe,Wataru Yamada,FormatComplete,,,,,,,Aug 6 6:12,
uistf2447,10/25,14,Flexible Sensing,10:40:00 AM,12:00:00 PM,4,11:00:00 AM,11:20:00 AM,long,long,uistf2895,2,1402,,,uistf2447,A,Data Storage and Interaction using Magnetized Fabric,Justin,Chan,jucha@cs.washington.edu,uistf2447-paper.pdf,12,letter,Times-Roman,,"Justin Chan, Shyamnath Gollakota","jucha@cs.washington.edu, gshyam@cs.washington.edu",71452,Justin,,Chan,jucha@cs.washington.edu,,University of Washington,Seattle,Washington,United States,,,,,,38995,Shyamnath,,Gollakota,gshyam@cs.washington.edu,,University of Washington,Seattle,Washington,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper enables data storage and interaction with smart fabric, without the need for onboard electronics or batteries. To do this, we present the first smart fabric design that harnesses the ferromagnetic properties of conductive thread. Specifically, we manipulate the polarity of magnetized fabric and encode different forms of data including 2D images and bit strings. These bits can be read by swiping a commodity smartphone across the fabric, using its inbuilt magnetometer. Our results show that magnetized fabric retains its data even after washing, drying and ironing. Using a glove made of magnetized fabric, we can also perform six gestures in front of a smartphone, with a classification accuracy of 90.1%. Finally, using magnetized thread, we create fashion accessories like necklaces, ties, wristbands and belts with data storage capabilities as well as enable authentication applications. \ ",jucha@cs.washington.edu,"1. Android Sensor types. https://source.android.com/ devices/sensors/sensor-types.html#composite_ sensor_type_summary. \ 2. BCP Conductive Sewing Thread for Touch Screen Gloves, Anti-static Overalls. https://www.amazon.com/gp/product/B01FYREP1I. \ 3. Gap Calculator. https://www.kjmagnetics.com/blog. asp?p=gap-calculator. \ 4. Hatch Embroidery Digitizer. https://www.wilcom.com/en-us/hatch.aspx. \ 5. LIS3MDL. http://www.st.com/content/ccc/resource/ technical/document/datasheet/54/2a/85/76/e3/97/ 42/18/DM00075867.pdf/files/DM00075867.pdf/jcr: content/translations/en.DM00075867.pdf. \ 6. MMC3616xPJ. http: //www.memsic.com/magnetic-sensors/MMC3416xPJ. \ 7. Nexus 5X. https://www.google.com/nexus/5x/. \ 8. Plastek Cards. https: //plastekcards.com/product-cards/key-cards/. \ 9. Real-time magnet position detection using smartphone. https://www.youtube.com/watch?v=fJlgDIO3oL8. \ 10. RFID Hotel. http://www.rfidhotel.com/. \ 11. SE400. http://www.brother-usa.com/homesewing/ ModelDetail.aspx?ProductID=SE400. \ 12. Simple Cost Analysis for RFID Options. https://www.amitracks.com/2013/10/ simple-cost-analysis-for-rfid-options/. \ 13. Touchscreen Computing: Gateway ZX4931 And HP TouchSmart 310. http://www.tomshardware.com/reviews/ gateway-zx4931-hp-touchsmart-310-touchscreen, 2922-12.html. \ 14. ZX Distance and Gesture Sensor. https://www.sparkfun.com/products/12780. \ 15. Joanna Berzowska and Maksim Skorobogatiy. Karma Chameleon: Bragg Fiber Jacquard-Woven Photonic Textiles. In Proceedings of the Fourth International Conference on Tangible, Embedded, and Embodied Interaction (TEI ’10). \ 16. Leah Buechley, Mike Eisenberg, Jaime Catchen, and Ali Crockett. The LilyPad Arduino: using computational textiles to investigate engagement, aesthetics, and diversity in computer science education. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’08). \ 17. Liwei Chan, Rong-Hao Liang, Ming-Chang Tsai, Kai-Yin Cheng, Chao-Huai Su, Mike Y Chen, Wen-Huang Cheng, and Bing-Yu Chen. 2013. FingerPad: private and subtle interaction using ﬁngertips. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST ’13). \ 18. Jun Chen, Yi Huang, Nannan Zhang, Haiyang Zou, Ruiyuan Liu, Changyuan Tao, Xing Fan, and Zhong Lin Wang. Micro-cable structured textile for simultaneously harvesting solar and mechanical energy (Nature Energy ’16). \ 19. Ke-Yu Chen, Kent Lyons, Sean White, and Shwetak Patel. uTrack: 3D input using two magnetic sensors. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST ’13). \ 20. Laura Devendorf, Joanne Lo, Noura Howell, Jung Lin Lee, Nan-Wei Gong, M Emre Karagozler, Shiho Fukuhara, Ivan Poupyrev, Eric Paulos, and Kimiko Ryokai. I don’t Want to Wear a Screen: Probing Perceptions of and Possibilities for Dynamic Displays on Clothing. In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI ’16). \ 21. Chris Harrison and Scott E Hudson. Abracadabra: wireless, high-precision, and unpowered ﬁnger input for very small mobile devices. In Proceedings of the 22Nd Annual ACM Symposium on User Interface Software and Technology (UIST ’09). \ 22. Sungjae Hwang, Myungwook Ahn, and Kwangyun Wohn. Magnetic Marionette: Magnetically Driven Elastic Controller on Mobile Device. In Proceedings of the Companion Publication of the 2013 International Conference on Intelligent User Interfaces Companion (IUI ’13 Companion). \ 23. Sungjae Hwang, Myungwook Ahn, and Kwang-yun Wohn. MagGetz: Customizable Passive Tangible Controllers on and Around Conventional Mobile Devices. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST ’13). \ 24. Sungjae Hwang, Andrea Bianchi, Myungwook Ahn, and Kwangyun Wohn. MagPen: Magnetically Driven Pen Interactions on and Around Conventional Smartphones. In Proceedings of the 15th International Conference on Human-computer Interaction with Mobile Devices and Services (MobileHCI ’13). \ 25. Sunyoung Kim, Eric Paulos, and Mark D Gross. WearAir: expressive t-shirts for air quality sensing. In Proceedings of the Fourth International Conference on Tangible, Embedded, and Embodied Interaction (TEI ’10). \ 26. Han-Chih Kuo, Rong-Hao Liang, Long-Fei Lin, and Bing-Yu Chen. GaussMarbles: Spherical Magnetic Tangibles for Interacting with Portable Physical Constraints. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. \ 27. Joanne Leong, Patrick Parzer, Florian Perteneder, Teo Babic, Christian Rendl, Anita Vogl, Hubert Egger, Alex Olwal, and Michael Haller. 2016. proCover: Sensory Augmentation of Prosthetic Limbs Using Smart Textile Covers. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology. ACM, 335–346. \ 28. Rong-Hao Liang, Liwei Chan, Hung-Yu Tseng, Han-Chih Kuo, Da-Yuan Huang, De-Nian Yang, and Bing-Yu Chen. GaussBricks: Magnetic Building Blocks for Constructive Tangible Interactions on Portable Displays. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’14). \ 29. Rong-Hao Liang, Han-Chih Kuo, Liwei Chan, De-Nian Yang, and Bing-Yu Chen. GaussStones: Shielded Magnetic Tangibles for Multi-token Interactions on Portable Displays. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST ’14). \ 30. Vincent Liu, Aaron Parks, Vamsi Talla, Shyamnath Gollakota, David Wetherall, and Joshua R. Smith. 2013. Ambient Backscatter: Wireless Communication out of Thin Air. In Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM. 39–50. \ 31. Matthew Mauriello, Michael Gubbels, and Jon E Froehlich. Social fabric ﬁtness: the design and evaluation of wearable E-textile displays to support group running. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’14). \ 32. Maggie Orth, Rehmi Post, and Emily Cooper. Fabric computing interfaces. In CHI 98 Conference Summary on Human Factors in Computing Systems. \ 33. Roshan Lalintha Peiris, Mili John Tharakan, Newton Fernando, and Adrian David Chrok. Ambikraf: A nonemissive fabric display for fast changing textile animation. In IFIP 9th International Conference on Embedded and Ubiquitous Computing (EUC ’11). \ 34. Ernest Rehmatulla Post, Maggie Orth, PR Russo, and Neil Gershenfeld. 2000. E-broidery: Design and fabrication of textile-based computing. IBM Syst. J. 39, 3-4 (July 2000), 840–860. \ 35. Ivan Poupyrev, Nan-Wei Gong, Shiho Fukuhara, Mustafa Emre Karagozler, Carsten Schwesig, and Karen E. Robinson. Project Jacquard: Interactive Digital Textiles at Scale. In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI ’16). \ 36. T Scott Saponas, Chris Harrison, and Hrvoje Benko. 2011. PocketTouch: through-fabric capacitive touch input. In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (UIST ’11). \ 37. Anran Wang, Vikram Iyer, Vamsi Talla, Joshua R. Smith, and Shyamnath Gollakota. FM Backscatter: Enabling Connected Cities and Smart Fabrics. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). \ 38. Sang Ho Yoon, Yunbo Zhang, Ke Huo, and Karthik Ramani. 2016. TRing: Instant and Customizable Interactions with Objects Using an Embedded Magnet and a Finger-Worn Device. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). \ ",Smart fabrics; E-textiles; Wearables computing; Conductive thread; Gesture interaction; Data storage,H.5.m,uistf2447-file1.zip,uistf2447-file2.jpg,uistf2447-file3.mp4,,Store data onto textiles by magnetizing portions of conductive thread. Read the bits by swiping a smartphone across its surface.,,The first smart fabric design that enables data storage and interaction using the ferromagnetic properties of conductive thread without the need for onboard electronics or batteries.,"- The paper has been shortened to 7 pages excluding references. \ - Added the keywords, classification, copyright information and space for acknowledgements. \ - Added a section about flexible RFID tags in the related work. \ - Added a new figure (Fig. 17) showing that gesture recognition works even when the smartphone is in a pocket. \ - Removed the application about magnetic identifiers. \ - Removed Algorithm 2. \ - Compressed the introduction, description and evaluation sections.",Justin Chan,Justin Chan,FormatComplete,"CNS-1420654, CNS-1407583, CNS-14524",National Science Foundation,,,,,Aug 8 18:11,
uistf4355,10/25,14,Flexible Sensing,10:40:00 AM,12:00:00 PM,4,11:20:00 AM,11:40:00 AM,long,long,uistf2447,3,1403,,,uistf4355,A,iSoft: A Customizable Soft Sensor with Real-time Continuous Contact and Stretching Sensing,Sang Ho,Yoon,yoon87@purdue.edu,uistf4355-paper.pdf,14,letter,Times-Roman,,"Sang Ho Yoon, Ke Huo, Yunbo Zhang, Guiming Chen, Luis Paredes, Subramanian Chidambaram, Karthik Ramani","yoon87@purdue.edu, khuo@purdue.edu, will.yunbo.zhang@gmail.com, chen1956@purdue.edu, lparede@purdue.edu, schidamb@purdue.edu, ramani@purdue.edu",40852,Sang Ho,,Yoon,yoon87@purdue.edu,School of Mechanical Engineering,Purdue University,West Lafayette,Indiana,United States,,,,,,48741,Ke,,Huo,khuo@purdue.edu,School of Mechanical Engineering,Purdue University,West Lafayette,Indiana,United States,,,,,,47366,Yunbo,,Zhang,will.yunbo.zhang@gmail.com,School of Mechanical Engineering,Purdue University,West Lafayette,Indiana,United States,,,,,,71613,Guiming,,Chen,chen1956@purdue.edu,School of Mechanical Engineering,Purdue University,West Lafayette,Indiana,United States,,,,,,55594,Luis,,Paredes,lparede@purdue.edu,School of Mechanical Engineering,Purdue University,West Lafayette,Indiana,United States,,,,,,71740,Subramanian,,Chidambaram,schidamb@purdue.edu,School of Mechanical Engineering,Purdue University,West Lafayette,Indiana,United States,,,,,,29031,Karthik,,Ramani,ramani@purdue.edu,School of Mechanical Engineering,Purdue University,West Lafayette,Indiana,United States,School of Electrical and Computer Engineering,Purdue University,West Lafayette,Indiana,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present iSoft, a single volume soft sensor capable of sensing real-time continuous contact and unidirectional stretching. We propose a low-cost and an easy way to fabricate such piezoresistive elastomer-based soft sensors for instant interactions. We employ an electrical impedance tomography (EIT) technique to estimate changes of resistance distribution on the sensor caused by fingertip contact. To compensate for the rebound elasticity of the elastomer and achieve real-time continuous contact sensing, we apply a dynamic baseline update for EIT. The baseline updates are triggered by fingertip contact and movement detections. Further, we support unidirectional stretching sensing using a model-based approach which works separately with continuous contact sensing. We also provide a software toolkit for users to design and deploy personalized interfaces with customized sensors. Through a series of experiments and evaluations, we validate the performance of contact and stretching sensing. Through example applications, we show the variety of examples enabled by iSoft.",yoon87@purdue.edu,"1. Andy Adler and Robert Guardo. 1996. Electrical impedance tomography: regularized imaging and contrast detection. IEEE Transactions on Medical Imaging 15, 2 (Apr 1996), 170–179. http://doi.org/10.1109/42.491418 \ 2. Alamusi, Ning Hu, Hisao Fukunaga, Satoshi Atobe, Yaolu Liu, and Jinhua Li. 2011. Piezoresistive Strain Sensors Made from Carbon Nanotubes Based Polymer Nanocomposites. Sensors 11, 11 (2011), 10691–10723. http://www.mdpi.com/1424-8220/11/11/10691 \ 3. Hassan Alirezaei, Akihiko Nagakubo, and Yasuo Kuniyoshi. 2007. A highly stretchable tactile distribution sensor for smooth surfaced humanoids. In 2007 7th IEEE-RAS International Conference on Humanoid Robots. 167–173. http://doi.org/10.1109/ICHR.2007.4813864 \ 4. Chin-yu Chien, Rong-Hao Liang, Long-Fei Lin, Liwei Chan, and Bing-Yu Chen. 2015. FlexiBend: Enabling Interactivity of Multi-Part, Deformable Fabrications Using Single Shape-Sensing Strip. In Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology (UIST ’15). ACM, New York, NY, USA, 659–663. http://doi.org/10.1145/2807442.2807456 \ 5. Alex Chortos, Jia Liu, and Zhenan Bao. 2016. Pursuing prosthetic electronic skin. Nature Materials 15, 9 (2016), 937–950. http://doi.org/10.1038/nmat4671 \ 6. Artem Dementyev, Hsin-Liu (Cindy) Kao, and Joseph A. Paradiso. 2015. SensorTape: Modular and Programmable 3D-Aware Dense Sensor Network on a Tape. In Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology (UIST ’15). ACM, New York, NY, USA, 649–658. http://doi.org/10.1145/2807442.2807507 \ 7. Artem Dementyev and Joseph A. Paradiso. 2014. WristFlex: Low-power Gesture Input with Wrist-worn Pressure Sensors. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST ’14). ACM, New York, NY, USA, 161–166. http://doi.org/10.1145/2642918.2647396 \ 8. Paul H. Dietz, Benjamin Eidelson, Jonathan Westhues, and Steven Bathiche. 2009. A Practical Pressure Sensitive Computer Keyboard. In Proceedings of the 22nd Annual ACM Symposium on User Interface Software and Technology (UIST ’09). ACM, New York, NY, USA, 55–58. http://doi.org/10.1145/1622176.1622187 \ 9. Peter M. Edic, Gary J. Saulnier, Jonathan C. Newell, and David Isaacson. 1995. A real-time electrical impedance tomograph. IEEE Transactions on Biomedical Engineering 42, 9 (Sept 1995), 849–859. http://doi.org/10.1109/10.412652 \ 10. Marcello Ferro, Giovanni Pioggia, Alessandro Tognetti, Nicola Carbonaro, and Danilo De Rossi. 2009. A Sensing Seat for Human Authentication. IEEE Transactions on Information Forensics and Security 4, 3 (Sept 2009), 451–459. http://doi.org/10.1109/TIFS.2009.2019156 \ 11. Sean Follmer, Daniel Leithinger, Alex Olwal, Nadia Cheng, and Hiroshi Ishii. 2012. Jamming User Interfaces: Programmable Particle Stiffness and Sensing for Malleable and Shape-changing Devices. In Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology (UIST ’12). ACM, New York, NY, USA, 519–528. http://doi.org/10.1145/2380116.2380181 \ 12. Nan-Wei Gong, Jürgen Steimle, Simon Olberding, Steve Hodges, Nicholas Edward Gillian, Yoshihiro Kawahara, and Joseph A. Paradiso. 2014. PrintSense: A Versatile Sensing Technique to Support Multimodal Flexible Surface Interaction. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’14). ACM, New York, NY, USA, 1407–1410. http://doi.org/10.1145/2556288.2557173 \ 13. LJ Gray and E Lutz. 1990. On the treatment of corners in the boundary element method. J. Comput. Appl. Math. 32, 3 (1990), 369–386. http://doi.org/10.1016/0377-0427(90)90043-Y \ 14. Jaehyun Han, Jiseong Gu, and Geehyuk Lee. 2014. Trampoline: A Double-sided Elastic Touch Device for Creating Reliefs. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST ’14). ACM, New York, NY, USA, 383–388. http://doi.org/10.1145/2642918.2647381 \ 15. PJRC Inc. 2017. Product Detail. (2017). Retrieved April 1, 2017 from http://www.pjrc.com/teensy/index.html. \ 16. Inpil Kang, Mark J Schulz, Jay H Kim, Vesselin Shanov, and Donglu Shi. 2006. A carbon nanotube strain sensor for structural health monitoring. Smart Materials and Structures 15, 3 (2006), 737. http://doi.org/10.1088/0964-1726/15/3/009 \ 17. Thorsten Karrer, Moritz Wittenhagen, Leonhard Lichtschlag, Florian Heller, and Jan Borchers. 2011. Pinstripe: Eyes-free Continuous Input on Interactive Clothing. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’11). ACM, New York, NY, USA, 1313–1322. http://doi.org/10.1145/1978942.1979137 \ 18. Pasi Kauppinen, Jari Hyttinen, and Jaakko Malmivuo. 2006. Sensitivity distribution visualizations of impedance tomography measurement strategies. International Journal of Bioelectromagnetism 8, 1 (2006), 1–9. \ 19. Yoshihiro Kawahara, Steve Hodges, Benjamin S. Cook, Cheng Zhang, and Gregory D. Abowd. 2013. Instant Inkjet Circuits: Lab-based Inkjet Printing to Support Rapid Prototyping of UbiComp Devices. In Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp ’13). ACM, New York, NY, USA, 363–372. http://doi.org/10.1145/2493432.2493486 \ 20. Tsz-Ho Kwok, Charlie CL Wang, Dongping Deng, Yunbo Zhang, and Yong Chen. 2015. Four-dimensional printing for freeform surfaces: Design optimization of origami and kirigami structures. Journal of Mechanical Design 137, 11 (2015), 111413. http://doi.org/10.1115/1.4031023 \ 21. David Ledo, Fraser Anderson, Ryan Schmidt, Lora Oehlberg, Saul Greenberg, and Tovi Grossman. 2017. Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17). ACM, New York, NY, USA, 2583–2593. http://doi.org/10.1145/3025453.3025652 \ 22. Joanne Leong, Patrick Parzer, Florian Perteneder, Teo Babic, Christian Rendl, Anita Vogl, Hubert Egger, Alex Olwal, and Michael Haller. 2016. proCover: Sensory Augmentation of Prosthetic Limbs Using Smart Textile Covers. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 335–346. http://doi.org/10.1145/2984511.2984572 \ 23. Jhe-Wei Lin, Chiuan Wang, Yi Yao Huang, Kuan-Ting Chou, Hsuan-Yu Chen, Wei-Luan Tseng, and Mike Y. Chen. 2015. BackHand: Sensing Hand Gestures via Back of the Hand. In Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology (UIST ’15). ACM, New York, NY, USA, 557–564. http://doi.org/10.1145/2807442.2807462 \ 24. Darren J. Lipomi, Michael Vosgueritchian, Benjamin C-K. Tee, Sondra L. Hellstrom, Jennifer A. Lee, Courtney H. Fox, and Zhenan Bao. 2011. Skin-like pressure and strain sensors based on transparent elastic ﬁlms of carbon nanotubes. Nature nanotechnology 6, 12 (2011), 788–792. http://doi.org/10.1038/nnano.2011.184 \ 25. F. Lorussi, Enzo Pasquale Scilingo, M. Tesconi, A. Tognetti, and D. De Rossi. 2005. Strain sensing fabric for hand posture and gesture monitoring. IEEE Transactions on Information Technology in Biomedicine 9, 3 (Sept 2005), 372–381. http://doi.org/10.1109/TITB.2005.854510 \ 26. Sachi Mizobuchi, Shinya Terasaki, Turo Keski-Jaskari, Jari Nousiainen, Matti Ryynanen, and Miika Silfverberg. 2005. Making an Impression: Force-controlled Pen Input for Handheld Devices. In CHI ’05 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’05). ACM, New York, NY, USA, 1661–1664. http://doi.org/10.1145/1056808.1056991 \ 27. Vinh P. Nguyen, Sang Ho Yoon, Ansh Verma, and Karthik Ramani. 2014. BendID: Flexible Interface for Localized Deformation Recognition. In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp ’14). ACM, New York, NY, USA, 553–557. http://doi.org/10.1145/2632048.2636092 \ 28. Simon Olberding, Nan-Wei Gong, John Tiab, Joseph A. Paradiso, and Jürgen Steimle. 2013. A Cuttable Multi-touch Sensor. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST ’13). ACM, New York, NY, USA, 245–254. http://doi.org/10.1145/2501988.2502048 \ 29. Steve Park, Michael Vosguerichian, and Zhenan Bao. 2013. A review of fabrication and applications of carbon nanotube ﬁlm-based ﬂexible electronics. Nanoscale 5, 5 (2013), 1727–1752. http://doi.org/10.1039/C3NR33560G \ 30. Yong-Lae Park, Bor-Rong Chen, and Robert J. Wood. 2012. Design and Fabrication of Soft Artiﬁcial Skin Using Embedded Microchannels and Liquid Conductors. IEEE Sensors Journal 12, 8 (2012), 2711–2718. http://doi.org/10.1109/JSEN.2012.2200790 \ 31. Pedro Piacenza, Yuchen Xiao, Steve Park, Ioannis Kymissis, and Matei Ciocarlie. 2016. Contact localization through spatially overlapping piezoresistive signals. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 195–201. http://doi.org/10.1109/IROS.2016.7759055 \ 32. Ivan Poupyrev, Nan-Wei Gong, Shiho Fukuhara, Mustafa Emre Karagozler, Carsten Schwesig, and Karen E. Robinson. 2016. Project Jacquard: Interactive Digital Textiles at Scale. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 4216–4227. http://doi.org/10.1145/2858036.2858176 \ 33. Ganna Pugach, Alexandre Pitti, and Philippe Gaussier. 2015. Neural learning of the topographic tactile sensory information of an artiﬁcial skin through a self-organizing map. Advanced Robotics 29, 21 (2015), 1393–1409. http://doi.org/10.1080/01691864.2015.1092395 \ 34. Raf Ramakers, Fraser Anderson, Tovi Grossman, and George Fitzmaurice. 2016. RetroFab: A Design Tool for Retroﬁtting Physical Interfaces Using Actuators, Sensors and 3D Printing. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 409–419. http://doi.org/10.1145/2858036.2858485 \ 35. Gonzalo Ramos, Matthew Boulos, and Ravin Balakrishnan. 2004. Pressure Widgets. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’04). ACM, New York, NY, USA, 487–494. http://doi.org/10.1145/985692.985754 \ 36. Christian Rendl, David Kim, Patrick Parzer, Sean Fanello, Martin Zirkl, Gregor Scheipl, Michael Haller, and Shahram Izadi. 2016. FlexCase: Enhancing Mobile Interaction with a Flexible Sensing and Display Cover. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 5138–5150. http://doi.org/10.1145/2858036.2858314 \ 37. Peter Roberts, Dana D. Damian, Wanliang Shan, Tong Lu, and Carmel Majidi. 2013. Soft-matter capacitive sensor for measuring shear and pressure deformation. In 2013 IEEE International Conference on Robotics and Automation. 3529–3534. http://doi.org/10.1109/ICRA.2013.6631071 \ 38. Ilya Rosenberg and Ken Perlin. 2009. The UnMousePad: An Interpolating Multi-touch Force-sensing Input Pad. In ACM SIGGRAPH 2009 Papers (SIGGRAPH ’09). ACM, New York, NY, USA, Article 65, 9 pages. http://doi.org/10.1145/1576246.1531371 \ 39. Valkyrie Savage, Sean Follmer, Jingyi Li, and Björn Hartmann. 2015. Makers’ Marks: Physical Markup for Designing and Fabricating Functional Objects. In Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology (UIST ’15). ACM, New York, NY, USA, 103–108. http://doi.org/10.1145/2807442.2807508 \ 40. Martin Schmitz, Mohammadreza Khalilbeigi, Matthias Balwierz, Roman Lissermann, Max Mühlhäuser, and Jürgen Steimle. 2015. Capricate: A Fabrication Pipeline to Design and 3D Print Capacitive Touch Sensors for Interactive Objects. In Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology (UIST ’15). ACM, New York, NY, USA, 253–258. http://doi.org/10.1145/2807442.2807503 \ 41. David Silvera-Tawil, David Rye, Manuchehr Soleimani, and Mari Velonaki. 2015. Electrical Impedance Tomography for Artiﬁcial Sensitive Robotic Skin: A Review. IEEE Sensors Journal 15, 4 (April 2015), 2001–2016. http://doi.org/10.1109/JSEN.2014.2375346 \ 42. David Silvera-Tawil, David Rye, and Mari Velonaki. 2012. Interpretation of the modality of touch on an artiﬁcial arm covered with an EIT-based sensitive skin. The International Journal of Robotics Research 31, 13 (2012), 1627–1641. http://doi.org/10.1177/0278364912455441 \ 43. Ronit Slyper, Ivan Poupyrev, and Jessica Hodgins. 2011. Sensing Through Structure: Designing Soft Silicone Sensors. In Proceedings of the Fifth International Conference on Tangible, Embedded, and Embodied Interaction (TEI ’11). ACM, New York, NY, USA, 213–220. http://doi.org/10.1145/1935701.1935744 \ 44. Yuta Sugiura, Masahiko Inami, and Takeo Igarashi. 2012. A Thin Stretchable Interface for Tangential Force Measurement. In Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology (UIST ’12). ACM, New York, NY, USA, 529–536. http://doi.org/10.1145/2380116.2380182 \ 45. Raw Material Suppliers. 2017. Print-On Silicone Ink. (2017). Retrieved April 1, 2017 from http: //rawmaterialsuppliers.com/print-on-silicone-rubber. \ 46. Tyler N Tallman, Sila Gungor, K W Wang, and Charles E Bakis. 2015. Damage detection via electrical impedance tomography in glass ﬁber/epoxy laminates with carbon black ﬁller. Structural Health Monitoring 14, 1 (2015), 100–109. http://doi.org/10.1177/1475921714554142 \ 47. Karen Vanderloock, Vero Vanden Abeele, Johan A.K. Suykens, and Luc Geurts. 2013. The Skweezee System: Enabling the Design and the Programming of Squeeze Interactions. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST ’13). ACM, New York, NY, USA, 521–530. http://doi.org/10.1145/2501988.2502033 \ 48. M Vauhkonen, W R B Lionheart, L M Heikkinen, P J Vauhkonen, and J P Kaipio. 2001. A MATLAB package for the EIDORS project to reconstruct two-dimensional EIT images. Physiological Measurement 22, 1 (2001), 107. http://stacks.iop.org/0967-3334/22/i=1/a=314 \ 49. Wacker. 2017. ELASTOSIL LR 3162 A/B. (2017). Retrieved April 1, 2017 from http://www.wacker.com. \ 50. Martin Weigel, Tong Lu, Gilles Bailly, Antti Oulasvirta, Carmel Majidi, and Jürgen Steimle. 2015. iSkin: Flexible, Stretchable and Visually Customizable On-Body Touch Sensors for Mobile Computing. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15). ACM, New York, NY, USA, 2991–3000. http://doi.org/10.1145/2702123.2702391 \ 51. Michael Wessely, Theophanis Tsandilas, and Wendy E. Mackay. 2016. Stretchis: Fabricating Highly Stretchable User Interfaces. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 697–704. http://doi.org/10.1145/2984511.2984521 \ 52. Takeo Yamada, Yuhei Hayamizu, Yuki Yamamoto, Yoshiki Yomogida, Ali Izadi-Najafabadi, Don N. Futaba, and Kenji Hata. 2011. A stretchable carbon nanotube strain sensor for human-motion detection. Nature Nano 6, 5 (2011), 296–301. http://doi.org/10.1038/nnano.2011.36 \ 53. K. Yamaguchi, J. J. C. Busﬁeld, and A. G. Thomas. 2003. Electrical and mechanical behavior of ﬁlled elastomers. I. The effect of strain. Journal of Polymer Science Part B: Polymer Physics 41, 17 (2003), 2079–2089. http://doi.org/10.1002/polb.10571 \ 54. Lining Yao, Ryuma Niiyama, Jifei Ou, Sean Follmer, Clark Della Silva, and Hiroshi Ishii. 2013. PneUI: Pneumatically Actuated Soft Composite Materials for Shape Changing Interfaces. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST ’13). ACM, New York, NY, USA, 13–22. http://doi.org/10.1145/2501988.2502037 \ 55. Sang Ho Yoon, Ke Huo, Vinh P. Nguyen, and Karthik Ramani. 2015. TIMMi: Finger-worn Textile Input Device with Multimodal Sensing in Mobile Interaction. In Proceedings of the Ninth International Conference on Tangible, Embedded, and Embodied Interaction (TEI ’15). ACM, New York, NY, USA, 269–272. http://doi.org/10.1145/2677199.2680560 \ 56. Sang Ho Yoon, Yunbo Zhang, Ke Huo, and Karthik Ramani. 2016. TRing: Instant and Customizable Interactions with Objects Using an Embedded Magnet and a Finger-Worn Device. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 169–181. http://doi.org/10.1145/2984511.2984529 \ 57. Yang Zhang, Gierad Laput, and Chris Harrison. 2017. Electrick: Low-Cost Touch Sensing Using Electric Field Tomography. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17). ACM, New York, NY, USA, 1–14. http://doi.org/10.1145/3025453.3025842 \ 58. Yang Zhang, Robert Xiao, and Chris Harrison. 2016. Advancing Hand Gesture Recognition with High Resolution Electrical Impedance Tomography. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 843–850. http://doi.org/10.1145/2984511.2984574 \ ",Wearables; Soft Sensor; Sensing Technique; Input Device; Customization,H.5.2,uistf4355-file1.tex,uistf4355-file2.jpg,uistf4355-file3.mp4,,iSoft is a single volume soft sensor capable of sensing real-time continuous contact and unidirectional stretching. iSoft supports a low-cost and an easy fabrication process for making customizable soft sensors.,,iSoft is a single volume soft sensor capable of sensing real-time continuous contact and unidirectional stretching. iSoft supports a low-cost and an easy fabrication process for making customizable soft sensors.,"1. Abstract & Introduction (pg.1) \ -	As suggested by AC1, we clarified the sensing capability in Abstract. \ -	We have added explanations on our work with respect to Electrick (CHI’17) in Introduction. \  \ 2. Sensing Principle (pg.4) \ -	We have modified Figure 4 with the correct symbol (V_H = V_i). (R1) \ -	We have corrected flowchart explanations to improve the readability. (R1) \ -	The multitouch capability is discussed at the end of the section. (AC1) \  \ 3. Material Property (pg.4) \ -	We have added volume resistivity of our material and show a comparison with recent polymer piezoresistive sensors based on carbon nanotubes. (AC1) \  \ 3. Task Evaluation (pg.9) \ -	As suggested by R2, we have added ANOVA before direct comparisons. \ -	We have also added post-hoc pairwise comparisons. \ -	As suggested by AC1, we have added our findings in the light of findings from Electrick.  \  \ 4. Discussion \ -	The applicability of our prototype and the time delay requirement are now discussed. (R1 & R3) \ -	We have also discussed the current stretching sensing capability and its limitation. (R1 & R3) \ -	Potential improvements for calibration process has been added. (R1 & R3) \ -	We have removed a subsection “Sensor Surface Texture” in Discussion section to satisfy the paper length requirement. \  \ 5. Miscellaneous \ -	We have changed the title to “iSoft: A Customizable Soft Sensor with Real-time Continuous Contact and Stretching Sensing” which better represents paper contents. \ -	We have changed all standard deviation notations.  \ -	We have modified the fonts and colors of figures for better readability (Fig 13,16,17) \ -	Reference format has been edited fully. \ -	All editorial comments have been reflected in the current submission \  \ We have edited the whole manuscript professionally to improve the overall paper quality. We have highlighted the parts with changes. We greatly appreciate the comments given to us for improving this paper.",Sang Ho Yoon,Ke Huo,FormatComplete,NRI #1637961 & IGERT #1144843,National Science Foundation,"Purdue University, School of Mechanical Engineering",Donald W. Feddersen Chair,,,Aug 9 17:13,
uistf4170,10/25,14,Flexible Sensing,10:40:00 AM,12:00:00 PM,4,11:40:00 AM,12:00:00 PM,long,long,uistf4355,4,1404,,,uistf4170,A,CanalSense: Face-Related Movement Recognition System based on Sensing Air Pressure in Ear Canals,Toshiyuki,Ando,ando@iplab.cs.tsukuba.ac.jp,uistf4170-paper.pdf,11,letter,,,"Toshiyuki Ando, Yuki Kubo, Buntarou Shizuki, Shin Takahashi","ando@iplab.cs.tsukuba.ac.jp, kubo@iplab.cs.tsukuba.ac.jp, shizuki@cs.tsukuba.ac.jp, shin@cs.tsukuba.ac.jp",62866,Toshiyuki,,Ando,ando@iplab.cs.tsukuba.ac.jp,IPLAB,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,53389,Yuki,,Kubo,kubo@iplab.cs.tsukuba.ac.jp,IPLAB,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,26480,Buntarou,,Shizuki,shizuki@cs.tsukuba.ac.jp,IPLAB,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,6407,Shin,,Takahashi,shin@cs.tsukuba.ac.jp,IPLAB,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present a jaw, face, or head movement (face-related movement) recognition system called CanalSense. It recognizes face-related movements using barometers embedded in earphones. We find that face-related movements change air pressure inside the ear canals, which shows characteristic changes depending on the type and degree of the movement. We also find that such characteristic changes can be used to recognize face-related movements. We conduct an experiment to measure the accuracy of recognition. As a result, random forest shows per-user recognition accuracies of 87.6% for eleven face-related movements and 87.5% for four OpenMouth levels.",ando@iplab.cs.tsukuba.ac.jp," \ WARNING: Reference 3 is very long.  Please verify that it was extracted correctly. \  \ 1. Ken Aoki and Masashi Mine. 2010. Mastication Frequency Detecting Device. (2010). JP-2010154985-A，2010. (In Japanese). \ 2. Daniel Ashbrook, Carlos Tejada, Dhwanit Mehta, Anthony Jiminez, Goudam Muralitharam, Sangeeta Gajendra, and Ross Tallents. 2016. Bitey: An Exploration of Tooth Click Gestures for Hands-free User Interface Control. In Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI ’16). ACM, New York, NY, USA, 158–169. DOI:http://dx.doi.org/10.1145/2935334.2935389 \ 3. Abdelkareem Bedri, David Byrd, Peter Presti, Himanshu Sahni, Zehua Gue, and Thad Starner. 2015a. Stick It in Your Ear: Building an In-ear Jaw Movement Sensor. In Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers (UbiComp/ISWC ’15 Adjunct). ACM, New York, NY, USA, 1333–1338. DOI: http://dx.doi.org/10.1145/2800835.2807933 \ 4. Abdelkareem Bedri, Himanshu Sahni, Pavleen Thukral, Thad Starner, David Byrd, Peter Presti, Gabriel Reyes, Maysam Ghovanloo, and Zehua Guo. 2015b. Toward Silent-Speech Control of Consumer Wearables. Computer 48, 10 (Oct. 2015), 54–62. DOI: http://dx.doi.org/10.1109/MC.2015.310 \ 5. Abdelkareem Bedri, Apoorva Verlekar, Edison Thomaz, Valerie Avva, and Thad Starner. 2015c. Detecting Mastication: A Wearable Approach. In Proceedings of the 17th ACM International Conference on Multimodal Interaction (ICMI ’15). ACM, New York, NY, USA, 247–250. DOI: http://dx.doi.org/10.1145/2818346.2820767 \ 6. Henry S. Brenman, Robert C. Mackowiak, and M. H. F. Friedman. 1968. Condylar Displacement Recordings as an Analog of Mandibular Movements. Journal of Dental Research 47, 4 (1968), 599–602. DOI: http://dx.doi.org/10.1177/00220345680470041501 \ 7. Andreas Bulling, Daniel Roggen, and Gerhard Tröster. 2008. It’s in Your Eyes: Towards Context-awareness and Mobile HCI using Wearable EOG Goggles. In Proceedings of the 10th International Conference on Ubiquitous Computing (UbiComp ’08). ACM, New York, NY, USA, 84–93. DOI: http://dx.doi.org/10.1145/1409635.1409647 \ 8. Jingyuan Cheng, Ayano Okoso, Kai Kunze, Niels Henze, Albrecht Schmidt, Paul Lukowicz, and Koichi Kise. 2014. On the Tip of My Tongue: A Non-invasive Pressure-based Tongue Interface. In Proceedings of the 5th Augmented Human International Conference (AH ’14). ACM, New York, NY, USA, Article 12, 4 pages. DOI:http://dx.doi.org/10.1145/2582051.2582063 \ 9. Sony Corporation. 2016. Xperia Ear. (2016). http://www.sonymobile.com/us/products/smart-products/ xperia-ear/ (accessed 2017-4-4). \ 10. Andrew Crossan, Mark McGill, Stephen Brewster, and Roderick Murray-Smith. 2009. Head Tilting for Interaction in Mobile Contexts. In Proceedings of the 11th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI ’09). ACM, New York, NY, USA, Article 6, 10 pages. DOI: http://dx.doi.org/10.1145/1613858.1613866 \ 11. Mayank Goel, Chen Zhao, Ruth Vinisha, and Shwetak N. Patel. 2015. Tongue-in-Cheek: Using Wireless Signals to Enable Non-Intrusive and Flexible Facial Gestures Detection. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15). ACM, New York, NY, USA, 255–258. DOI: http://dx.doi.org/10.1145/2702123.2702591 \ 12. Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations Newsletter 11, 1 (Nov. 2009), 10–18. DOI:http://dx.doi.org/10.1145/1656274.1656278 \ 13. Pan Hu, Guobin Shen, Xiaofan Jiang, Shao-fu Shih, Donghuan Lu, Feng Zhao, Dezhi Hong, Qiang Li, Shahriar Nirjon, Robert Dickerson, and John A. Stankovic. 2012. Septimu2 - Earphones for Continuous and Non-intrusive Physiological and Environmental Monitoring. In Proceedings of the 10th ACM Conference on Embedded Network Sensor Systems (SenSys ’12). ACM, New York, NY, USA, 387–388. DOI:http://dx.doi.org/10.1145/2426656.2426722 \ 14. Satoshi Hyuga, Masaki Ito, Masayuki Iwai, and Kaoru Sezaki. 2015. Estimate a User’s Location using Smartphone’s Barometer on a Subway. In Proceedings of the 5th International Workshop on Mobile Entity Localization and Tracking in GPS-less Environments (MELT ’15). ACM, New York, NY, USA, Article 2, 4 pages. DOI:http://dx.doi.org/10.1145/2830571.2830576 \ 15. Apple Inc. 2016. AirPods. (2016). http://www.apple.com/airpods/ (accessed 2017-4-4). \ 16. Thibaut Jacob, Gilles Bailly, Eric Lecolinet, Géry Casiez, and Marc Teyssier. 2016. Desktop Orbital Camera Motions using Rotational Head Movements. In Proceedings of the 2016 Symposium on Spatial User Interaction (SUI ’16). ACM, New York, NY, USA, 139–148. DOI: http://dx.doi.org/10.1145/2983310.2985758 \ 17. Takashi Kato. 2008. Manducating Motion Detection Apparatus. (2008). JP-200848791-A，2008. (In Japanese). \ 18. Yojiro Kawamura. 1972. About Occlusion Physiology. The Nippon Dental Review 359 (1972), 25–33. (In Japanese). \ 19. Gierad Laput, Xiang ‘Anthony’ Chen, and Chris Harrison. 2016. SweepSense: Ad Hoc Conﬁguration Sensing using Reﬂected Swept-Frequency Ultrasonics. In Proceedings of the 21st International Conference on Intelligent User Interfaces (IUI ’16). ACM, New York, NY, USA, 332–335. DOI: http://dx.doi.org/10.1145/2856767.2856812 \ 20. Roman Lissermann, Jochen Huber, Aristotelis Hadjakos, Suranga Nanayakkara, and Max Mühlhäuser. 2014. EarPut: Augmenting Ear-worn Devices for Ear-based Interaction. In Proceedings of the 26th Australian Computer-Human Interaction Conference on Designing Futures: The Future of Design (OzCHI ’14). ACM, New York, NY, USA, 300–307. DOI: http://dx.doi.org/10.1145/2686612.2686655 \ 21. Edmund LoPresti, David M. Brienza, Jennifer Angelo, Lars Gilbertson, and Jonathan Sakai. 2000. Neck Range of Motion and Use of Computer Head Controls. In Proceedings of the Fourth International ACM Conference on Assistive Technologies (ASSETS ’00). ACM, New York, NY, USA, 121–128. DOI: http://dx.doi.org/10.1145/354324.354352 \ 22. Michael J. Lyons, Chi-Ho Chan, and Nobuji Tetsutani. 2004. MouthType: Text Entry by Hand and Mouth. In Proceedings of the ACM Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA ’04). ACM, New York, NY, USA, 1383–1386. DOI: http://dx.doi.org/10.1145/985921.986070 \ 23. Hiroyuki Manabe and Masaaki Fukumoto. 2006. Full-time Wearable Headphone-type Gaze Detector. In Proceedings of the ACM Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA ’06). ACM, New York, NY, USA, 1073–1078. DOI: http://dx.doi.org/10.1145/1125451.1125655 \ 24. Hiroyuki Manabe and Masaaki Fukumoto. 2012. Headphone Taps: A Simple Technique to Add Input Function to Regular Headphones. In Proceedings of the 14th International Conference on Human-computer Interaction with Mobile Devices and Services Companion (MobileHCI ’12). ACM, New York, NY, USA, 177–180. DOI: http://dx.doi.org/10.1145/2371664.2371703 \ 25. Hiroyuki Manabe, Masaaki Fukumoto, and Tohru Yagi. 2013. Conductive Rubber Electrodes for Earphone-based Eye Gesture Input Interface. In Proceedings of the 2013 International Symposium on Wearable Computers (ISWC ’13). ACM, New York, NY, USA, 33–40. DOI: http://dx.doi.org/10.1145/2493988.2494329 \ 26. Kohei Matsumura, Daisuke Sakamoto, Masahiko Inami, and Takeo Igarashi. 2012. Universal Earphones: Earphones with Automatic Side and Shared Use Detection. In Proceedings of the 2012 ACM International Conference on Intelligent User Interfaces (IUI ’12). ACM, New York, NY, USA, 305–306. DOI: http://dx.doi.org/10.1145/2166966.2167025 \ 27. Denys J.C. Matthies. 2013. InEar BioFeedController: A Headset for Hands-free and Eyes-free Interaction with Mobile Devices. In CHI ’13 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’13). ACM, New York, NY, USA, 1293–1298. DOI: http://dx.doi.org/10.1145/2468356.2468587 \ 28. Kosuke Nakajima, Yuichi Itoh, Yusuke Hayashi, Kazuaki Ikeda, Kazuyuki Fujita, and Takao Onoye. 2013. Emoballoon: A Balloon-Shaped Interface Recognizing Social Touch Interactions. In Proceeding of the 10th International Conference on Advances in Computer Entertainment (ACE 2013). Springer-Verlag New York, Inc., New York, NY, USA, 182–197. DOI: http://dx.doi.org/10.1007/978-3-319-03161-3_13 \ 29. Guerman G. Nemirovski. 2001. System and Method for Detecting an Action of the Head and Generating an Output in Response Thereto. (2001). WO 200139662-A2. \ 30. Ming-Zher Poh, Kyunghee Kim, Andrew D. Goessling, Nicholas C. Swenson, and Rosalind W. Picard. 2009. Heartphones: Sensor Earphones and Mobile Application for Non-obtrusive Health Monitoring. In Proceedings of the 2009 International Symposium on Wearable Computers (ISWC ’09). IEEE Computer Society, Washington, DC, USA, 153–154. DOI: http://dx.doi.org/10.1109/ISWC.2009.35 \ 31. JunRong Qi. 2016. Cross-correlation between Mandibular Condylar Movements and Distortion of External Auditory Meatus. Ph.D. Dissertation. Matsumoto Dental University. (In Japanese). \ 32. Ioannis Rigas and Oleg V. Komogortsev. 2017. Current Research in Eye Movement Biometrics. Image and Vision Computing 58 (Feb. 2017), 129–141. DOI: http://dx.doi.org/10.1016/j.imavis.2016.03.014 \ 33. Masahiko Sakai. 1999. Chewing Sensor. (1999). JP-11318862-A, 1999. (In Japanese). \ 34. Katsuhiro Suzuki, Fumihiko Nakamura, Jiu Otsuka, Katsutoshi Masai, Yuta Itoh, Yuta Sugiura, and Maki Sugimoto. 2016. Facial Expression Mapping Inside Head Mounted Display by Embedded Optical Sensors. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16 Adjunct). ACM, New York, NY, USA, 91–92. DOI: http://dx.doi.org/10.1145/2984751.2985714 \ 35. Ai Tayama, Hiromasa Yamashita, Tomoo Sato, Gontaro Kitazumi, Toshio Chiba, and Akira Toki. 2014. Development and Accuracy of A Miniature Earphone-Type Bilogical Information Sensor. Journal of the Showa University Society 74, 1 (2014), 60–66. DOI: http://dx.doi.org/10.14930/jshowaunivsoc.74.60 (In Japanese). \ 36. Shigeo Wakamoto. 1995. Mastication Count Meter. (1995). JP-07213510-A，1995. (In Japanese). \ 37. Muchen Wu, Parth H. Pathak, and Prasant Mohapatra. 2015. Monitoring Building Door Events using Barometer Sensor in Smartphones. In Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp ’15). ACM, New York, NY, USA, 319–323. DOI: http://dx.doi.org/10.1145/2750858.2804257 \ 38. Haibo Ye, Tao Gu, Xianping Tao, and Jian Lu. 2014. B-Loc: Scalable Floor Localization using Barometer on Smartphone. In 11th IEEE International Conference on Mobile Ad Hoc and Sensor Systems (MASS ’14). IEEE Computer Society, Washington, DC, USA, 127–135. DOI:http://dx.doi.org/10.1109/MASS.2014.49 \ ",Jaw movement; mouth movement; facial movement; head movement; barometer; hands-free; eyes-free; earphones; outer ear interface; wearable computing,H.5.2.,uistf4170-file1.zip,uistf4170-file2.jpg,uistf4170-file3.mp4,,CanalSense: Face-Related Movement Recognition System based on Sensing Air Pressure in Ear Canals,,CanalSense is a novel face-related movement recognition system using barometers embedded in earphones. This system recognizes face-related movements based on characteristic air pressure changes inside ear canals.,"The reviewer's questions/concerns are following: \ 1) P1/R1/R2/R3/S1: Ecological validity outside of the controlled lab environment. Discuss limitations and influencing factors such as user movements, concurrent speech, and spontaneous usage.  \ 2) P1/R1/S1/Coordinator: If the authors want to claim user authentication purpose, it may require more data samples, more rigorous validation process, and higher accuracy. \ 3) R1: Clarification on Face Right gesture recognition \ 4) P1/R2: The effect of the placement of ear plugs on accuracy results \ 5) R2: Comfort of wearing earplugs that are completely airtight \ 6) R3: Missing experimental & technical details. \  \ In addition, the additional reviewer and coordinator's questions/concerns are: \ 7) S2: Cut down length (Figures and Further Possibility). \ 8) S2: Better explain the math in the 'Extraction' \ 9) S2: Consider cutting down on the machine learning description. \  \ Other comments \ 10) Coordinator: Writing needs significant proofreading and please check for small formatting mistakes. \  \  \ We satisfied these conditions as follows: \  \ 1) P1/R1/R2/R3/S1: Ecological validity outside of the controlled lab environment. Discuss limitations and influencing factors such as user movements, concurrent speech, and spontaneous usage.  \  \ We added discussion about various factors as 'Influence by Various Factors' in DISCUSSION. \  \ # In our rebuttal: \ We will discuss the following three factors in DISCUSSION. \ 1. Having a cold, stuffy nose, swallow, and changing the body temperature \ (e.g., drinking, performing hard exercise) will cause the air pressure \ change or make air pressure changes different from usual ones, and thus \ cause unwanted operations. \ 2. Standing, walking, running, biking, riding, talking, and other facial \ expressions will also make air pressure changes for each movement. In \ walking or running, the user's head moves up and down, which might \ generate waveforms similar to FU and FD. While casual small facial \ expression changes such as a wink or nose twitching have no influences on \ waveforms, large or strong facial expressions such as laughter or \ squeezing eyes tightly have influences. To reduce erroneous detection, we \ will try to collect data during such facial expressions. \ 3. An elevator, an airplane, and bad weather effect barometer values. \ About music factor, we replace 'The subject was one author ...' by 'We \ recruited 6 participants (2 females) ranging in age from 20-23. .. The \ result showed no significant difference between the two conditions (using \ paired T-tests; music condition: accuracy 87.7%; no-music condition: \ accuracy 89.9%; p=.382>.05). ..' in Influence by Sound. \  \ # We changed in Page 7-8 \ ADD: \ Influence by Various Factors \ In the evaluation, we conducted the experiments only in a controlled lab environment. In order to actually use CanalSense, it is necessary to examine the factors that can influence the accuracy. \  \ Physical Condition Factors \ Having a cold, or a stuffy nose, relieving ear pressure, swallowing, and changing the body temperature (e.g., drinking, performing intense exercise) will cause air pressure changes or make air pressure changes inside the ear canals different from usual without the user's intention, and thus cause unwanted operations. \  \ User's Movement Factors \ Standing, walking, running, biking, riding, talking and laughing will also cause air pressure changes inside the ear canals for each movement. In walking or running, the user's head moves up and down, which might generate waveforms similar to Face Up and Face Down. Although casual small facial expression changes such as a wink or nose twitching have no influences on waveforms, large or strong facial expressions such as bursting into laughter or squeezing the eyes tightly do have influences. In order to reduce erroneous detection, we will attempt to collect data during such facial expressions to improve our system.  \  \ Environmental Factors \ Some environments, such as on an elevator, aboard an airplane, and in bad weather, would affect the air pressure inside the ear canals. In our implementation, we did not use raw barometer values considering atmospheric pressure changes from day to day, but the short-time pressure changes were out of consideration. We will verify the barometer value changes in these environments and revise our implementation to accommodate the factors. \  \ Sound Factors \ The influence on air pressure by the sound played by the earphones should be considered. Because sound vibrates the air inside the ear canals, there is a possibility that the air pressure will be changed by the sound. As a pilot study to evaluate the influences, we conducted a small experiment to compare the accuracy in recognizing seven face-related movements (i.e., Open Mouth, Close Mouth, Slide Jaw Left, Slide Jaw Right, Keep Mouth Closed, Open to Close Mouth, and Read Sentences). We recruited six participants (four males and two females) ranging in age from 20–23. We compared two conditions: music and no-music. In the music condition, Symphony No. 5 by Ludwig van Beethoven was played. The RF results showed no significant difference between the two conditions (using a paired t-test; music condition: accuracy 89.9%; no-music condition: accuracy 87.7%; p=0.38>0.05). In future, we plan to conduct experiments to verify the influences of other types of sound. \  \  \ 2) P1/R1: If the authors want to claim user authentication purpose, it may require more data samples, more rigorous validation process, and higher accuracy. \ Coordinator: Suggest to remove the entire User Identification & Authentication, considering that  the accuracy is not there yet and this method has several limitations. \  \ # In our rebuttal: \ We rename FURTHER POSSIBILITY to FURTHER APPLICATION SCENARIOS and move 'User Identification/Authentication' there. Also, we describe more rigorous validation process there with its accuracy. \  \ REMOVE:  \ We had removed the entire descriptions of user identification/authentication according to the coordinator's comment. \  \  \ 3) R1: Clarification on Face Right gesture recognition. \  \ We added two paragraphs discussing Face Right as 'Low Accuracy of Face Right' in DISCUSSION AND LIMITATIONS. \  \ # In our rebuttal: \ We performed a Kruskal-Wallis test between facial motions in F1-score. It showed significant differences between these motions; however a post-hoc Tukey test using adjusted p-values showed no significant differences. We further discuss the reason why FR showed the lowest recognition rate.  \  \ # We chnaged in Page 7. \ ADD: \ Face Right seemed to exhibit low accuracy compared with the other movements. To confirm this observation, we performed a Kruskal-Wallis test between facial motions in F1-score. The results showed significant differences between these movements; however, a post-hoc Tukey test using adjusted p-values showed no significant differences. \                          \ We further discuss the reason why Face Right showed the lowest recognition rate. One possible reason for the low accuracy of Face Right is that it is confused with other movements with similar muscle movements. As shown in Table 2, Face Right is confused with Tilt Head Right and Face Up. Both Face Right and Tilt Head Right are movements in the right direction where the movement of the muscle is similar (this also appears in Face Left and Tilt Head Left). Face Up shows the second lowest recognition rate overall, and was confused with various movements including Face Right.  \  \ 4) P1/R2: The effect of the placement of ear plugs on accuracy results. \  \ We modified the experiment procedure and added a paragraph to 'Airtightness'. \  \ # In our rebuttal: \ A participant re-wore (i.e., removed and reattached) the earphones after each round. Therefore, our system is robust to re-attachment. However, it is necessary to examine the effect of the position changes due to long-term use (e.g., four hours) and inserted positions (deep/shallow). \  \ # We changed in Page 5 and 8.  \ FROM:                    \ We asked the participant to take a break after every round and to re-wear the earphone-type barometers in every break.  \ TO:                                  \ We asked the participant to take a break after every round and to re-wear (i.e., remove and reattach) the earphones after each round.  \  \ ADD: \ In addition, placement changes of the earphones would affect the airtightness. In the experiment, participants re-wore (i.e., removed and reattached) the earphones after each round. Therefore, our system would be robust to re-attachment. However, it is necessary to examine the effect of the placement changes due to long-time uses (e.g., four hours) and inserted conditions (deep or shallow).  \  \  \ 5) R2: Comfort of wearing earplugs that are completely airtight. \  \ We added a paragraph discussing comfort as 'Comfort' in DISCUSSION AND LIMITATION. \  \ # In our Rebuttal \ We used only one size of eartips while a user should use eartips that matches the user's ear size. We obtained two comments about comfort. P2, who wrote his ear canals were small in his demographic information, commented 'Smaller eartips would be better.' These would cause the eartips did not be inserted well into his ear canals and thus the airtightness was low, which lowered the recognition rate. By contrast, P10, who wrote nothing about his ear canals, commented 'The earphones was too tight.' This would mean that the airtightness of P10 was kept high, which leaded to a high recognition rate. However, the comfortness in a long-term use should be explored as future work. \  \ # We changed in Page 8.  \ Add: \ We used the commercial eartips that were carefully designed for comfort. Although we used only one size of eartips in the experiments for unifying the experimental environment, a user should use eartips that match the user's ear size. We obtained two comments about comfort. P2, who wrote his ear canals were small in his demographic information, commented 'Smaller eartips would be better.' This problem prevented the eartips from being inserted well into his ear canals, and thus the airtightness was low, which reduced the recognition rate. By contrast, P10, who wrote nothing about his ear canals, commented 'The earphones were too tight.' This would mean that the airtightness of P10 was kept high, resulting in a high recognition rate. However, comfort in long-time use should be explored as future work.  \  \  \ 6) R3: Missing experimental & technical details. \ We modified the things pointed out.  \ For example, the numerator of Equation 1 was modified for g_j-g_i and the variables of our equations were modified for collect values. The other points were modified based on the R3's comments. Also, we added the details of statistical analysis, such as our t-test used 2-tails. \  \  \ 7) S2: Cut down length (figures and Further Possibility). \  \ We modified the some figures and removed FURTHER POSSIBILITY . \  \ # In our rebuttal:  \ About further possibility, we rename FURTHER POSSIBILITY to FURTHER APPLICATION SCENARIOS and move 'User Identification/Authentication' there. Also, we describe more rigorous validation process there with its accuracy. \  \ # We changed in figure 2-13 and further possibility. \  \ We removed FURTHER POSSIBILITY. \  \ We modified figures 2-13. \ Figure 3 was merged with Figure 2 (new Figure 2). \ We added explanation of 'Extraction' into Figure 5 (new Figure 4). \ Figure 6-9 was changed into a table (Table 1). \ Figure 10-13 was removed.  \  \ In addition, we showed relationships on the amount of data in training a classifier v.s. its accuracy as a new figure (new Figure 5) according to P1. \  \  \ 8) S2: Better explain the math in the 'Extraction' \  \ We thoroughly modified the explanation of 'Extraction' including the math. \  \  \ 9) S2: Consider cutting down on the machine learning description. \  \ We cut down the description of DTW+kNN, and we add a brief summary about DTW+kNN to RESULTS. \  \ For example, in ABSTRACT, \ FROM: \ From its results, per-user recognition accuracies with eleven face-related movements were 87.6% by using Random Forest and 90.7% by using Dynamic Time Warping and kNN, four Open Mouth levels were 87.5% by using Random Forest, 86.5% by using Dynamic Time Warping and kNN.  \ TO: \ We mainly used random forest to calculate the accuracy; per-user recognition accuracies for eleven face-related movements were 87.6%, and for four Open Mouth levels were 87.5%.  \  \ The other parts of DTW+kNN were removed like this. \  \ ADD:  \ The overall accuracies of movement recognition are shown in Table 1. The accuracies for a combination of Dynamic Time Warping and k-nearest neighbor as k = 1 (DTW+kNN) were noted together with RF. As DTW can compare waveforms directory (unlike RF), we tested it. However, because the results of DTW+kNN were similar to those of RF and DTW is computationally intensive and thus real-time classification is difficult in a casual computational platform, we mainly discuss RF hereafter.  \  \  \ 10) Coordinator: Writing needs significant proofreading and please check for small formatting mistakes. \  \ We asked proofreading service to proofread, and we modified our writing based on the results of the proofreading. We also fixed many mistakes we made. ",Toshiyuki Ando,Yuki Kubo,FormatComplete,,Takahashi Industrial and Economic Research Foundation,,,,,Aug 8 16:28,
uistf4658,10/25,15,Code/education,1:30:00 PM,3:00:00 PM,4+1,1:30:00 PM,1:50:00 PM,long,long,none,1,1501,Honorable Mention,,uistf4658,A,DS.js: Turn Any Webpage into an Example-Centric Live Programming Environment for Learning Data Science,Philip,Guo,philip@pgbovine.net,uistf4658-paper.pdf,12,letter,,,"Xiong Zhang, Philip J Guo","xiongzhang@rochester.edu, philip@pgbovine.net",60236,Xiong,,Zhang,xiongzhang@rochester.edu,Computer Science,University of Rochester,Rochester,New York,United States,,,,,,21112,Philip,J,Guo,philip@pgbovine.net,Cognitive Science,UC San Diego,La Jolla,California,USA,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Data science courses and tutorials have grown popular in recent years, yet they are still taught using production-grade programming tools (e.g., R, MATLAB, and Python IDEs) within desktop computing environments. Although powerful, these tools present high barriers to entry for novices, forcing them to grapple with the extrinsic complexities of software installation and configuration, data file management, data parsing, and Unix-like command-line interfaces. To lower the barrier for novices to get started with learning data science, we created DS.js, a bookmarklet that embeds a data science programming environment directly into any existing webpage. By transforming any webpage into an example-centric IDE, DS.js eliminates the aforementioned complexities of desktop-based environments and turns the entire web into a rich substrate for learning data science. DS.js automatically parses HTML tables and CSV/TSV data sets on the target webpage, attaches code editors to each data set, provides a data table manipulation and visualization API designed for novices, and gives instructional scaffolding in the form of bidirectional previews of how the user's code and data relate.",philip@pgbovine.net,"1. 2013. A History of Live Programming. http://liveprogramming.github.io/liveblog/2013/ 01/a-history-of-live-programming/. (Jan. 2013). \ 2. 2017. The 50 Most Popular MOOCs of All Time. http://www.onlinecoursereport.com/ the-50-most-popular-moocs-of-all-time/. (2017). \ 3. 2017. binder: Turn a GitHub repo into a collection of interactive notebooks. http://mybinder.org/. (2017). \ 4. 2017. The Complete List of Data Science Bootcamps & Fellowships. http://www.skilledup.com/articles/ list-data-science-bootcamps. (2017). \ 5. 2017. Coursera course: The Data Scientist’s Toolbox. https: //www.coursera.org/learn/data-scientists-tools. (2017). \ 6. 2017. JSHint, a JavaScript Code Quality Tool. http://jshint.com/. (2017). \ 7. 2017. Microsoft Azure Notebooks. https://notebooks.azure.com/. (2017). \ 8. 2017. NumJs - Like NumPy, in JavaScript. https://github.com/nicolaspanel/numjs. (2017). \ 9. 2017. Project Jupyter. http://jupyter.org/. (2017). \ 10. 2017. A Python library for introductory data science. https://github.com/data-8/datascience. (2017). \ 11. 2017. RStudio: Open source and enterprise-ready professional software for R. https://www.rstudio.com/. (2017). \ 12. 2017. What is the maximum length of a URL in different browsers? http://stackoverflow.com/questions/ 417142/what-is-the-maximum-length-of-a-urlin-different-browsers. (2017). \ 13. 2017. Yhat End-to-End Data Science Platform: Rodeo. https://www.yhat.com/products/rodeo. (2017). \ 14. Ani Adhikari and John DeNero. 2017. Computational and Inferential Thinking: The Foundations of Data Science. https://www.inferentialthinking.com/. (2017). \ 15. Michael Bolin, Matthew Webber, Philip Rha, Tom Wilson, and Robert C. Miller. 2005. Automation and Customization of Rendered Web Pages. In Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology (UIST ’05). ACM, New York, NY, USA, 163–172. DOI: http://dx.doi.org/10.1145/1095034.1095062 \ 16. Michael Bostock, Vadim Ogievetsky, and Jeffrey Heer. 2011. D3 Data-Driven Documents. IEEE Transactions on Visualization and Computer Graphics 17, 12 (Dec. 2011), 2301–2309. DOI: http://dx.doi.org/10.1109/TVCG.2011.185 \ 17. Joel Brandt, Mira Dontcheva, Marcos Weskamp, and Scott R. Klemmer. 2010. Example-centric Programming: Integrating Web Search into the Development Environment. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’10). ACM, New York, NY, USA, 513–522. DOI: http://dx.doi.org/10.1145/1753326.1753402 \ 18. Michael J. Cafarella, Alon Halevy, and Jayant Madhavan. 2011. Structured Data on the Web. Commun. ACM 54, 2 (Feb. 2011), 72–79. DOI: http://dx.doi.org/10.1145/1897816.1897839 \ 19. Michael J. Cafarella, Alon Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang. 2008. WebTables: Exploring the Power of Tables on the Web. Proc. VLDB Endow. 1, 1 (Aug. 2008), 538–549. DOI: http://dx.doi.org/10.14778/1453856.1453916 \ 20. Andrew Cantino. 2017. SelectorGadget: point and click CSS selectors. http://selectorgadget.com/. (2017). \ 21. Bryan Chan, Leslie Wu, Justin Talbot, Mike Cammarano, and Pat Hanrahan. 2008. Vispedia: Interactive Visual Exploration of Wikipedia Data via Search-Based Integration. IEEE Transactions on Visualization and Computer Graphics 14, 6 (Nov. 2008), 1213–1220. DOI: http://dx.doi.org/10.1109/TVCG.2008.178 \ 22. R. DeLine and D. Fisher. 2015. Supporting exploratory data analysis with live programming. In Proceedings of the IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC) (VL/HCC ’15). 111–119. DOI: http://dx.doi.org/10.1109/VLHCC.2015.7357205 \ 23. Danyel Fisher, Badrish Chandramouli, Robert DeLine, Jonathan Goldstein, Andrei Aron, Mike Barnett, John Platt, James Terwilliger, and John Wernsing. 2014. Tempe: An Interactive Data Science Environment for Exploration of Temporal and Streaming Data. Technical Report. \ 24. Philip Guo. 2013. Data Science Workﬂow: Overview and Challenges. Blog @ Communications of the ACM. (Oct. 2013). \ 25. Philip J. Guo, Sean Kandel, Joseph M. Hellerstein, and Jeffrey Heer. 2011. Proactive Wrangling: Mixed-initiative End-user Programming of Data Transformation Scripts. In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (UIST ’11). ACM, New York, NY, USA, 65–74. DOI: http://dx.doi.org/10.1145/2047196.2047205 \ 26. David Huynh, Stefano Mazzocchi, and David Karger. 2007. Piggy Bank: Experience the Semantic Web Inside Your Web Browser. Web Semant. 5, 1 (March 2007), 16–27. DOI: http://dx.doi.org/10.1016/j.websem.2006.12.002 \ 27. David F. Huynh, Robert C. Miller, and David R. Karger. 2006. Enabling Web Browsers to Augment Web Sites’ Filtering and Sorting Functionalities. In Proceedings of the 19th Annual ACM Symposium on User Interface Software and Technology (UIST ’06). ACM, New York, NY, USA, 125–134. DOI: http://dx.doi.org/10.1145/1166253.1166274 \ 28. Sean Kandel, Andreas Paepcke, Joseph Hellerstein, and Jeffrey Heer. 2011. Wrangler: Interactive Visual Speciﬁcation of Data Transformation Scripts. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’11). ACM, New York, NY, USA, 3363–3372. DOI: http://dx.doi.org/10.1145/1978942.1979444 \ 29. Sean Kandel, Andreas Paepcke, Joseph Hellerstein, and Jeffrey Heer. 2012. Enterprise Data Analysis and Visualization: An Interview Study. In IEEE Visual Analytics Science & Technology (VAST). http://vis.stanford.edu/papers/ enterprise-analysis-interviews \ 30. Jun Kato, Takeo Igarashi, and Masataka Goto. 2016. Programming with Examples to Develop Data-Intensive User Interfaces. (2016), 34–42. \ 31. Mary Beth Kery, Amber Horvath, and Brad Myers. 2017. Variolite: Supporting Exploratory Programming by Data Scientists. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17). \ 32. Donald E. Knuth. 1984. Literate Programming. Comput. J. 27, 2 (May 1984), 97–111. DOI: http://dx.doi.org/10.1093/comjnl/27.2.97 \ 33. James Lin, Jeffrey Wong, Jeffrey Nichols, Allen Cypher, and Tessa A. Lau. 2009. End-user Programming of Mashups with Vegemite. In Proceedings of the 14th International Conference on Intelligent User Interfaces (IUI ’09). ACM, New York, NY, USA, 97–106. DOI: http://dx.doi.org/10.1145/1502650.1502667 \ 34. Bernard Marr. 2016. How The Citizen Data Scientist Will Democratize Big Data. https://www.forbes.com/sites/bernardmarr/2016/ 04/01/how-the-citizen-data-scientist-will -democratize-big-data/#479003e665b8. (April 2016). \ 35. Wes McKinney. 2013. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. O’Reilly. \ 36. Donald A. Norman. 2002. The Design of Everyday Things. Basic Books, Inc., New York, NY, USA. \ 37. Chris Okasaki. 1998. Purely Functional Data Structures. Cambridge University Press, New York, NY, USA. \ 38. Mark Pilgrim. 2005. Greasemonkey Hacks: Tips & Tools for Remixing the Web with Firefox (Hacks). O’Reilly Media, Inc. \ 39. Casey Reas and Ben Fry. 2014. Processing: A Programming Handbook for Visual Designers and Artists. The MIT Press. \ 40. Xin Rong, Shiyan Yan, Stephen Oney, Mira Dontcheva, and Eytan Adar. 2016. CodeMend: Assisting Interactive Programming with Bimodal Embedding. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 247–258. DOI: http://dx.doi.org/10.1145/2984511.2984544 \ 41. Arvind Satyanarayan, Dominik Moritz, Kanit Wongsuphasawat, and Jeffrey Heer. 2017. Vega-Lite: A Grammar of Interactive Graphics. IEEE Transactions on Visualization and Computer Graphics 23, 1 (Jan. 2017), 341–350. DOI: http://dx.doi.org/10.1109/TVCG.2016.2599030 \ 42. Chris Stolte, Diane Tang, and Pat Hanrahan. 2008. Polaris: A System for Query, Analysis, and Visualization of Multidimensional Databases. Commun. ACM 51, 11 (Nov. 2008), 75–84. DOI: http://dx.doi.org/10.1145/1400214.1400234 \ 43. Steven L. Tanimoto. 2013. A perspective on the evolution of live programming. In 2013 1st International Workshop on Live Programming (LIVE). 31–34. DOI: http://dx.doi.org/10.1109/LIVE.2013.6617346 \ 44. Michael Toomim, Steven M. Drucker, Mira Dontcheva, Ali Rahimi, Blake Thomson, and James A. Landay. 2009. Attaching UI Enhancements to Websites with End Users. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’09). ACM, New York, NY, USA, 1859–1868. DOI: http://dx.doi.org/10.1145/1518701.1518987 \ 45. Rachel Treisman. 2017. Yale to offer new major in data science. http://yaledailynews.com/blog/2017/03/ 08/yale-to-offer-new-major-in-data-science/. (2017). \ 46. Jake VanderPlas. 2016. Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly. \ 47. Hadley Wickham. 2014. Tidy Data. Journal of Statistical Software 59, 1 (2014), 1–23. DOI: http://dx.doi.org/10.18637/jss.v059.i10 \ 48. Jeffrey Wong and Jason I. Hong. 2007. Making Mashups with Marmite: Towards End-user Programming for the Web. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’07). ACM, New York, NY, USA, 1435–1444. DOI: http://dx.doi.org/10.1145/1240624.1240842 \ ",data science; live programming; novice programmers,H.5.m,uistf4658-file1.zip,,,,,,"DS.js turns any webpage into a live programming environment for learning data science, thus allowing you to use the entire web as a substrate for learning.","List of major changes for camera-ready, which are all highlighted in red in the PDF attached as auxiliary materials. It is also available here in Dropbox: \  \ https://www.dropbox.com/s/xicnzlig8eupcb1/dsjs_TRACK_CHANGES_2017-07-16.pdf?dl=0 \  \ - With permission from 1AC, we changed the paper title to “DS.js: Turn Any Webpage into an Example-Centric Live Programming Environment for Learning Data Science”. \  \ - Upon reviewer requests, added “example-centric” approach to the framing of this paper. \  \ - Added a more thorough discussion of web-based IDEs (WIDEs) for data science and differentiated DS.js from that prior work. \  \ - Clarified the third set of observations from our formative study by renaming that section to “Code, data, and exposition are separated,” since that more directly reflects what our interview subjects described. \  \ - Added a more thorough discussion of Limitations on Page 8 regarding scale, lack of security, and potential advantages of using a browser extension rather than a bookmarklet. \  \ - Added a summary of our user study results as well as a more thorough conclusion, taking reviewer suggestions into account. \  \  \ Thanks for your consideration! \ ",Xiong Zhang,Philip Guo,FormatComplete,,,,,,,Aug 8 2:06,
uistf3019,10/25,15,Code/education,1:30:00 PM,3:00:00 PM,4+1,1:50:00 PM,2:10:00 PM,long,long,uistf4658,2,1502,,,uistf3019,A,Torta: Generating Mixed-Media GUI and Command-Line App Tutorials Using Operating-System-Wide Activity Tracing,Philip,Guo,philip@pgbovine.net,uistf3019-paper.pdf,12,letter,,,"Alok Mysore, Philip J Guo","amysore@eng.ucsd.edu, philip@pgbovine.net",71861,Alok,,Mysore,amysore@eng.ucsd.edu,Computer Science & Engineering,UC San Diego,La Jolla,California,USA,,,,,,21112,Philip,J,Guo,philip@pgbovine.net,Cognitive Science,UC San Diego,La Jolla,California,USA,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Tutorials are vital for helping people perform complex software-based tasks in domains such as programming, data science, system administration, and computational research. However, it is tedious to create detailed step-by-step tutorials for tasks that span multiple interrelated GUI and command-line applications. To address this challenge, we created Torta, an end-to-end system that automatically generates step-by-step GUI and command-line app tutorials by demonstration, provides an editor to trim, organize, and add validation criteria to these tutorials, and provides a web-based viewer that can validate step-level progress and automatically run certain steps. The core technical insight that underpins Torta is that combining operating-system-wide activity tracing and screencast recording makes it easier to generate mixed-media (text+video) tutorials that span multiple GUI and command-line apps. An exploratory study on 10 computer science teaching assistants (TAs) found that they all preferred the experience and results of using Torta to record programming and sysadmin tutorials relevant to classes they teach rather than manually writing tutorials. A follow-up study on 6 students found that they all preferred following the Torta tutorials created by those TAs over the manually-written versions.",philip@pgbovine.net,"1. 2017. Autodesk Screencast: A simple way to share what you know. https: //knowledge.autodesk.com/community/screencast. (2017). \ 2. 2017. Developing APIs is hard. Postman makes it easy. https://www.getpostman.com/. (2017). \ 3. 2017. FFmpeg: A complete, cross-platform solution to record, convert and stream audio and video. https://ffmpeg.org/. (2017). \ 4. Lawrence Bergman, Vittorio Castelli, Tessa Lau, and Daniel Oblinger. 2005. DocWizards: A System for Authoring Follow-me Documentation Wizards. In Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology (UIST ’05). ACM, New York, NY, USA, 191–200. DOI: http://dx.doi.org/10.1145/1095034.1095067 \ 5. Michael Bolin, Matthew Webber, Philip Rha, Tom Wilson, and Robert C. Miller. 2005. Automation and Customization of Rendered Web Pages. In Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology (UIST ’05). ACM, New York, NY, USA, 163–172. DOI: http://dx.doi.org/10.1145/1095034.1095062 \ 6. Bryan M. Cantrill, Michael W. Shapiro, and Adam H. Leventhal. 2004. Dynamic Instrumentation of Production Systems. In Proceedings of the Annual Conference on USENIX Annual Technical Conference (ATEC ’04). USENIX Association, Berkeley, CA, USA. http: //dl.acm.org/citation.cfm?id=1247415.1247417 \ 7. Pei-Yu Chi, Sally Ahn, Amanda Ren, Mira Dontcheva, Wilmot Li, and Bj¨orn Hartmann. 2012. MixT: Automatic Generation of Step-by-step Mixed Media Tutorials. In Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology (UIST ’12). ACM, New York, NY, USA, 93–102. DOI: http://dx.doi.org/10.1145/2380116.2380130 \ 8. Pei-Yu Chi, Bongshin Lee, and Steven M. Drucker. 2014. DemoWiz: Re-performing Software Demonstrations for a Live Presentation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’14). ACM, New York, NY, USA, 1581–1590. DOI: http://dx.doi.org/10.1145/2556288.2557254 \ 9. Jennifer Fernquist, Tovi Grossman, and George Fitzmaurice. 2011. Sketch-sketch Revolution: An Engaging Tutorial System for Guided Sketching and Application Learning. In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (UIST ’11). ACM, New York, NY, USA, 373–382. DOI: http://dx.doi.org/10.1145/2047196.2047245 \ 10. Adam Fourney, Ben Lafreniere, Parmit Chilana, and Michael Terry. 2014. InterTwine: Creating Interapplication Information Scent to Support Coordinated Use of Software. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST ’14). ACM, New York, NY, USA, 429–438. DOI: http://dx.doi.org/10.1145/2642918.2647420 \ 11. Floraine Grabler, Maneesh Agrawala, Wilmot Li, Mira Dontcheva, and Takeo Igarashi. 2009. Generating Photo Manipulation Tutorials by Demonstration. In ACM SIGGRAPH 2009 Papers (SIGGRAPH ’09). ACM, New York, NY, USA, Article 66, 9 pages. DOI: http://dx.doi.org/10.1145/1576246.1531372 \ 12. Tovi Grossman, Justin Matejka, and George Fitzmaurice. 2010. Chronicle: Capture, Exploration, and Playback of Document Workﬂow Histories. In Proceedings of the 23Nd Annual ACM Symposium on User Interface Software and Technology (UIST ’10). ACM, New York, NY, USA, 143–152. DOI: http://dx.doi.org/10.1145/1866029.1866054 \ 13. Philip J. Guo. 2012. Software Tools to Facilitate Research Programming. Ph.D. Dissertation. Stanford University. \ 14. Philip J. Guo and Margo Seltzer. 2012. BURRITO: Wrapping Your Lab Notebook in Computational Infrastructure. In Proceedings of the 4th USENIX Workshop on the Theory and Practice of Provenance (TaPP’12). USENIX Association, Berkeley, CA, USA. http: //dl.acm.org/citation.cfm?id=2342875.2342882 \ 15. Jeff Huang and Michael B. Twidale. 2007. Graphstract: Minimal Graphical Help for Computers. In Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology (UIST ’07). ACM, New York, NY, USA, 203–212. DOI: http://dx.doi.org/10.1145/1294211.1294248 \ 16. Scott Klemmer. 2017. UCSD Interaction Design COGS120/CSE170 - Winter 2017. http://ixd.ucsd.edu/home/w17/index.php. (2017). \ 17. Rebecca P. Krosnick. 2014. VideoDoc: Combining Videos and Lecture Notes for a Better Learning Experience. Master’s thesis. MIT Department of Electrical Engineering and Computer Science, Cambridge, MA. \ 18. Nate Kushman and Dina Katabi. 2010. Enabling Conﬁguration-independent Automation by Non-expert Users. In Proceedings of the 9th USENIX Conference on Operating Systems Design and Implementation (OSDI’10). USENIX Association, Berkeley, CA, USA, 223–236. http: //dl.acm.org/citation.cfm?id=1924943.1924959 \ 19. Oren Laadan, Ricardo A. Baratto, Dan B. Phung, Shaya Potter, and Jason Nieh. 2007. DejaView: A Personal Virtual Computer Recorder. In Proceedings of Twenty-ﬁrst ACM SIGOPS Symposium on Operating Systems Principles (SOSP ’07). ACM, New York, NY, USA, 279–292. DOI: http://dx.doi.org/10.1145/1294261.1294289 \ 20. Benjamin Lafreniere, Tovi Grossman, and George Fitzmaurice. 2013. Community Enhanced Tutorials: Improving Tutorials with Multiple Demonstrations. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’13). ACM, New York, NY, USA, 1779–1788. DOI: http://dx.doi.org/10.1145/2470654.2466235 \ 21. Ben Lafreniere, Tovi Grossman, Justin Matejka, and George Fitzmaurice. 2014. Investigating the Feasibility of Extracting Tool Demonstrations from In-situ Video Content. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’14). ACM, New York, NY, USA, 4007–4016. DOI: http://dx.doi.org/10.1145/2556288.2557142 \ 22. Ian Li, Jeffrey Nichols, Tessa Lau, Clemens Drews, and Allen Cypher. 2010. Here’s What I Did: Sharing and Reusing Web Activity with ActionShot. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’10). ACM, New York, NY, USA, 723–732. DOI: http://dx.doi.org/10.1145/1753326.1753432 \ 23. Cuong Nguyen and Feng Liu. 2015. Making Software Tutorial Video Responsive. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15). ACM, New York, NY, USA, 1565–1568. DOI: http://dx.doi.org/10.1145/2702123.2702209 \ 24. Amy Pavel, Colorado Reed, Bj¨orn Hartmann, and Maneesh Agrawala. 2014. Video Digests: A Browsable, Skimmable Format for Informational Lecture Videos. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST ’14). ACM, New York, NY, USA, 573–582. DOI: http://dx.doi.org/10.1145/2642918.2647400 \ 25. Suporn Pongnumkul, Mira Dontcheva, Wilmot Li, Jue Wang, Lubomir Bourdev, Shai Avidan, and Michael F. Cohen. 2011. Pause-and-play: Automatically Linking Screencast Video Tutorials with Applications. In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (UIST ’11). ACM, New York, NY, USA, 135–144. DOI: http://dx.doi.org/10.1145/2047196.2047213 \ 26. Cheng-Yao Wang, Wei-Chen Chu, Hou-Ren Chen, Chun-Yen Hsu, and Mike Y. Chen. 2014. EverTutor: Automatically Creating Interactive Guided Tutorials on Smartphones by User Demonstration. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’14). ACM, New York, NY, USA, 4027–4036. DOI: http://dx.doi.org/10.1145/2556288.2557407 \ ",tutorial generation; mixed-media tutorials; software tutorials,H.5.m,uistf3019-file1.zip,,,,,,Torta allows you to quickly create mixed-media video+text software tutorials for an assortment of GUI and command-line apps by demonstration.,"List of major changes for camera-ready, which are all highlighted in red in the PDF in the auxiliary materials. Also can be found here in Dropbox: \  \ https://www.dropbox.com/s/0g6vehd5cg2delt/torta_TRACK_CHANGES_2017-07-16.pdf?dl=0 \  \ - With permission from 1AC, we changed the paper title to “Torta: Generating Mixed-Media GUI and Command-Line App Tutorials Using Operating-System-Wide Activity Tracing” to remove the overly-broad term “multi-application” and replace it with more precise terminology about our paper’s contributions. \  \ - Made the contributions of our paper much more specific, removing all instances of the term “multi-application” throughout and replacing it with a more precise description of the kinds of GUI and command-line app tutorials that Torta is capable of making. \  \ - Removed all instances of the term “learning” from the paper so that we are not making any claims about learning. This further tightens the scope of this paper’s contributions be about creating and consuming a specific kind of tutorial. \  \ - Added a few additional citations as requested by reviewers. \  \ - Shortened the Formative Interviews section and removed a figure there to free up some space, and also because R1 thought that it was not a critical section. \  \ - Clarified some implementation details. \  \ - Renamed the evaluation section to “Exploratory User Studies” to emphasize the exploratory nature of our studies. \  \ - Removed the problematic parenthetical (“Note that since Torta subsumes screencast videos, that is a less informative comparison”) from study description and added an explicit limitation upfront that we did not compare against screencast video recording. \  \ - Renamed the consumer study to a “Pilot Study,” as one reviewer requested. \  \ - Added anecdotes for both the producer and consumer studies where subjects compared their experiences with using Torta to their prior experiences with screencast videos. \  \ - Moved the Discussion section after the Exploratory User Studies section \  \ - Added a more detailed discussion of limitations in the Discussion about the limits of Torta’s approach. \  \  \ Thanks for your consideration! \ ",Alok Mysore,Philip Guo,FormatComplete,,,,,,,Aug 8 3:34,
uistf3771,10/25,15,Code/education,1:30:00 PM,3:00:00 PM,4+1,2:10:00 PM,2:30:00 PM,long,long,uistf3019,3,1503,,,uistf3771,A,Codestrates: Literate Computing with Webstrates,Roman,Rädle,contact@romanraedle.com,uistf3771-paper.pdf,11,letter,Times-Roman,,"Roman Rädle, Midas Nouwens, Kristian B Antonsen, James R Eagan, Clemens N Klokmose","contact@romanraedle.com, midasnouwens@gmail.com, kba@cc.au.dk, james.eagan@telecom-paristech.fr, clemens@cavi.au.dk",24374,Roman,,Rädle,contact@romanraedle.com,Digital Design and Information Studies,Aarhus University,Aarhus,,Denmark,,,,,,63740,Midas,,Nouwens,midasnouwens@gmail.com,Digital Design and Information Studies,Aarhus University,Aarhus,,Denmark,,,,,,71867,Kristian,B,Antonsen,kba@cc.au.dk,Digital Design and Information Studies,Aarhus University,Aarhus,,Denmark,,,,,,5893,James,R,Eagan,james.eagan@telecom-paristech.fr,,"LTCI, CNRS, Telecom ParisTech, Université Paris-Saclay",Paris,,France,,,,,,12031,Clemens,N,Klokmose,clemens@cavi.au.dk,Digital Design and Information Studies,Aarhus University,Aarhus,,Denmark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We introduce Codestrates, a literate computing approach to developing interactive software. Codestrates blurs the distinction between the use and development of applications. It builds on the literate computing approach, commonly found in interactive notebooks such as Jupyter notebook. Literate computing weaves together prose and live computation in the same document. However, literate computing in interactive notebooks are limited to computation and it is challenging to extend their user interface, reprogram their functionality, or develop stand-alone applications. Codestrates builds literate computing capabilities on top of Webstrates and demonstrates how it can be used for (i) collaborative interactive notebooks, (ii) extending its functionality from within itself, and (iii) developing reprogrammable applications.",contact@romanraedle.com,"1. Margaret M. Burnett and Brad A. Myers. 2014. Future of End-user Software Engineering: Beyond the Silos. In Proceedings of the on Future of Software Engineering (FOSE 2014). ACM, New York, NY, USA, 201–211. DOI:http://dx.doi.org/10.1145/2593882.2593896 \ 2. Kerry Shih-Ping Chang and Brad A. Myers. 2014. Creating Interactive Web Data Applications with Spreadsheets. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST ’14). ACM, New York, NY, USA, 87–96. DOI: http://dx.doi.org/10.1145/2642918.2647371 \ 3. C. A. Ellis and S. J. Gibbs. 1989. Concurrency Control in Groupware Systems. SIGMOD Rec. 18, 2 (June 1989), 399–407. DOI: http://dx.doi.org/10.1145/66926.66963 \ 4. Adele Goldberg. 1995. Why Smalltalk? Commun. ACM 38, 10 (Oct. 1995), 105–107. DOI: http://dx.doi.org/10.1145/226239.226260 \ 5. Danny Goodman. 1987. The Complete HyperCard  Handbook. Random House Inc., New York, NY, USA.  \ 6. Dan Ingalls, Ted Kaehler, John Maloney, Scott Wallace, and Alan Kay. 1997. Back to the Future: The Story of Squeak, a Practical Smalltalk Written in Itself. SIGPLAN Not. 32, 10 (Oct. 1997), 318–326. DOI: http://dx.doi.org/10.1145/263700.263754 \ 7. Clemens N. Klokmose, James R. Eagan, Siemen Baader, Wendy Mackay, and Michel Beaudouin-Lafon. 2015. Webstrates: Shareable Dynamic Media. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST ’15). ACM, New York, NY, USA, 280–290. DOI: http://dx.doi.org/10.1145/2807442.2807446 \ 8. Thomas Kluyver, Benjamin Ragan-Kelley, Fernando Pérez, Brian Granger, Matthias Bussonnier, Jonathan Frederic, Kyle Kelley, Jessica Hamrick, Jason Grout, Sylvain Corlay, and et al. 2016. Jupyter Notebooks–a publishing format for reproducible computational workﬂows. Positioning and Power in Academic Publishing: Players, Agents and Agendas (2016), 87–90. DOI: http://dx.doi.org/10.3233/978-1-61499-649-1-87 \ 9. Donald E. Knuth. 1984. Literate Programming. Comput. J. 27, 2 (1984), 97–111. DOI: http://dx.doi.org/10.1093/comjnl/27.2.97 \ 10. Robert Krahn, Dan Ingalls, Robert Hirschfeld, Jens Lincke, and Krzysztof Palacz. 2009. Lively Wiki a Development Environment for Creating and Sharing Active Web Content. In Proceedings of the 5th International Symposium on Wikis and Open Collaboration (WikiSym ’09). ACM, New York, NY, USA, Article 9, 10 pages. DOI: http://dx.doi.org/10.1145/1641309.1641324 \ 11. John H. Maloney and Randall B. Smith. 1995. Directness and Liveness in the Morphic User Interface Construction Environment. In Proceedings of the 8th Annual ACM Symposium on User Interface and Software Technology (UIST ’95). ACM, New York, NY, USA, 21–28. DOI: http://dx.doi.org/10.1145/215585.215636 \ 12. Jarrod K. Millman and Fernando Pérez. 2014. Developing open-source scientiﬁc practice. Implementing Reproducible Research 149 (2014). \ 13. Brad Myers, Scott E. Hudson, and Randy Pausch. 2000. Past, Present, and Future of User Interface Software Tools. ACM Trans. Comput.-Hum. Interact. 7, 1 (March 2000), 3–28. DOI: http://dx.doi.org/10.1145/344949.344959 \ 14. David A. Nichols, Pavel Curtis, Michael Dixon, and John Lamping. 1995. High-latency, Low-bandwidth Windowing in the Jupiter Collaboration System. In Proceedings of the 8th Annual ACM Symposium on User Interface and Software Technology (UIST ’95). ACM, New York, NY, USA, 111–120. DOI: http://dx.doi.org/10.1145/215585.215706 \ 15. Dan R. Olsen, Jr. 2007. Evaluating User Interface Systems Research. In Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology (UIST ’07). ACM, New York, NY, USA, 251–258. DOI: http://dx.doi.org/10.1145/1294211.1294256 \ 16. Arvind Satyanarayan, Dominik Moritz, Kanit Wongsuphasawat, and Jeffrey Heer. 2017. Vega-Lite: A Grammar of Interactive Graphics. IEEE Transactions on Visualization and Computer Graphics 23, 1 (jan 2017), 341–350. DOI: http://dx.doi.org/10.1109/tvcg.2016.2599030 \ 17. Eric Schulte and Dan Davison. 2011. Active Documents with Org-Mode. Computing in Science & Engineering 13, 3 (may 2011), 66–73. DOI: http://dx.doi.org/10.1109/mcse.2011.41 \ 18. Antero Taivalsaari, Tommi Mikkonen, Dan Ingalls, and Krzysztof Palacz. 2008. Web Browser As an Application Platform: The Lively Kernel Experience. Technical Report. Mountain View, CA, USA. \ 19. Lea Verou, Amy X. Zhang, and David R. Karger. 2016. Mavo: Creating Interactive Data-Driven Web Applications by Authoring HTML. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 483–496. DOI: http://dx.doi.org/10.1145/2984511.2984551 \ 20. Jeannette M. Wing. 2006. Computational Thinking. Commun. ACM 49, 3 (March 2006), 33–35. DOI: http://dx.doi.org/10.1145/1118178.1118215 \ ",Literate programming; literate computing; interactive notebooks; real-time collaboration; reprogrammable systems,H.5.m,uistf3771-file1.zip,uistf3771-file2.jpg,uistf3771-file3.mp4,,Collaborative authoring of a physics report as an example use of Codestrates,,"We introduce Codestrates, a literate computing approach to developing interactive software. Codestrates blurs the dis­tinction between the use and development of interactive applications.","CHANGES (July 11, 2017) \  \  - The introduction has been shortened significantly; we removed the high-level focus on computational thinking and strengthened our contribution statement to make it clearer what this work adds to the UIST community \  - We added a significantly more detailed description of how Webstrates works to the implementation section. \  - The implementation of Codestrates has been rewritten for clarity and reproducibility \  - The related work has been restructured, a section discussing differences with code playgrounds has been added, references suggested by the reviewers have been added, and the distinction from Jupyter has been clarified. \  - A discussion was added which describes: the benefits of Codestrates, differences to version control systems, usability issues, and a system-oriented evaluation based on Olsen's criteria for evaluation of user interface systems research. \  - Minor language and grammar issues have been fixed throughout the paper. \  \ CHANGES (July 17, 2017) \  \ - Changed the paragraph describing literate computing to add more explanation about what it is in interactive notebooks, and added a footnote to describe how it is different from literate programming. \ - Reworded the contribution statement in the introduction to emphasize how Codestrates allows for sharing application state (in contrast to code playgrounds). \ - Rephrased the paragraph ""web-based dev playgrounds & reactive programming"" to clarify that application state is not persistent and shared with other clients \ - Changed remote collaboration paragraph to explain how applications in Codestrate can have visually distinct appearance on different clients \ - Updated Figures 3, 4, and 5. Figure 4 and 5 have device frames to provide a clearer context of use. \ - Added URL to public release of Codestrates: http://codestrates.org \ - Native English speaker proof-read the paper \ - Improved accessibility of the document according to guidelines provided by program chairs \  \ CHANGES (July 18, 2017) \  \ - Added more elaborate explanation of literate programming and literate computing to the introduction \ - Added borders to individual images in Figure 1 \ - Minor editorial changes (e.g.,  sentence structure, punctuation)",Roman Rädle,Clemens Klokmose,FormatComplete,,Aarhus University Research Foundation,,,,,Aug 6 8:30,
uistf3971,10/25,15,Code/education,1:30:00 PM,3:00:00 PM,4+1,2:30:00 PM,2:50:00 PM,long,long,uistf3771,4,1504,,,uistf3971,A,ZIPT: Zero-Integration Performance Testing of Mobile App Designs,Biplab,Deka,deka2@illinois.edu,uistf3971-paper.pdf,12,letter,,,"Biplab Deka, Zifeng Huang, Chad D Franzen, Jeffrey Nichols, Yang Li, Ranjitha Kumar","deka2@illinois.edu, zhuang45@illinois.edu, cdfranz2@illinois.edu, jeff@jeffreynichols.com, yangli@acm.org, ranjitha@illinois.edu",49332,Biplab,,Deka,deka2@illinois.edu,Department of Electrical and Computer Engineering,University of Illinois at Urbana-Champaign,Champaign,Illinois,United States,,,,,,49333,Zifeng,,Huang,zhuang45@illinois.edu,Computer Science,University of Illinois at Urbana-Champaign,Champaign,Illinois,United States,,,,,,71865,Chad,D,Franzen,cdfranz2@illinois.edu,Department of Computer Science,University of Illinois at Urbana Champaign,Urbana,Illinois,United States,,,,,,2720,Jeffrey,,Nichols,jeff@jeffreynichols.com,,"Google, Inc.",Mountain View,California,United States,,,,,,3340,Yang,,Li,yangli@acm.org,,Google Research,Mountain View,California,United States,,,,,,13333,Ranjitha,,Kumar,ranjitha@illinois.edu,Computer Science,University of Illinois at Urbana-Champaign,Urbana,Illinois,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Mobile app designers often look at examples during the early stages of the design process. In this work, we present an approach for low-overhead collection of performance data for such mobile app designs examples found in the wild. It requires no development overhead since designers can directly work with the app they are interested in instead of building prototypes based on it. It uses anonymous, unsupervised, crowdworkers to use apps while collecting detailed data about the user interactions and the app UIs in the background. Performance measures of interests (such as time on task, completion rates etc.) are then computed from these interaction traces. We demonstrate the usefulness of this approach through case studies highlighting the differences in user behaviors between apps and present examples of usability issues that it can help discover.",deka2@illinois.edu,"1. AppAnalytics. http://appanalytics.io/. \ 2. Appsee. https://www.appsee.com/. \ 3. Google Analytics for Android Apps. https://developers.google.com/analytics/ devguides/collection/android/v4/. \ 4. Mixpanel. https://mixpanel.com/. \ 5. Web form design guidelines: an eyetracking study, 2009. https://www.cxpartners.co.uk/our-thinking/ web forms design guidelines an eyetracking study/. \ 6. Introducing Android Instant Apps, 2010. http://android-developers.blogspot.com/2016/05/ android-instant-apps-evolving-apps.html. \ 7. 20 Best Practices for Mobile App Search, 2017. https://www.raywenderlich.com/153260/20-bestpractices-for-mobile-app-search. \ 8. Search outside the box with new Pinterest visual discovery tools, 2017. https://blog.pinterest.com/en/search-outsidebox-new-pinterest-visual-discovery-tools. \ 9. Chang, S., Dai, P., Hong, L., Sheng, C., Zhang, T., and Chi, E. H. Appgrouper: Knowledge-based interactive clustering tool for app search results. In Proc. IUI (2016). \ 10. Deka, B., Huang, Z., and Kumar, R. ERICA: Interaction mining mobile apps. In Proc. UIST (2016). \ 11. Dow, S., Kulkarni, A., Klemmer, S., and Hartmann, B. Shepherding the crowd yields better work. In Proc. CSCW (2012). \ 12. Eckert, C., and Stacey, M. Sources of inspiration: A language of design. Design Studies 21, 5 (2000), 523–538. \ 13. Eckert, C., Stacey, M., and Earl, C. References to past designs. Studying Designers 5 (2005), 3–21. \ 14. Grossauer, C., Holzmann, C., Steiner, D., and Guetz, A. Interaction visualization and analysis in automation industry. In Proc. MUM (2015). \ 15. Iitsuka, S., and Matsuo, Y. Website optimization problem and its solutions. In Proc. KDD (2015). \ 16. Kittur, A., Nickerson, J. V., Bernstein, M., Gerber, E., Shaw, A., Zimmerman, J., Lease, M., and Horton, J. The future of crowd work. In Proc. CSCW (2013). \ 17. Komarov, S., Reinecke, K., and Gajos, K. Z. Crowdsourcing performance evaluations of user interfaces. In Proc. CHI (2013). \ 18. Miller, S. R., and Bailey, B. P. Searching for inspiration: An in-depth look at designers example ﬁnding practices. In Proc. IDETC (2014). \ 19. Qin, Z., Tang, Y., Novak, E., and Li, Q. Mobiplay: A remote execution based record-and-replay tool for mobile applications. In Proc. ICSE (2016). \ 20. Quinn, A. J., and Bederson, B. B. Human computation: a survey and taxonomy of a growing ﬁeld. In Proc. CHI (2011). \ 21. Riehmann, P., Hanﬂer, M., and Froehlich, B. Interactive sankey diagrams. In Proc. INFOVIS (2005). \ 22. Schmidt, M. The sankey diagram in energy and material ﬂow management. Journal of industrial ecology 12, 1 (2008), 82–94. \ ",App design; design support tools; zero-integration performance testing;,D.2.2,uistf3971-file1.zip,,,,,,We present an approach for low-overhead collection of performance data for such mobile app designs examples found in the wild. ,We have incorporated many of the suggestions received from reviewers into our draft. The large changes are: \  \ - We discuss the differences between ZIPT and other techniques (A/B testing and usability testing) in more detail in the BACKGROUND AND RELATED WORK section. \  \ - In the discussion section we mention how a large scale evaluation of ZIPT should be done in the future for better understanding its benefits and limitations \ ,Biplab Deka,Ranjitha Kumar,FormatComplete,,,,,,,Jul 17 20:39,
uistf3396,10/25,15,Code/education,1:30:00 PM,3:00:00 PM,4+1,2:50:00 PM,3:00:00 PM,short,short,uistf3971,5,1505,,,uistf3396,A,Omnicode: A Novice-Oriented Live Programming Environment with Always-On Run-Time Value Visualizations,Philip,Guo,philip@pgbovine.net,uistf3396-paper.pdf,9,letter,,,"Hyeonsu Kang, Philip J Guo","hyk149@eng.ucsd.edu, philip@pgbovine.net",71866,Hyeonsu,,Kang,hyk149@eng.ucsd.edu,Computer Science & Engineering,UC San Diego,La Jolla,California,United States,,,,,,21112,Philip,J,Guo,philip@pgbovine.net,Cognitive Science,UC San Diego,La Jolla,California,USA,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Visualizations of run-time program state help novices form proper mental models and debug their code. We push this technique to the extreme by posing the following question: What if a live programming environment for an imperative language always displays the entire history of all run-time values for all program variables all the time? To explore this question, we built a prototype live IDE called Omnicode (""Omniscient Code"") that continually runs the user's Python code and uses a scatterplot matrix to visualize the entire history of all of its numerical values, along with meaningful numbers derived from other data types. To filter the visualizations and hone in on specific points of interest, the user can brush and link over the scatterplots or select portions of code. They can also zoom in to view detailed stack and heap visualizations at each execution step. An exploratory study on 10 novice programmers discovered that they found Omnicode to be useful for debugging, forming mental models, explaining their code to others, and discovering moments of serendipity that would not have been likely within an ordinary IDE.",philip@pgbovine.net,"1. 2013. A History of Live Programming. http://liveprogramming.github.io/liveblog/2013/ 01/a-history-of-live-programming/. (Jan. 2013). \ 2. 2017. doctest - Test interactive Python examples. https://docs.python.org/2/library/doctest.html. (2017). \ 3. Shaaron Ainsworth and Andrea Th Loizou. 2003. The effects of self-explaining when learning with text or diagrams. Cognitive Science 27, 4 (2003), 669–681. DOI: http://dx.doi.org/10.1207/s15516709cog2704_5 \ 4. Mordechai Ben-Ari and Jorma Sajaniemi. 2004. Roles of Variables As Seen by CS Educators. In Proceedings of the 9th Annual SIGCSE Conference on Innovation and Technology in Computer Science Education (ITiCSE ’04). ACM, New York, NY, USA, 52–56. DOI: http://dx.doi.org/10.1145/1007996.1008013 \ 5. Benjamin Biegel, Benedikt Lesch, and Stephan Diehl. 2015. Live object exploration: Observing and manipulating behavior and state of Java objects. In 2015 IEEE International Conference on Software Maintenance and Evolution (ICSME). 581–585. DOI: http://dx.doi.org/10.1109/ICSM.2015.7332518 \ 6. Margaret M. Burnett, John W. Atwood Jr, and Zachary T. Welch. 1998. Implementing Level 4 Liveness in Declarative Visual Programming Languages. In Proceedings of the IEEE Symposium on Visual Languages (VL ’98). IEEE Computer Society, Washington, DC, USA, 126–. http://dl.acm.org/citation.cfm?id=832279.834482 \ 7. Pauli Byckling, Petri Gerdt, and Jorma Sajaniemi. 2005. Roles of Variables in Object-oriented Programming. In Companion to the 20th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications (OOPSLA ’05). ACM, New York, NY, USA, 350–355. DOI: http://dx.doi.org/10.1145/1094855.1094972 \ 8. Michelene T.H. Chi, Miriam Bassok, Matthew W. Lewis, Peter Reimann, and Robert Glaser. 1989. Self-Explanations: How Students Study and Use Examples in Learning to Solve Problems. Cognitive Science 13, 2 (1989), 145–182. DOI: http://dx.doi.org/10.1207/s15516709cog1302_1 \ 9. Michelene T.H. Chi, Nicholas De Leeuw, Mei-Hung Chiu, and Christian Lavancher. 1994. Eliciting Self-Explanations Improves Understanding. Cognitive Science 18, 3 (1994), 439–477. DOI: http://dx.doi.org/10.1207/s15516709cog1803_3 \ 10. Benedict Du Boulay. 1986. Some Difﬁculties of Learning to Program. Journal of Educational Computing Research 2, 1 (1986), 57–73. http://www.tandfonline.com/doi/abs/10.1207/ S15327809JLS0904_3 \ 11. Michael D. Ernst, Jeff H. Perkins, Philip J. Guo, Stephen McCamant, Carlos Pacheco, Matthew S. Tschantz, and Chen Xiao. 2007. The Daikon system for dynamic detection of likely invariants. Science of Computer Programming 69, 1–3 (Dec. 2007), 35–45. \ 12. Chris Granger. 2017. Light Table: The next generation code editor. http://lighttable.com/. (July 2017). \ 13. Philip J. Guo. 2013. Online Python Tutor: Embeddable Web-based Program Visualization for CS Education. In Proceedings of the 44th ACM Technical Symposium on Computer Science Education (SIGCSE ’13). ACM, New York, NY, USA, 579–584. DOI: http://dx.doi.org/10.1145/2445196.2445368 \ 14. Jeffrey Heer, Michael Bostock, and Vadim Ogievetsky. 2010. A Tour Through the Visualization Zoo. Commun. ACM 53, 6 (June 2010), 59–67. DOI: http://dx.doi.org/10.1145/1743546.1743567 \ 15. Jeffrey Heer and Ben Shneiderman. 2012. Interactive Dynamics for Visual Analysis. Commun. ACM 55, 4 (April 2012), 45–54. DOI: http://dx.doi.org/10.1145/2133806.2133821 \ 16. Christopher D. Hundhausen and Jonathan L. Brown. 2007. What You See Is What You Code: A “live” algorithm development and visualization environment for novice learners. Journal of Visual Languages & Computing 18, 1 (2007), 22 – 47. DOI: http://dx.doi.org/10.1016/j.jvlc.2006.03.002 \ 17. Christopher D. Hundhausen, Sarah A. Douglas, and John T. Stasko. 2002. A Meta-Study of Algorithm Visualization Effectiveness. Journal of Visual Languages & Computing 13, 3 (2002), 259 – 290. DOI: http://dx.doi.org/10.1006/jvlc.2002.0237 \ 18. Daniel H. H. Ingalls. 1981. Design Principles Behind Smalltalk. Byte Magazine 6, 8 (1981), 286–298. \ 19. Andrew J. Ko and Brad A. Myers. 2008. Debugging Reinvented: Asking and Answering Why and Why Not Questions About Program Behavior. In Proceedings of the 30th International Conference on Software Engineering (ICSE ’08). ACM, New York, NY, USA, 301–310. DOI: http://dx.doi.org/10.1145/1368088.1368130 \ 20. Jan-Peter Kr¨amer, Joachim Kurz, Thorsten Karrer, and Jan Borchers. 2014. How live coding affects developers’ coding behavior. In Proceedings of the 2014 IEEE Symposium on Visual Languages - Human Centric Computing (VLHCC ’14). IEEE Computer Society, Washington, DC, USA, 5–8. \ 21. Tom Lieber, Joel R. Brandt, and Rob C. Miller. 2014. Addressing Misconceptions About Code with Always-on Programming Visualizations. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’14). ACM, New York, NY, USA, 2481–2490. DOI: http://dx.doi.org/10.1145/2556288.2557409 \ 22. Lauri Malmi, Ville Karavirta, Ari Korhonen, Jussi Nikander, Otto Seppl, and Panu Silvasti. 2004. Visual Algorithm Simulation Exercise System with Automatic Assessment: TRAKLA2. In Informatics in Education. 048. \ 23. Andr´es Moreno, Niko Myller, Erkki Sutinen, and Mordechai Ben-Ari. 2004. Visualizing Programs with Jeliot 3. In Proceedings of the Working Conference on Advanced Visual Interfaces (AVI ’04). ACM, New York, NY, USA, 373–376. DOI: http://dx.doi.org/10.1145/989863.989928 \ 24. Greg Nelson, Benjamin Xie, and Andrew J. Ko. 2017. Comprehension First: Evaluating a Novel Pedagogy and Tutoring System for Program Tracing in CS1. In Proceedings of the 2017 International Conference on International Computing Education Research (ICER ’17). ACM, New York, NY, USA. \ 25. David Saff and Michael D. Ernst. 2004. An Experimental Evaluation of Continuous Testing During Development. In Proceedings of the 2004 ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA ’04). ACM, New York, NY, USA, 76–85. DOI: http://dx.doi.org/10.1145/1007512.1007523 \ 26. Clifford A. Shaffer, Matthew L. Cooper, Alexander Joel D. Alon, Monika Akbar, Michael Stewart, Sean Ponce, and Stephen H. Edwards. 2010. Algorithm Visualization: The State of the Field. Trans. Comput. Educ. 10, 3, Article 9 (Aug. 2010), 22 pages. DOI: http://dx.doi.org/10.1145/1821996.1821997 \ 27. Juha Sorva. 2013. Notional Machines and Introductory Programming Education. Trans. Comput. Educ. 13, 2, Article 8 (July 2013), 31 pages. DOI: http://dx.doi.org/10.1145/2483710.2483713 \ 28. Juha Sorva, Ville Karavirta, and Lauri Malmi. 2013. A Review of Generic Program Visualization Systems for Introductory Programming Education. Trans. Comput. Educ. 13, 4, Article 15 (Nov. 2013), 64 pages. DOI: http://dx.doi.org/10.1145/2490822 \ 29. Juha Sorva and Teemu Sirki¨a. 2010. UUhistle: A Software Tool for Visual Program Simulation. In Proceedings of the 10th Koli Calling International Conference on Computing Education Research (Koli Calling ’10). ACM, New York, NY, USA, 49–54. DOI: http://dx.doi.org/10.1145/1930464.1930471 \ 30. Steven L. Tanimoto. 2013. A perspective on the evolution of live programming. In 2013 1st International Workshop on Live Programming (LIVE). 31–34. DOI: http://dx.doi.org/10.1109/LIVE.2013.6617346 \ 31. Bret Victor. 2012. Learnable Programming: Designing a programming system for understanding programs. http://worrydream.com/LearnableProgramming/. (Sept. 2012). \ 32. E. M. Wilcox, J. W. Atwood, M. M. Burnett, J. J. Cadiz, and C. R. Cook. 1997. Does Continuous Visual Feedback Aid Debugging in Direct-manipulation Programming Systems?. In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI ’97). ACM, New York, NY, USA, 258–265. DOI: http://dx.doi.org/10.1145/258549.258721 \ ",live programming; always-on visualizations,H.5.m,uistf3396-file1.zip,,,,,,Omnicode is a live programming environment for novices that displays always-on visualizations of the entire history of all numerical program values throughout execution.,"List of major changes for camera-ready, which are all highlighted in red in the PDF in the auxiliary materials. Also available here on Dropbox: \  \ https://www.dropbox.com/s/debb9ageehygvn4/omnicode_TRACK_CHANGES_2017-07-17.pdf?dl=0 \  \  \ - With permission from 1AC, we changed the paper title to “Omnicode: A Novice-Oriented Live Programming Environment with Always-On Run-Time Value Visualizations” to add “live programming” since many of the reviewers’ requests for revisions involve better framing our work in the context of live programming. \  \ - With permission from 1AC, we expanded this paper from 6 to 7 pages in order to accommodate all reviewers’ revision requests (but we will still present a short talk). \  \ - Greatly reduced the novelty claims of Omnicode and made the scope of this paper’s contributions more precise, as requested by reviewers. \  \ - Removed the “DISPLAY ALL THE VALUES!” image on the first page. \  \ - Added a more thorough discussion of live programming environments to show how Omnicode fits into this line of existing work. \  \ - Added other categories of related work suggested by reviewers, such as algorithm visualization (AV) systems, and a more thorough discussion of Bret Victor’s essay. \  \ - Added a more complete discussion of Omnicode’s design motivation, especially our design of the scatterplot matrix visualizations. \  \ - Renamed the evaluation section to “Exploratory User Study” to more accurately represent the exploratory nature of our study. \  \ - Added a discussion of Study Limitations at the end of the paper’s evaluation section. \  \ - Merged the Discussion and Conclusion sections into one and added some additional related work as described in our rebuttal. \  \  \ Thanks for your consideration! \ ",Philip Guo,Hyeonsu Kang,FormatComplete,IIS-1660819,National Science Foundation,,,,,Aug 8 3:34,
uistf3858,10/25,16,VR/AR,1:30:00 PM,3:00:00 PM,4+1,1:30:00 PM,1:50:00 PM,long,long,none,1,1601,,,uistf3858,A,NaviFields: Relevance fields for adaptive VR navigation,Roberto,Montano Murillo,R.Montano-Murillo@sussex.ac.uk,uistf3858-paper.pdf,12,letter,,,"Roberto Antonio Montano Murillo, Elia Gatti, Miguel Oliver Segovia, Marianna Obrist, Jose Pascual Molina Masso, Diego Martinez Plasencia","R.Montano-Murillo@sussex.ac.uk, elia.gatti1986@gmail.com, miguel.oliver.segovia111@gmail.com, m.obrist@sussex.ac.uk, josepascual.molina@uclm.es, dm372@sussex.ac.uk",63459,Roberto,Antonio,Montano Murillo,R.Montano-Murillo@sussex.ac.uk,"Interact Lab, School of Engineering and Informatics",University of Sussex,Brighton,,United Kingdom,,,,,,46946,Elia,,Gatti,elia.gatti1986@gmail.com,"SCHI lab, Dept of Informatics",University of Sussex,Brighton,,United Kingdom,,,,,,71889,Miguel,,Oliver Segovia,miguel.oliver.segovia111@gmail.com,Departamento de Sistemas Informaticos,Universidad de Castilla de la Mancha,Albacete,Albacete,Spain,,,,,,7742,Marianna,,Obrist,m.obrist@sussex.ac.uk,"SCHI Lab, School of Engineering and Informatics",University of Sussex,Brighton,,United Kingdom,,,,,,50582,Jose,Pascual,Molina Masso,josepascual.molina@uclm.es,Departamento de Sistemas Informaticos,Universidad de Castilla-La Mancha,Albacete,Albacete,Spain,,,,,,28400,Diego,,Martinez Plasencia,dm372@sussex.ac.uk,Department of Informatics,Sussex University,Brighton,,United Kingdom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Virtual Reality allow users to explore virtual environments naturally, by moving their head and body. However, the size of the environments they can explore is limited by real world constraints, such as the tracking technology or the physical space available. Existing techniques removing these limitations often break the metaphor of natural navigation in VR (e.g. steering techniques), involve control commands (e.g., teleporting) or hinder precise navigation (e.g., scaling user’s displacements). This paper proposes NaviFields, which quantify the requirements for precise navigation of each point of the environment, allowing natural navigation within relevant areas, while scaling users’ displacements when travelling across non-relevant spaces. This expands the size of the navigable space, retains the natural navigation metaphor and still allows for areas with precise control of the virtual head. We present a formal description of our NaviFields technique, which we compared against two alternative solutions (i.e., homogeneous scaling and natural navigation). Our results demonstrate our ability to cover larger spaces, introduce minimal disruption when travelling across bigger distances and improve very significantly the precise control of the viewpoint inside relevant areas.",R.Montano-Murillo@sussex.ac.uk,"1. Bowman, Doug A, Kruijff, Ernst, LaViola Jr, Joseph J, and Poupyrev, Ivan, An introduction to 3-D user interface design. Presence: Teleoperators and virtual environments, 2001. 10(1): p. 96-108. DOI: https://doi.org/10.1162/105474601750182342 \ 2. Bowman, Doug A, Koller, David, and Hodges, Larry F. Travel in immersive virtual environments: An evaluation of viewpoint motion control techniques. in Virtual Reality Annual International Symposium, 1997., IEEE 1997. 1997. IEEE. DOI: https://doi.org/10.1109/VRAIS.1997.583043 \ 3. Bystrom, Karl-Erik, Barfield, Woodrow, and Hendrix, Claudia, A conceptual model of the sense of presence in virtual environments. Presence: Teleoperators and Virtual Environments, 1999. 8(2): p. 241-244. DOI: http://dx.doi.org/10.1162/105474699566107 \ 4. Cirio, Gabriel, Marchal, Maud, Regia-Corte, Tony, and Lécuyer, Anatole. The magic barrier tape: a novel metaphor for infinite navigation in virtual worlds with a restricted walking workspace. in ACM Symposium on Virtual Reality Software and Technology. 2009. ACM. DOI: https://doi.org/10.1145/1643928.1643965 \ 5. Cutting, James E, How the eye measures reality and virtual reality. Behavior Research Methods, 1997. 29(1): p. 27-36. DOI: https://doi.org/10.3758/BF03200563 \ 6. Darken, Rudolph P, Cockayne, William R, and Carmein, David. The omni-directional treadmill: a locomotion device for virtual worlds. in ACM symposium on User interface software and technology. 1997. ACM. DOI: https://doi.org/10.1145/263407.263550 \ 7. Darken, Rudy P and Sibert, John L. A toolset for navigation in virtual environments. in ACM symposium on User interface software and technology. 1993. ACM. DOI: https://doi.org/10.1145/168642.168658 \ 8. Feasel, Jeff, Whitton, Mary C, and Wendt, Jeremy D. LLCM-WIP: Low-latency, continuous-motion walkingin-place. in 3D User Interfaces, 2008. 3DUI 2008. IEEE Symposium on. 2008. IEEE. DOI: https://doi.org/10.1109/3DUI.2008.4476598 \ 9. Fernandes, Kiran J, Raja, Vinesh, and Eyre, Julian, Cybersphere: the fully immersive spherical projection system. Communications of the ACM, 2003. 46(9): p. 141-146. DOI: https://doi.org/10.1145/903893.903929 \ 10. Freitag, Sebastian, Weyers, Benjamin, and Kuhlen, Torsten W. Automatic speed adjustment for travel through immersive virtual environments based on viewpoint quality. in 3D User Interfaces (3DUI), 2016 IEEE Symposium on. 2016. IEEE. DOI: https://doi.org/10.1109/3DUI.2016.7460033 \ 11. Fuhrmann, Anton, Schmalstieg, Dieter, and Gervautz, Michael, Strolling through cyberspace with your hands in your pockets: Head directed navigation in virtual environments. Virtual Environments '98 (Proceedings of the 4th EUROGRAPHICS Workshop on Virtual Environments), 1998, URL: http://www.cg.tuwien.ac.at/research/vr/hdn/ \ 12. Hayhoe, Mary, Gillam, Barbara, Chajka, Kelly, and Vecellio, Elia, The role of binocular vision in walking. Visual neuroscience, 2009. 26(1): p. 73-80. DOI: https://doi.org/10.1017/S0952523808080838 \ 13. Interrante, Victoria, Ries, Brian, and Anderson, Lee. Seven league boots: A new metaphor for augmented locomotion through moderately large scale immersive virtual environments. in 3D User Interfaces, 2007. 3DUI'07. IEEE Symposium on. 2007. IEEE. DOI: https://doi.org/10.1109/3DUI.2007.340791 \ 14. Kawato, Mitsuo, Internal models for motor control and trajectory planning. Current opinion in neurobiology, 1999. 9(6): p. 718-727. DOI: https://doi.org/10.1016/S0959-4388(99)00028-8 \ 15. Kopper, Regis, Ni, Tao, Bowman, Doug A, and Pinho, Marcio. Design and evaluation of navigation techniques for multiscale virtual environments. in Virtual Reality Conference, 2006. 2006. Ieee. DOI: https://doi.org/10.1109/VR.2006.47 \ 16. Lathrop, William B and Kaiser, Mary K, Perceived orientation in physical and virtual environments: changes in perceived orientation as a function of idiothetic information available. Presence, 2002. 11(1): p. 19-32. DOI: https://doi.org/10.1162/105474602317343631 \ 17. Laurel, Brenda, Strickland, Rachel, and Tow, Rob. Placeholder: Landscape and narrative in virtual environments. in Digital illusion. 1998. ACM Press/Addison-Wesley Publishing Co. DOI: https://doi.org/10.1145/178951.178967 \ 18. LaViola Jr, Joseph J, Feliz, Daniel Acevedo, Keefe, Daniel F, and Zeleznik, Robert C. Hands-free multiscale navigation in virtual environments. in Symposium on Interactive 3D graphics. 2001. ACM. DOI: https://doi.org/10.1145/364338.364339 \ 19. Lécuyer, Anatole, Burkhardt, J-M, Henaff, J-M, and Donikian, Stéphane. Camera motions improve the sensation of walking in virtual environments. in Virtual Reality Conference. 2006. IEEE. DOI: https://doi.org/10.1109/VR.2006.31 \ 20. López, Luis Bausá, 2013, Travel simulation inside an Immersive Video Environment (IVE), in Sitcom Lab. Institut für Geoinformatik (IFGI) of the Westfälische Wilhelms-Universität Münster http://hdl.handle.net/10362/9192. \ 21. Mackinlay, Jock D, Card, Stuart K, and Robertson, George G. Rapid controlled movement through a virtual 3D workspace. in ACM SIGGRAPH computer graphics. 1990. ACM. DOI: https://doi.org/10.1145/97880.97898 \ 22. Peck, Tabitha C, Fuchs, Henry, and Whitton, Mary C, Evaluation of reorientation techniques and distractors for walking in large virtual environments. IEEE Transactions on Visualization and Computer Graphics, 2009. 15(3): p. 383-394. DOI: https://doi.org/10.1109/TVCG.2008.191 \ 23. Peck, Tabitha C, Fuchs, Henry, and Whitton, Mary C. Improved redirection with distractors: A large-scalereal-walking locomotion interface and its effect on navigation in virtual environments. in Virtual Reality Conference (VR), 2010 IEEE. 2010. IEEE. DOI: https://doi.org/10.1109/VR.2010.5444816 \ 24. Razzaque, Sharif, Kohn, Zachariah, and Whitton, Mary C. Redirected walking. in Proceedings of EUROGRAPHICS. 2001. Manchester, UK. DOI: http://dx.doi.org/10.2312/egs.20011036 \ 25. Riecke, Bernhard, Bodenheimer, Bobby, McNamara, Timothy, Williams, Betsy, Peng, Peng, and Feuereissen, Daniel, Do we need to walk for effective virtual reality navigation? physical rotations alone may suffice. Spatial cognition VII, 2010: p. 234-247. DOI: https://doi.org/10.1007/978-3-642-14749-4_21 \ 26. Sanz, Ferran Argelaguet. Adaptive navigation for virtual environments. in IEEE Symposium on 3D User Interfaces. 2014. DOI: https://doi.org/10.1109/3DUI.2014.7027325 \ 27. Slater, Mel, Usoh, Martin, and Steed, Anthony, Taking steps: the influence of a walking technique on presence in virtual reality. ACM Transactions on ComputerHuman Interaction (TOCHI), 1995. 2(3): p. 201-219. DOI: https://doi.org/10.1145/210079.210084 \ 28. Song, Deyang and Norman, Michael. Nonlinear interactive motion control techniques for virtual space navigation. in Virtual Reality Annual International Symposium, 1993., 1993 IEEE. 1993. IEEE. DOI: https://doi.org/10.1109/VRAIS.1993.380790 \ 29. Stanney, Kay M., Virtual environments, in The humancomputer interaction handbook, A. Jacko Julie and Sears Andrew, Editors. 2003, L. Erlbaum Associates Inc. p. 621-634, \ 30. Steinicke, Frank, Bruder, Gerd, Jerald, Jason, Frenz, Harald, and Lappe, Markus, Estimation of detection thresholds for redirected walking techniques. IEEE transactions on visualization and computer graphics, 2010. 16(1): p. 17-27. DOI: https://doi.org/10.1109/TVCG.2009.62 \ 31. Sukan, Mengu, Elvezio, Carmine, Oda, Ohan, Feiner, Steven, and Tversky, Barbara. Parafrustum: Visualization techniques for guiding a user to a constrained set of viewing positions and orientations. in ACM symposium on User interface software and technology. 2014. ACM. DOI: https://doi.org/10.1145/2642918.2647417 \ 32. Suma, Evan A, Clark, Seth, Krum, David, Finkelstein, Samantha, Bolas, Mark, and Warte, Zachary. Leveraging change blindness for redirection in virtual environments. in Virtual Reality Conference (VR), 2011 IEEE. 2011. IEEE. DOI: https://doi.org/10.1109/VR.2011.5759455 \ 33. Templeman, James N, Denbrook, Patricia S, and Sibert, Linda E, Virtual locomotion: Walking in place through virtual environments. Presence: teleoperators and virtual environments, 1999. 8(6): p. 598-617. DOI: http://dx.doi.org/10.1162/105474699566512 \ 34. Terziman, Léo, Marchal, Maud, Emily, Mathieu, Multon, Franck, Arnaldi, Bruno, and Lécuyer, Anatole. Shake-your-head: Revisiting walking-in-place for desktop virtual reality. in ACM Symposium on Virtual Reality Software and Technology. 2010. ACM. DOI: https://doi.org/10.1145/1889863.1889867 \ 35. Tregillus, Sam and Folmer, Eelke. Vr-step: Walking-inplace using inertial sensing for hands free navigation in mobile vr environments. in Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. 2016. ACM. DOI: https://doi.org/10.1145/2858036.2858084 \ 36. Usoh, Martin, Arthur, Kevin, Whitton, Mary C, Bastos, Rui, Steed, Anthony, Slater, Mel, and Brooks Jr, Frederick P. Walking> walking-in-place> flying, in virtual environments. in Conference on Computer graphics and interactive techniques. 1999. ACM Press/Addison-Wesley Publishing Co. DOI: https://doi.org/10.1145/311535.311589 \ 37. Wendt, Jeremy D, Whitton, Mary C, and Brooks, Frederick P. Gud wip: Gait-understanding-driven walking-in-place. in Virtual Reality Conference (VR), 2010 IEEE. 2010. IEEE. DOI: https://doi.org/10.1109/VR.2010.5444812 \ 38. Williams, Betsy, Narasimham, Gayathri, McNamara, Tim P, Carr, Thomas H, Rieser, John J, and Bodenheimer, Bobby. Updating orientation in large virtual environments using scaled translational gain. in Symposium on Applied Perception in Graphics and Visualization. 2006. ACM. DOI: https://doi.org/10.1145/1140491.1140495 \ ",Virtual Reality; Navigation techniques; Navigation fields,I.3.6 Methodology and Techniques: Interaction techniques. ,uistf3858-file1.docx,,uistf3858-file3.mp4,uistf3858-file4.zip,,The additional material contains: \ -The analytical description of drift. \ -The experiment data analysis.,NaviFields is an adaptive VR  navigation technique that maintains the physical displacement metaphor while expanding the size of the VE that users can navigate.,"- Added author names, affiliations and keywords (First page) \  \ - Highlighted our rationale: the paper contributes a testbed evaluation of low-level navigation tasks, to explore the suitability of navifields as a VR navigation technique. Changes to: (Intro, pp2, left col, par2), (User Study, pp4, right col, par 3&4) \  \ - Better justification of the techniques we compare against, adding other techniques (besides [34]) and elaborating on discussion of egocentric-vs-alocentric techniques. Changes to:(User Study, pp4, right col, par 4). \  \ - Further explanation of related approaches (Interrante [34] and Williams [11]) in related work. Further elaboration on ego-centric vs alocentric techniques(pp2, left col, par3). \  \ - Explicitly mention that the technnique does not change the direction of motion. It uses translational gain, but no rotational gain: (pp3, right col, par 4, after Eq(2)) \  \ -Further explanation on the behaviour of our linear transfer function. We highlight it creates parabolic motion and connects this design decision to other previous techniques using dynamic viewport control (pp4, left col, par 4).  \  \ - Improved the justification of higher T_RD. We refer to the measurements from Fig4B (example environment for S=4), and us them to illustrate higher T_RD, both for homogeneous scaling and NaviField. Changes to (pp7, right col, par 2). \  \ - Elaborated on cross-country validation. We describe similar space and performance used in both deployments (pp4, right col, last paragraph). We added our use of t-tests (showing no significant differences) to (pp7, left col, par2). \  \ -Added absolute values of the baseline in a box to the bottom of each graph (Fig6 and 7), to help assess effect sizes. We explain the meaning of these boxes in (pp7, left col, par3) \  \ - Improved explanation of sinusoidal side movements of the user's head causing higher T_D  (pp7, left col, par 6) \  \ - Correction for user's side movements added to the discussion (pp10, left col, par2). We also added testing drift correction techniques as another aspect requiring further research (same paragraph). \  \ - Added the possibility to automatically create navigation fields from geometry (pp10, left col, par 6). This is followed by our previous examples of alternative ways to create the field (analysing user's navigation+clustering techniques; and explicit in-world creation, as part of game mechanics). \  \ - Proof-read, correcting various minor typos. We also reinforced the use of active voice and removed hyphens. ",Roberto Montano Murillo,Diego Martinez Plasencia,FormatComplete,638605,European Research Council (ERC) ,The Spanish Government,FPU13/03141,Mexican National Council of Science and Technology (CONACYT),,Aug 8 14:28,
uistf1663,10/25,16,VR/AR,1:30:00 PM,3:00:00 PM,4+1,1:50:00 PM,2:10:00 PM,long,long,uistf3858,2,1602,,,uistf1663,A,Erg-O: Ergonomic Optimization of Immersive Virtual Environments,Roberto,Montano Murillo,R.Montano-Murillo@sussex.ac.uk,uistf1663-paper.pdf,13,letter,,,"Roberto Antonio Montano Murillo, Sriram Subramanian, Diego Martinez Plasencia","R.Montano-Murillo@sussex.ac.uk, sriram@sussex.ac.uk, d.martinez-plasencia@sussex.ac.uk",63459,Roberto,Antonio,Montano Murillo,R.Montano-Murillo@sussex.ac.uk,"Interact Lab, School of Engineering and Informatics",University of Sussex,Brighton,,United Kingdom,,,,,,2491,Sriram,,Subramanian,sriram@sussex.ac.uk,"Interact Lab, School of Engineering and Informatics",University of Sussex,Brighton,East Sussex,United Kingdom,,,,,,28400,Diego,,Martinez Plasencia,d.martinez-plasencia@sussex.ac.uk,"Interact Lab, School of Engineering and Informatics",University of Sussex,Brighton,East Sussex,United Kingdom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Interaction in VR involves large body movements, easily inducing fatigue and discomfort. We propose Erg-O, a manipulation technique that leverages visual dominance to maintain the visual location of the elements in VR, while making them accessible from more comfortable locations. Our solution works in an open-ended fashion (no prior knowledge of the object the user wants to touch), can be used with multiple objects, and still allows interaction with any other point within user’s reach. We use optimization approaches to compute the best physical location to interact with each visual element, and space partitioning techniques to distort the visual and physical spaces based on those mappings and allow multi-object retargeting. In this paper we describe the Erg-O technique, propose two retargeting strategies and report the results from a user study on 3D selection under different conditions, elaborating on their potential and application to specific usage scenarios.",R.Montano-Murillo@sussex.ac.uk," \ 1. Azmandian, Mahdi, Mark Hancock, Hrvoje Benko, Eyal Ofek, and Andrew D Wilson. Haptic retargeting: Dynamic repurposing of passive haptics for enhanced virtual reality experiences. in Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. 2016. ACM. DOI: http://dx.doi.org/10.1145/2858036.2858226 \ 2. Bachynskyi, Myroslav, Gregorio Palmas, Antti Oulasvirta, and Tino Weinkauf, Informing the design of novel input methods with muscle coactivation clustering. ACM Transactions on Computer-Human Interaction (TOCHI), 2015. 21(6): p. 30. DOI: http://dx.doi.org/10.1145/2687921 \ 3. Badler, Norman I, Cary B Phillips, and Bonnie Lynn Webber, Simulating humans: computer graphics animation and control. 1993: Oxford University Press, \ 4. Ban, Yuki, Takashi Kajinami, Takuji Narumi, Tomohiro Tanikawa, and Michitaka Hirose. Modifying an identified curved surface shape using pseudo-haptic effect. in Haptics Symposium (HAPTICS), 2012 IEEE. 2012. IEEE. DOI: http://dx.doi.org/10.1007/978-3-64231401-8_3 \ 5. Banakou, Domna, Raphaela Groten, and Mel Slater, Illusory ownership of a virtual child body causes overestimation of object sizes and implicit attitude changes. Proceedings of the National Academy of Sciences, 2013. 110(31): p. 12846-12851. DOI: http://dx.doi.org/10.1073/pnas.1306779110 \ 6. Banakou, Domna and Mel Slater, Body ownership causes illusory self-attribution of speaking and influences subsequent real speaking. Proceedings of the National Academy of Sciences, 2014. 111(49): p. 17678-17683. DOI: http://dx.doi.org/10.1073/pnas.1414936111 \ 7. Barbagli, Federico, Ken Salisbury, Cristy Ho, Charles Spence, and Hong Z Tan, Haptic discrimination of force direction and the influence of visual information. ACM Transactions on Applied Perception (TAP), 2006. 3(2): p. 125-135. DOI: http://dx.doi.org/10.1145/1141897.1141901 \ 8. Bertsimas, Dimitris and John Tsitsiklis, Simulated annealing. Statistical science, 1993. 8(1): p. 10-15. DOI: http://dx.doi.org/10.1214/ss/1177011077 \ 9. Borg, Gunnar, Borg's perceived exertion and pain scales. 1998: Human kinetics, \ 10. Boring, Sebastian, Marko Jurmu, and Andreas Butz. Scroll, tilt or move it: using mobile phones to continuously control pointers on large public displays. in Proceedings of the 21st Annual Conference of the Australian Computer-Human Interaction Special Interest Group: Design: Open 24/7. 2009. ACM. DOI: http://dx.doi.org/10.1145/1738826.1738853 \ 11. Bowman, Doug A and Larry F Hodges. An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. in Proceedings of the 1997 symposium on Interactive 3D graphics. 1997. ACM. DOI: http://dx.doi.org/10.1145/253284.253301 \ 12. Bricken, Meredith, Virtual reality learning environments: potentials and challenges. ACM SIGGRAPH Computer Graphics, 1991. 25(3): p. 178184. DOI: http://dx.doi.org/10.1145/126640.126657 \ 13. Burdorf, Alex and Judith Laan, Comparison of methods for the assessment of postural load on the back. Scandinavian journal of work, environment & health, 1991: p. 425-429, URL: http://www.jstor.org/stable/40965930 \ 14. Burns, Eric, Sharif Razzaque, Abigail T Panter, Mary C Whitton, Matthew R McCallus, and Frederick P Brooks. The hand is slower than the eye: A quantitative exploration of visual dominance over proprioception. in Virtual Reality, 2005. Proceedings. VR 2005. IEEE. 2005. IEEE. DOI: http://dx.doi.org/10.1109/VR.2005.1492747 \ 15. Bustamante, Ernesto A and Randall D Spain. Measurement invariance of the Nasa TLX. in Proceedings of the Human Factors and Ergonomics Society Annual Meeting. 2008. SAGE Publications Sage CA: Los Angeles, CA. DOI: http://dx.doi.org/10.1177/154193120805201946 \ 16. Cerný, Vladimír, Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm. Journal of optimization theory and applications, 1985. 45(1): p. 41-51. DOI: http://dx.doi.org/10.1007/BF00940812 \ 17. Cheng, Lung-Pan, Eyal Ofek, Christian Holz, Hrvoje Benko, and Andrew D Wilson. Sparse Haptic Proxy: Touch Feedback in Virtual Environments Using a General Passive Prop. in Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. 2017. ACM.DOI: 10.1145/3025453.3025753 \ 18. Cheng, Yi-Ling and Kelly S Mix, Spatial training improves children's mathematics ability. Journal of Cognition and Development, 2014. 15(1): p. 2-11. DOI: http://dx.doi.org/10.1080/15248372.2012.725186 \ 19. Colman, Andrew M, A dictionary of psychology. 2015: Oxford University Press, USA, \ 20. Dunne, Alan, Son Do-Lenh, Gearóid Ó'Laighin, Chia Shen, and Paolo Bonato. Upper extremity rehabilitation of children with cerebral palsy using accelerometer feedback on a multitouch display. in Engineering in Medicine and Biology Society (EMBC), 2010 Annual International Conference of the IEEE. 2010. IEEE. DOI: http://dx.doi.org/10.1109/IEMBS.2010.5626724 \ 21. Dünser, Andreas, Karin Steinbügl, Hannes Kaufmann, and Judith Glück. Virtual and augmented reality as spatial ability training tools. in Proceedings of the 7th ACM SIGCHI New Zealand chapter's international conference on Computer-human interaction: design centered HCI. 2006. ACM. DOI: http://dx.doi.org/10.1145/1152760.1152776 \ 22. Flasar, Jan. Interaction Techniques for Object Selection/Manipulation in Non-Immersive Virtual Environments with Force Feedback. 2001. Eurohaptics, URL:http://www.eurohaptics.vision.ee.ethz.ch/2001/flas ar.pdf \ 23. Hincapié-Ramos, Juan David, Xiang Guo, Paymahn Moghadasian, and Pourang Irani. Consumed endurance: a metric to quantify arm fatigue of mid-air interactions. in Proceedings of the 32nd annual ACM conference on Human factors in computing systems. 2014. ACM. DOI: http://dx.doi.org/10.1145/2556288.2557130 \ 24. Kilteni, Konstantina, Jean-Marie Normand, Maria V Sanchez-Vives, and Mel Slater, Extending body space in immersive virtual reality: a very long arm illusion. PloS one, 2012. 7(7): p. e40867. DOI: http://dx.doi.org/10.1371/journal.pone.0040867 \ 25. Kirkpatrick, Scott, C Daniel Gelatt, and Mario P Vecchi, Optimization by simulated annealing. science, 1983. 220(4598): p. 671-680, URL: http://www.jstor.org/stable/1690046 \ 26. Kohli, Luv, Mary C Whitton, and Frederick P Brooks. Redirected touching: The effect of warping space on task performance. in 3D User Interfaces (3DUI), 2012 IEEE Symposium on. 2012. IEEE. DOI: http://dx.doi.org/10.1109/3DUI.2012.6184193 \ 27. Kohli, Luv. Redirected touching: Warping space to remap passive haptics. in 3D User Interfaces (3DUI), 2010 IEEE Symposium on. 2010. IEEE. DOI: http://dx.doi.org/10.1109/3DUI.2010.5444703 \ 28. Kokkinara, Elena, Konstantina Kilteni, Kristopher J Blom, and Mel Slater, First Person Perspective of Seated Participants Over a Walking Virtual Body Leads to Illusory Agency Over the Walking. Scientific Reports, 2016. 6. DOI: http://dx.doi.org/10.1038/srep28879 \ 29. Lee, Yongseok, Inyoung Jang, and Dongjun Lee. Enlarging just noticeable differences of visualproprioceptive conflict in VR using haptic feedback. in World Haptics Conference (WHC), 2015 IEEE. 2015. IEEE. DOI: http://dx.doi.org/10.1109/WHC.2015.7177685 \ 30. Lopez, Christopher, Pär Halje, and Olaf Blanke, Body ownership and embodiment: vestibular and multisensory mechanisms. Neurophysiologie Clinique/Clinical Neurophysiology, 2008. 38(3): p. 149161. DOI: http://dx.doi.org/10.1016/j.neucli.2007.12.006 \ 31. Matsuoka, Yoky, Sonya J Allin, and Roberta L Klatzky, The tolerance for visual feedback distortions in a virtual environment. Physiology & behavior, 2002. 77(4): p. 651-655. DOI: http://dx.doi.org/10.1016/S00319384(02)00914-9 \ 32. McAtamney, Lynn and E Nigel Corlett, RULA: a survey method for the investigation of work-related upper limb disorders. Applied ergonomics, 1993. 24(2): p. 91-99. DOI: http://dx.doi.org/10.1016/0003-6870(93)90080-S \ 33. Normand, Jean-Marie, Elias Giannopoulos, Bernhard Spanlang, and Mel Slater, Multisensory stimulation can induce an illusion of larger belly size in immersive virtual reality. PloS one, 2011. 6(1): p. e16128. DOI: http://dx.doi.org/10.1371/journal.pone.0016128 \ 34. Ott, Michela and Laura FREINA. A literature review on immersive virtual reality in education: state of the art and perspectives. in Conference proceedings of» eLearning and Software for Education «(eLSE). 2015. Universitatea Nationala de Aparare Carol I, URL: https://www.ceeol.com/search/article-detail?id=289829 \ 35. Plantard, Pierre, Edouard Auvinet, Anne-Sophie Le Pierres, and Franck Multon, Pose estimation with a kinect for ergonomic studies: Evaluation of the accuracy using a virtual mannequin. Sensors, 2015. 15(1): p. 1785-1803. DOI: http://dx.doi.org/10.3390/s150101785 \ 36. Poupyrev, Ivan, Mark Billinghurst, Suzanne Weghorst, and Tadao Ichikawa. The go-go interaction technique: non-linear mapping for direct manipulation in VR. in Proceedings of the 9th annual ACM symposium on User interface software and technology. 1996. ACM. DOI: http://dx.doi.org/10.1145/237091.237102 \ 37. Richardson, Anthony E, Daniel R Montello, and Mary Hegarty, Spatial knowledge acquisition from maps and from navigation in real and virtual environments. Memory & cognition, 1999. 27(4): p. 741-750. DOI: http://dx.doi.org/10.3758/BF03211566 \ 38. Robles-De-La-Torre, Gabriel and Vincent Hayward, Force can overcome object geometry in the perception of shape through active touch. Nature, 2001. 412(6845): p. 445-448. DOI: http://dx.doi.org/10.1038/35086588 \ 39. Sanchez-Vives, Maria V and Mel Slater, From presence to consciousness through virtual reality. Nature Reviews Neuroscience, 2005. 6(4): p. 332-339. DOI: http://dx.doi.org/10.1038/nrn1651 \ 40. Satava, Richard M, Virtual reality surgical simulator. Surgical endoscopy, 1993. 7(3): p. 203-205. DOI: http://dx.doi.org/10.1007/BF00594110 \ 41. Seymour, Neal E, Anthony G Gallagher, Sanziana A Roman, Michael K O’brien, Vipin K Bansal, Dana K Andersen, and Richard M Satava, Virtual reality training improves operating room performance: results of a randomized, double-blinded study. Annals of surgery, 2002. 236(4): p. 458-464, URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC142260 0/ \ 42. Spillmann, Jonas, Stefan Tuchschmid, and Matthias Harders, Adaptive space warping to enhance passive haptics in an arthroscopy surgical simulator. IEEE transactions on visualization and computer graphics, 2013. 19(4): p. 626-633. DOI: http://dx.doi.org/10.1109/TVCG.2013.23 \ 43. Steuer, Jonathan, Defining virtual reality: Dimensions determining telepresence. Journal of communication, 1992. 42(4): p. 73-93. DOI: http://dx.doi.org/10.1111/j.1460-2466.1992.tb00812.x \ 44. Tolani, Deepak, Ambarish Goswami, and Norman I Badler, Real-time inverse kinematics techniques for anthropomorphic limbs. Graphical models, 2000. 62(5): p. 353-388.DOI: https://doi.org/10.1006/gmod.2000.0528 \ 45. Valkov, Dimitar, Alexander Giesler, and Klaus H Hinrichs. Imperceptible depth shifts for touch interaction with stereoscopic objects. in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2014. ACM. DOI: http://dx.doi.org/10.1145/2556288.2557134 \ 46. Wachs, Juan Pablo, Mathias Kölsch, Helman Stern, and Yael Edan, Vision-based hand-gesture applications. Communications of the ACM, 2011. 54(2): p. 60-71. DOI: http://dx.doi.org/10.1145/1897816.1897838 \ 47. Wiktorin, Christina, Lena Karlqvist, and Jörgen Winkel, Validity of self-reported exposures to work postures and manual materials handling. Scandinavian journal of work, environment & health, 1993: p. 208-214, URL: http://www.jstor.org/stable/40966137 \ 48. Zhai, Shumin and Paul Milgram. Quantifying coordination in multiple DOF movement and its application to evaluating 6 DOF input devices. in Proceedings of the SIGCHI conference on Human factors in computing systems. 1998. ACM Press/Addison-Wesley Publishing Co. DOI: http://dx.doi.org/10.1145/274644.274689 \ ",Virtual reality; ergonomics; optimization,H.5.2. [User interfaces]: Ergonomics.,uistf1663-file1.docx,,uistf1663-file3.mp4,,,,Erg-O is a manipulation technique that allows ergonomic retargeting for a variable number of interactive elements within the user’s arms reach by leveraging visual dominance to maintain the visual location of elements in VR.,"-Added author names, affiliations and acknowledgements. \  \ - Highlight main novelties of the technique (Open-ended, multi-object retargetting; isomorphic visual-to-physical mapping; and optimization-based computation of retargetting mappings). Changes to (pp3, right col, bullet points) and Conclusion. \  \ - Provide an early summary of the behaviour of the technique (we describe the role of tetrahedrons to match physical and visual space; describe internal points are retargeted to enable multi-object retargeting; clarify points in the boundary are not retargeted (limit interaction to normally reachable space)). Changes to (pp3, right col, par4&5). \  \ - Clarified differences between Erg-O and Haptic retargeting and Sparse Haptic Proxy (reference added). We highlight Erg-O does not need to know target of interaction and allows free hand movements in space (instead of stream-lined interaction).  \  \ - Improved Figure 3 to show two matching tetrahedrons. We show the difference in shapes between tetrahedrons, and particular examples of how a vertex maps to equivalent vertex, and an edge to matching edge. Improved in-text explanation of this behaviour in (pp4, right col, par2). \  \ -Improved Figure 4C and 4D, showing a top 2D view of the space, and matching tetrahedrons in both spaces. These show how the matching tetrahedrons have different shapes, as a result of internal retargeted objects. This figure also shows an example path for a user's hand and the behaviour of the virtual hand. The virtual hand goes from a starting point to touch a retargeted object, then to touch the limit of the reachable space, and then touches a second object. The path of the virtual hand shows the effect of redirections when boundaries between tetrahedrons are crossed.  \  \ -Clarified differences between points in the boundary (allow users to still reach all their naturally reachable space) and internal points (multi-object retargeting). Changes to (pp5, left col, par 1&2) and (pp5, left col, par6). These changes refer to Figure 4C&D, to highlight how internal points create matching tetrahedrons with different shapes. \  \ - Improved Figure 4E and 4F, to show tetrahedron partition and how redirections happen.  \  \ - Explanation of the hand motion example in Figure 4C and 4C added, to better illustrate the effect of redirections. Changes to (pp5, left col, par6). \  \ -Clarify (reiterate) that only internal points are retargeted at the beginning of COMPUTING RETARGETING MAPPINGS (pp5, right col, par 4)  \  \ - Clarified modified RULA is only used to compute retargeting mappings. User study uses traditional RULA (pp6, left col, par5). \  \ - Clarification on RULA repeated in section Experimental Design, and explain RULA is used to check whether our example retargeting strategies actually improve ergonomics (pp8, right col, par4). \  \ - Clarify retargeting strategies presented are just two examples throughout the paper (pp3, right col, par6); (pp6, right col, par2); (pp7, left col, par2); (pp8, left col, last par); DISCUSSION and CONCLUSION. \  \ - Clarified differences between S_R strategy and naive motor-space scaling, indicating presence of redirections and limits in reachable space (pp6, right col, par 6).  \  \ - Modified Figure 6, to show the results of the two retargeting strategies from top and side views (instead of perspective view), to help assess differences between strategies.  \  \ - Modified explanation of TCT, using Fitt's law index of difficulty. Changes to (pp9, left col, last par) and   (pp9, right col, par 4). \  \ - Modified discussion as described in the rebuttal. Reinforce E_R and S_R are just two examples. Acknowledge use of other weights, functions and metrics (RULA, CE) would result in different results. Highlight how simple retargeting strategies can be successful (S_R). Highlight importance of spatial preservation metrics, giving an example of ill-posed mappings. \ ",Roberto Montano Murillo,Diego Martinez Plasencia,FormatComplete,,Mexican National Council of Science and Technology (CONACyT),,,,,Aug 8 14:25,
uistf3635,10/25,16,VR/AR,1:30:00 PM,3:00:00 PM,4+1,2:10:00 PM,2:30:00 PM,long,long,uistf1663,3,1603,,,uistf3635,A,More than a Feeling: The MiFace Framework for Defining Facial Communication Mappings,Crystal,Butler,crystal.butler@nyu.edu,uistf3635-paper.pdf,14,letter,,,"Crystal Butler, Stephanie Ann Michalowicz, Lakshmi Subramanian, Winslow Burleson","crystal.butler@nyu.edu, sam676@nyu.edu, lakshmi@cs.nyu.edu, wb50@nyu.edu",52945,Crystal,,Butler,crystal.butler@nyu.edu,Courant Institute of Mathematical Sciences,New York University,New York,New York,United States,,,,,,53016,Stephanie,Ann,Michalowicz,sam676@nyu.edu,Courant Institute of Mathematical Sciences,New York University,New York,New York,United States,,,,,,14522,Lakshmi,,Subramanian,lakshmi@cs.nyu.edu,Computer Science,New York University,New York,New York,United States,,,,,,1836,Winslow,,Burleson,wb50@nyu.edu,Motivational Environments,New York University,New York City,NY,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Facial expressions transmit a variety of social, grammatical, and affective signals. For technology to leverage this rich source of communication, tools that better model the breadth of information they convey are required. MiFace is a novel framework for creating expression lexicons that map signal values to parameterized facial muscle movements. In traditional mapping paradigms using posed photographs, naïve judges select from predetermined label sets and movements are inferred by trained experts. The set of generally accepted expressions established in this way is limited to six basic displays of affect. In contrast, our approach generatively simulates muscle movements on a 3D avatar. By applying natural language processing techniques to crowdsourced free-response labels for the resulting images, we efficiently converge on an expression’s value across signal categories. Two studies returned 218 discriminable facial expressions with 51 unique labels. The six basic emotions are included, but we additionally define such nuanced expressions as embarrassed, curious, and hopeful. \ ",crystal.butler@nyu.edu,"1. Paul André, Aniket Kittur, and Steven P. Dow. 2014. Crowd synthesis: Extracting categories and clusters from complex data. In Proceedings of the ACM Conference on Computer Supported Cooperative Work (CSCW ’14). ACM Press, New York, NY, 989–998. DOI:https://doi.org/10.1145/2531602.2531653 \ 2. Satanjeev Banerjee and Ted Pedersen. 2002. An adapted lesk algorithm for word sense disambiguation using wordnet. In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing (CICLing ’02). Springer-Verlag, London, UK, 136–145. \ 3. Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the 18th international joint conference on Artificial intelligence (IJCAI’03). Morgan Kaufmann Publishers Inc., San Francisco, CA, 805– 810. \ 4. Marian Stewart Bartlett, Joseph C. Hager, Paul Ekman, and Terrence J. Sejnowski. 1999. Measuring facial expressions by computer image analysis. Psychophysiology 36, 2 (Mar. 1999), 253–263. DOI:https://doi.org/10.1017/S0048577299971664 \ 5. C. Fabian Benitez-Quiroz, Ronnie B. Wilbur, and Aleix M. Martinez. 2016. The not face: A grammaticalization of facial expressions of emotion. Cognition 150 (May 2016), 77–84. DOI:https://doi.org/10.1016/j.cognition.2016.02.004 \ 6. Winslow Burleson and Rosalind Picard. 2007. Evidence for gender specific approaches to the development of emotionally intelligent learning companions. IEEE Intell. Syst. 22, 4 (Aug. 2007), 62– 69. DOI:https://doi.org/10.1109/MIS.2007.69 \ 7. Hiram Calvo, Oscar Méndez, and Marco A. MorenoArmendáriz. 2016. Integrated concept blending with vector space models. Comput. Speech Lang. 40 (Nov. 2016), 79–96. DOI:https://doi.org/10.1016/j.csl.2016.01.004 \ 8. Justin Cheng and Michael S. Bernstein. 2015. Flock: Hybrid crowd-machine learning classifiers. In Proceedings of the 2015 ACM International Conference on Computer-Supported Cooperative Work and Social Computing (CSCW ’15). ACM Press, New York, NY, 600–611. DOI:https://doi.org/10.1145/2675133.2675214 \ 9. Lydia B. Chilton, Greg Little, Darren Edge, Daniel S. Weld, and James A. Landay. 2013. Cascade: Crowdsourcing taxonomy creation. In Proceedings of the 31st Annual CHI Conference on Human Factors in Computing Systems (CHI ’13). ACM Press, New York, NY, 1999–2008. DOI:https://doi.org/10.1145/2470654.2466265 \ 10. Jeffrey F. Cohn, Adena J. Zlochower, James Lien, and Takeo Kanade. 1999. Automated face analysis by feature point tracking has high concurrent validity with manual FACS coding. Psychophysiology 36, 1 (Jan. 1999), 35–43. DOI:https://doi.org/10.1017/S0048577299971184 \ 11. Darren Cosker, Eva Krumhuber, and Adrian Hilton. 2011. A FACS valid 3D dynamic action unit database with applications to 3D dynamic morphable facial modeling. In Proceedings of the IEEE International Conference on Computer Vision (ICCV ’11). IEEE Computer Society, Los Alamitos, CA, 2296–2303. DOI:https://doi.org/10.1109/ICCV.2011.6126510 \ 12. Darren Cosker, Eva Krumhuber, and Adrian Hilton. 2010. Perception of linear and nonlinear motion properties using a FACS validated 3D facial model. In Proceedings of the Symposium on Applied Perception in Graphics and Visualization (APGV ’10). ACM Press, New York, NY, 101–108. DOI:https://doi.org/10.1145/1836248.1836268 \ 13. Charles Darwin and Paul Ekman. 2009. The expression of the emotions in man and animals. Oxford University Press, New York, NY. \ 14. Deepak P. and Prasad M. Deshpande. 2015. Operators for similarity search: Semantics, techniques and usage scenarios, Springer International Publishing, Cham, Switzerland. DOI:10.1007/978-3-319-21257-9 \ 15. Shichuan Du, Yong Tao, and Aleix M. Martinez. 2014. Compound facial expressions of emotion. In Proceedings of the National Academy of Sciences (PNAS ’14). National Academy of Sciences, Washington, DC, E1454–E1462. DOI:https://doi.org/10.1073/pnas.1322355111 \ 16. Paul Ekman. 1994. All emotions are basic. In The Nature of Emotion: Fundamental Questions. Oxford University Press, New York, NY, 15–19. \ 17. Paul Ekman, Wallace V. Friesen, and Joseph C. Hager. 2002. A human face. In Facial Action Coding System: The manual on CD ROM. \ 18. Clarence (Skip) Ellis and Paulo Barthelmess. 2003. The neem dream. In Proceedings of the 2003 conference on diversity in computing (TAPIA ’03). ACM Press, New York, NY, 23–29. DOI:https://doi.org/10.1145/948542.948548 \ 19. Ethan Fast, Binbin Chen, and Michael S. Bernstein. 2016. Empath: Understanding topic signals in largescale text. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM Press, New York, NY, 4647–4657. DOI:https://doi.org/10.1145/2858036.2858535 \ 20. Beverley Fehr and James A. Russell. 1984. Concept of emotion viewed from a prototype perspective. J. Exp. Psychol Gen. 113, 3 (Sept. 1984), 464–486. DOI:https://doi.org/10.1037/0096-3445.113.3.464 \ 21. Gretchen N. Foley and Julie P. Gentile. 2010. Nonverbal communication in psychotherapy. Psychiatry (Edgmont) 7, 6 (Jun. 2010), 38–44. \ 22. Nico H. Frijda. 1987. Emotion, cognitive structure, and action tendency. Cognition & Emotion 1, 2 (Apr. 1987), 115–143. DOI:https://doi.org/10.1080/02699938708408043 \ 23. Takeo Fujiwara, Rie Mizuki, Takahiro Miki, and Claude Chemtob. 2015. Association between facial expression and PTSD symptoms among young children exposed to the Great East Japan Earthquake: A pilot study. Front. in Psychol. 6 (Oct. 2015). DOI:https://doi.org/10.3389/fpsyg.2015.01534 \ 24. Jeffrey M. Girard, Jeffrey F. Cohn, Laszlo A. Jeni, Michael A. Sayette, and Fernando De la Torre. 2015. Spontaneous facial expression in unscripted social interactions can be measured automatically. Behav. Res. Methods 47, 4 (Dec. 2015), 1136–1147. DOI:https://doi.org/10.3758/s13428-014-0536-1 \ 25. Steven L. Gordon. 1981. The sociology of sentiments and emotion. Social psychology: sociological perspectives. Transaction Publishers, New Brunswick, NJ, 562-592. \ 26. Jonathan Haidt and Dacher Keltner. 1999. Culture and facial expression: Open-ended methods find more expressions and a gradient of recognition. Cognition & Emotion 13, 3 (May 1999), 225–266. DOI:https://doi.org/10.1080/026999399379267 \ 27. Shlomo Hareli and Ursula Hess. 2012. The social signal value of emotions. Cognition & Emotion 26, 3 (Apr. 2012), 385–389. DOI:https://doi.org/10.1080/02699931.2012.665029 \ 28. Arlie Russell Hochschild. 2012. The managed heart: commercialization of human feeling, Univ. of California Press, Berkeley, CA. \ 29. Rachael E. Jack, Oliver G. B.Garrod, Hui Yu, Roberto Caldara, and Philippe G. Schyns. 2012. Facial expressions of emotion are not culturally universal. In Proceedings of the National Academy of Sciences (PNAS ’12). National Academy of Sciences, Washington, DC, 7241–7244. DOI:https://doi.org/10.1073/pnas.1200155109 \ 30. Philip L. Jackson, Pierre-Emmanuel Michon, Erik Geslin, Maxime Carignan, and Danny Beaudoin. 2015. EEVEE: The empathy-enhancing virtual evolving environment. Front Hum Neurosci 9 (Mar. 2015). DOI:https://doi.org/10.3389/fnhum.2015.00112 \ 31. Aniket Kittur, Jeffrey V. Nickerson, Michael Bernstein, Elizabeth Gerber, Aaron Shaw, John Zimmerman, Matt Lease, and John Horton. 2013. The future of crowd work. In Proceedings of the ACM Conference on Computer Supported Cooperative Work (CSCW ’13). ACM Press, New York, NY, 1301–1317. DOI:https://doi.org/10.1145/2441776.2441923 \ 32. Andrea Kleinsmith and Nadia Bianchi-Berthouze. 2013. Affective body expression perception and recognition: A survey. IEEE Trans. Affective Comput. 4, 1 (Jan. 2013), 15–33. DOI:https://doi.org/10.1109/T-AFFC.2012.16 \ 33. Nicholas Kong, Marti A. Hearst, and Maneesh Agrawala. 2014. Extracting references between text and charts via crowdsourcing. In Proceedings of the 32nd Annual ACM Conference on Human Factors in Computing Systems (CHI ’14). ACM Press, New York, NY, 31–40. DOI:https://doi.org/10.1145/2556288.2557241 \ 34. Walter S. Lasecki, Young Chol Song, Henry Kautz, and Jeffrey P. Bigham. 2013. Real-time crowd labeling for deployable activity recognition. In Proceedings of the ACM Conference on Computer Supported Cooperative Work (CSCW ’13). ACM Press, New York, NY, 1203–1212. DOI:https://doi.org/10.1145/2441776.2441912 \ 35. Hao Li, Jihun Yu, Yuting Ye, and Chris Bregler. 2013. Realtime facial animation with on-the-fly correctives. ACM T. Graphics 32, 4 (Jul. 2013), 1. DOI:https://doi.org/10.1145/2461912.2462019 \ 36. Wei Lu, Yuanyuan Cai, Xiaoping Che, and Yuxun Lu. 2016. Joint semantic similarity assessment with raw corpus and structured ontology for semantic-oriented service discovery. Personal and Ubiquitous Computing 20, 3 (May 2016), 311–323. DOI:https://doi.org/10.1007/s00779-016-0921-0 \ 37. Gale M. Lucas, Jonathan Gratch, Aisha King, and Louis-Philippe Morency. 2014. Research report: It’s only a computer: Virtual humans increase willingness to disclose. Comput. Hum. Behav. 37 (Aug. 2014), 94– 100. DOI:https://doi.org/10.1016/j.chb.2014.04.043 \ 38. Patrick Lucey, Jeffrey F. Cohn, Takeo Kanade, Jason Saragih, Zara Ambadar, and Iain Matthews. 2010. The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression. In Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops (CVPRW ’10). IEEE Computer Society, Los Alamitos, CA, 94–101. DOI:https://doi.org/10.1109/CVPRW.2010.5543262 \ 39. Catherine Lutz. 1988. Unnatural emotions: Everyday sentiments on a Micronesian atoll & their challenge to western theory, University of Chicago Press, Chicago, IL. \ 40. Daniel McDuff, Rana El Kaliouby, and Rosalind W. Picard. 2012. Crowdsourcing facial responses to online videos. IEEE Trans. Affective Comput. 3, 4 (Jan. 2012), 456–468. DOI:https://doi.org/10.1109/TAFFC.2012.19 \ 41. Seiko Minoshita, Nobuaki Morita, Toshiyuki Yamashita, Maiko Yoshikawa, Tadashi Kikuchi, and Shinji Satoh. 2005. Recognition of affect in facial expression using the Noh Mask Test: Comparison of individuals with schizophrenia and normal controls. Psychiat. Clin. Neuros. 59, 1 (Feb. 2005), 4–10. DOI:https://doi.org/10.1111/j.1440-1819.2005.01325.x \ 42. Saif M. Mohammad and Peter D. Turney. 2010. Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text (CAAGET '10). Association for Computational Linguistics, Stroudsburg, PA, 26–34. \ 43. Magalie Ochs, Catherine Pelachaud, and Gary Mckeown. 2017. A user perception-based approach to create smiling embodied conversational agents. ACM Trans Interact. Intell. Syst. 7, 1 (Jan. 2017), 1–33. DOI:https://doi.org/10.1145/2925993 \ 44. Doris Peham et al. 2015. Facial affective behavior in mental disorder. J. Nonverbal Behav. 39, 4 (Dec. 2015), 371–396. DOI:https://doi.org/10.1007/s10919015-0216-6 \ 45. Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation, version 1.2. (2014). Retrieved October 16, 2016 from http://nlp.stanford.edu/projects/glove/ \ 46. Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP ’14). Association for Computational Linguistics, 1532–1543. DOI: 10.3115/v1/D14-1162 \ 47. Robert Plutchik. 2001. The nature of emotions. Am. Sci. 89, 4 (Jul. 2001), 344–350. DOI:https://doi.org/10.1511/2001.4.344 \ 48. Marie Postma-Nilsenová, Eric Postma, and Kiek Tates. 2015. Automatic detection of confusion in elderly users of a web-based health instruction video. Telemed. J. e-Health 21, 6 (Jun. 2015), 514–519. DOI:https://doi.org/10.1089/tmj.2014.0061 \ 49. Princeton University. 2015. About WordNet. (Mar. 2015). Retrieved November 10, 2016 from https://wordnet.princeton.edu/ \ 50. Etienne B. Roesch, Lucas Tamarit, Lionel Reveret, Didier Grandjean, David Sander, and Klaus R. Scherer. 2011. FACSGen: A tool to synthesize emotional facial expressions through systematic manipulation of facial action units. J. Nonverbal Behav. 35, 1 (Mar. 2011), 1– 16. DOI:https://doi.org/10.1007/s10919-010-0095-9 \ 51. James A. Russell. 1995. Facial expressions of emotion: What lies beyond minimal universality? Psychol. Bull. 118, 3 (Nov. 1995), 379–391. DOI:http://dx.doi.org/10.1037/0033-2909.118.3.379 \ 52. James A. Russell. 1994. Is there universal recognition of emotion from facial expression? A review of the cross-cultural studies. Psychol. Bull. 115, 1 (Feb. 1994), 102–141. DOI:https://doi.org/10.1037/00332909.115.1.102 \ 53. Saba Safdar, Wolfgang Friedlmeier, David Matsumoto, Seung Hee Yoo, Catherine T. Kwantes, and Hisako Kakai. 2009. Variations of emotional display rules within and across cultures: A comparison between Canada, USA, and Japan. Can. J. of Behav. Sci. 41, 1 (Jan. 2009), 1–10. DOI:https://doi.org/10.1037/a0014387 \ 54. Evangelos Sariyanidi, Hatice Gunes, and Andrea Cavallaro. 2015. Automatic analysis of facial affect: A survey of registration, representation, and recognition. IEEE Trans. Pattern Anal. Mach. Intell. 37, 6 (Jun. 2015), 1113–1133. DOI:https://doi.org/10.1109/TPAMI.2014.2366127 \ 55. José Serra, Verónica Orvalho, and Darren Cosker. 2016. Behavioural facial animation using motion graphs and mind maps. In Proceedings of the 9th International Conference on Motion in Games (MIG '16). ACM, New York, NY, 161–166. DOI:https://doi.org/10.1145/2994258.2994270 \ 56. Phillip Shaver, Judith Schwartz, Donald Kirson, and Cary O’Connor. 1987. Emotion knowledge: Further exploration of a prototype approach. J. Pers. Soc. Psychol. 52, 6 (Jun. 1987), 1061–1086. DOI:http://dx.doi.org/10.1037/0022-3514.52.6.1061 \ 57. Judith Sinzig, Dagmar Morsch, and Gerd Lehmkuhl. 2008. Do hyperactivity, impulsivity and inattention have an impact on the ability of facial affect recognition in children with autism and ADHD? Eur. Child Adolesc. Psychiatry 17, 2 (Mar. 2008), 63–72. DOI:https://doi.org/10.1007/s00787-007-0637-9 \ 58. Petr Slovák and Geraldine Fitzpatrick. 2015. Teaching and Developing Social and Emotional Skills with Technology. ACM Trans. Comput.-Hum. Interact. 22, 4 (Jun. 2015), 1–34. DOI:https://doi.org/10.1145/2744195 \ 59. Andreas Sonderegger, Klaus Heyden, Alain Chavaillaz, and Juergen Sauer. 2016. AniSAM & AniAvatar: Animated visualizations of affective states. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM Press, New York, NY, 4828–4837. DOI:https://doi.org/10.1145/2858036.2858365 \ 60. Matteo Sorci, David McCallum, and Alastair Gordon. 2011. Say it to my face! In Proceedings of 2011 Australian Market & Social Research Society National Conference (AMSRS '11). AMSRS, Sydney, Australia, 1–21. \ 61. Carlo Strapparava, Alessandro Valitutti, and Oliviero Stock. 2006. The affective weight of lexicon. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC ’06). 423426. \ 62. Alexander Todorov, Ron Dotsch, Jenny M. Porter, Nikolaas N. Oosterhof, and Virginia B. Falvello. 2013. Validation of data-driven computational models of social perception of faces. Emotion 13, 4 (Aug. 2013), 724–738. DOI:https://doi.org/10.1037/a0032335 \ 63. Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. J. Artif. Int. Res. 37 (Mar. 2010), 141–188. DOI:https://doi.org/10.1613/jair.2934 \ 64. Alessandro Vinciarelli, Maja Pantic, Dirk Heylen, Catherine Pelachaud, Isabella Poggi, Francesca D’Errico, and Marc Schroeder. 2012. Bridging the gap between social animal and unsocial machine: A survey of social signal processing. IEEE Trans. Affective Comput. 3, 1 (Jan. 2012), 69–87. DOI:https://doi.org/10.1109/T-AFFC.2011.27 \ 65. Justin Walden, Eun Hwa Jung, S. Shyam Sundar, and Ariel Celeste Johnson. 2015. Mental models of robots among senior citizens: An interview study of interaction expectations and design implications. Interact. Stud. 16, 1 (Aug. 2015), 68–88. DOI:https://doi.org/10.1075/is.16.1.04wal \ 66. Thibaut Weise, Sofien Bouaziz, Hao Li, and Mark Pauly. 2011. Realtime performance-based facial animation. In Proceedings of the International Conference on Computer Graphics & Interactive Techniques (SIGGRAPH ’11). ACM Press, New York, NY, 1–10. DOI:https://doi.org/10.1145/1964921.1964972 \ 67. Jacob Whitehill, Zewelanji Serpell, Yi-Ching Lin, Aysha Foster, and Javier R. Movellan. 2014. The faces of engagement: Automatic recognition of student engagement from facial expressions. IEEE T. Affect. Comput. 5, 1 (Jan. 2014), 86–98. DOI:https://doi.org/10.1109/TAFFC.2014.2316163 \ 68. Marzanna Wiechetek Ostos, Françoise Schenk, Tania Baenziger, and Armin von Gunten. 2011. An exploratory study on facial emotion recognition capacity in beginning Alzheimer’s disease. Eur. Neurol. 65, 6 (Jun. 2011), 361–367. DOI:https://doi.org/10.1159/000327979 \ 69. Hui Yu, Oliver G.B. Garrod, and Philippe G. Schyns. 2012. Perception-driven facial expression synthesis. Comput. Graph. 36, 3 (May 2012), 152–162. DOI:https://doi.org/10.1016/j.cag.2011.12.002 \ 70. Zhihong Zeng, Maja Pantic, Glenn I. Roisman, and Thomas S. Huang. 2009. A survey of affect recognition methods: Audio, visual, and spontaneous expressions. IEEE Trans. Pattern Anal. Mach. Intell. 31, 1 (Mar. 2008), 39–58. DOI:https://doi.org/10.1109/TPAMI.2008.52 \ 71. Shuo Zhou, Timothy Bickmore, Michael PaascheOrlow, and Brian Jack. 2014. Agent-user concordance and satisfaction with a virtual hospital discharge nurse. In Proceedings of the 14th International Conference on Intelligent Virtual Agents (IVA ’14). Lecture Notes in Computer Science. Springer Verlag, Berlin, Germany, 528–541. DOI:https://doi.org/10.1007/9783-319-09767-1_63 \ ","Facial expression recognition, virtual humans, 3D modeling, avatars, affective computing, natural language processing, social signal processing",Human-centered computing:Empirical studies in HCI; Applied computing:Psychology,uistf3635-file1.docx,uistf3635-file2.jpg,,uistf3635-file4.zip,"One of the MiFace images labeled ""happy"" demonstrates the prototypical muscle activations of a genuine smile.","This file contains all facial expression images labeled in MiFace studies one and two, along with their associated crowdsourced text labels, muscle activation codes, and hierarchical agglomerative clustering dendrograms.","MiFace is a framework for building facial expression lexicons that can be used as the foundation for developing intelligent agents capable of recognizing, modeling, and synthesizing human social signals.","The primary changes we made to our paper were to add an outline of our methods, and to separate our results from methods. The new results section follows methods. \  \ The description of our research question has been more clearly identified in the section relabeled Contribution: A New Way of Mapping FEs (facial expressions). We used some text taken directly from our rebuttal in this section. \  \ In addition, we added five references requested by the reviewers, and expanded the related work section. We added material on the topic of computer vision and using avatars to study facial expressions, per reviewer suggestions. \  \ Further description of the methods used for our Mechanical Turk studies is given, along with an added figure to illustrate the study interface. \  \ More attention has been given to defining specialist language. Discussion of limitations of our avatar in its current form is more detailed.",Crystal Butler,Stephanie Michalowicz,FormatComplete,,,,,,,Aug 9 20:50,
uistf3584,10/25,16,VR/AR,1:30:00 PM,3:00:00 PM,4+1,2:30:00 PM,2:40:00 PM,short,short,uistf3635,4,1604,,"probably short. Only 7 pages plus refs (?)

keep long? -KZG",uistf3584,A,One Reality: Augmenting How the Physical World is Experienced by combining Multiple Mixed Reality Modalities,Joan Sol,Roo,joan-sol.roo@inria.fr,uistf3584-paper.pdf,9,letter,Times-Roman,,"Joan Sol Roo, Martin Hachet","joan-sol.roo@inria.fr, Martin.Hachet@inria.fr",51557,Joan Sol,,Roo,joan-sol.roo@inria.fr,potioc,INRIA Bordeaux Sud-Ouest,Talence,,France,,,,,,5694,Martin,,Hachet,Martin.Hachet@inria.fr,,Inria,Bordeaux,,France,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Most of our daily activities take place in the physical world, which inherently imposes physical constraints. In contrast, the digital world is very flexible, but usually isolated from its physical counterpart. To combine these two realms, many Mixed Reality (MR) techniques have been explored, at different levels in the continuum. In this work we present an integrated Mixed Reality ecosystem that allows users to incrementally transition from pure physical to pure virtual experiences in a unique reality. This system stands on a conceptual framework composed of 6 levels. This paper presents these levels as well as the related interaction techniques.",joan-sol.roo@inria.fr,"1. Dragomir Anguelov, Carole Dulong, Daniel Filip, Christian Frueh, Stéphane Lafon, Richard Lyon, Abhijit Ogale, Luc Vincent, and Josh Weaver. 2010. Google street view: Capturing the world at street level. Computer 43, 6 (2010), 32–38. \ 2. Albert Barille. 1987. Once Upon a Time... Life. (1987). \ 3. Hrvoje Benko, Edward W Ishak, and Steven Feiner. 2004. Collaborative mixed reality visualization of an archaeological excavation. In Proceedings of the 3rd IEEE/ACM international Symposium on Mixed and Augmented Reality. IEEE Computer Society, 132–140. \ 4. Hrvoje Benko, Eyal Ofek, Feng Zheng, and Andrew D Wilson. 2015. Fovear: Combining an optically see-through near-eye display with projector-based spatial augmented reality. In UIST ’15. ACM, 129–135. \ 5. Mark Billinghurst, Hirokazu Kato, and Ivan Poupyrev. 2001. The magicbook-moving seamlessly between reality and virtuality. IEEE CGandA 21, 3 (2001), 6–8. \ 6. Oliver Bimber and Ramesh Raskar. 2006. Modern  approaches to augmented reality. In ACM SIGGRAPH  2006 Courses. ACM, 1.  \ 7. Mark Blythe. 2014. Research through design ﬁction:  narrative in real and imaginary abstracts. In CHI ’14.  ACM, 703–712.  \ 8. Sophia Sophia Agnes Brueckner. 2014. Out of network: technologies to connect with strangers. Ph.D. Dissertation. Massachusetts Institute of Technology. \ 9. D Alex Butler, Shahram Izadi, Otmar Hilliges, David  Molyneaux, Steve Hodges, and David Kim. 2012.  Shake’n’sense: reducing interference for overlapping  structured light depth cameras. In CHI ’12. ACM,  1933–1936.  \ 10. Andreas Butz, Tobias Hollerer, Steven Feiner, Blair MacIntyre, and Clifford Beshers. 1999. Enveloping users and computers in a collaborative 3D augmented reality. In Augmented Reality, 1999.(IWAR’99) Proceedings. 2nd IEEE and ACM International Workshop on. IEEE, 35–44. \ 11. Joanna Cole and Bruce Degen. 1994. The Magic School Bus. (1994). \ 12. Carolina Cruz-Neira, Daniel J Sandin, and Thomas A DeFanti. 1993. Surround-screen projection-based virtual reality: the design and implementation of the CAVE. In Proceedings of the 20th annual conference on Computer graphics and interactive techniques. ACM, 135–142. \ 13. Nicolas J Dedual, Ohan Oda, and Steven K Feiner. 2011. Creating hybrid user interfaces with a 2D multi-touch tabletop and a 3D see-through head-worn display. In ISMAR 2011. IEEE, 231–232. \ 14. Mike Eissele, Oliver Siemoneit, and Thomas Ertl. 2006. Transition of mixed, virtual, and augmented reality in smart production environments-an interdisciplinary view. In Robotics, Automation and Mechatronics, 2006 IEEE Conference on. IEEE, 1–6. \ 15. Renaud Gervais, Joan Sol Roo, and Martin Hachet. 2016. Tangible Viewports: Getting Out of Flatland in Desktop Environments. In TEI’16. ACM, 176–184. \ 16. Valentin Heun, Shunichi Kasahara, and Pattie Maes. 2013. Smarter objects: using AR technology to program physical objects and their interactions. In CHI’13 Extended Abstracts. ACM, 961–966. \ 17. David Holman and Roel Vertegaal. 2008. Organic User Interfaces: Designing Computers in Any Way, Shape, or Form. Commun. ACM 51, 6 (June 2008), 48–55. DOI: http://dx.doi.org/10.1145/1349026.1349037 \ 18. David Holman, Roel Vertegaal, Mark Altosaar, Nikolaus Troje, and Derek Johns. 2005. Paper windows: interaction techniques for digital paper. In CHI ’05. ACM, 591–599. \ 19. Hikaru Ibayashi, Yuta Sugiura, Daisuke Sakamoto, Natsuki Miyata, Mitsunori Tada, Takashi Okuma, Takeshi Kurata, Masaaki Mochimaru, and Takeo Igarashi. 2015. Dollhouse VR: a multi-view, multi-user collaborative design workspace with VR technology. In SIGGRAPH Asia 2015 Emerging Technologies. ACM, 8. \ 20. Adrian Ilie, Kok-Lim Low, Greg Welch, Anselmo Lastra, Henry Fuchs, and Bruce Cairns. 2004. Combining head-mounted and projector-based displays for surgical training. Presence: Teleoperators and Virtual Environments 13, 2 (2004), 128–145. \ 21. Hiroshi Ishii, Dávid Lakatos, Leonardo Bonanni, and Jean-Baptiste Labrune. 2012. Radical atoms: beyond tangible bits, toward transformable materials. interactions 19, 1 (2012), 38–51. \ 22. Robert JK Jacob, Audrey Girouard, Leanne M Hirshﬁeld, Michael S Horn, Orit Shaer, Erin Treacy Solovey, and Jamie Zigelbaum. 2008. Reality-based interaction: a framework for post-WIMP interfaces. In CHI ’08. ACM, 201–210. \ 23. Brett Jones, Rajinder Sodhi, Michael Murdock, Ravish Mehra, Hrvoje Benko, Andrew Wilson, Eyal Ofek, Blair MacIntyre, Nikunj Raghuvanshi, and Lior Shapira. 2014. RoomAlive: magical experiences enabled by scalable, adaptive projector-camera units. In UIST ’14. ACM, 637–644. \ 24. Shunichi Kasahara, Ryuma Niiyama, Valentin Heun, and Hiroshi Ishii. 2013. exTouch: Spatially-aware Embodied Manipulation of Actuated Objects Mediated by Augmented Reality. In TEI ’13 (TEI ’13). ACM, New York, NY, USA, 223–228. \ 25. Kiyoshi Kiyokawa, Haruo Takemura, and Naokazu Yokoya. 2000. SeamlessDesign for 3D object creation. IEEE multimedia 7, 1 (2000), 22–33. \ 26. Ryohei Komiyama, Takashi Miyaki, and Jun Rekimoto. 2017. JackIn Space: Designing a Seamless Transition Between First and Third Person View for Effective Telepresence Collaborations. In Proceedings of the 8th Augmented Human International Conference (AH ’17). ACM, New York, NY, USA, Article 14, 9 pages. DOI: http://dx.doi.org/10.1145/3041164.3041183 \ 27. Jérémy Laviole and Martin Hachet. 2012. PapARt: interactive 3D graphics and multi-touch augmented paper for artistic creation. In 3D User Interfaces (3DUI), 2012 IEEE Symposium on. IEEE, 3–6. \ 28. Johnny C Lee, Scott E Hudson, Jay W Summet, and Paul H Dietz. 2005. Moveable interactive projected displays using projector based tracking. In UIST ’05. ACM, 63–72. \ 29. Michael R Marner, Bruce H Thomas, and Christian Sandor. 2009. Physical-virtual tools for spatial augmented reality user interfaces. In ISMAR, Vol. 9. \ 30. Paul Milgram, Haruo Takemura, Akira Utsumi, and Fumio Kishino. 1995. Augmented reality: A class of displays on the reality-virtuality continuum. In Photonics for industrial applications. International Society for Optics and Photonics, 282–292. \ 31. Richard A Newcombe, Dieter Fox, and Steven M Seitz. 2015. Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. In Proceedings of the IEEE conference on computer vision and pattern recognition. \ 32. Ben Piper, Carlo Ratti, and Hiroshi Ishii. 2002. Illuminating clay: a 3-D tangible interface for landscape analysis. In CHI ’02. ACM, 355–362. \ 33. Ramesh Raskar, Greg Welch, Matt Cutts, Adam Lake, Lev Stesin, and Henry Fuchs. 1998b. The ofﬁce of the future: A uniﬁed approach to image-based modeling and spatially immersive displays. In Proceedings of the 25th annual conference on Computer graphics and interactive techniques. ACM, 179–188. \ 34. Ramesh Raskar, Greg Welch, and Henry Fuchs. 1998a. Spatially augmented reality. In First IEEE Workshop on Augmented Reality (IWARâ ˘Z98). Citeseer, 11–20.A ´ \ 35. Ramesh Raskar, Greg Welch, Kok-Lim Low, and Deepak Bandyopadhyay. 2001. Shader lamps: Animating real objects with image-based illumination. In Rendering Techniques 2001. Springer, 89–102. \ 36. Jun Rekimoto and Katashi Nagao. 1995. The world through the computer: Computer augmented interaction with real world environments. In UIST ’95. ACM, 29–36. \ 37. Jun Rekimoto and Masanori Saitoh. 1999. Augmented surfaces: a spatially continuous work space for hybrid computing environments. In CHI’99. ACM, 378–385. \ 38. Joan Sol Roo, Renaud Gervais, Jérémy Frey, and Martin Hachet. 2017. Inner Garden: Connecting Inner States to a Mixed Reality Sandbox for Mindfulness. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 1459–1470. \ 39. Joan Sol Roo and Martin Hachet. 2017. Towards a hybrid space combining Spatial Augmented Reality and virtual reality. In 3DUI’17. IEEE, 195–198. \ 40. Carl Sagan, Steven Soter, and Ann Druyan. 1989. Cosmos: A personal voyage. (1989). \ 41. Ari Shapiro, Andrew Feng, Ruizhe Wang, Hao Li, Mark Bolas, Gerard Medioni, and Evan Suma. 2014. Rapid avatar capture and simulation using commodity depth sensors. Computer Animation and Virtual Worlds 25 (2014). \ 42. Ross T Smith, Guy Webber, Maki Sugimoto, Michael Marner, and Bruce H Thomas. 2013. [Invited Paper] Automatic Sub-pixel Projector Calibration. ITE Transactions on Media Technology and Applications 1, 3 (2013), 204–213. \ 43. Ivan E Sutherland. 1965. The ultimate display. Multimedia: From Wagner to virtual reality (1965). \ 44. Brygg Ullmer and Hiroshi Ishii. 1997. The metaDESK: models and prototypes for tangible user interfaces. In UIST ’97. ACM, 223–232. \ 45. Brygg Ullmer and Hiroshi Ishii. 2000. Emerging frameworks for tangible user interfaces. IBM systems journal 39, 3.4 (2000), 915–931. \ 46. Bret Victor. 2014. Humane Representation of Thought: A Trail Map for the 21st Century. In UIST ’14. ACM, New York, NY, USA, 1. \ 47. James A Walsh, Stewart von Itzstein, and Bruce H Thomas. 2014. Ephemeral interaction using everyday objects. In Proceedings of the Fifteenth Australasian User Interface Conference-Volume 150. Australian Computer Society, Inc., 29–37. \ 48. Mark Weiser. 1993. Some computer science issues in ubiquitous computing. Commun. ACM 36, 7 (1993). \ 49. Mark Weiser and John Seely Brown. 1997. The coming age of calm technology. In Beyond calculation. Springer. \ 50. Pierre Wellner. 1993. Interacting with Paper on the DigitalDesk. Commun. ACM 36, 7 (July 1993), 87–96. \ 51. Andrew Wilson, Hrvoje Benko, Shahram Izadi, and Otmar Hilliges. 2012. Steerable augmented reality with the beamatron. In UIST ’12. ACM, 413–422. \ 52. Robert Xiao, Chris Harrison, and Scott E. Hudson. 2013a. WorldKit: Rapid and Easy Creation of Ad-hoc Interactive Applications on Everyday Surfaces. In CHI ’13 (CHI ’13). ACM, New York, NY, USA, 879–888. \ 53. Robert Xiao, Chris Harrison, and Scott E Hudson. 2013b. WorldKit: rapid and easy creation of ad-hoc interactive applications on everyday surfaces. In CHI ’13. ACM, 879–888. \ 54. Jianlong Zhou, Ivan Lee, Bruce Thomas, Roland Menassa, Anthony Farrant, and Andrew Sansome. 2011. Applying Spatial Augmented Reality to Facilitate In-situ Support for Automotive Spot Welding Inspection. In Proceedings of the 10th International Conference on Virtual Reality Continuum and Its Applications in Industry (VRCAI ’11). ACM, New York, NY, USA, 195–200. \ ",Augmented Reality; Mixed Reality; Spatial Augmented Reality; Virtual Reality; Through-the-lens Technique; Tangible User Interfaces; Organic User Interfaces,H.5.1; H.5.2,uistf3584-file1.zip,uistf3584-file2.jpg,uistf3584-file3.mp4,,"The system allows one or more users to interact and transition between multiple mixed reality modalities. This provides increasing flexibility, while keeping the interaction framed in the physical world.",,"This paper presents a mixed reality conceptual framework and its implementation, focusing on the seamless, progressive transition between physical and digital spaces.","As discussed with the reviewers, small modifications were performed, namely: \ - Related work: it was extended with the requested references \ - General: The contribution statements were toned down given the absence of user study, and two statements were removed (mention of ""time manipulation"" and ""all-or-nothing dichotomy"") \ - section 3, level 4: Teleportation mode was clarified  \ - Section 3, overview: now briefly describes the relationship of this project and the related work \ - Conclusion: we explicitly indicated that a proper evaluation was out of the scope of the current paper, and a comprehensive user study will be the focus of a future work \ - The length of the paper was kept at 7 pages",Joan Sol Roo,Martin Hachet,FormatComplete,ANR-14-CE24-0013,ANR,,,,,Aug 8 11:41,
uistf4994,10/25,16,VR/AR,1:30:00 PM,3:00:00 PM,4+1,2:40:00 PM,2:50:00 PM,short,short,uistf3584,5,1605,,,uistf4994,A,Mutual Human Actuation,Lung-Pan,Cheng,lung-pan.cheng@hpi.uni-potsdam.de,uistf4994-paper.pdf,9,letter,,,"Lung-Pan Cheng, Sebastian Marwecki, Patrick Baudisch","lung-pan.cheng@hpi.uni-potsdam.de, Sebastian.Marwecki@hpi.de, patrick.baudisch@hpi.uni-potsdam.de",17012,Lung-Pan,,Cheng,lung-pan.cheng@hpi.uni-potsdam.de,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,33875,Sebastian,,Marwecki,Sebastian.Marwecki@hpi.de,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,1437,Patrick,,Baudisch,patrick.baudisch@hpi.uni-potsdam.de,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Human actuation is the idea of using people to provide large-scale force feedback to users. The Haptic Turk system, for example, used four human actuators to lift and push a virtual reality user; TurkDeck used ten human actuators to place and animate props for a single user. While the experience of human actuators was decent, it was still inferior to the experience these people could have had, had they participated as a user. In this paper, we address this issue by making everyone a user. We introduce mutual human actuation, a version of human actuation that works without dedicated human actuators. The key idea is to run pairs of users at the same time and have them provide human actuation to each other. Our system, Mutual Turk, achieves this by (1) offering shared props through which users can exchange forces while obscuring the fact that there is a human on the other side, and (2) synchronizing the two users’ timelines such that their way of manipulating the shared props is consistent across both virtual worlds. We demonstrate mutual human actuation with an example experience in which users pilot kites though storms, tug fish out of ponds, are pummeled by hail, battle monsters, hop across chasms, push loaded carts, and ride in moving vehicles.",lung-pan.cheng@hpi.de,"1. Bergamasco, M. The GLAD-IN-ART Project. Virtual Reality SE-19. 251–258. \ 2. Lung-Pan Cheng, Patrick Lühne, Pedro Lopes, Christoph Sterz, and Patrick Baudisch. 2014. Haptic turk: a motion platform based on people. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '14). ACM, New York, NY, USA, 3463-3472. DOI=http://dx.doi.org/10.1145/2556288.2557101 \ 3. Lung-Pan Cheng, Thijs Roumen, Hannes Rantzsch, Sven Köhler, Patrick Schmidt, Robert Kovacs, Johannes Jasper, Jonas Kemper, and Patrick Baudisch. 2015. TurkDeck: Physical Virtual Reality Based on People. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST '15). ACM, New York, NY, USA, 417-426. DOI: https://doi.org/10.1145/2807442.2807463 \ 4. Fabien Danieau, Julien Fleureau, Philippe Guillotel, Nicolas Mollet, Anatole Lécuyer, and Marc Christie. 2012. HapSeat: producing motion sensation with multiple force-feedback devices embedded in a seat. In Proceedings of the 18th ACM symposium on Virtual reality software and technology (VRST '12). ACM, New York, NY, USA,69-76. DOI=http://dx.doi.org/10.1145/2407336.2407350 \ 5. H. G. Hoffman, ""Physically touching virtual objects using tactile augmentation enhances the realism of virtual environments,"" Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180), Atlanta, GA, USA, 1998, pp. 59-63. DOI: 10.1109/VRAIS.1998.658423 \ 6. C. E. Hughes, C. B. Stapleton, D. E. Hughes and E. M. Smith, ""Mixed reality in education, entertainment, and training,"" in IEEE Computer Graphics and Applications, vol. 25, no. 6, pp. 24-30, Nov.-Dec. 2005. DOI: 10.1109/MCG.2005.139 \ 7. Insko, B.E. Passive haptics significantly enhances virtual environments. Dissertation at University of North Carolina at Chapel Hill, 2001. \ 8. Ivan E. Sutherland. 1965. The ultimate display. In Proceedings of the Congress of the International Federation of Information Processing (IFIP), 506–508. https://doi.org/10.1109/MC.2005.274 \ 9. Luv Kohli, Eric Burns, Dorian Miller, and Henry Fuchs. 2005. Combining passive haptics with redirected walking. In Proceedings of the 2005 international conference on Augmented tele-existence (ICAT '05). ACM, New York, NY, USA, 253-254. DOI=http://dx.doi.org/10.1145/1152399.1152451 \ 10. Kok-Lim Low, Greg Welch, Anselmo Lastra, and Henry Fuchs. 2001. Life-sized projector-based dioramas. In Proceedings of the ACM symposium on Virtual reality software and technology (VRST '01). ACM, New York, NY, USA, 93-101. DOI=http://dx.doi.org/10.1145/505008.505026 \ 11. W. A. McNeely, ""Robotic graphics: a new approach to force feedback for virtual reality,"" Proceedings of IEEE Virtual Reality Annual International Symposium, Seattle, WA, 1993, pp. 336-341. DOI: 10.1109/VRAIS.1993.380761 \ 12. MotiveDirect, https://github.com/XmanLCH/MotiveDirect \ 13. M. Ortega and S. Coquillart, ""Prop-based haptic interaction with co-location and immersion: an automotive application,"" IEEE International Workshop on Haptic Audio Visual Environments and their Applications, 2005, pp. 6. DOI: 10.1109/HAVE.2005.1545646 \ 14. J. Pair, U. Neumann, D. Piepol and B. Swartout, ""FlatWorld: combining Hollywood set-design techniques with VR,"" in IEEE Computer Graphics and Applications, vol. 23, no. 1, pp. 12-15, Jan/Feb 2003. DOI: 10.1109/MCG.2003.1159607 \ 15. Adalberto L. Simeone, Eduardo Velloso, and Hans Gellersen. 2015. Substitutional Reality: Using the Physical Environment to Design Virtual Reality Experiences. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15). ACM, New York, NY, USA, 3307-3316. DOI: https://doi.org/10.1145/2702123.2702389. \ 16. Rajinder Sodhi, Ivan Poupyrev, Matthew Glisson, and Ali Israr. 2013. AIREAL: interactive tactile experiences in free air. ACM Trans. Graph. 32, 4, Article 134 (July 2013), 10 pages. DOI=http://dx.doi.org/10.1145/2461912.2462007 \ 17. Stewart, D. A platform with six degrees of freedom. Proceedings of the institution of mechanical engineers 180, 1 (1965), 371–386. \ 18. Dzmitry Tsetserukou, Katsunari Sato, Alena Neviarouskaya, Naoki Kawakami, and Susumu Tachi. 2009. FlexTorque: innovative haptic interface for realistic physical interaction in virtual reality. In ACM SIGGRAPH ASIA 2009 Sketches (SIGGRAPH ASIA '09). ACM, New York, NY, USA, Article 10, 1 pages. DOI: https://doi.org/10.1145/1667146.1667159 \ 19. Martin Usoh, Kevin Arthur, Mary C. Whitton, Rui Bastos, Anthony Steed, Mel Slater, and Frederick P. Brooks, Jr.. 1999. Walking > walking-in-place > flying, in virtual environments. In Proceedings of the 26th annual conference on Computer graphics and interactive techniques (SIGGRAPH '99). ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 359-364. DOI=http://dx.doi.org/10.1145/311535.311589 \ 20. Wolfgang Reisig. 1991. Petri nets and algebraic specifications. Theoretical Computer Science 80, 1: 1– 34. DOI=https://doi.org/http://dx.doi.org/10.1016/03043975(91)90203-E \ 21. Lining Yao, Sayamindu Dasgupta, Nadia Cheng, Jason Spingarn-Koff, Ostap Rudakevych, and Hiroshi Ishii. 2011. Rope Revolution: tangible and gestural rope interface for collaborative play. In Proceedings of the 8th International Conference on Advances in Computer Entertainment Technology (ACE '11), Teresa Romão, Nuno Correia, Masahiko Inami, Hirokasu Kato, Rui Prada, Tsutomu Terada, Eduardo Dias, and Teresa Chambel (Eds.). ACM, New York, NY, USA, , Article 11 , 8 pages. DOI=http://dx.doi.org/10.1145/2071423.2071437. \ 22. Bob G. Witmer, Christian J. Jerome, and Michael J. Singer. 2005. The factor structure of the presence questionnaire. Presence: Teleoper. Virtual Environ. 14, 3 (June 2005), 298-312. DOI=http://dx.doi.org/10.1162/105474605323384654 \ ",Virtual reality; haptics; immersion; Haptic Turk,H.5.2,uistf4994-file1.docx,uistf4994-file2.jpg,uistf4994-file3.mp4,,Two users provide force feedback to each other while they are immersed into their own virtual reality experience without noticing the other's existence. ,,"Mutual human actuation provides large-scale haptic feedback by synchronizing users' timelines to let users actuate each other. This eliminates the need for dedicated human actuators, instead allowing everyone to enjoy the experience of a user.","In the Contribution and Limitation section, we added more description about our limitation.  \  \ In the Implementation section, we added our performance information.  \  \ We added two new sections: 1) Designing Mutual Turk Experiences and 2) User Study which was described in our rebuttal.  \ ",Lung-Pan Cheng,Sebastian Marwecki,FormatComplete,,,,,,,Jul 25 13:08,
uistf1526,10/25,17,Crowd,3:40:00 PM,4:50:00 PM,3+1,3:40:00 PM,3:50:00 PM,short,short,none,1,1701,,"first paper should not be short. Apparently this is to match HCOMP;
Yes, this is the one place where the short paper is deliberately at the beginning of the session.",uistf1526,A,WearMail: On-the-Go Access to Information in Your Email with a Privacy-Preserving Human Computation Workflow,Saiganesh,Swaminathan,saiganes@cs.cmu.edu,uistf1526-paper.pdf,9,letter,Times-Roman,,"Saiganesh Swaminathan, Raymond Fok, Fanglin Chen, Ting-Hao Huang, Irene Lin, Rohan Jadvani, Walter Lasecki, Jeffrey Bigham","saiganes@cs.cmu.edu, rayfok@umich.edu, chenfanglintc@gmail.com, tinghaoh@andrew.cmu.edu, irenelin0258@gmail.com, rjadvani@andrew.cmu.edu, wlasecki@umich.edu, jbigham@cs.cmu.edu",37459,Saiganesh,,Swaminathan,saiganes@cs.cmu.edu,Human-Computer Interaction Institute,Carnegie Mellon University,Pittsburgh,Pennsylvania,United States,,,,,,71872,Raymond,,Fok,rayfok@umich.edu,Computer Science and Engineering,University of Michigan,Ann Arbor,Michigan,United States,,,,,,24507,Fanglin,,Chen,chenfanglintc@gmail.com,Human-Computer Interaction Institute,Carnegie Mellon University,Pittsburgh,Pennsylvania,United States,,,,,,44450,Ting-Hao,,Huang,tinghaoh@andrew.cmu.edu,Carnegie Mellon University ,Language Technologies Institute,Pittsburgh,PA,USA,,,,,,71873,Irene,,Lin,irenelin0258@gmail.com,,Carnegie Mellon University,Pittsburgh,Pennsylvania,United States,,,,,,71874,Rohan,,Jadvani,rjadvani@andrew.cmu.edu,,Carnegie Mellon University,Pittsburgh,Pennsylvania,United States,,,,,,21848,Walter,,Lasecki,wlasecki@umich.edu,Computer Science and Engineering,University of Michigan,Ann Arbor,MI,USA,,,,,,8964,Jeffrey,,Bigham,jbigham@cs.cmu.edu,Human-Computer Interaction Institute,Carnegie Mellon University,Pittsburgh,PA,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Email is more than just a communication medium. Email serves as an external memory for people---it contains our reservation numbers, meeting details, phone numbers, and more. Often, people need access to this information while on the go, which is cumbersome from mobile devices with limited I/O bandwidth. In this paper, we introduce WearMail, a conversational interface to retrieve specific information in email. WearMail is mostly automated but is made robust to information extraction tasks via a novel privacy-preserving human computation workflow. In WearMail, crowdworkers never have direct access to emails, but rather (i) generate an email filter to help the system find messages that may contain the desired information, and (ii) generate examples of the requested information that are then used to create custom, low-level information extractors that run automatically within the set of filtered emails. We explore the impact of varying levels of obfuscation on result quality, demonstrating that workers are able to deal with highly-obfuscated information nearly as well as with the original. WearMail introduces general mechanisms that let the crowd search and select private data without having direct access to the data itself. ",saiganes@cs.cmu.edu,"1. Bernstein, M. S., Brandt, J., Miller, R. C., and Karger, D. R. Crowds in two seconds: Enabling realtime crowd-powered interfaces. In Proceedings of the 24th annual ACM symposium on User interface software and technology, ACM (2011), 33–42. \ 2. Bernstein, M. S., Teevan, J., Dumais, S., Liebling, D., and Horvitz, E. Direct answers for search queries in the long tail. In Proceedings of the 2012 ACM annual conference on Human Factors in Computing Systems (2012), 237–246. \ 3. Bigham, J. P., Jayant, C., Ji, H., Little, G., Miller, A., Miller, R. C., Miller, R., Tatarowicz, A., White, B., White, S., et al. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, ACM (2010), 333–342. \ 4. Brin, S., and Page, L. The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems 30, 1 (1998), 107–117. \ 5. Cochran, R. a., D’Antoni, L., Livshits, B., Molnar, D., and Veanes, M. Program Boosting: Program Synthesis via Crowd-Sourcing. In POPL (2015), 677–688. \ 6. Dabbish, L. A., Kraut, R. E., Fussell, S., and Kiesler, S. Understanding email use: predicting action on a message. In Proceedings of the SIGCHI conference on Human factors in computing systems, ACM (2005), 691–700. \ 7. Demartini, G., Trushkowsky, B., Kraska, T., and Franklin, M. J. CrowdQ: Crowdsourced Query Understanding. In 6th Biennial Conference on Innovative Data Systems Research (CIDR ’13) (2013). \ 8. Doudalis, S., Mehrotra, S., Haney, S., and Machanavajjhala, A. Releasing True Data with Formal Privacy Guarantees. In Privacy-Preserving IR Workshop at SIGIR (2016). \ 9. Dumais, S., Cutrell, E., Cadiz, J. J., Jancke, G., Sarin, R., and Robbins, D. C. Stuff i’ve seen: A system for personal information retrieval and re-use. SIGIR Forum 49, 2 (Jan. 2016), 28–35. \ 10. Elsweiler, D., Baillie, M., and Ruthven, I. What makes re-ﬁnding information difﬁcult? a study of email re-ﬁnding. In European Conference on Information Retrieval, Springer (2011), 568–579. \ 11. Horvitz, E., Jacobs, A., and Hovel, D. Attention-sensitive alerting. In Proceedings of the Fifteenth conference on Uncertainty in artiﬁcial intelligence, Morgan Kaufmann Publishers Inc. (1999), 305–313. \ 12. Huang, T. H., Lasecki, Walter S., Azaria, A. and Bigham, J.P. “Is there anything else I can help you with?”: Challenges in Deploying an On-Demand Crowd-Powered Conversational Agent. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2016), (2016). \ 13. Jenny Rose Finkel, T. G., and Manning, C. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005) (2005), 363–370. \ 14. Jeong, J. W., Morris, M. R., Teevan, J., and Liebling, D. A crowd-powered socially embedded search engine. Proceedings of the 7th International Conference on Weblogs and Social Media, ICWSM 2013 (2013), 263–272. \ 15. Kaufman, J. 10,000 most common english words. https://github.com/ﬁrst20hours/google-10000-english. \ 16. Kaur, H., Gordon, M., Yang, Y., Bigham, J. P., Teevan, J., Kamar, E., and Lasecki, W. S. Crowdmask: Using crowds to preserve privacy in crowd-powered systems via progressive ﬁltering. In Proceedings of the AAAI Conference on Human Computation (HCOMP 2017)., HCOMP ’17 (2017). \ 17. Kim, Y., Collins-thompson, K., and Arbor, A. Crowdsourcing for Robustness in Web Search. In TREC (2013). \ 18. Kittur, A., Nickerson, J. V., Bernstein, M., Gerber, E., Shaw, A., Zimmerman, J., Lease, M., and Horton, J. The future of crowd work. In Proceedings of the 2013 conference on Computer supported cooperative work (CSCW 2013), 1301–1318. \ 19. Klimt, B., and Yang, Y. Introducing the enron corpus. In CEAS (2004). \ 20. Kokkalis, N., K¨ohn, T., Pfeiffer, C., Chornyi, D., Bernstein, M. S., and Klemmer, S. R. Emailvalet: Managing email overload through private, accountable crowdsourcing. In Proceedings of the 2013 Conference on Computer Supported Cooperative Work, CSCW ’13, ACM (New York, NY, USA, 2013), 1291–1300. \ 21. Koutra, D., Vogelstein, J. T., and Faloutsos, C. Deltacon: A principled massive-graph similarity function. In Proceedings of the 13th SIAM International Conference on Data Mining (SDM), SIAM (2013), 162–170. \ 22. Laput, G., Lasecki, W. S., Bigham, J. P., Wiese, J., Xiao, R., and Harrison, C. Zensors: Adaptive, rapidly deployable, human-intelligent sensor feeds. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM (2015). \ 23. Lasecki, W. S., Wesley, R., Nichols, J., Kulkarni, A., Allen, J. F. and Bigham, J. P. Chorus: A Crowd-powered Conversational Assistant. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST 2013), 151–162. \ 24. Lasecki, W. S., Gordon, M., Koutra, D., Jung, M. F., Dow, S. P., and Bigham, J. P. Glance: Rapidly coding behavioral video with the crowd. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology, UIST ’14, ACM (New York, NY, USA, 2014), 551–562. \ 25. Lasecki, W. S., Gordon, M., Leung, W., Lim, E., Bigham, J. P., and Dow, S. P. Exploring privacy and accuracy trade-offs in crowdsourced behavioral video coding. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, CHI ’15, ACM (New York, NY, USA, 2015), 1945–1954. \ 26. Lasecki, W. S., Murray, K. I., White, S., Miller, R. C., and Bigham, J. P. Real-time crowd control of existing interfaces. In Proceedings of the 24th annual ACM symposium on User interface software and technology, ACM (2011), 23–32. \ 27. Lasecki, W. S., Song, Y. C., Kautz, H., and Bigham, J. P. Real-time crowd labeling for deployable activity recognition. In Proceedings of the 2013 conference on Computer supported cooperative work, ACM (2013), 1203–1212. \ 28. Lasecki, W. S., Teevan, J., and Kamar, E. Information extraction and manipulation threats in crowd-powered systems. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work and Social Computing, CSCW ’14, ACM (New York, NY, USA, 2014), 248–256. \ 29. Lasecki, W. S., Weingard, L., Ferguson, G., and Bigham, J. P. Finding dependencies between actions using the crowd. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM (2014), 3095–3098. \ 30. Myers, B. A., Ko, A. J., and Burnett, M. M. Invited research overview: End-user programming. In CHI ’06 Extended Abstracts on Human Factors in Computing Systems, CHI EA ’06, ACM (New York, NY, USA, 2006), 75–80. \ 31. Nebeling, M., To, A., Guo, A., de Freitas, A. A., Teevan, J., Dow, S. P., and Bigham, J. P. Wearwrite: Crowd-assisted writing from smartwatches. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI ’16, ACM (New York, NY, USA, 2016), 3834–3846. \ 32. Parameswaran, A., Teh, M. H., Garcia-Molina, H., and Widom, J. DataSift: An expressive and accurate crowd-powered search toolkit. In Proceedings of the First AAAI Conference on Human Computation and Crowdsourcing (2013), 112–120. \ 33. Qingyao Ai, Susan T. Dumais, N. C., and Liebling, D. Characterizing email search using large-scale behavioral logs and surveys. In Proceedings of the World Wide Web Conference (WWW 2017) (2017). \ 34. Sahami, M., Dumais, S., Heckerman, D., and Horvitz, E. A bayesian approach to ﬁltering junk e-mail. In Learning for Text Categorization: Papers from the 1998 workshop, vol. 62 (1998), 98–105. \ 35. Sproull, L., and Kiesler, S. New ways of working in the networked organization. Cambridge, MA (1991). \ 36. Teevan, J., Alvarado, C., Ackerman, M. S., and Karger, D. R. The perfect search engine is not enough: a study of orienteering behavior in directed search. In Proceedings of the SIGCHI conference on Human factors in computing systems, ACM (2004), 415–422. \ 37. Teevan, J., Collins-Thompson, K., White, R. W., Dumais, S. T., and Kim, Y. Slow Search. In Proceedings of the Symposium on Human-Computer Interaction and Information Retrieval - HCIR ’13 (2013), 1–10. \ 38. Thomas, G. F., King, C. L., Baroni, B., Cook, L., Keitelman, M., Miller, S., and Wardle, A. Reconceptualizing e-mail overload. Journal of Business and Technical Communication 20, 3 (2006), 252–287. \ ",Wearables; Email; Crowdsourcing; Information Extraction,"""H.5.m.""",uistf1526-file1.zip,uistf1526-file2.jpg,uistf1526-file3.mp4,,"We present WearMail, a system that let’s you easily extract information from your email on-the-go",,"In this paper, we introduce WearMail, a conversational interface to retrieve specific information in email. WearMail performs robust information extraction tasks via a novel privacy-preserving human computation workflow. ","Changelog \ ======================== \ - Introduced the EmailValet paper as part of related work as recommended by the AC and discussed the main difference  \ - Expanded the ranking section with a new fig 5 and explained it in detail \ - Introduced and clarified that we used IR based metrics like HIT@3 and MRR when reporting the results.  \ - Wrote several paragraphs on Failure modes, Recovery Strategies, Latency and Future work near the end.  \ - Added captions to the tables and made the figure 1 more legible \ - Clarified Binary queries as pointed our by R2 \ - Fixed fig 5 color scheme between red and green for accessibility to colors that show contrast. \ - Fixed several other typos, references and grammatical errors.  \ - Attaching a changed version of the pdf highlighted in yellow in auxiliary material ",Saiganesh Swaminathan,Jeffrey Bigham,FormatComplete,,,,,,,Aug 9 16:50,
uistf1006,10/25,17,Crowd,3:40:00 PM,4:50:00 PM,3+1,3:50:00 PM,4:10:00 PM,long,long,uistf1526,2,1702,,,uistf1006,A,SketchExpress: Remixing Animations for More Effective Crowd-Powered Prototyping of Interactive Interfaces,Stephanie,O'Keefe,sdokeefe@umich.edu,uistf1006-paper.pdf,12,letter,Times-Roman,,"Sang Won Lee, Yujin Zhang, Isabelle Wong, Yiwei Yang, Stephanie D O'Keefe, Walter S Lasecki","snaglee@umich.edu, yujinz@umich.edu, isawong@umich.edu, yanyiwei@umich.edu, sdokeefe@umich.edu, wlasecki@umich.edu",49734,Sang Won,,Lee,snaglee@umich.edu,Computer Science and Engineering,University of Michigan,Ann Arbor,Michigan,United States,,,,,,65330,Yujin,,Zhang,yujinz@umich.edu,Computer Science and Engineering,University of Michigan,Ann Arbor,Michigan,United States,,,,,,65333,Isabelle,,Wong,isawong@umich.edu,Computer Science and Engineering,University of Michigan,Ann Arbor,Michigan,United States,,,,,,60516,Yiwei,,Yang,yanyiwei@umich.edu,Computer Science and Engineering,University of Michigan,Ann Arbor,Michigan,United States,,,,,,71735,Stephanie,D,O'Keefe,sdokeefe@umich.edu,Computer Science and Engineering,University of Michigan,Ann Arbor,Michigan,United States,,,,,,21848,Walter,S,Lasecki,wlasecki@umich.edu,Computer Science and Engineering,University of Michigan,Ann Arbor,MI,USA,School of Information,University of Michigan,Ann Arbor,Michigan,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Low-fidelity prototyping at the early stages of user interface (UI) design can help designers and system builders quickly explore their ideas.  However, interactive behaviors in such prototypes are often replaced by textual descriptions because it usually takes even professionals hours or days to create animated interactive elements due to the complexity of creating them.  In this paper, we introduce SketchExpress, a crowd-powered prototyping tool that enables crowd workers to create reusable interactive behaviors easily and accurately. With the system, a requester—designers or end-users—describes aloud how an interface should behave and crowd workers make the sketched prototype interactive within minutes using a demonstrate-remix-replay approach.  These behaviors are manually demonstrated, refined using remix functions, and then can be replayed later. The recorded behaviors persist for future reuse to help users communicate with the animated prototype. We conducted a study with crowd workers recruited from Mechanical Turk, which demonstrated that workers could create animations using SketchExpress in 2.9 minutes on average with 27% gain in the quality of animations compared to the baseline condition of manual demonstration.",snaglee@umich.edu,"WARNING: Reference 32 starts with a non-alphanumeric character.  Please check it. \  \ 1. 2017. Amazon’s Mechanical Turk. (2017).  http://www.mturk.com.  \ 2. Connelly Barnes, David E. Jacobs, Jason Sanders, Dan B Goldman, Szymon Rusinkiewicz, Adam Finkelstein, and Maneesh Agrawala. 2008. Video Puppetry: A Performative Interface for Cutout Animation. In ACM SIGGRAPH Asia 2008 Papers (SIGGRAPH Asia ’08). ACM, New York, NY, USA, Article 124, 9 pages. DOI: http://dx.doi.org/10.1145/1457515.1409077 \ 3. Michael S. Bernstein, Joel R. Brandt, Robert C. Miller, and David R. Karger. 2011. Crowds in Two Seconds: Enabling Realtime Crowd-Powered Interfaces. In User Interface Software and Technology (UIST). 33–42. DOI: http://dx.doi.org/10.1145/1866029.1866080 \ 4. Michael S. Bernstein, Greg Little, Robert C. Miller, Björn Hartmann, Mark S. Ackerman, David R. Karger, David Crowell, and Katrina Panovich. 2010. Soylent: a word processor with a crowd inside. In User Interface Software and Technology (UIST). 313–322. DOI: http://dx.doi.org/10.1145/1866029.1866078 \ 5. Jeffrey P. Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C. Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, and Tom Yeh. 2010. VizWiz: nearly real-time answers to visual questions. In User Interface Software and Technology (UIST). 333–342. DOI: http://dx.doi.org/10.1145/1866029.1866080 \ 6. Yan Chen, Sang Won Lee, Yin Xie, YiWei Yang,  Walter S. Lasecki, and Steve Oney. 2017. Codeon:  On-Demand Software Development Assistance. In  Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17). ACM, New York, NY, USA, 6220–6231. DOI: http://dx.doi.org/10.1145/3025453.3025972 \ 7. Allen Cypher, Daniel C. Halbert, David Kurlander, Henry Lieberman, David Maulsby, Brad A. Myers, and Alan Turransky (Eds.). 1993. Watch What I Do: Programming by Demonstration. MIT Press, Cambridge, MA, USA. \ 8. Richard C. Davis, Brien Colwell, and James A. Landay. 2008. K-sketch: A ’Kinetic’ Sketch Pad for Novice Animators. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’08). ACM, New York, NY, USA, 413–422. DOI: http://dx.doi.org/10.1145/1357054.1357122 \ 9. Richard C. Davis, T. Scott Saponas, Michael Shilman, and James A. Landay. 2007. SketchWizard: Wizard of Oz Prototyping of Pen-based User Interfaces. In User Interface Software and Technology (UIST). 119–128. DOI:http://dx.doi.org/10.1145/1294211.1294233 \ 10. Mitchell Gordon, Jeffrey P. Bigham, and Walter S. Lasecki. 2015. LegionTools: A Toolkit + UI for Recruiting and Routing Crowds to Synchronous Real-Time Tasks. In Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST ’15 Adjunct). ACM, New York, NY, USA, 81–82. DOI: http://dx.doi.org/10.1145/2815585.2815729 \ 11. Saul Greenberg, Sheelagh Carpendale, Nicolai Marquardt, and Bill Buxton. 2011. Sketching user experiences: The workbook. Elsevier. \ 12. Philip J. Guo. 2013. Online Python Tutor: Embeddable Web-based Program Visualization for Cs Education. In Proceeding of the 44th ACM Technical Symposium on Computer Science Education (SIGCSE ’13). ACM, New York, NY, USA, 579–584. DOI: http://dx.doi.org/10.1145/2445196.2445368 \ 13. Rubaiat Habib Kazi, Fanny Chevalier, Tovi Grossman, and George Fitzmaurice. 2014. Kitty: Sketching Dynamic and Interactive Illustrations. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST ’14). ACM, New York, NY, USA, 395–405. DOI: http://dx.doi.org/10.1145/2642918.2647375 \ 14. Joy Kim, Justin Cheng, and Michael S. Bernstein. 2014. Ensemble: Exploring Complementary Strengths of Leaders and Crowds in Creative Collaboration. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing (CSCW ’14). ACM, New York, NY, USA, 745–755. DOI: http://dx.doi.org/10.1145/2531602.2531638 \ 15. Aniket Kittur, Bongwon Suh, Bryan A. Pendleton, and Ed H. Chi. 2007. He Says, She Says: Conﬂict and Coordination in Wikipedia. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’07). ACM, New York, NY, USA, 453–462. DOI:http://dx.doi.org/10.1145/1240624.1240698 \ 16. James A. Landay and Brad A. Myers. 1995. Interactive Sketching for the Early Stages of User Interface Design. In Human Factors in Computing Systems (CHI). ACM Press/Addison-Wesley Publishing Co., 43–50. DOI: http://dx.doi.org/10.1145/223904.223910 \ 17. Walter S. Lasecki, Juho Kim, Nick Rafter, Onkur Sen, Jeffrey P. Bigham, and Michael S. Bernstein. 2015. Apparition: Crowdsourced User Interfaces That Come to Life As You Sketch Them. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15). ACM, New York, NY, USA, 1925–1934. DOI: http://dx.doi.org/10.1145/2702123.2702565 \ 18. Walter S. Lasecki, Kyle I. Murray, Samuel White, Robert C. Miller, and Jeffrey P. Bigham. 2011. Real-time Crowd Control of Existing Interfaces. In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (UIST ’11). ACM, New York, NY, USA, 23–32. DOI: http://dx.doi.org/10.1145/2047196.2047200 \ 19. Walter S. Lasecki, Phyo Thiha, Yu Zhong, Erin Brady, and Jeffrey P. Bigham. 2013. Answering Visual Questions with Conversational Crowd Assistants. In Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS ’13). ACM, New York, NY, USA, Article 18, 8 pages. DOI:http://dx.doi.org/10.1145/2513383.2517033 \ 20. Matthew Lease, Jessica Hullman, Jeffrey P. Bigham, Michael S. Bernstein, Juho Kim, Walter S. Lasecki, Saeideh Bakhshi, Tanushree Mitra, and Robert C Miller. 2013. Mechanical turk is not anonymous. (2013). http://dx.doi.org/10.2139/ssrn.2228728 \ 21. S. W. Lee and J. Freeman. 2013. Real-Time Music Notation in Mixed Laptop Acoustic Ensembles. Computer Music Journal 37, 4 (Dec 2013), 24–36. DOI: http://dx.doi.org/10.1162/COMJ_a_00202 \ 22. Sang Won Lee, Jason Freeman, Andrew Colella, Shannon Yao, and Akito Van Troyer. 2012. Evaluating Collaborative Laptop Improvisation with LOLC. In Proceedings of the Symposium on Laptop Ensembles and Orchestras. 55–62. \ 23. James Lin, Mark W. Newman, Jason I. Hong, and James A. Landay. 2000. DENIM: Finding a Tighter Fit Between Tools and Practice for Web Site Design. In Human Factors in Computing Systems (CHI). 510–517. DOI:http://dx.doi.org/10.1145/332040.332486 \ 24. Greg Little, Lydia B. Chilton, Max Goldman, and Robert C. Miller. 2010. TurKit: human computation algorithms on mechanical turk. In User Interface Software and Technology (UIST). 57–66. DOI: http://dx.doi.org/10.1145/1866029.1866040 \ 25. Mark MacKay. 2017. Method Draw. https://github.com/duopixel/Method-Draw. (2017). \ 26. Lennart Molin. 2004. Wizard-of-Oz Prototyping for Co-operative Interaction Design of Graphical User Interfaces. In Proceedings of the Third Nordic Conference on Human-computer Interaction (NordiCHI ’04). ACM, New York, NY, USA, 425–428. DOI: http://dx.doi.org/10.1145/1028014.1028086 \ 27. B. Myers, S. Y. Park, Y. Nakano, G. Mueller, and A. Ko. 2008. How designers design and program interactive behaviors. In 2008 IEEE Symposium on Visual Languages and Human-Centric Computing. 177–184. DOI:http://dx.doi.org/10.1109/VLHCC.2008.4639081 \ 28. Brad A. Myers, Andrew J. Ko, and Margaret M. Burnett. 2006. Invited Research Overview: End-user Programming. In CHI ’06 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’06). ACM, New York, NY, USA, 75–80. DOI: http://dx.doi.org/10.1145/1125451.1125472 \ 29. Brad A. Myers, Richard McDaniel, and David Wolber. 2000. Programming by Example: Intelligence in Demonstrational Interfaces. Commun. ACM 43, 3 (March 2000), 82–89. DOI: http://dx.doi.org/10.1145/330534.330545 \ 30. Bonnie A Nardi. 1993. A small matter of programming: perspectives on end user computing. MIT press. \ 31. Michael Nebeling, Alexandra To, Anhong Guo, Adrian A. de Freitas, Jaime Teevan, Steven P. Dow, and Jeffrey P. Bigham. 2016. WearWrite: Crowd-Assisted Writing from Smartwatches. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 3834–3846. DOI: http://dx.doi.org/10.1145/2858036.2858169 \ 32. Željko Obrenovic and Jean-Bernard Martens. 2011. Sketching Interactive Systems with Sketchify. ACM Trans. Comput.-Hum. Interact. 18, 1, Article 4 (May 2011), 38 pages. DOI: http://dx.doi.org/10.1145/1959022.1959026 \ 33. E. Sohn and Y. C. Choy. 2012. Sketch-n-Stretch: Sketching Animations Using Cutouts. IEEE Computer Graphics and Applications 32, 3 (May 2012), 59–69. DOI:http://dx.doi.org/10.1109/MCG.2010.106 \ 34. Jaime Teevan, Shamsi T. Iqbal, and Curtis von Veh. 2016. Supporting Collaborative Writing with Microtasks. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 2657–2668. DOI: http://dx.doi.org/10.1145/2858036.2858108 \ 35. Bret Victor. 2012. Inventing on principle. (2012). \ ",Crowdsourcing; Prototyping/Implementation; Animation,D.2.2;H.5.m;Misc,uistf1006-file1.zip,,uistf1006-file3.avi,,,,SketchExpress is a crowdpowered prototyping tool in which crowd workers create interactive prototypes using a demonstrate-remix-replay approach. Animations created in SketchExpress improve by 27% in quality compared to manual demonstration.,"- We discussed Apparition earlier in the introduction to differentiate this work from the precursor.  \ - We revised the goal of the paper to focus more on the crowd workers side rather than over promising study results regarding the requester's side in the introduction. \ - We address the reviewer's concerns of the requester's side of the system in the future works session why we think we need a separate study.  \ - we described the envisioned interaction between the requester and the workers in the intro, referring the previous work.  \ - We described how this new practice differs from alternative prototyping methods, such as static sketches and physical mockups.  \ - We described limitations of the previous system earlier in the introduction as well and reinforced our descriptions of related works, such as Kitty.  \ - We added more information about the study design, statistical tests, and elaborated on technical details about the retainer pool and quality control. \ ",Sang Won Lee,Walter Lasecki,FormatComplete,,,,,,,Aug 8 22:16,
uistf4070,10/25,17,Crowd,3:40:00 PM,4:50:00 PM,3+1,4:10:00 PM,4:30:00 PM,long,long,uistf1006,3,1703,Honorable Mention,,uistf4070,A,Crowd Research: Open and Scalable University Laboratories,Rajan,Vaish,vaish.rajan@gmail.com,uistf4070-paper.pdf,15,letter,,,"Rajan Vaish, Snehalkumar `Neil' S. Gaikwad, Geza Kovacs, Andreas Veit, Ranjay A Krishna, Imanol Arrieta Ibarra, Camelia Simoiu, Michael Wilber, Serge Belongie, Sharad C. Goel, James Davis, Michael S Bernstein","rvaish@cs.stanford.edu, gaikwad@mit.edu, geza@cs.stanford.edu, av443@cornell.edu, ranjaykrishna@stanford.edu, imanol@stanford.edu, csimoiu@stanford.edu, mjw285@cornell.edu, sjb344@cornell.edu, scgoel@stanford.edu, davis@cs.ucsc.edu, msb@cs.stanford.edu",20699,Rajan,,Vaish,rvaish@cs.stanford.edu,,Stanford University,Stanford,California,United States,,,,,,52946,Snehalkumar `Neil',S.,Gaikwad,gaikwad@mit.edu,"Massachusetts Institute of Technology, Cambridge, MA, United States",Massachusetts Institute of Technology,Cambridge,Massachusetts,United States,,,,,,24941,Geza,,Kovacs,geza@cs.stanford.edu,,Stanford University,Palo Alto,California,United States,,,,,,48706,Andreas,,Veit,av443@cornell.edu,"Department of Computer Science, Cornell Tech",Cornell University,New York City,New York,United States,,,,,,53422,Ranjay,A,Krishna,ranjaykrishna@stanford.edu,Computer Science,Stanford University,Stanford,California,United States,,,,,,52869,Imanol,,Arrieta Ibarra,imanol@stanford.edu,Management Science and Engineering,Stanford,Stanford,CA,United States,,,,,,52955,Camelia,,Simoiu,csimoiu@stanford.edu,Management Science and Engineering,Stanford,Stanford,California,United States,,,,,,56955,Michael,,Wilber,mjw285@cornell.edu,Computer Science Department,Cornell Tech,NYC,New York,United States,,,,,,35882,Serge,,Belongie,sjb344@cornell.edu,"Department of Computer Science, Cornell Tech",Cornell University,New York City,New York,United States,,,,,,52610,Sharad C.,,Goel,scgoel@stanford.edu,Management Science and Engineering,Stanford,Stanford,California,United States,,,,,,9037,James,,Davis,davis@cs.ucsc.edu,Computer Science,"University of California, Santa Cruz",Santa Cruz,California,United States,,,,,,7809,Michael,S,Bernstein,msb@cs.stanford.edu,Computer Science,Stanford University,Stanford,California,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Research experiences today are limited to a privileged few at select universities. Providing open access to research experiences would enable global upward mobility and increased diversity in the scientific workforce. How can we coordinate a crowd of diverse volunteers on open-ended research? How could a PI have enough visibility into each person's contributions to recommend them for further study? We present Crowd Research, a crowdsourcing technique that coordinates open-ended research through an iterative cycle of open contribution, synchronous collaboration, and peer assessment. To aid upward mobility and recognize contributions in publications, we introduce a decentralized credit system: participants allocate credits to each other, which a graph centrality algorithm translates into a collectively-created author order. Over 1,500 people from 62 countries have participated, 74% from institutions with low access to research. Over two years and three projects, this crowd has produced articles at top-tier Computer Science venues, and participants have gone on to leading graduate programs.",vaish.rajan@gmail.com ,"1. 2017. The Times Higher Education World University Rankings. (2017). https://www.timeshighereducation. com/world-university-rankings \ 2. Ricardo A Baeza-Yates, Carlos Castillo, Vicente López, Martin Shubik, John Hopcroft, and Daniel Sheldon. 2007. Pagerank Increase under Different Collusion Topologies. In AIRWeb, Vol. 5. Springer, Sage Publications, 68–81. \ 3. Julie A Bianchini. 2011. Expanding underrepresented minority participation: America’s science and technology talent at the crossroads. National Academies Press, Washington, District of Columbia. \ 4. Rick Bonney, Heidi Ballard, Rebecca Jordan, Ellen McCallie, Tina Phillips, Jennifer Shirk, and Candie C Wilderman. 2009. Public Participation in Scientiﬁc Research: Deﬁning the Field and Assessing Its Potential for Informal Science Education. (2009). \ 5. William G Bowen and Derek Bok. 2016. The shape of the river: Long-term consequences of considering race in college and university admissions. Princeton University Press. \ 6. Nama Budhathoki. 2016. Who are the mappers and why do they map in OpenStreetMap. (2016). https://www.youtube.com/watch?v=LvakiUOsDrM \ 7. Yan Chen, Steve Oney, and Walter S Lasecki. 2016. Towards providing on-demand expert support for software developers. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 3192–3203. \ 8. CIA. 2016. The world factbook. (2016). https://www.cia. gov/library/publications/the-world-factbook/ \ 9. Open Science Collaboration. 2015. Estimating the reproducibility of psychological science. Science 349, 6251 (2015). \ 10. Seth Cooper, Firas Khatib, Adrien Treuille, Janos Barbero, Jeehyung Lee, Michael Beenen, Andrew Leaver-Fay, David Baker, Zoran Popovi´c, and Foldit players. 2010. Predicting protein structures with a multiplayer online game. Nature 466, 7307 (2010), 756–760. \ 11. Joe Cox, Eun Young Oh, Brooke Simmons, Chris Lintott, Karen Masters, Anita Greenhill, Gary Graham, and Kate Holmes. 2015. Deﬁning and measuring success in online citizen science: A case study of Zooniverse projects. Computing in Science & Engineering 17, 4 (2015), 28–41. \ 12. Catherine Durnell Cramton and Pamela J Hinds. 2004. Subgroup dynamics in internationally distributed teams: Ethnocentrism or cross-national learning? Research in organizational behavior 26 (2004), 231–263. \ 13. Justin Cranshaw and Aniket Kittur. 2011. The polymath project: lessons from a successful online collaboration in mathematics. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1865–1874. \ 14. Ruth Cronje, Spencer Rohlinger, Alycia Crall, and Greg Newman. 2011. Does Participation in Citizen Science Improve Scientiﬁc Literacy? A Study to Compare Assessment Methods. Applied Environmental Education & Communication 10, 3 (7 2011), 135–145. \ 15. Tawanna R Dillahunt, Bingxin Chen, and Stephanie Teasley. 2014. Model thinking: demographics and performance of MOOC students unable to afford a formal education. In Proceedings of the ﬁrst ACM conference on Learning@ scale conference. ACM, 145–146. \ 16. Tawanna R Dillahunt, Sandy Ng, Michelle Fiesta, and Zengguang Wang. 2016. Do Massive Open Online Course Platforms Support Employability?. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing. ACM Press, New York, New York, USA, 232–243. \ 17. Ying Ding, Erjia Yan, Arthur Frazho, and James Caverlee. 2009. PageRank for ranking authors in co-citation networks. Journal of the American Society for Information Science and Technology 60, 11 (11 2009), 2229–2243. \ 18. Steven Dow, Anand Kulkarni, Scott Klemmer, and Björn Hartmann. 2012. Shepherding the crowd yields better work. In Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work. ACM Press, New York, New York, USA, 1013. \ 19. M Kevin Eagan, Sylvia Hurtado, Mitchell J Chang, Gina A Garcia, Felisha A Herrera, Juan C Garibay, and Juan C. Garibay. 2013. Making a Difference in Science Education: The Impact of Undergraduate Research Programs. American Educational Research Journal 50, 4 (8 2013), 683–713. \ 20. Snehalkumar (Neil) Gaikwad, Durim Morina, Rohit Nistala, Megha Agarwal, Alison Cossette, Radhika Bhanu, Saiph Savage, Vishwajeet Narwal, Karan Rajpal, Jeff Regino, Aditi Mithal, Adam Ginzberg, Aditi Nath, Karolina R. Ziulkoski, Trygve Cossette, Dilrukshi Gamage, Angela Richmond-Fuller, Ryo Suzuki, Jeerel Herrejón, Kevin Le, Claudia Flores-Saviaga, Haritha Thilakarathne, Kajal Gupta, William Dai, Ankita Sastry, Shirish Goyal, Thejan Rajapakshe, Niki Abolhassani, Angela Xie, Abigail Reyes, Surabhi Ingle, Verónica Jaramillo, Martin Godinez, Walter Angel, Carlos Toxtli, Juan Flores, Asmita Gupta, Vineet Sethia, Diana Padilla, Kristy Milland, Kristiono Setyadi, Nuwan Wajirasena, Muthitha Batagoda, Rolando Cruz, James Damon, Divya Nekkanti, Tejas Sarma, Mohamed Saleh, Gabriela Gongora-Svartzman, Soroosh Bateni, Gema Toledo Barrera, Alex Pe´na, Ryan Compton, Deen Aariff, Luis Palacios, Manuela Paula Ritter, Nisha K.K., Alan Kay, Jana Uhrmeister, Srivalli Nistala, Milad Esfahani, Elsa Bakiu, Christopher Diemert, Luca Matsumoto, Manik Singh, Krupa Patel, Ranjay Krishna, Geza Kovacs, Rajan Vaish, and Michael Bernstein. 2015. Daemo: A Self-Governed Crowdsourcing Marketplace. In Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology. ACM, 101–102. \ 21. Snehalkumar (Neil) S. Gaikwad, Mark E. Whiting, Dilrukshi Gamage, Catherine A. Mullings, Dinesh Majeti, Shirish Goyal, Aaron Gilbee, Nalin Chhibber, Adam Ginzberg, Angela Richmond-Fuller, Sekandar Matin, Vibhor Sehgal, Tejas Seshadri Sarma, Ahmed Nasser, Alipta Ballav, Jeff Regino, Sharon Zhou, Kamila Mananova, Preethi Srinivas, Karolina Ziulkoski, Dinesh Dhakal, Alexander Stolzoff, Senadhipathige S. Niranga, Mohamed Hashim Salih, Akshansh Sinha, Rajan Vaish, and Michael S. Bernstein. 2017. The Daemo Crowdsourcing Marketplace. In Companion of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing. ACM, 1–4. \ 22. S S Gaikwad, D Morina, A Ginzberg, C Mullings, S Goyal, D Gamage, C Diemert, M Burton, S Zhou, M Whiting, K Ziulkoski, A Ballav, A Gilbee, S S Niranga, V Sehgal, J Lin, L Kristianto, J Regino, N Chhibber, D Majeti, S Sharma, K Mananova, D Dhakal, W Dai, V Purynova, S Sandeep, V Chandrakanthan, T Sarma, S Matin, A Nassar, R Nistala, A Stolzoff, K Milland, V Mathur, R Vaish, and M S Bernstein. 2016. Boomerang: Rebounding the Consequences of Reputation Feedback on Crowdsourcing Platforms. In Proceedings of the 29th Annual ACM Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA. \ 23. C R Gerstner and D V Day. 1997. Meta-analytic review of leader-member exchange theory: Correlates and construct issues. Journal of Applied Psychology 82, 6 (1997), 827–844. \ 24. Eric Gilbert. 2013. Widespread underprovision on Reddit. In Proceedings of the 2013 conference on Computer supported cooperative work. ACM, 803–808. \ 25. Timothy Gowers and Michael Nielsen. 2009. Massively collaborative mathematics. Nature 461, 7266 (2009), 879–881. \ 26. Patricia Gurin, Eric Dey, Sylvia Hurtado, and Gerald Gurin. 2002. Diversity and higher education: Theory and impact on educational outcomes. Harvard Educational Review 72, 3 (2002), 330–367. \ 27. Russel S Hathaway, Biren A Nagda, and Sandra R Gregerman. 2002. The relationship of undergraduate research participation to graduate and professional education pursuit: an empirical study. Journal of College Student Development 43, 5 (2002), 614. \ 28. Pamela J Hinds and Diane E Bailey. 2003. Out of sight, out of sync: Understanding conﬂict in distributed teams. Organization science 14, 6 (2003), 615–632. \ 29. Pamela J Hinds and Mark Mortensen. 2005. Understanding conﬂict in geographically distributed teams: The moderating effects of shared identity, shared context, and spontaneous communication. Organization science 16, 3 (2005), 290–307. \ 30. Lu Hong and Scott E Page. 2004. Groups of diverse problem solvers can outperform groups of high-ability problem solvers. Proceedings of the National Academy of Sciences 101, 46 (2004), 16385–16389. \ 31. Anne-Barrie Hunter, Sandra L. Laursen, and Elaine Seymour. 2007. Becoming a scientist: The role of undergraduate research in students’ cognitive, personal, and professional development. Science Education 91, 1 (1 2007), 36–74. \ 32. Lilly C Irani and M Silberman. 2013. Turkopticon: Interrupting worker invisibility in amazon mechanical turk. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 611–620. \ 33. Firas Khatib, Frank DiMaio, Seth Cooper, Maciej Kazmierczyk, Miroslaw Gilski, Szymon Krzywda, Helena Zabranska, Iva Pichova, James Thompson, Zoran Popovi´c, Mariusz Jaskolski, and David Baker. 2011. Crystal structure of a monomeric retroviral protease solved by protein folding game players. Nature Structural & Molecular Biology 18, 10 (9 2011), 1175–1177. \ 34. Sara Kiesler, Robert Kraut, Paul Resnick, and Aniket Kittur. 2012. Regulating behavior in online communities. Building Successful Online Communities: Evidence-Based Social Design. MIT Press, Cambridge, MA (2012). \ 35. Aniket Kittur, Jeffrey V. Nickerson, Michael Bernstein, Elizabeth Gerber, Aaron Shaw, John Zimmerman, Matt Lease, and John Horton. 2013. The future of crowd work. In Proceedings of the 2013 conference on Computer supported cooperative work. ACM Press, New York, New York, USA, 1301. \ 36. Aniket Kittur, Boris Smus, Susheel Khamkar, and Robert E Kraut. 2011. Crowdforge: Crowdsourcing complex work. In Proceedings of the 24th annual ACM symposium on User interface software and technology. ACM, 43–52. \ 37. René F. Kizilcec and Sherif Halawa. 2015. Attrition and Achievement Gaps in Online Learning. In Proceedings of the Second ACM Conference on Learning @ Scale. ACM Press, New York, New York, USA, 57–66. \ 38. Jon M Kleinberg. 1999. Authoritative sources in a hyperlinked environment. Journal of the ACM (JACM) 46, 5 (1999), 604–632. \ 39. Chinmay Kulkarni, Koh Pang Wei, Huy Le, Daniel Chia, Kathryn Papadopoulos, Justin Cheng, Daphne Koller, and Scott R Klemmer. 2013. Peer and self assessment in massive online classes. ACM Transactions on Computer-Human Interaction (TOCHI) 20, 6 (2013), 33. \ 40. Chinmay E Kulkarni, Michael S Bernstein, and Scott R Klemmer. 2015. PeerStudio: rapid peer feedback emphasizes revision and improves performance. In Proceedings of the Second (2015) ACM Conference on Learning@ Scale. ACM, 75–84. \ 41. Karim R Lakhani, Hila Lifshitz-Assaf, and Michael Tushman. 2013. Open innovation and organizational boundaries: task decomposition, knowledge distribution and the locus of innovation. Handbook of economic organization: Integrating economic and organizational theory (2013), 355–382. \ 42. Kate Land, Anže Slosar, Chris Lintott, Dan Andreescu, Steven Bamford, Phil Murray, Robert Nichol, M Jordan Raddick, Kevin Schawinski, Alex Szalay, Daniel Thomas, and Jan Vandenberg. 2008. Galaxy Zoo: the large-scale spin statistics of spiral galaxies in the Sloan Digital Sky Survey. Monthly Notices of the Royal Astronomical Society 388, 4 (2008), 1686–1692. \ 43. R. Eric Landrum and Lisa R. Nelsen. 2002. The Undergraduate Research Assistantship: An Analysis of the Beneﬁts. Teaching of Psychology 29, 1 (2 2002), 15–19. \ 44. Adam Lashinsky. 2012. Inside Apple: How America’s Most Admired–and Secretive–Company Really Works. John Murray Publishers. \ 45. Thomas D LaToza, W Ben Towne, Christian M Adriano, and André Van Der Hoek. 2014. Microtask programming: Building software with a crowd. In Proceedings of the 27th annual ACM symposium on User interface software and technology. ACM, 43–54. \ 46. Edith Law, Andrea Wiggins, Mary L. Gray, and Alex Williams. 2017. Crowdsourcing as a Tool for Research: Implications of Uncertainty. In Proceedings of the 20th ACM Conference on Computer-Supported Cooperative Work & Social Computing (CSCW ’17). ACM, New York, NY, USA. \ 47. Jeehyung Lee, Wipapat Kladwang, Minjae Lee, Daniel Cantu, Martin Azizyan, Hanjoo Kim, Alex Limpaecher, Snehal Gaikwad, Sungroh Yoon, Adrien Treuille, and others. 2014. RNA design rules from a massive open laboratory. Proceedings of the National Academy of Sciences 111, 6 (2014), 2122–2127. \ 48. Jin Li. 2003. US and Chinese cultural beliefs about learning. Journal of educational psychology 95, 2 (2003), 258. \ 49. David Lopatto. 2004. Survey of Undergraduate Research Experiences (SURE): ﬁrst ﬁndings. Cell biology education 3, 4 (2004), 270–7. \ 50. Andrew Mao, Winter Mason, Siddharth Suri, Duncan J. Watts, and TW Malone. 2016. An Experimental Study of Team Size and Performance on a Complex Task. PLOS ONE 11, 4 (4 2016), e0153048. \ 51. David Martin, Benjamin V. Hanrahan, Jacki O’Neill, and Neha Gupta. 2014. Being a turker. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. ACM Press, New York, New York, USA, 224–235. \ 52. Robert C Miller, Haoqi Zhang, Eric Gilbert, and Elizabeth Gerber. 2014. Pair research: matching people for collaboration, learning, and productivity. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. ACM, 1043–1048. \ 53. Michael Nebeling, Alexandra To, Anhong Guo, Adrian A de Freitas, Jaime Teevan, Steven P Dow, and Jeffrey P Bigham. 2016. WearWrite: Crowd-assisted writing from smartwatches. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 3834–3846. \ 54. Nigini Oliveira, Eunice Jun, and Katharina Reinecke. 2017. Citizen Science Opportunities in Volunteer-Based Online Experiments. (2017). \ 55. Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The PageRank citation ranking: bringing order to the web. (1999). \ 56. Scott E Page. 2008. The difference: How the power of diversity creates better groups, ﬁrms, schools, and societies. Princeton University Press. \ 57. Vineet Pandey, Amnon Amir, Justine Debelius, Embriette R Hyde, Tomasz Kosciolek, Rob Knight, and Scott Klemmer. 2017. Gut Instinct: Creating scientiﬁc theories with online learners. (2017). \ 58. David A Patterson. 1994. How to have a bad career in research/academia. In Keynote, 1994 USENIX Symposium on Operating System Design and Implementation. \ 59. M Jordan Raddick, Georgia Bracey, Pamela L Gay, Chris J Lintott, Phil Murray, Kevin Schawinski, Alexander S Szalay, and Jan Vandenberg. 2009. Galaxy zoo: Exploring the motivations of citizen science volunteers. arXiv preprint arXiv:0909.2925 (2009). \ 60. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. \ 61. Katharina Reinecke and Krzysztof Z Gajos. 2015. LabintheWild: Conducting large-scale online experiments with uncompensated samples. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing. ACM, 1364–1378. \ 62. Daniela Retelny, Sébastien Robaszkiewicz, Alexandra To, Walter S Lasecki, Jay Patel, Negar Rahmati, Tulsee Doshi, Melissa Valentine, and Michael S Bernstein. 2014. Expert crowdsourcing with ﬂash teams. In Proceedings of the 27th annual ACM symposium on User interface software and technology. ACM, 75–85. \ 63. Olga Russakovsky, Li-Jia Li, and Li Fei-Fei. 2015. Best of both worlds: Human-machine collaboration for object annotation. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2121–2131. \ 64. Susan H Russell, Mary P Hancock, and James McCullough. 2007. Beneﬁts of undergraduate research experiences. Science 316, 5824 (2007), 548–549. \ 65. Heather Sarsons. 2015. Gender differences in recognition for group work. Working Paper, Harvard University (2015). \ 66. Alok Shankar Mysore, Vikas S Yaligar, Imanol Arrieta Ibarra, Camelia Simoiu, Sharad Goel, Ramesh Arvind, Chiraag Sumanth, Arvind Srikantan, Bhargav HS, Mayank Pahadia, Tushar Dobha, Atif Ahmed, Mani Shankar, Himani Agarwal, Rajat Agarwal, Sai Anirudh-Kondaveeti, Shashank Arun-Gokhale, Aayush Attri, Arpita Chandra, Yogitha Chilukur, Sharath Dharmaji, Deepak Garg, Naman Gupta, Paras Gupta, Glincy Mary Jacob, Siddharth Jain, Shashank Joshi, Tarun Khajuria, Sameeksha Khillan, Sandeep Konam, Praveen Kumar-Kolla, Sahil Loomba, Rachit Madan, Akshansh Maharaja, Vidit Mathur, Bharat Munshi, Mohammed Nawazish, Venkata Neehar-Kurukunda, Venkat Nirmal-Gavarraju, Sonali Parashar, Harsh Parikh, Avinash Paritala, Amit Patil, Rahul Phatak, Mandar Pradhan, Abhilasha Ravichander, Krishna Sangeeth, Sreecharan Sankaranarayanan, Vibhor Sehgal, Ashrith Sheshan, Suprajha Shibiraj, Aditya Singh, Anjali Singh, Prashant Sinha, Pushkin Soni, Bipin Thomas, Kasyap Varma-Dattada, Sukanya Venkataraman, Pulkit Verma, and Ishan Yelurwar. 2015. Investigating the “Wisdom of Crowds” at Scale. In Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST ’15 Adjunct). ACM, New York, NY, USA, 75–76. \ 67. Hua-Wei Shen and Albert-László Barabási. 2014. Collective credit allocation in science. Proceedings of the National Academy of Sciences 111, 34 (2014), 12325–12330. \ 68. Jonathan Silvertown. 2009. A new dawn for citizen science. Trends in Ecology & Evolution 24, 9 (9 2009), 467–471. \ 69. Günter K Stahl, Martha L Maznevski, Andreas Voigt, and Karsten Jonsen. 2010. Unraveling the effects of cultural diversity in teams: A meta-analysis of research on multicultural work groups. Journal of international business studies 41, 4 (2010), 690–709. \ 70. Matthias Stevens, Michalis Vitos, Julia Altenbuchner, Gillian Conquest, Jerome Lewis, and Muki Haklay. 2014. Taking participatory citizen science to extremes. IEEE Pervasive Computing 13, 2 (2014), 20–29. \ 71. Brian L. Sullivan, Christopher L. Wood, Marshall J. Iliff, Rick E. Bonney, Daniel Fink, and Steve Kelling. 2009. eBird: A citizen-based bird observation network in the biological sciences. Biological Conservation 142, 10 (10 2009), 2282–2292. \ 72. Yizhou Sun, Yintao Yu, and Jiawei Han. 2009. Ranking-based clustering of heterogeneous information networks with star network schema. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 797–806. \ 73. Ryo Suzuki, Niloufar Salehi, Michelle S Lam, Juan C Marroquin, and Michael S Bernstein. 2016. Atelier: Repurposing Expert Crowdsourcing Tasks as Micro-internships. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 2645–2656. \ 74. Jaime Teevan, Shamsi T Iqbal, and Curtis von Veh. 2016. Supporting collaborative writing with microtasks. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 2657–2668. \ 75. Ginka Toegel and Jay A Conger. 2003. 360-degree assessment: Time for reinvention. Academy of Management Learning & Education 2, 3 (2003), 297–311. \ 76. Bill Tomlinson, Joel Ross, Paul Andre, Eric Baumer, Donald Patterson, Joseph Corneli, Martin Mahaux, Syavash Nobarany, Marco Lazzari, Birgit Penzenstadler, and others. 2012. Massively distributed authorship of academic papers. In Extended Abstracts of the 2012 ACM annual conference on Human Factors in Computing Systems. ACM, 11–20. \ 77. Jeanne L Tsai. 2007. Ideal affect: Cultural causes and behavioral consequences. Perspectives on Psychological Science 2, 3 (2007), 242–259. \ 78. Melissa A Valentine, Daniela Retelny, Alexandra To, Negar Rahmati, Tulsee Doshi, and Michael S Bernstein. 2017. Flash Organizations: Crowdsourcing Complex Work by Structuring Crowds As Organizations. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 3523–3537. \ 79. Andreas Veit, Michael J Wilber, Rajan Vaish, Serge J Belongie, James Davis, Vishal Anand, Anshu Aviral, Prithvijit Chakrabarty, Yash Chandak, Sidharth Chaturvedi, Chinmaya Devaraj, Ankit Dhall, Utkarsh Dwivedi, Sanket Gupte, Sharath N Sridhar, Karthik Paga, Anuj Pahuja, Aditya Raisinghani, Ayush Sharma, Shweta Sharma, Darpana Sinha, Nisarg Thakkar, K Bala Vignesh, Utkarsh Verma, Kanniganti Abhishek, Amod Agrawal, Arya Aishwarya, Aurgho Bhattacharjee, Sarveshwaran Dhanasekar, Venkata Karthik Gullapalli, Shuchita Gupta, Chandana G, Kinjal Jain, Simran Kapur, Meghana Kasula, Shashi Kumar, Parth Kundaliya, Utkarsh Mathur, Alankrit Mishra, Aayush Mudgal, Aditya Nadimpalli, Munakala Sree Nihit, Akanksha Periwal, Ayush Sagar, Ayush Shah, Vikas Sharma, Yashovardhan Sharma, Faizal Siddiqui, Virender Singh, Abhinav S., Pradyumna Tambwekar, Rashida Taskin, Ankit Tripathi, and Anurag D Yadav. 2015. On Optimizing Human-Machine Task Assignments. HCOMP 2015 Extended Abstracts (2015). \ 80. Mark E Whiting, Dilrukshi Gamage, Snehalkumar (Neil) S Gaikwad, Aaron Gilbee, Shirish Goyal, Alipta Ballav, Dinesh Majeti, Nalin Chhibber, Angela Richmond-Fuller, Freddie Vargus, Tejas Seshadri Sarma, Varshine Chandrakanthan, Teogenes Moura, Mohamed Hashim Salih, Gabriel Bayomi Tinoco Kalejaiye, Adam Ginzberg, Catherine A Mullings, Yoni Dayan, Kristy Milland, Henrique Oreﬁce, Jeff Regino, Sayna Parsi, Kunz Mainali, Vibhor Sehgal, Sekandar Matin, Akshansh Sinha, Rajan Vaish, and Michael S Bernstein. 2017. Crowd Guilds: Worker-led Reputation and Feedback on Crowdsourcing Platforms. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing (CSCW ’17). ACM, New York, NY, USA, 1902–1913. \ 81. Haizi Yu, Biplab Deka, Jerry O Talton, and Ranjitha Kumar. 2016. Accounting for taste: ranking curators and content in social networks. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 2383–2389. \ 82. Lixiu Yu, Aniket Kittur, and Robert E Kraut. 2014. Distributed analogical idea generation: inventing with crowds. In Proceedings of the 32nd annual ACM conference on Human factors in computing systems. ACM, 1245–1254. \ 83. Haoqi Zhang, Matthew W. Easterday, Elizabeth M. Gerber, Daniel Rees Lewis, and Leesha Maliakal. 2017. Agile Research Studios. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing - CSCW ’17. ACM Press, New York, New York, USA, 220–232. \ 84. Andrew L. Zydney, Joan S. Bennett, Abdus Shahid, and Karen W. Bauer. 2002. Impact of Undergraduate Research Experience in Engineering. Journal of Engineering Education 91, April (4 2002), 151–157. \ ",Crowdsourcing; citizen science; crowd research,H.5.3,uistf4070-file1.zip,uistf4070-file2.jpg,,uistf4070-file4.zip,"Crowd Research is a crowdsourcing technique that enables access and coordinates participants worldwide to work together on open-ended research problems, like a university laboratory at massive scale. ",The auxiliary material contains an appendix and a Crowd Research recommendation letter template in PDF format. ,"We present a crowdsourcing technique that enabled access for 1,500 participants worldwide to work together on open-ended research problems, like a university laboratory at massive scale. ",We have addressed the suggestions from the AC and other reviewers in this final version of the paper. Please find the changes highlighted in green here: https://www.dropbox.com/s/3hxp03u930gymn0/crowd-research_changes.pdf?dl=0,Rajan Vaish,Michael Bernstein,FormatComplete,N00014-16-1-2894,Office of Naval Research,Office of Naval Research,N00014-15-1-2711,,,Aug 2 16:08,
uistf4234,10/25,17,Crowd,3:40:00 PM,4:50:00 PM,3+1,4:30:00 PM,4:50:00 PM,long,long,uistf4070,4,1704,,,uistf4234,A,RICO: A Mobile App Dataset for Building Data-Driven Design Applications,Biplab,Deka,deka2@illinois.edu,uistf4234-paper.pdf,10,letter,,,"Biplab Deka, Zifeng Huang, Chad D Franzen, Joshua Hibschman, Dan Afergan, Yang Li, Jeffrey Nichols, Ranjitha Kumar","deka2@illinois.edu, zhuang45@illinois.edu, cdfranz2@illinois.edu, jh@u.northwestern.edu, afergan@gmail.com, yangli@acm.org, jeff@jeffreynichols.com, ranjitha@illinois.edu",49332,Biplab,,Deka,deka2@illinois.edu,Department of Electrical and Computer Engineering,University of Illinois at Urbana-Champaign,Champaign,Illinois,United States,,,,,,49333,Zifeng,,Huang,zhuang45@illinois.edu,Computer Science,University of Illinois at Urbana-Champaign,Champaign,Illinois,United States,,,,,,71865,Chad,D,Franzen,cdfranz2@illinois.edu,Department of Computer Science,University of Illinois at Urbana Champaign,Urbana,Illinois,United States,,,,,,51518,Joshua,,Hibschman,jh@u.northwestern.edu,EECS,Northwestern University,Evanston,IL,United States,,,,,,16883,Dan,,Afergan,afergan@gmail.com,,"Google, Inc.",Mountain View,California,United States,,,,,,3340,Yang,,Li,yangli@acm.org,,Google Research,Mountain View,California,United States,,,,,,2720,Jeffrey,,Nichols,jeff@jeffreynichols.com,,"Google, Inc.",Mountain View,California,United States,,,,,,13333,Ranjitha,,Kumar,ranjitha@illinois.edu,Computer Science,University of Illinois at Urbana-Champaign,Champaign,Illinois,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present a new, publicly available dataset of mobile app designs with the goal of advancing the development of data-driven mobile app design tools. This dataset was mined from over 9.7K existing Android apps from 27 categories using a combination of human and automated crawling of apps. The dataset contains more than 72K unique UI screens captured as screenshots and view hierarchies, metadata from the Google Play Store (category, ratings, etc.), and the set of human interaction traces captured in the crawl. We sketch the diverse set of applications that could be enabled by this dataset and demonstrate that the dataset’s scale can support deep learning techniques by training and evaluating an autoencoder for UI layout similarity.",deka2@illinois.edu," \ WARNING: Reference 15 is very long.  Please verify that it was extracted correctly. \  \ 1. Android Activities, 2016. https://developer.android.com/guide/components/ activities.html. \ 2. Database of Android Apps on Kaggle, 2016. https://www.kaggle.com/orgesleka/android-apps. \ 3. UI Overview, 2016. https://developer.android.com/ guide/topics/ui/overview.html. \ 4. Alharbi, K., and Yeh, T. Collect, decompile, extract, stats, and diff: Mining design pattern changes in Android apps. In Proc. MobileHCI (2015). \ 5. Amini, S. Analyzing Mobile App Privacy Using Computation and Crowdsourcing. PhD thesis, Carnegie Mellon University, 2014. \ 6. Azim, T., and Neamtiu, I. Targeted and depth-ﬁrst exploration for systematic testing of android apps. In ACM SIGPLAN Notices (2013). \ 7. Bell, S., and Bala, K. Learning visual similarity for product design with convolutional neural networks. ACM TOG (2015). \ 8. Bengio, Y. Learning deep architectures for ai. Foundations and Trends in Machine Learning 2, 1 (2009). \ 9. Bhoraskar, R., Han, S., Jeon, J., Azim, T., Chen, S., Jung, J., Nath, S., Wang, R., and Wetherall, D. Brahmastra: Driving apps to test the security of third-party components. In Proc. SEC (2014). \ 10. Brandt, J., Dontcheva, M., Weskamp, M., and Klemmer, S. R. Example-centric programming: integrating web search into the development environment. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM (2010), 513–522. \ 11. Deka, B., Huang, Z., and Kumar, R. ERICA: Interaction mining mobile apps. In Proc. UIST (2016). \ 12. Deka, B., Huang, Z., Nichols, J., Li, Y., and Kumar, R. ZIPT: Zero-intergration performance testing for mobile applications. In Proc. UIST (2017). \ 13. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proc. CVPR (2009). \ 14. Eckert, C., and Stacey, M. Sources of inspiration: A language of design. Design Studies 21, 5 (2000), 523–538. \ 15. Eckert, C., Stacey, M., and Earl, C. References to past designs. Studying Designers 5 (2005), 3–21. Query UI Retrieved UIs 1 2 3 4 5 10 100 1000 Text Non-Text (a) (b) (c) (d) (e) (f) (g) Figure 8: Results from querying the repository using nearest neighbor search in the learned lower-dimensional space for the UIs. The top ﬁve results as well as results at rank 10, 100, and 1000 are shown. We observe that the layout of the top ﬁve results are very similar to that of the query UI and the similarity decreases for lower ranked results. \ 16. Frank, M., Dong, B., Felt, A. P., and Song, D. Mining permission request patterns from android and facebook applications. In Proc. ICDM (2012). \ 17. Fu, B., Lin, J., Li, L., Faloutsos, C., Hong, J., and Sadeh, N. Why people hate your app: Making sense of user feedback in a mobile app store. In Proc. KDD (2013). \ 18. Funkhouser, T., Min, P., Kazhdan, M., Chen, J., Halderman, A., Dobkin, D., and Jacobs, D. A search engine for 3D models. ACM TOG (2003). \ 19. Koch, J., and Oulasvirta, A. Computational layout perception using gestalt laws. In Proc. CHI (Extended Abstracts) (2016). \ 20. Komarov, S., Reinecke, K., and Gajos, K. Z. Crowdsourcing performance evaluations of user interfaces. In Proc. CHI (2013). \ 21. Kumar, R., Satyanarayan, A., Torres, C., Lim, M., Ahmad, S., Klemmer, S. R., and Talton, J. O. Webzeitgeist: Design mining the web. In Proc. CHI (2013). \ 22. Lee, K., Flinn, J., Giuli, T., Noble, B., and Peplin, C. Amc: Verifying user interface properties for vehicular applications. In Proc. Mobisys (2013). \ 23. Leuthold, S., Schmutz, P., Bargas-Avila, J. A., Tuch, A. N., and Opwis, K. Vertical versus dynamic menus on the world wide web: Eye tracking study measuring the inﬂuence of menu design and task complexity on user performance and subjective preference. Computers in human behavior 27, 1 (2011), 459–472. \ 24. McAuley, J., Targett, C., Shi, Q., and Van Den Hengel, A. Image-based recommendations on styles and substitutes. In Proc SIGIR, ACM (2015), 43–52. \ 25. Miller, S. R., and Bailey, B. P. Searching for inspiration: An in-depth look at designers example ﬁnding practices. In ASME 2014 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference (2014). \ 26. Miniukovich, A., and De Angeli, A. Visual impressions of mobile app interfaces. In Proc. Nordic CHI (2014). \ 27. Miniukovich, A., and De Angeli, A. Computation of interface aesthetics. In Proc. CHI (2015). \ 28. Miniukovich, A., and De Angeli, A. Visual diversity and user interface quality. In Proc. British HCI (2015). \ 29. Miniukovich, A., and De Angeli, A. Pick me!: Getting noticed on google play. In Proc. CHI (2016). \ 30. Nair, V., and Hinton, G. E. Rectiﬁed linear units improve restricted boltzmann machines. In Proc. ICML (2010), 807–814. \ 31. Nguyen, T. A., and Csallner, C. Reverse engineering mobile application user interfaces with REMAUI. In Proc. ASE (2015). \ 32. O’Donovan, P., L¯ıbeks, J., Agarwala, A., and Hertzmann, A. Exploratory font selection using crowdsourced attributes. ACM TOG (2014). \ 33. ODonovan, P., Agarwala, A., and Hertzmann, A. Learning layouts for single-page graphic designs. IEEE TVCG (2014). \ 34. Reinecke, K., Yeh, T., Miratrix, L., Mardiko, R., Zhao, Y., Liu, J., and Gajos, K. Z. Predicting users’ ﬁrst impressions of website aesthetics with a quantiﬁcation of perceived visual complexity and colorfulness. In Proc. CHI (2013). \ 35. Ritchie, D., Kejriwal, A. A., and Klemmer, S. R. d. tour: Style-based exploration of design example galleries. In Proc. UIST (2011). \ 36. Sahami Shirazi, A., Henze, N., Schmidt, A., Goldberg, R., Schmidt, B., and Schmauder, H. Insights into layout patterns of mobile user interfaces by an automatic analysis of Android apps. In Proc. EICS (2013). \ 37. Szydlowski, M., Egele, M., Kruegel, C., and Vigna, G. Challenges for dynamic analysis of iOS applications. In Open Problems in Network Security. Springer, 2012, 65–77. \ 38. Talton, J., Yang, L., Kumar, R., Lim, M., Goodman, N., and Mˇech, R. Learning design patterns with bayesian grammar induction. In Proc. UIST, ACM (2012). \ 39. Tractinsky, N., Inbar, O., Tsimhoni, O., and Seder, T. Slow down, you move too fast: Examining animation aesthetics to promote eco-driving. In Proc. AutoUI, ACM (2011), 193–202. \ 40. van der Geest, T., and Loorbach, N. Testing the visual consistency of web sites. Technical communication 52, 1 (2005), 27–36. \ 41. Viennot, N., Garcia, E., and Nieh, J. A measurement study of google play. In ACM SIGMETRICS Performance Evaluation Review, vol. 42, ACM (2014), 221–233. \ 42. Yi, L., Guibas, L., Hertzmann, A., Kim, V. G., Su, H., and Yumer, E. Learning hierarchical shape segmentation and labeling from online repositories. In Proc. SIGGRAPH (2017). \ ",Mobile App Design; Design Mining; Design Search; App Dataset;,D.2.2,uistf4234-file1.zip,,,,,,"We present a new, publicly available dataset of mobile app designs with the goal of advancing the development of data-driven mobile app design tools. ","JULY 30th UPDATE \  \ - We added the statement ""The RICO dataset will be publicly available at http://interactionmining.org/rico until 2022."" \  \ - We detailed precisely how RICO supports all five classes of applications in the ""DATA-DRIVEN DESIGN APPLICATIONS"" section. \  \ - We took a pass over the ""ANDROID APP DATASETS"" section to clarify the differences between RICO and ERICA. \  \ Thank you for your patience! We hope that we've made all major changes required for acceptance. Before August 9th, we will take another cleaning pass to improve presentation: fix typos, tweak figures, improve layout, etc.  \  \ ---------------------------------------------------------------------------- \ We have rewritten large parts of the draft in accordance to the conditions mentioned by the primary. Below we describe which parts of the paper were modified for each specific request: \  \ (i) The contribution of the paper should be reframed as the large dataset \    of mobile app screenshots. The authors should not claim the hybrid data \    collection system as a research contribution as it presents limited \    technical novelty. The authors could simply describe the system as a tool \    that was used collect the dataset. \ >> We have changed the title, introduction, and the rest of the paper to reframe \   our primary contribution as the dataset \  \ (ii) Make the collected dataset publicly available and maintain the \    dataset. The authors should put the archived dataset at a publicly \    accessible online repository, include the URL to the dataset in the \    paper, and include how long the authors plan to maintain this dataset in \    the paper. \ >> The dataset will be publicly available at http://interactionmining.org/rico for the \   next two years. We have added this to the paper \  \ (iii) Provide the analysis of the dataset as claimed in the rebuttal. \ >> Table 4 describes the contents of the dataset.  Summary statistics of the \   dataset are in Figure 6. Both are discussed in the ""RICO DATASET"" section of the \   paper. The ""search heuristic"" has also been evaluated as promised in the rebuttal \   (in the ""Effectiveness of the Similarity Heuristic"" subsection on page 5). \  \ (iv) Provide the descriptive statistics comparing the difference between \    ERICA and RICO datasets. At minimum, report the size of both datasets and \    provide the comparison. Also clarify that the RICO dataset does not have \    human trace information unlike the ERICA dataset. \ >> Table 2 compares the RICO dataset with the ERICA dataset and two other \   datasets. The main differences are also elaborated upon in the ""Comparison With \   Other Mobile App Datasets"" subsection on page 2. \  \ (v) Describe potential applications as 2AC mentioned: (i) supporting \    design explorations like [16], (ii) understanding common design patterns \    for mobile apps in a particular category, and (iii) identifying how many \    different screens or menus an app to be built should have by comparing \    those in a similar category. \ >> Potential applications that motivated us to collect the dataset are discussed in \   detail in the ""MOTIVATING APPLICATIONS AND RELATED WORK"" section on \   page 2. They are also mentioned briefly in the ""DISCUSSION AND FUTURE \   WORK"" section on page 7.",Biplab Deka,Ranjitha Kumar,FormatComplete,,,,,,,Jul 30 5:32,
uistpp111,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp111,A,KinToon: A Kinect Facial Projector for Communication Enhancement for ASD Children,Caowei,Zhang,415965428@qq.com,uistpp0111-paper.pdf,3,letter,,,"Cheng Zheng, Caowei Zhang, Xuan Li, Fan Zhang, Bing Li, Chuqi Tang, Cheng Yao, Ting Zhang, Fangtian Ying","zzzzzccccc@gmail.com, 415965428@qq.com, xuanli@zju.edu.cn, zfan425@zju.edu.cn, happyice19870112@zju.edu.cn, 369710861@qq.com, yaoch@zju.edu.cn, zhangting5@zju.edu.cn, yingft@gmail.com",71397,Cheng,,Zheng,zzzzzccccc@gmail.com,,Zhejiang University,Hangzhou,,China,,,,,,58118,Caowei,,Zhang,415965428@qq.com,,Zhejiang University,Hangzhou,,China,,,,,,69501,Xuan,,Li,xuanli@zju.edu.cn,Zhejiang University,Hangzhou,,China,,,,,,,73641,Fan,,Zhang,zfan425@zju.edu.cn,,Zhejiang University,Hangzhou,Zhejiang,China,,,,,,57374,Bing,,Li,happyice19870112@zju.edu.cn,Digital Art Design,Computer Science,Hangzhou,Zhejiang,China,,,,,,69503,Chuqi,,Tang,369710861@qq.com,,Zhejiang University,Hangzhou,,China,,,,,,59812,Cheng,,Yao,yaoch@zju.edu.cn,"Industrial Design,Zhejiang University",Computer Science and Technology,Hangzhou,Zhejiang,China,,,,,,74107,Ting,,Zhang,zhangting5@zju.edu.cn,"Industrial Design,Zhejiang University, Computer Scienve and Technology, Hangzhou, Zhejiang, China, Industrial Design,Zhejiang University, Hangzhou, Zhejiang, China",Zhejiang University,Hangzhou,Zhejiang,China,,,,,,36206,Fangtian,,Ying,yingft@gmail.com,Zhejiang University,College of Computer Science and Technology,Hangzhou,,China,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Children with ASD (Autism Spectrum Disorder) have social communication difficulties partly due to their abnormal avoidance of eye contact on human faces, yet they have a normal visual processing strategy on cartoon face. In this paper, we present KinToon, a face-to-face communication enhancement system to help ASD children in their training lessons. Our system use Kinect to scan human face and extract key points from facial contour, and match them to corresponding key points of a cartoon face. A modified cartoon face is projected to the communicator's face to achieve the effect of dynamic ""makeup"". ASD children will finally talk to the communicator with dynamic cartoon makeup, which would reduce their stress of interacting with people and make them easier to understand emotions. The interactive devices were applied to an ASD training lesson, and our creative approach was examined to be relatively effective in encouraging ASD children to fetch more emotional information and have more eye contact with people by eye tracking.",415965428@qq.com,"1. Ajmera, R., Nigam, A., & Gupta, P. (2014, December). 3d face recognition using kinect. In Proceedings of the 2014 Indian Conference on Computer Vision Graphics and Image Processing (p. 76). ACM. \ 2. Chawarska, K., &Volkmar, F. (2005). Autism in infancy and early childhood. In Volkmar, F. R., Paul, R., Klin, A., & Cohen, D. (Eds.), Handbook of autism and pervasive developmental disorders. Vol. 1 (pp.223â€“ 246). Hoboken, NJ: John Wiley & Sons. \ 3. Bouaziz, S. (2015). Realtime Face Tracking and Animation (Doctoral dissertation). \ 4. Miyafuji, S., & Koike, H. (2015, November). Ballumiere: Real-time tracking and projection system for high-speed flying balls. In SIGGRAPH Asia 2015 Emerging Technologies (p. 2). ACM. \ 5. Rosset, D. B., Rondan, C., Da Fonseca, D., Santos, A., Assouline, B., & Deruelle, C. (2008). Typical emotion processing for cartoon but not for real faces in children with autistic spectrum disorders. Journal of autism and developmental disorders, 38(5), 919-925. \ 6. Zheng, C., Zhang, C. W., Li, X., Liu, X., Tang, C.Q., Wang, G.Y., Yao, C., Zhang, F., Xu, W.J., & Ying, F. T. (2017, July). Toon-Chat: A Cartoon-Masked Chat System for Children with Autism. ACM SIGGRAPH 2017 Posters. ACM. \",Facial Projector; Kinect; Mixed Reality; ASD Children; Communication Enhancement,H.5.m,uistpp0111-file1.doc,,,uistpp0111-file4.pdf,,poster file printed for exhibition (PDF),We wish to encourage ASD children to communicate more with others through our user interface. It is just the first step.,,Caowei Zhang,Cheng Zheng,FormatComplete,61332017,National Natural Science of China,National Key Technologies R&D Program,2015BAF14B01,,,Aug 16 21:53,
uistpp117,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp117,A,Towards Intermanual Apparent Motion of Thermal Pulses,Daniel,Gongora,daniel@rm.is.tohoku.ac.jp,uistpp0117-paper.pdf,3,letter,,,"Daniel Gongora, Roshan L Peiris, Kouta Minamizawa","daniel@rm.is.tohoku.ac.jp, roshan@kmd.keio.ac.jp, kouta@kmd.keio.ac.jp",71764,Daniel,,Gongora,daniel@rm.is.tohoku.ac.jp,Graduate School of Information Sciences,Keio University,Yokohama City,Kanagawa,Japan,,,,,,53252,Roshan,L,Peiris,roshan@kmd.keio.ac.jp,Graduate School of Media Design,Keio University,Yokohama City,Kanagawa,Japan,,,,,,62190,Kouta,,Minamizawa,kouta@kmd.keio.ac.jp,Keio University,Yokohama City,Kanagawa,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Perceptual illusions enable designers to go beyond hardware limitations to create rich haptic content. Nevertheless, spatio-temporal interactions for thermal displays have not been studied thoroughly. We focus on the apparent motion of hot and cold thermal pulses delivered at the thenar eminence of both hands. Here we show that 1000 ms hot and cold thermal pulses overlapping for about 40% of their actuation time are likely to produce a continuous apparent motion sensation. Furthermore, we show that the quality of the illusion (defined as the motion's temporal continuity) was more sensitive to changes in SOA for cold pulses in relation to hot pulses.",daniel@rm.is.tohoku.ac.jp,"1. Louise G Hagander, Hani A Midani, Michael A Kuskowski, and Gareth JG Parry. 2000. Quantitative sensory testing: effect of site and skin temperature on thermal thresholds. Clinical neurophysiology 111, 1 (2000), 17â€“22. \ 2. Lynette A Jones and Hsin-Ni Ho. 2008. Warm or cool, large or small? The challenge of thermal displays. IEEE Transactions on Haptics 1, 1 (2008), 53â€“70. \ 3. Lynette A Jones and Hong Z Tan. 2013. Application of psychophysical techniques to haptic research. IEEE Transactions on Haptics 6, 3 (2013), 268â€“284. \ 4. Susan J Lederman and Lynette A Jones. 2011. Tactile and haptic illusions. IEEE Transactions on Haptics 4, 4 (2011), 273â€“294. \ 5. Andrew J RÃ³zsa and Dan R Kenshalo. 1977. Bilateral spatial summation of cooling of symmetrical sites. Perception & Psychophysics 21, 5 (1977), 455â€“462. \",Thermal; Perceptual illusion; Intermanual,H.5.2,uistpp0117-file1.zip,,uistpp0117-file3.mp4,uistpp0117-file4.zip,,Poster that accompanies the submission: Towards Intermanual Apparent Motion of Thermal Pulses \ \ The ZIP file contains a single PDF file. \,One-second thermal pulses to the thenar eminence of the hands produce a continuous apparent motion sensation when their actuation time overlaps for about 40 percent of their duration.,,Roshan Peiris,Daniel Gongora,FormatComplete,Embodied Media project (JPMJAC1404),JST ACCEL,,,,,Aug 10 4:18,
uistpp120,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp120,A,Information Identification Support Method for Areas with Densely Located Signboards,Shigeo,Kitamura,k403662@kansai-u.ac.jp,uistpp0120-paper.pdf,2,letter,,,"Shigeo Kitamura, Mitsunori Matsushita","k403662@kansai-u.ac.jp, mat@res.kutc.kansai-u.ac.jp",73049,Shigeo,,Kitamura,k403662@kansai-u.ac.jp,Graduate School of Informatics,Kansai University,Takatsuki,Osaka,Japan,,,,,,73580,Mitsunori,,Matsushita,mat@res.kutc.kansai-u.ac.jp,Graduate School of Informatics,Kansai University,Takatsuki,Osaka,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We developed methods and implemented a system prototype to help people find specific signboards in areas with densely located signboards. In addition, we examined whether the proposed methods would reduce the search time of a specific signboard. The result showed that the proposed method was superior in cases where there were multiple signboards to be searched and background saturation was low.",k403662@kansai-u.ac.jp,"1. Hata, H., Koike, H., and Sato, Y. Visual guidance with unnoticed blur effect. In Proceedings of the International Working Conferences on Advanced Visual Interfaces, no. 8 in AVI â€² 16 (2016), 28â€“35. \ 2. Kawai, N., Sato, T., and Yokoya, N. Diminished reality based on image inpainting considering background geometry. IEEE Transactions on Visualization and Computer Graphics 22, 3 (2016), 1236â€“1247. \ 3. Umair, R., and Shi, C. Augmented reality-based indoor navigation using google glass as a wearable head-mounted display. In IEEE International Conference on Systems, Man, and Cybernetics (2015), 1452â€“1457. \",Augmented Reality; Diminished Reality; Visibility of visual information,H.5.m.,uistpp0120-file1.zip,uistpp0120-file2.jpg,uistpp0120-file3.mp4,uistpp0120-file4.pdf,This is the scenery of the experiment. Users can see whole sky ball image through the mobile device. We measured the time required for users to find specific signboards.,This is a poster design pdf file. You may need AXIS Font (http://typeproject.com/en/fonts/axisfont).,A large amount of signboard is present in urban areas. We developed methods and implemented a system prototype to help people find specific signboards in areas with densely located signboards.,,Shigeo Kitamura,Mitsunori Matsushita,FormatComplete,,,,,,,Aug 10 11:51,
uistpp125,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp125,A,Projective Windows: Arranging Windows in Space Using Projective Geometry,Joon Hyub,Lee,joonhyub.lee@kaist.ac.kr,uistpp0125-paper.pdf,3,letter,,,"Joon Hyub Lee, Sang-Gyun An, Yongkwan Kim, Seok-Hyung Bae","joonhyub.lee@kaist.ac.kr, sang-gyun.an@kaist.ac.kr, yongkwan.kim@kaist.ac.kr, seokhyung.bae@kaist.ac.kr",27369,Joon Hyub,,Lee,joonhyub.lee@kaist.ac.kr,Department of Industrial Design,KAIST (Korea Advanced Institute of Science and Technology),Daejeon,,"Korea, Republic of",,,,,,49079,Sang-Gyun,,An,sang-gyun.an@kaist.ac.kr,Department of Industrial Design,KAIST (Korea Advanced Institute of Science and Technology),Daejeon,,"Korea, Republic of",,,,,,25936,Yongkwan,,Kim,yongkwan.kim@kaist.ac.kr,KAIST (Korea Advanced Institute of Science and Technology),Daejeon,,Republic of Korea,,,,,,,17771,Seok-Hyung,,Bae,seokhyung.bae@kaist.ac.kr,Department of Industrial Design,KAIST (Korea Advanced Institute of Science and Technology),Daejeon,,Republic of Korea,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In augmented and virtual reality, there may be many 3D planar windows with 2D texts, images, and videos on them. Projective Windows is a technique using projective geometry to bring any near or distant window instantly to the fingertip and then to scale and position it simultaneously with a single, continuous flow of hand motion.",joonhyub.lee@kaist.ac.kr,"1. Luigi Atzori, Antonio Iera, and Giacomo Morabito. 2010. The internet of things: a survey. Computer Networks, 54(15), 2787-2805. \ 2. Till Ballendat, Nicolai Marquardt, and Saul Greenberg. 2010. Proxemic interaction: designing for a proximity and orientation-aware environment. In Proc. ITSâ€™10, 121-130. \ 3. Li-Wei Chan, Hui-Shan Kao, Mike Y. Chen, Ming-Sui Lee, Jane Hsu, and Yi-Ping Hung. 2010. Touching the void: direct-touch interaction for intangible displays. In Proc. CHIâ€™10, 2625-2634. \ 4. Barrett Ens, Rory Finnegan, and Pourang Irani. 2014. The personal cockpit: a spatial interface for effective task switching on head-worn displays. In Proc. CHIâ€™14, 3171-3180. \ 5. Barrett Ens, Juan David HincapiÃ©-Ramos, and Pourang Irani. 2014. Ethereal planes: a design framework for 2D information space in 3D mixed reality environments. In Proc. SUIâ€™14, 2-12. \ 6. Steven Feiner, Blair MacIntyre, Marcus Haupt, and Eliot Solomon. 1993. Windows on the world: 2D windows for 3D augmented reality. In Proc. UISTâ€™93, 145-155. \ 7. Tovi Grossman and Ravin Balakrishnan. 2005. The bubble cursor: enhancing target acquisition by dynamic resizing of the cursorâ€™s activation area. In Proc. CHIâ€™05, 281-290. \ 8. Jan Gugenheimer, David Dobbelstein, Christian Winkler, Gabriel Haas, and Enrico Rukzio. 2016. FaceTouch: enabling touch interaction in display fixed UIs for mobile virtual reality. In Proc. UISTâ€™16, 49-60. \ 9. Chris Harrison, Hrvoje Benko, and Andrew D. Wilson. 2011. OmniTouch: wearable multitouch interaction everywhere. In Proc. UISTâ€™11, 441-450. \ 10. Valentin Heun, James Hobin, and Pattie Maes. 2013. Reality editor: programming smarter objects. In Adj. Proc. UbiCompâ€™13, 307-310. \ 11. Jinha Lee, Alex Olwal, Hiroshi Ishii, and Cati Boulanger. 2013. SpaceTop: integrating 2D and spatial 3D interactions in a see-through desktop environment. In Proc. CHIâ€™13, 189-192. \ 12. Joon Hyub Lee, Seok-Hyung Bae, Jinyung Jung, Hayan Choi. 2012. Transparent display interaction without binocular parallax. In Adj. Proc. UISTâ€™12, 9798. \ 13. Joon Hyub Lee and Seok-Hyung Bae. 2013. Binocular cursor: enabling selection on transparent displays troubled by binocular parallax. In Proc. CHIâ€™13, 31693172. \ 14. Frank Chun Yat Li, David Dearman, and Khai N. Truong. 2009. Virtual shelves: interactions with orientation aware devices. In Proc. UISTâ€™09, 125-128. \ 15. Kyle Orland. 2014. Will VR make flat panels obsolete? Oculusâ€™ founder gives it 20 years. Ars Technica. Retrieved July 12, 2017 from https://arstechnica.com /gaming/2014/04/will-vr-make-flat-panels-obsoleteoculus-founder-gives-it-20-years/ \ 16. Jeffrey S. Pierce, Andrew Forsberg, Matthew J. Conway, Seung Hong, Robert Zeleznik, and Mark R. Mine. 1997. Image plane interaction techniques in 3D immersive environments. In Proc. I3Dâ€™97, 39-43. \ 17. George Robertson, Maarten van Dantzich, Daniel Robbins, Mary Czerwinski, Ken Hinckley, Kirsten Risden, David Thiel, and Vadim Gorokhovsky. 2000. The task gallery: a 3D window manager. In Proc. CHIâ€™00, 494-501. \ 18. Marcos Serrano, Barrett Ens, Xing-Dong Yang, and Pourang Irani. 2015. Gluey: developing a head-worn display interface to unify the interaction experience in distributed display environments. In Proc. MobileHCIâ€™15, 161-170. \ 19. Marcos Serrano, Barrett Ens, Xing-Dong Yang, and Pourang Irani. 2015. Desktop-Gluey: augmenting desktop environments with wearable devices. In Adj. Proc. MobileHCIâ€™15, 1175-1178. \ 20. Mark Weiser. 1991. The computer for the 21st century. Scientific American, 265(3), 94-104. \",Augmented reality; virtual reality; 3D windows management.,H.5.2,uistpp0125-file1.docx,uistpp0125-file2.jpg,uistpp0125-file3.mp4,uistpp0125-file4.pdf,"In Projective Windows, the user wishing to adjust the scale and position of an AR window grabs the window, moves it, makes it bigger by bringing it closer, and projects it to the desired position.",The auxiliary material is a PDF file of the posterboard to be presented at the ACM UIST 2017.,"Some speculate that AR and VR might replace all screen-based devices in the future. In that future, what would the spatial OS look like? We present Projective Windows.",,Joon Hyub Lee,Sang-Gyun An,FormatComplete,,,,,,,Aug 11 13:47,
uistpp128,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp128,A,Smellization of Warnings against Overuse Power used to Promote Energy Saving Behavior,Tsutomu,Miyasato,miyasato@kit.ac.jp,uistpp0128-paper.pdf,2,letter,,,Tsutomu Miyasato,miyasato@kit.ac.jp,73429,Tsutomu,,Miyasato,miyasato@kit.ac.jp,Graduate School of Art and Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In Japan, the necessity of saving energy is rising due to the nuclear accident caused by the Great East Japan Earthquake that occurred on March 11, 2011. Reduction of energy usage is required due to rapid increases in electricity consumption due to the scorching summer heat in recent years. The common ways to provide information on energy consumption mainly occur through ""visualization"" of information. On the contrary, olfactory stimulation can be performed while working, and it is effective also when the degree of arousal is low. This study considers applications on the basis of the concept of ""smellization"" of information using olfactive stimulation. In this paper, we introduce the configuration and operation examples of a system developed for evoking public energy conservation behavior using smell.",miyasato@kit.ac.jp,"1. Kansai Electric Power Co., Inc. http://www.kepco.co.jp/energy_supply/supply/denkiyoh o/index.html \ 2. Kato R., Issues of the penetration of Home Energy Management System-Summarizing issues by the investigation of past demonstration projects-, Central Research Institute of Electric Power Industry Research Report Y12011 (2011) (in Japanese) http://criepi.denken.or.jp/jp/kenkikaku/report/detail/Y12 011.html# \ 3. Kaye, J.N, Symbolic Olfactory Display, Master thesis, Media Lab., MIT (2001). http://alumni.media.mit.edu/~jofish/thesis/symbolic_olf actory_display.html \ 4. Yagita Y., Iwafune Y., The Role of Visualization with Home Energy Audit to Promote Energy Conservation Behavior. Journal of Japan Society of Energy and Resources, 32, 4 (2011), 25-33 (in Japanese). \ 5. Yanagida Y., Projection-Based Olfactory Display with Nose Tracking, In Proc. IEEE VR2004, (2004), 43-50. Abbreviation for â€œparticulate matter, 2.5 micrometers or lessâ€ in the air, causing respiratory illness. \","Olfactory display; energy saving behavior; information presentation using smell; ""smellization""",H.5.m,uistpp0128-file1.doc,,,,,,"Since the smell to use is determined by trial and error, I would like advices about appropriate smells from smell experts.",,Tsutomu Miyasato,Tsutomu Miyasato,FormatComplete,,,,,,,Aug 9 3:15,
uistpp133,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp133,A,Automatically Visualizing Audio Travel Podcasts,Jihyeon,Lee,janel@cs.stanford.edu,uistpp0133-paper.pdf,3,letter,Arial-Bold,TimesNewRoman,"Jihyeon Lee, Mitchell Gordon, Maneesh Agrawala","janel@cs.stanford.edu, mgord@cs.stanford.edu, maneesh@cs.stanford.edu",73498,Jihyeon,,Lee,janel@cs.stanford.edu,Computer Science,Stanford University,Stanford,CA,United States,,,,,,38984,Mitchell,,Gordon,mgord@cs.stanford.edu,Computer Science,Stanford University,Stanford,CA,United States,,,,,,53076,Maneesh,,Agrawala,maneesh@cs.stanford.edu,Stanford University,Palo Alto,California,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Audio Podcasts have gained popularity because they are a compelling form of storytelling and are easy to consume. However, they are not as easy to produce since resources are invested in the research, recording, and editing process and the average length of an episode is over an hour. Some audio podcasts could benefit from visuals to increase engagement and learning, but manually curating them can be arduous and time-consuming. We introduce a tool for automatically visualizing audio podcasts, currently focused on the genre of travelogues. Our system works by first time-aligning the transcript of a given podcast, using NLP techniques to extract entities and track how interesting or relevant they are throughout the podcast, and then retrieving visual data appropriately to describe them, either through transitions on a map or professionally taken photographs with captions. By automatically creating a visual narrative to accompany a podcast, we hope our tool can provide listeners with a better sense of the podcast's topic.",janel@cs.stanford.edu,"WARNING: Reference 1 is very long. Please verify that it was extracted correctly. \ \ 1. Accessing Text Corpora and Lexical Resources. Natural Language Toolkit. Retrieved July 10, 2017 from http://www.nltk.org/book/ch02.html Figure 1. Example of two types of visualizations, an animation on a map upon a location change (Italy to California) and a transition to a professionally taken photo and accompanying caption (â€œAmalfi Coastâ€). The left side shows the transcript, where black text is successfully aligned, gray is uncertain, and pink is the current word as the podcast plays. \ 2. E Kobus Barnard, Pinar Duygulu, David Forsyth, Nando de Freitas, David M. Blei, and Michael I. Jordan. 2003. Matching Words and Pictures. Journal of Machine Learning Research 3: 1107â€“1135. http://doi.org/10.1162/153244303322533214 \ 3. Chris Christensen. 2016. Travel to Naples and the Amalfi Coast, Italy. Amateur Traveler Travel Podcast. Retrieved July 10, 2017 from http://europe.amateurtraveler.com/naples-amalfi-coastitaly/ \ 4. Ann Friedman. 2015. The economics of the podcast boom. Columbia Journalism Review. Retrieved July 10, 2017 from https://www.cjr.org/first_person/the_economics_of_the _podcast_boom.php \ 5. Gentle. Retrieved July 10, 2017 from https://github.com/lowerquality/gentle \ 6. Getty Images API. Retrieved July 10, 2017 from http://developers.gettyimages.com/en/ \ 7. Chris Giliberti. 2016. 6 Reasons Why Podcasting Is The Future Of Storytelling. Forbes. Retrieved July 10, 2017 from https://www.forbes.com/sites/under30network/2016/03 /31/6-reasons-why-podcasting-is-the-future-ofstorytelling/#285adb8c31fe \ 8. Daniel J. Lewis. 2017. Is There an Ideal Length for Podcast Episodes? The Audacity to Podcast. Retrieved July 10, 2017 from https://theaudacitytopodcast.com/is-there-an-ideallength-for-podcast-episodes-tap305/ \ 9. OpenLayers. Retrieved July 10, 2017 from https://openlayers.org/ \ 10. Podcast Consumer 2016. Edison Research. Retrieved July 10, 2017 from http://www.edisonresearch.com/the-podcastconsumer-2016/ \ 11. Podcast Consumer 2017. Edison Research. Retrieved July 10, 2017 from http://www.edisonresearch.com/the-podcastconsumer-2017/ \ 12. M. Pounsford. 2007. Using storytelling, conversation and coaching to engage: How to initiate meaningful conversations inside your organization. Strategic Communication Management 11: 22-28. \ 13. NLTK TextTiling. Retrieved July 10, 2017 from http://www.nltk.org/_modules/nltk/tokenize/texttiling.h tml \ 14. The power of visual storytelling. Curve. Retrieved July 10, 2017 from http://curve.gettyimages.com/article/thepower-of-visual-storytelling \ 15. John Patrick Pullen. 2014. You Asked: What Are Podcasts? Time. Retrieved July 10, 2017 from http://time.com/3608287/podcasts-serial-whatare/ \ 16. Stanford CoreNLP. Retrieved July 10, 2017 from https://github.com/stanfordnlp/CoreNLP \ 17. Ariel Stulberg. 2015. Podcasting is getting huge. Here's why. Vox. Retrieved July 10, 2017 from https://www.vox.com/business-andfinance/2015/12/15/10126144/serial-podcast-huge-hit \ 18. Rev Transcription Service. Retrieved July 10, 2017 from https://www.rev.com/transcription \ 19. CondÃ© Nast Traveler. 2017. Travelogue. Player FM. Retrieved July 12, 2017 from https://player.fm/series/travelogue \ 20. Tiffanie Wen. 2015. Inside the Podcast Brain: Why Do Audio Stories Captivate? The Atlantic. Retrieved July 10, 2017 from https://www.theatlantic.com/entertainment/archive/201 5/04/podcast-brain-why-do-audio-storiescaptivate/389925/ \","Podcasts, Travelogues, Visualization",H.5.m,uistpp0133-file1.docx,,,uistpp0133-file4.pdf,,"Poster for Automatically Visualizing Audio Travel Podcasts, by Jihyeon Janel Lee.",We present a tool that automatically creates a visual narrative to accompany a travel podcast to improve storytelling in travelogues without increasing the burden on podcast producers.,,Jihyeon Lee,Mitchell Gordon,FormatComplete,,,,,,,Aug 11 3:40,
uistpp134,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp134,A,Exploring of Simulating Passing through Feeling on the Wrist: Using thermal feedback,Wei,Peng,houi51899@gmail.com,uistpp0134-paper.pdf,3,letter,,,"Wei Peng, Roshan L Peiris, Kouta Minamizawa","houi51899@gmail.com, roshan@kmd.keio.ac.jp, kouta@kmd.keio.ac.jp",64267,Wei,,Peng,houi51899@gmail.com,Keio University Graduate School of Media Design,Graduate School,Yokohama,Kanagawa,Japan,,,,,,53252,Roshan,L,Peiris,roshan@kmd.keio.ac.jp,Keio University Graduate School of Media Design,Graduate School,Yokohama,Kanagawa,Japan,,,,,,62190,Kouta,,Minamizawa,kouta@kmd.keio.ac.jp,Graduate School,Yokohama,Kanagawa,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Wearable devices combining with VR/AR technology become a research hotspot these years. In some research, tactile displays are put on the skin and synchronized with VR/AR environment. Researchers try to use these display to simulate varied embodied feeling to enhance the immersion in the VR/AR environment. In the field of game entertainment, based on the scenario, sometimes the feeling of passing through the body need to be presented to the user. However this is physically impossible. Thus we make a exploration attempting to simulate this feeling by thermal feedback. Here we use two thermal modules bonding on the two side of the wrist( inside and outside). When we actuate two modules sequentially, user would perceive the stimuli and interpret this into a feeling of passing though. In the paper, we will introduce the interface and describe the experiment to determine the principle for thermo-tactile illusion of passing through.",houi51899@gmail.com,"1. Israr, A., and Poupyrev, I. Tactile brush: Drawing on skin with a tactile grid display. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI â€™11, ACM (New York, NY, USA, 2011), 2019â€“2028. \ 2. Peiris, R. L., Peng, W., Chen, Z., Chan, L., and Minamizawa, K. Thermovr: Exploring integrated thermal haptic feedback with head mounted displays. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, CHI â€™17, ACM (New York, NY, USA, 2017), 5452â€“5456. \ 3. Watanabe, J., Fukuzawa, Y., Kajimoto, H., and Ando, H. Presentation of feeling when pierced using apparent movement passing through the body. Journal of Information Processing 49, 10 (oct 2008), 3542â€“3545. \",thermal haptics; wearable device; haptic illusion; Embodied Media,H.5.2,uistpp0134-file1.zip,,,uistpp0134-file4.pdf,,"The poster file here is a regular PDF file. In the poster we have a brief introduction of the paper including the setup, evaluation, experiment result and so on.","An attempt to simulate a ""passing through body"" feeling which is impractical in real life but common in game scenarios.",,Wei Peng,Roshan Peiris,FormatComplete,JPMJAC1404,JST ACCEL Embodied Media Project,,,,,Aug 11 8:25,
uistpp135,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp135,A,Grouping Applications Using Geometrical Information of Applications on Tabletop Systems,Jonggyu,Park,jonggyu@skku.edu,uistpp0135-paper.pdf,2,letter,,,"Jonggyu Park, Inhyeok Kim, Young Ik Eom","jonggyu@skku.edu, kkojiband@skku.edu, yieom@skku.edu",73432,Jonggyu,,Park,jonggyu@skku.edu,College of Software/Sungkyunkwan University/Distributed Computing Lab.,Sungkyunkwan University,Suwon,,"Korea, Republic of",,,,,,43409,Inhyeok,,Kim,kkojiband@skku.edu,College of Software/Sungkyunkwan University/Distributed Computing Lab.,Sungkyunkwan University,Suwon,,"Korea, Republic of",,,,,,43411,Young Ik,,Eom,yieom@skku.edu,Sungkyunkwan University,Suwon,,"Korea, Republic of",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this paper, we propose a grouping scheme that classifies applications into groups for individual users by utilizing their geometrical information on a tabletop system. The proposed scheme investigates the geometrical information of each application, such as its position on the display and its rotational information, and then groups the applications of each individual user by utilizing a classifier with the geometrical information. We evaluate the proposed scheme with lab experiments, and the results show that, on average, 95.6% of applications are well classified into their users.",jonggyu@skku.edu,"1. Evans, A. C., Davis, K., Fogarty, J., and Wobbrock, J. O. Group Touch : Distinguishing Tabletop Users in Group Settings via Statistical Modeling of Touch Pairs. In Proc. CHI 2017, ACM (2017), 35â€“47. \ 2. Kim, J., Kim, I., Kim, T., and Eom, Y. I. NEMOSHELL Demo: Windowing System for Concurrent Applications on Multi-user Interactive Surfaces. In Proc. ITS 2014, ACM (2014), 451â€“454. \ 3. Padraig, C., and Delany, S. J. k-Nearest Neighbour Classiï¬ers. Technical Report UCD-CSI-2007-4. University College Dublin, 2007. \ 4. Richter, S. R., Holz, C., and Baudisch, P. Bootstrapper: Recognizing Tabletop Users by Their Shoes. In Proc. CHI 2012, ACM (2012), 1249â€“1252. \ 5. Russell, K., Sheelagh, C., Stacey D., S., and Saul, G. How People Use Orientation on Tables: Comprehension, Coordination and Communication. In Proc. GROUP 2003, ACM (2003), 369â€“378. \",Multi-user environment; tabletop systems; application classification.,H.5.3,uistpp0135-file1.zip,,,uistpp0135-file4.pdf,,"The submitted file is PDF extension. So, the user should have a PDF reader software to view the file. The file is a poster file which describes my research work in this paper. It will be printed and presented in the poster session of the conference.","On tabletop systems, our scheme analyzes the geometrical information of applications (their position on the display and rotational information) and groups the applications by their users by using the information.",,Jonggyu Park,Jonggyu Park,FormatComplete,IITP-2015-0-00284,"MSIT and IITP, Korea",,,,,Aug 9 21:25,
uistpp136,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp136,A,HapticDrone - An Encountered-Type Kinesthetic Haptic Interface with Controllable Force Feedback: Initial Example for 1D Haptic Feedback,Muhammad,Abdullah,mabdullah.nust@gmail.com,uistpp0136-paper.pdf,3,letter,,,"Muhammad Abdullah, Minji Kim, Waseem Hassan, Yoshihiro Kuroda, Seokhee Jeon","mabdullah.nust@gmail.com, mj.kim1102@gmail.com, Waseem.hassan119@gmail.com, ykuroda@bpe.es.osaka-u.ac.jp, jeon@khu.ac.kr",71823,Muhammad,,Abdullah,mabdullah.nust@gmail.com,Computer Engineering,KYUNG HEE UNIVERSITY,Yongin-si,Gyeonggi-do,"Korea, Republic of",,,,,,71828,Minji,,Kim,mj.kim1102@gmail.com,Computer Engineering / Haptics and Virtual Reality Lab,KYUNG HEE UNIVERSITY,Yongin-si,Gyeonggi-do,"Korea, Republic of",,,,,,71829,Waseem,,Hassan,Waseem.hassan119@gmail.com,Kyung hee universiry,Yongin-si,Gyeonggi-do,South korea,,,,,,,61724,Yoshihiro,,Kuroda,ykuroda@bpe.es.osaka-u.ac.jp,Graduate School of Engineering Science,Osaka University,Osaka,,Japan,,,,,,18839,Seokhee,,Jeon,jeon@khu.ac.kr,Computer Engineering,KYUNG HEE UNIVERSITY,Yongin-si,Gyeonggi-do,"Korea, Republic of",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present HapticDrone, a concept to generate controllable and comparable force feedback for direct haptic interaction with a drone. As a proof-of-concept study this paper focuses on creating haptic feedback only in 1D direction. To this end, an encountered-type, safe and un-tethered haptic display is implemented. An overview of the system and details on how to control the force output of drones is provided. Our current prototype generates forces up to 1.53 N upwards and 2.97 N downwards. This concept serves as a first step towards introducing drones as mainstream haptic devices.",mabdullah.nust@gmail.com,"1. M. Bouzit, G. Popescu, G. Burdea, and R. Boian. 2002. The Rutgers Master II-ND force feedback glove. In Proceedings 10th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems. HAPTICS 2002. 145â€“152. DOI: http://dx.doi.org/10.1109/HAPTIC.2002.998952 \ 2. Tom Carter, Sue Ann Seah, Benjamin Long, Bruce Drinkwater, and Sriram Subramanian. 2013. UltraHaptics: Multi-point Mid-air Haptic Feedback for Touch Surfaces. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST â€™13). ACM, New York, NY, USA, 505â€“514. DOI: http://dx.doi.org/10.1145/2501988.2502018 \ 3. Geomagic. 2017. The Geomagic Touch (formerly Sensable Phantom Omni). (2017). http://www.geomagic.com/en/products/phantom-omni/overview.html. \ 4. Antonio Gomes, Calvin Rubens, Sean Braley, and Roel Vertegaal. 2016. BitDrones: Towards Using 3D Nanocopter Displays As Interactive Self-Levitating Programmable Matter. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI â€™16). ACM, New York, NY, USA, 770â€“780. DOI: http://dx.doi.org/10.1145/2858036.2858519 \ 5. Pascal Knierim, Thomas Kosch, Valentin Schwind, Markus Funk, Francisco Kiss, Stefan Schneegass, and Niels Henze. 2017. Tactile Drones - Providing Immersive Tactile Feedback in Virtual Reality Through Quadcopters. In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA â€™17). ACM, New York, NY, USA, 433â€“436. DOI: http://dx.doi.org/10.1145/3027063.3050426 \ 6. Thomas H Massie, J Kenneth Salisbury, and others. 1994. The phantom haptic interface: A device for probing virtual objects. In Proceedings of the ASME winter annual meeting, symposium on haptic interfaces for virtual environment and teleoperator systems, Vol. 55. Citeseer, 295â€“300. \ 7. Rajinder Sodhi, Ivan Poupyrev, Matthew Glisson, and Ali Israr. 2013. AIREAL: Interactive Tactile Experiences in Free Air. ACM Trans. Graph. 32, 4, Article 134 (July 2013), 10 pages. DOI: http://dx.doi.org/10.1145/2461912.2462007 \ 8. Susumu Tachi, Taro Maeda, Ryokichi Hirata, and Hiroshi Hoshino. 1994. A construction method of virtual haptic space. In proceedings of the 4th International Conference on Artiï¬cial Reality and Tele-Existence (ICATâ€™94). 131â€“138. \ 9. Kotaro Yamaguchi, Ginga Kato, Yoshihiro Kuroda, Kiyoshi Kiyokawa, and Haruo Takemura. 2016. A Non-grounded and Encountered-type Haptic Display Using a Drone. In Proceedings of the 2016 Symposium on Spatial User Interaction (SUI â€™16). ACM, New York, NY, USA, 43â€“46. DOI: http://dx.doi.org/10.1145/2983310.2985746 \","""Haptics; Virtual Reality; Kinesthetic; Encountered; Drone""","""H.5.2""",uistpp0136-file1.zip,uistpp0136-file2.jpg,uistpp0136-file3.mp4,uistpp0136-file4.zip,The user experiencing stiffness of a virtual object by interacting with the drone. The drone renders force in the upward direction.,Poster design,Our research focuses on expanding the novel field of drone haptics. We are creating controllable force feedback in midair from drones that can be compared with mainstream haptic devices.,,Muhammad Abdullah,Seokhee Jeon,FormatComplete,,,,,,,Aug 11 7:34,
uistpp138,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp138,A,Walk-In Music : Walking experience with synchronized music and its effect of pseudo-gravity,Haruto,Murata,murata-haru@kmd.keio.ac.jp,uistpp0138-paper.pdf,3,letter,,,"Haruto Murata, Youssef Bouzarte, Junichi Kanebako, Kouta Minamizawa","murata-haru@kmd.keio.ac.jp, bouzarte@kmd.keio.ac.jp, kanebako@kmd.keio.ac.jp, kouta@kmd.keio.ac.jp",73532,Haruto,,Murata,murata-haru@kmd.keio.ac.jp,Keio Graduate School of Media Design,Graduate school,Yokohama,Kanagawa,Japan,,,,,,73638,Youssef,,Bouzarte,bouzarte@kmd.keio.ac.jp,"Kreio Graduate School of Media Design, Graduate School, Yokohama, Kanagawa, Japan",Graduate school,Yokohama,Kanagawa,Japan,,,,,,73640,Junichi,,Kanebako,kanebako@kmd.keio.ac.jp,Graduate school,Yokohama,Kanagawa,Japan,,,,,,,62190,Kouta,,Minamizawa,kouta@kmd.keio.ac.jp,Keio University Graduate School of Media Design,Graduate School,Yokohama,Kanagawa,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"""Walk-In Music"" is a system that provides a new walking experience through synchronized music and pseudo-gravity. This system synchronizes each step with the music being listened to and creates a feeling of generating music through walking. It creates a Walk-In state where music and walking are consistent at all times. In this state, when changing the speed of music, the pedestrian may feel pseudo-gravity based on pseudo-haptics. Our results indicate that by changing the speed of music during the Walk-In state, the walking speed became faster and slower. We call this a Walk-Shift. This demonstrated the possibility of controlling personal walking by music. Walk-In Music has created a pleasant experience by music, and proposed a new relationship between people and music that leads to behavior changes. \",murata-haru@kmd.keio.ac.jp,"1. Anatole Lecuyer, Sabine Coquillart, Abderrahmane Kheddar. 2000. Pseudo-Haptic Feedback : Can Isometric Input Devices Simulate Force Feedback?. In Processings of the IEEE Virtual Reality 2000 Conference. IEEE. 83-90 \ 2. Nuria Oliver, Fernando Flores-Mangas. 2006. MPTrain: AMobile, Music and Physiology-Based Personal Trainer. In Proceeding of the 8th conference on Human-computer interaction with mobile devices and services. ACM. 21-28 \ 3. Ana Tajadura-Jimenez, Maria Basia, Ophelia Deroy, Merle Fairhurst, Nicolai Marquardt, Nadia BianchiBerthouze. 2015. As Light as your Footsteps: Altering Walking Sounds to Change Perceived Body Weight, Emotional State and Gait. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM. 2943-2952. \ 4. Fritz Menzer, Anna Brooks, PÃ¤r Halje, Christof Faller, Martin Vetterli, and Olaf Blanke. 2010. Feeling in control of your footsteps: conscious gait monitoring and the auditory consequences of footsteps. In Proceeding of Cognitive Neuroscience. CNS. 184-192. \",Embodied interaction; Generate music; Induce,H.5.5; H.1.2,uistpp0138-file1.doc,uistpp0138-file2.jpg,,uistpp0138-file4.pdf,Walk-In music generate music by gait and synchronize walking with music.,Walk-In Music poster data(pdf),Walk-In Music system provides a unique experience of induced behavior through music. This system make two states. Users can feel comfortable in Walk-In state and pseudo-gravity in Walk-Shift state.,,Haruto Murata,Youssef Bouzarte,FormatComplete,26700018,JSPS KAKENHI,,,,,Aug 11 19:59,
uistpp144,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp144,A,Evorus: A Crowd-powered Conversational Assistant That Automates Itself Over Time,Ting-Hao,Huang,tinghaoh@andrew.cmu.edu,uistpp0144-paper.pdf,3,letter,,,"Ting-Hao K Huang, Joseph Chee Chang, Saiganesh Swaminathan, Jeffrey P Bigham","tinghaoh@andrew.cmu.edu, josephcc@cs.cmu.edu, saiganes@cs.cmu.edu, jbigham@cs.cmu.edu",44450,Ting-Hao,K.,Huang,tinghaoh@andrew.cmu.edu,Language Technologies Institute,Carnegie Mellon University,Pittsburgh,Pennsylvania,United States,,,,,,44356,Joseph Chee,,Chang,josephcc@cs.cmu.edu,Human-Computer Interaction Institute,Carnegie Mellon University,Pittsburgh,Pennsylvania,United States,,,,,,37459,Saiganesh,,Swaminathan,saiganes@cs.cmu.edu,Carnegie Mellon University,Pittsburgh,Pennsylvania,United States,,,,,,,8964,Jeffrey,P,Bigham,jbigham@cs.cmu.edu,Human-Computer Interaction Institute,Carnegie Mellon University,Pittsburgh,PA,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Crowd-powered conversational assistants have found to be more robust than automated systems, but do so at the cost of higher response latency and monetary costs. One promising direction is to combined the two approaches for high quality and low cost solutions. However, traditional offline approaches of building automated systems with the crowd requires first collecting training data from the crowd, and then training a model before an online system can be launched. In this paper, we introduce Evorus, a crowd-powered conversational assistant with online-learning capability that automate itself over time. Evorus expands a previous crowd-powered conversation system by reducing its reliance on the crowd over time while maintaining the robustness and reliability of human intelligence, by (i) allowing new chatbots to be added to help contribute possible answers, (ii) learning to reuse past responses to similar queries over time, and (iii) learning to reduce the amount of crowd oversight necessary to retain quality. Our deployment study with 28 users show that automated responses were chosen 12.84% of the time, and voting cost was reduced by 6%. Evorus introduced a new framework for constructing crowd-powered conversation systems that can gradually automate themselves using machine learning, a concept that we believe can be generalize to other types of crowd-powered systems for future research.",tinghaoh@cs.cmu.edu,"1. Azaria, A., and Hong, J. Recommender system with personality. In Proceedings of the 10th ACM conference on Recommender systems, ACM (2016). \ 2. Carpenter, R. Cleverbot, 2006. [Online; accessed 08-March-2017]. \ 3. Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J. Liblinear - a library for large linear classiï¬cation, 2008. The Weka classiï¬er works with version 1.33 of LIBLINEAR. \ 4. Huang, T.-H. K., Lasecki, W. S., Azaria, A., and Bigham, J. P. â€œis there anything else i can help you with?â€: Challenges in deploying an on-demand crowd-powered conversational agent. In Proceedings of AAAI Conference on Human Computation and Crowdsourcing 2016 (HCOMP 2016), AAAI (2016). \ 5. Lasecki, W. S., Wesley, R., Nichols, J., Kulkarni, A., Allen, J. F., and Bigham, J. P. Chorus: A crowd-powered conversational assistant. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology, UIST â€™13, ACM (New York, NY, USA, 2013), 151â€“162. \",crowdsourcing; crowd-powered system; conversational agent,H.5.m,uistpp0144-file1.zip,uistpp0144-file2.jpg,,uistpp0144-file4.pdf,The core concept of Evorus is to have crowd workers work with automated virtual agents on the fly to hold sophisticated conversations with users.,Poster design of our UIST'17 submission.,Evorus expands a previous crowd-powered conversation system by reducing its reliance on the crowd over time while maintaining the robustness and reliability of human intelligence.,,Ting-Hao Huang,Jeffrey Bigham,FormatComplete,InMind Project,Yahoo!,,,,,Aug 9 17:07,
uistpp145,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp145,A,GraspForm: Shape and Hardness Rendering on Handheld Device toward Universal Interface for 3D Haptic TV,Takuya,Handa,handa.t-es@nhk.or.jp,uistpp0145-paper.pdf,3,letter,,,"Takuya Handa, Makiko Azuma, Toshihiro Shimizu, Satoru Kondo","handa.t-es@nhk.or.jp, azuma.m-ia@nhk.or.jp, shimizu.t-hy@nhk.or.jp, kondou.s-ey@nhk.or.jp",73564,Takuya,,Handa,handa.t-es@nhk.or.jp,Human Interface Research Division,NHK Science & Technology Research Laboratories,Setagaya-ku,Tokyo,Japan,,,,,,73565,Makiko,,Azuma,azuma.m-ia@nhk.or.jp,Human Interface Research Division,NHK Science & Technology Research Laboratories,Setagaya-ku,Tokyo,Japan,,,,,,73566,Toshihiro,,Shimizu,shimizu.t-hy@nhk.or.jp,NHK Science & Technology Research Laboratories,Setagaya-ku,Tokyo,Japan,,,,,,,73567,Satoru,,Kondo,kondou.s-ey@nhk.or.jp,Human Interface Research Division,NHK Science & Technology Research Laboratories,Setagaya-ku,Tokyo,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The main goal of our research is to develop a haptic display that conveys the shapes, hardness, and textures of objects displayed on near-future 3D haptic TVs. We present a novel handheld device, GraspForm. This device renders the surface shapes and hardness of a virtual object that is represented in an absolute position in real space. GraspForm has a 2Ã—2 matrix of actuated hemispheres for one fingertip, two actuated pads for the palm, and a force feedback actuator for the thumb. Our first experimental results showed that eight participants succeeded in recognizing the side geometries of a cylinder and a square prism regardless of the availability of visual information.",handa.t-es@nhk.or.jp,"1. Iwata, H., Yano, H., Nakaizumi, F., and Kawamura, R. 2001. Project feelex: Adding haptic surface to graphics. In Proc. of ACM SIGGRAPH â€™01. 469-476. \ 2. Nakagaki, K., Vink, L., Counts, J., Windham, D. Leithinger, D., Follmer, S., Ishii, H., Materiable: Rendering Dynamic Material Properties in Response to Direct Physical Touch with Shape Changing Interfaces, Proc. of the CHI Conference, pp.2764-2772, 2016. \ 3. Massie, T. and Salisbury, J.K. 1994. The PAHToM Haptic Interface: A Device for Probing Virtual Objects. Dynamics and Control 1994. In Proc. of the ASME Winter Annual Meeting '94. \ 4. CyberGrasp Glove, CyberGlove Systems Inc. http://www.cyberglovesystems.com/cybergrasp/. Last accessed July 4, 2017. \ 5. Handa, T., Sakai, T., Shimizu, T., and Shinoda., H. Recognition of Three-dimensional Geometry using Multipoint Force feedback on a Fingertip, IEICE Technical Report, HIP, Vol. 112, No. 483, pp. 13-16, 2012 (in Japanese) \ 6. Handa, T., Murase, K., Azuma, M., Shimizu, T., Kondo, S., and Shinoda, H. A Haptic Three-Dimensional Shape Display with Three Fingers Grasping. Proc. of IEEE VR 2017. \ 7. Benko, H., Holz, C., Sinclair, M., Ofek, E. NormalTouch and TextureTouch: High-fidelity 3D Haptic Shape Rendering on Handheld Virtual Reality Controllers. Proc. of ACM UIST '16. \ 8. HAPTIC Trigger, ALPS ELECTRIC CO., LTD. http://www.alps.com/gps_e/haptic.html. Last accessed July 4, 2017. HAPTIC is registered as a trademark in Japan, China and the European Union (JP4619342/JP5471286, CN4730821/CN4730822/CN4730823, CTM4399606), and is used in commerce as a trademark in the United States, being enforceable under common law. \",Haptics; Tactile Display; Virtual Reality; Universal Design,"H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial, and Virtual Realities; H.5.2 [User Interfaces]: Haptic I/O",uistpp0145-file1.docx,uistpp0145-file2.jpg,uistpp0145-file3.mp4,uistpp0145-file4.pdf,"This device, GraspForm renders the surface shape and hardness of a virtual object that is represented in an absolute position in real space.",This file is a poster board PDF.,"We developed a novel haptic interface that enables users to feel the size, shape, and hardness of 3D objects represented in absolute positions in real space without visual cues.",,Takuya Handa,Makiko Azuma,FormatComplete,,,,,,,Aug 10 2:05,
uistpp152,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp152,A,A Thermally Enhanced Weather Checking System in VR,Zikun,Chen,zkzk.chen@gmail.com,uistpp0152-paper.pdf,3,letter,,,"Zikun Chen, Roshan L Peiris, Kouta Minamizawa","zkzk.chen@gmail.com, roshan@kmd.keio.ac.jp, kouta@kmd.keio.ac.jp",60148,Zikun,,Chen,zkzk.chen@gmail.com,Graduate School of Media Design,Keio University,Yokohama City,Kanagawa,Japan,,,,,,53252,Roshan,L,Peiris,roshan@kmd.keio.ac.jp,Graduate School of Media Design,Keio University,Yokohama City,Kanagawa,Japan,,,,,,62190,Kouta,,Minamizawa,kouta@kmd.keio.ac.jp,Keio University,Yokohama City,Kanagawa,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this project, by combining thermal feedback with Virtual Reality (VR) and utilizing thermal stimuli to present temperature data of weather, we attempted to provide a multi-sensory experience for enhancing users' perception of environment in virtual space. \ By integrating thermal modules with the current VR head mounted display to provide thermal feedback directly on the face, and by setting thermal stimulus to provide similar feeling towards real air temperature, we developed an application in which users are able to ""feel"" the weather in VR environment. \ An user experiment was also conducted to evaluate our design, according to which we verified that thermal feedback can improve users' experience in perceiving environment, and this research also provided a new approach for setting thermal feedback for presenting environmental information in virtual space.",zkzk.chen@gmail.com,"1. Cassinelli, A., Reynolds, C., and Ishikawa, M. Augmenting spatial awareness with haptic radar. In Wearable Computers, 2006 10th IEEE International Symposium on, IEEE (2006), 61â€“64. \ 2. Chen, Z., Peiris, R. L., and Minamizawa, K. A thermal pattern design for providing dynamic thermal feedback on the face with head mounted displays. In Proceedings of the Tenth International Conference on Tangible, Embedded, and Embodied Interaction, ACM (2017), 381â€“388. \ 3. de Jesus Oliveira, V. A., Nedel, L., Maciel, A., and Brayda, L. Localized magniï¬cation in vibrotactile hmds for accurate spatial awareness. In International Conference on Human Haptic Sensing and Touch Enabled Computer Applications, Springer (2016), 55â€“64. \ 4. Dinh, H. Q., Walker, N., Hodges, L. F., Song, C., and Kobayashi, A. Evaluating the importance of multi-sensory input on memory and the sense of presence in virtual environments. In Virtual Reality, 1999. Proceedings., IEEE, IEEE (1999), 222â€“228. \ 5. Ho, C., Tan, H. Z., and Spence, C. Using spatial vibrotactile cues to direct visual attention in driving scenes. Transportation Research Part F: Trafï¬c Psychology and Behaviour 8, 6 (2005), 397â€“412. \ 6. Jones, L. A., and Ho, H.-N. Warm or cool, large or small? the challenge of thermal displays. IEEE Transactions on Haptics 1, 1 (2008), 53â€“70. \ 7. Mann, S., Huang, J., Janzen, R., Lo, R., Rampersad, V., Chen, A., and Doha, T. Blind navigation with a wearable range camera and vibrotactile helmet. In Proceedings of the 19th ACM international conference on Multimedia, ACM (2011), 1325â€“1328. \ 8. Peiris, R. L., Peng, W., Chen, Z., Chan, L., and Minamizawa, K. Thermovr: Exploring integrated thermal haptic feedback with head mounted displays. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, ACM (2017), 5452â€“5456. \ 9. Sato, K., and Maeno, T. Presentation of sudden temperature change using spatially divided warm and cool stimuli. Haptics: Perception, Devices, Mobility, and Communication (2012), 457â€“468. \",Haptic; Thermal; Environmental information; Virtual Reality; VR,H.5.2,uistpp0152-file1.zip,,,uistpp0152-file4.pdf,,This material includes the poster which summarize the core information of the submission A Thermally Enhanced Weather Checking System in VR.,This project provides a new approach to set thermal feedback and shows the potential of utilizing thermal feedback to present information and enhance users perception of the environment in VR,,Zikun Chen,Roshan Peiris,FormatComplete,JPMJAC1404,JST ACCEL Embodied Media project,,,,,Aug 10 5:51,
uistpp153,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp153,A,Jaguar: Indoor Navigation System for Organizations,Brandon,Dalton,daltonb@usc.edu,uistpp0153-paper.pdf,3,letter,,,"Brandon Dalton, Chen-Hsiang Yu, Mira Yun","daltonb@usc.edu, yuj6@wit.edu, yunm@wit.edu",73520,Brandon,,Dalton,daltonb@usc.edu,Viterbi School of Engineering,Wentworth Institute of Technology,Boston,MA,U.S.A.,,,,,,10994,Chen-Hsiang,,Yu,yuj6@wit.edu,Computer Science and Networking,Wentworth Institute of Technology,Boston,MA,U.S.A.,,,,,,62097,Mira,,Yun,yunm@wit.edu,Wentworth Institute of Technology,Boston,Massachusetts,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Global Positioning System (GPS) technology is widely used for outdoor navigation, but it is still challenging to apply this technology to a mid-scale or indoor environment. Using GPS in this way raises issues, such as reliability, deployment cost, and maintenance. Recently, companies like Google have begun to provide accurate indoor mapping. However, current implementations rely on both Wi-Fi and cellular technologies which have a hard time identifying the user's exact location in an indoor environment. There are two research questions in this paper: (1) How do we design a flexible and cost efficient indoor navigation system for organizations? (2) How to find an optimized path in a mid-scale/local environment. Here we propose Jaguar, which is a novel navigation system that utilizes a customized KML map with NFC technologies to address above questions. Our system includes an Android mobile application, a web-based map authoring tool and an implementation of a Cartesian plane based path finding algorithm. The initial testing of the system shows successful adaptation for a school campus environment.",daltonb@usc.edu,"WARNING: Reference 12 starts with a non-alphanumeric character. Please check it. \ \ 1. Vann, S. (2014). Indoor Google Maps. \ 2. U.S. Environmental Protection Agency. 1989. Report to Congress on indoor air quality: Volume 2. EPA/400/189/001C. Washington, DC.NFC indoor navigation: NFC Internal: An Indoor Navigation System \ 3. B. Ozdenizci, V. Coskun and K. Ok, ""NFC indoor navigation: NFC Internal: An Indoor Navigation System"", in sensors, Istanbul, 2015, pp. 7571-7595. \ 4. Chen-Hsiang Yu and Mira Yun. ""A Wizard for School Opening Events,"" The 15th Annual Hawaii International Conference on Education (HICE), Jan. 2017. \ 5. Nic Bonzani, Edward Kang, Chen-Hsiang Yu, and Mira Yun. ""Smart Guide: Mid-Scale NFC Navigation System,"" IEEE MIT Undergraduate Research Technology Conference (IEEE URTC 2015), Nov. 2015. \ 6. Near Field Communication (NFC) Technology and Measurements - White Paper, Rohde & Schwarz, June 2011. \ 7. Y. Dinitz and R. Itzhak, ""Hybrid Bellman-Ford-Dijkstra Algorithm"", in Ben-Gurion University of the Negev. \ 8. Dimitris Sacharidis and Panagiotis Bouros. 2013. Routing directions: keeping it fast and simple. In Proceedings of the 21st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (SIGSPATIAL'13). ACM, New York, NY, USA, 164-173. \ 9. John Hershberger, Matthew Maxel, and Subhash Suri. 2007. Finding the k shortest simple paths: A new algorithm and its implementation. ACM Trans. Algorithms 3, 4, Article 45 (November 2007). \ 10. Mikkel Thorup. 1999. Undirected single-source shortest paths with positive integer weights in linear time. J. ACM 46, 3 (May 1999), 362-394. \ 11. Stacey D. Lyle and Nathan Eby. 2010. Conversion of cadastral data to KML file type for use in Google Earth and Google Maps for Mobile as a land information system. In Proceedings of the 1st International Conference and Exhibition on Computing for Geospatial Research & Application (COM.Geo '10). ACM, New York, NY, USA, Article 30, 4 pages. \ 12. ""KML Reference Google Developers"", Google Developers, 2017. \",Mid-scale Navigation; Near-field Communication (NFC),H5.2,uistpp0153-file1.docx,,,uistpp0153-file4.pdf,,"This poster (in PDF format) supports the paper presented with the research: ""Jaguar: Indoor Navigation System for Organizations"".","Navigating indoor spaces has become a necessity in modern life. The Jaguar system seeks to provide an innovative, low cost and efficient way to navigate indoor and mid-scale environments.",,Brandon Dalton,Chen-Hsiang Yu,FormatComplete,,,,,,,Aug 12 14:21,
uistpp154,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp154,A,A Digital Pen and Paper Email System for Older Adults,Taciana,Falcao,tacianapontual@gmail.com,uistpp0154-paper.pdf,3,letter,,,"Taciana Pontual FalcÃ£o, Xiaofeng Yong, Elisabeth Sulmont, Robert Douglas Ferguson, Karyn Moffatt","tacianapontual@gmail.com, yongxiaofeng@gmail.com, elisabeth.sulmont@mail.mcgill.ca, robert.douglas.ferguson@mail.mcgill.ca, karyn.moffatt@mcgill.ca",13468,Taciana,,Pontual FalcÃ£o,tacianapontual@gmail.com,School of Information Studies,McGill University,Montreal,Quebec,Canada,,,,,,33135,Xiaofeng,,Yong,yongxiaofeng@gmail.com,School of Information Studies,McGill University,Montreal,Quebec,Canada,,,,,,73654,Elisabeth,,Sulmont,elisabeth.sulmont@mail.mcgill.ca,McGill University,Montreal,Quebec,Canada,,,,,,,36170,Robert,Douglas,Ferguson,robert.douglas.ferguson@mail.mcgill.ca,School of Information,McGill,Montreal,Quebec,Canada,,,,,,2986,Karyn,,Moffatt,karyn.moffatt@mcgill.ca,School of Information Studies,McGill University,Montreal,Quebec,Canada,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"As communication technologies continue to rapidly evolve, older adults face challenges to access systems and devices, which may increase their social isolation. Our project investigates the design of a digital pen and paper-based communication system that allows users to connect to their family and friends' e-mail inboxes. Given the unique needs of older adults, we opted for a participatory design approach, prototyping the system with 22 older adults through a series of design workshops in two locations. Four individuals used our resulting system over a period of two weeks. Based on their feedback and a review of design workshops, we are currently in the process of refining our interface and preparing for a larger deployment study.",tacianapontual@gmail.com,"1. Adams, K. B., Leibbrandt, S., Moon, H. A critical review of the literature on social and leisure activity and wellbeing in later life. Ageing & Society 31 (2011), 683â€“ 712. \ 2. Baecker, R., Sellen, K., Crosskey, S., Boscart, V., Neves, B. B. Technology to Reduce Social Isolation and Loneliness. In Proc. ASSETSâ€™14, ACM Press (2014), 27 â€“ 34. \ 3. Brewer, R. Garcia, R. C., Schwaba, D. G., Piper, A. M. Exploring Traditional Phones as an E-Mail Interface for Older Adults. ACM Trans. on Accessible Computing 8, 2 (2016). \ 4. Cloutier-Fisher, D., Kobayashi, K., Smith, A. The subjective dimension of social isolation: A qualitative investigation of older adultsâ€™ experiences in small social support networks. J. of Ageing Studies 25, 4 (2011), 407-414. \ 5. Klamka, K., Dachselt, R. IllumiPaper: Illuminated Interactive Paper. In Proc. CHI â€™17, ACM Press (2017), 5605â€“5618. \ 6. Lindley, S., Harper, R., Sellen, A. Desiring to be in touch in a changing communications landscape: attitudes of older adults. In Proc. CHI'09, ACM Press (2009), 1693 - 1702. \ 7. Livescribe wifi smartpen basics. http://www.livescribe.com/enca/media/pdf/sky_wifi_smartpen_basics_guide.pdf \ 8. Newell, A., Arnott, J., Carmichael, A., Morgan, M. Methodologies for Involving Older Adults in the Design Process. In Proc. UAHCI 2007 (2007). \ 9. Sixsmith, A., Gutman, G. (Eds.) Technologies for Active Aging. International perspectives on aging, volume 9. Springer, New York, NY, USA (2013). \",Older adults; Participatory design; Communication technologies; Digital pen; Pen and paper interaction,H.5.2,uistpp0154-file1.docx,,,uistpp0154-file4.pdf,,We provide a PDF of the posterboard to be presented at the conference.,How can we help older adults communicate with family and friends in the current plethora of complex devices and networks? We propose a solution based on paper for email communication. \,,Taciana Pontual FalcÃ£o,Elisabeth Sulmont,FormatComplete,,AGE-WELL,NSERC,,Canada Research Chairs Program,,Aug 10 16:51,
uistpp157,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp157,A,Raising the Heat : Electrical Muscle Stimulation for Simulated Heat Withdrawal Response,Pascal,Fortin,pe.fortin@mail.mcgill.ca,uistpp0157-paper.pdf,3,letter,,,"Pascal E Fortin, Jeffrey R Blum, Jeremy R Cooperstock","pefortin@cim.mcgill.ca, jeffbl@cim.mcgill.ca, jer@cim.mcgill.ca",72834,Pascal,E. ,Fortin,pefortin@cim.mcgill.ca,,McGill University,Montreal,Quebec,Canada,,,,,,29496,Jeffrey,R,Blum,jeffbl@cim.mcgill.ca,Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT),McGill University,Montreal,Quebec,Canada,,,,,,4933,Jeremy,R,Cooperstock,jer@cim.mcgill.ca,McGill University,Montreal,Quebec,Canada,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Virtual Reality (VR) has numerous mechanisms for making a virtual scene more compellingly real. \ Most effort has been focused on visual and auditory techniques for immersive environments, although some commercial systems now include relatively crude haptic effects through handheld controllers or haptic suits. \ We present results from a pilot experiment demonstrating the use of Electrical Muscle Stimulation (EMS) to trick participants into thinking a surface is dangerously hot even though it is below 50C. \ This is accomplished by inducing an artificial heat withdrawal reflex by contracting the participant's bicep shortly after contact with the virtual hot surface. \ Although the effects of multiple experimental confounds need to be quantified in future work, results so far suggest that EMS could potentially be used to modify temperature perception in VR and AR contexts. \ Such an illusion has applications for VR gaming as well as emergency response and workplace training and simulation, in addition to providing new insights into the human perceptual system.",pe.fortin@mail.mcgill.ca,"1. Pedro Lopes, Alexandra Ion, and Patrick Baudisch. 2015. Impacto: Simulating Physical Impact by Combining Tactile Stimulation with Electrical Muscle Stimulation. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology - UIST â€™15 (UIST â€™15). ACM, New York, NY, USA, 11â€“19. DOI: http://dx.doi.org/10.1145/2807442.2807443 \ 2. Pedro Lopes, Sijing You, Lung-Pan Cheng, Sebastian Marwecki, and Patrick Baudisch. 2017. Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI â€™17 (CHI â€™17). ACM, New York, NY, USA, 1471â€“1482. DOI: http://dx.doi.org/10.1145/3025453.3025600 \ 3. Jordan Navarro, Franck Mars, and Jean-Michel Hoc. 2007. Lateral Control Assistance for Car Drivers: A Comparison of Motor Priming and Warning Systems. Human Factors 49, 5 (2007), 950â€“960. DOI: http://dx.doi.org/10.1518/001872007X230280 \ 4. Hsin-Yun Yao and Vincent Hayward. 2010. Design and analysis of a recoil-type vibrotactile transducer. The Journal of the Acoustical Society of America 128, 2 (aug 2010), 619â€“627. DOI: http://dx.doi.org/10.1121/1.3458852 \","thermal interface, electrical muscle stimulation, perception",H.5.2,uistpp0157-file1.zip,,,uistpp0157-file4.pdf,Experimental apparatus used to render temperatures and trigger electrical muscle stimulations.,pdf render of the poster,This project investigates the use of an EMSÂ­ induced heat withdrawal reflex to bias temperature perception in VR and AR. Promising preliminary results and future experiment plan are presented.,,Pascal Fortin,Jeffrey Blum,FormatComplete,,NSERC Discovery Grant,NSERC PGS-D,,McGill Engineering Doctoral Award,,Aug 18 14:37,
uistpp158,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp158,A,Grip Force Estimation by Emitting Vibration,Kentaro,Takemura,takemura@tokai.ac.jp,uistpp0158-paper.pdf,2,letter,,,"Nobuhiro Funato, Kentaro Takemura","nobuhiro-f@takemura-lab.org, takemura@tokai.ac.jp",73618,Nobuhiro,,Funato,nobuhiro-f@takemura-lab.org,Graduate School of Engineering,Tokai University,Hiratsuka,Kanagawa,Japan,,,,,,5353,Kentaro,,Takemura,takemura@tokai.ac.jp,Graduate School of Engineering,Tokai University,Hiratsuka,Kanagawa,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a method for determining grip force based on active bone-conducted sound sensing, which is an active acoustic sensing. In our previous studies, we estimated the joint angle, hand pose, and contact force by emitting a vibration to the body. We aspired to expand to an additional application of an active bone-conducted sound sensing, thus, we tried to estimate the grip force by creating a wrist-type device. The grip force was determined by using the power spectral density as the features, and gradient boosted regression trees (GBRT). Through evaluation experiments, the average error of the estimated grip force was around 15 N. Moreover, we confirmed that the grip strength could be determined with high accuracy.",nobuhiro-f@takemura-lab.org,"1. Funato, N., and Takemura, K. Estimating contact force of ï¬ngertip and providing tactile feedback simultaneously. In Proc. of the 29th Annual Symposium on User Interface Software and Technology (2016), 195â€“196. \ 2. Hoozemans, M. J., and Dieen, J. H. V. Prediction of handgrip forces using surface emg of forearm muscles. Journal of electromyography and kinesiology 15, 4 (2005), 358â€“366. \",Active acoustic sensing;Grip force;Vibration,H.5.2; B.4.2,uistpp0158-file1.zip,uistpp0158-file2.jpg,uistpp0158-file3.m4v,uistpp0158-file4.pdf,The result of estimating the grip force when the object is grasped.,This file is our poster material.,We propose a method for estimating grip force based on active bone-conducted sound sensing which is one of the active acoustic sensing.,,Nobuhiro Funato,Kentaro Takemura,FormatComplete,162103009,Ministry of Internal Affairs and Communications,Japan Society for the Promotion of Science,15K00285,,,Aug 10 6:43,
uistpp162,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp162,A,EYE DEAR : Smartphone text resizing interaction for the eye health of the presbyopia population,Jiyeon,Lee,all4me8911@gmail.com,uistpp0162-paper.pdf,3,letter,,,"Jiyeon Lee, Inseok Hong, Jongsung Lee, Jundong Cho","all4me8911@gmail.com, ghdlstjr@naver.com, kes98215@naver.com, jdcho07@gmail.com",62288,Jiyeon,,Lee,all4me8911@gmail.com,Department of Human ICT Convergence,Sungkyunkwan University,Suwon,Gyeonggi-do,"Korea, Republic of",,,,,,71386,Inseok,,Hong,ghdlstjr@naver.com,Department of Human ICT Convergence,Sungkyunkwan University,Suwon,Gyeonggi-do,"Korea, Republic of",,,,,,62268,Jongsung,,Lee,kes98215@naver.com,Sungkyunkwan University,Suwon-si,Gyeonggi-do,"Korea, Republic of",,,,,,,45015,Jundong,,Cho,jdcho07@gmail.com,Human ICT Convergence/Sungkyunkwan University,Dep. of Human ICT Convergence,Seoul,Suwon,"Korea, Republic of",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Presbyopia is a symptom in which the elasticity of the lens is weakened and the image is not formed. However, as the use of smart phones increases, the age at which presbyopia symptoms appear is gradually decreasing. The closer the distance is from the smartphone, the less the focus of the eyes will be which making the letters and pictures on the smartphone screen appear blurred. In this study, we conducted a study on the interactions that helped to improve health of eye for people with presbyopia or those who have a habit that can facilitating presbyopia. As the distance of the smartphone from the eye is increased, the font size is increased to upgrading readability and the prototype is tested by 20 experimenters.",all4me8911@gmail.com,"1. http://coopervision.co.kr/eye-health-and-vision/what-ispresbyopia \ 2. Cobb, Sarah. ""Harmon Revisted."" OEP's Advanced Therapist (1990). \ 3. Darroch, Iain, et al. ""The effect of age and font size on reading text on handheld computers."" Human-Computer Interaction-INTERACT 2005 (2005): 253-266. \ 4. Ho, Jimmy, et al. ""EyeProtector: Encouraging a Healthy Viewing Distance when Using Smartphones."" Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services. ACM, 2015 \ 5. Huang, Ding-Long, Pei-Luen Patrick Rau, and Ying Liu. ""Effects of font size, display resolution and task type on reading Chinese fonts from mobile devices."" International Journal of Industrial Ergonomics 39.1 (2009): 81-89. \ 6. Kwon, Ki-il, et al. ""The Functional Change of Accommodation and Convergence in the Mid-Forties by Using Smartphone."" Journal of Korean Ophthalmic Optics Society 21.2 (2016): 127-135. \",Interaction Design; Presbyopia; Harmon Distance; Behavior Change; User Test; User-Experience Design;,H.5.2. User Interfaces,uistpp0162-file1.doc,uistpp0162-file2.jpg,,uistpp0162-file4.pdf,The EYE DEAR's thumbnail,"At my poster, you can see the more detail images for my study.","In this study, we conducted on the interactions that helped to improve health of eye for people with presbyopia or those who have a habit that can facilitating presbyopia.",,Jiyeon Lee,Inseok Hong,FormatComplete,N0000717,"The Ministry of Trade, Industry and Energy(MOTIE), KOREA",,,,,Aug 13 16:41,
uistpp165,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp165,A,Non-Linear Editor for Text-Based Screencast,Jungkook,Park,pjknkda@kaist.ac.kr,uistpp0165-paper.pdf,3,letter,,,"Jungkook Park, Yeong Hoon Park, Alice Oh","pjknkda@kaist.ac.kr, park1799@umn.edu, alice.oh@kaist.edu",61207,Jungkook,,Park,pjknkda@kaist.ac.kr,,University of Minnesota,Minneapolis,Minnesota,United States,,,,,,61406,Yeong Hoon,,Park,park1799@umn.edu,Department of Computer Science & Engineering,University of Minnesota,Minneapolis,Minnesota,United States,,,,,,18266,Alice,,Oh,alice.oh@kaist.edu,KAIST,Daejeon,Chungnam,Republic of Korea,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Screencasts, where computer screen is broadcast to a large audience on the web, are becoming popular as an online educational tool. Among various types of screencast content, popular are the contents that involve text editing, including computer programming. There are emerging platforms that support such text-based screencasts by recording every character insertion/deletion from the creator and reconstructing its playback on the viewer's screen. However, these platforms lack rich support for creating and editing the screencast itself, mainly due to the difficulty of manipulating recorded text changes; the changes are tightly coupled in sequence, thus modifying arbitrary part of the sequence is not trivial. \ \ We present a non-linear editing tool for text-based screencasts. With the proposed selective history rewrite process, our editor allows users to substitute an arbitrary part of a text-based screencast while preserving overall consistency of the rest of the text-based screencast.",pjknkda@kaist.ac.kr,"Children with ASD (Autism Spectrum Disorder) have difficulties in expressing their feelings and needs, their teachers have to be very familiar with them to adjust teaching contents in related training lessons. In this paper, we present an adaptive training system with EEG (Electroencephalogram) devices for autistic children. We designed an EEG helmet to monitor children's attention levels, and a video chat system with virtual cartoon faces covered on teacher's face. Cartoon faces are synchronized with the performer's facial movements to help trainers express themselves in an exaggerated way. When the attention reduction is detected by the EEG helmet, cartoon face will adjust automatically, and try to draw their attention back through changing cartoon types, colors, brightness, etc. Each change and feedback from children will be traced by the helmet and analyzed for improvements. By continuous iterative learning, the system will become smarter in avoiding children's physical exhaustion. The system was introduced in the form of a specific training lesson to an ASD school, and preliminary experiment has indicated an encouraging result.",Selective history rewriting; Document history; History editing; Screencast,H.5.m.,uistpp0165-file1.zip,,,uistpp0165-file4.pdf,,This is a single PDF file of the poster that will be presented at the conference.,"We present a non-linear editing tool for text-based screencasts. With our tool, users can substitute an arbitrary part of text-based screencast while preserving overall consistency of the text editing history.",,Jungkook Park,Yeong Hoon Park,FormatComplete,,,,,,,Aug 10 1:09,
uistpp168,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp168,A,An EEG-based Adaptive Training System for ASD Children,Cheng,Zheng,zzzzzccccc@gmail.com,uistpp0168-paper.pdf,3,letter,,,"Cheng Zheng, Caowei Zhang, Xuan Li, Bing Li, Fan Zhang, Xin Liu, Cheng Yao, Yijun Zhao, Fangtian Ying","zzzzzccccc@gmail.com, 415965428@qq.com, xuanli@zju.edu.cn, happyice19870112@zju.edu.cn, zfan425@zju.edu.cn, u_shin@126.com, yaoch@zju.edu.cn, zhaoyj@zju.edu.cn, yingft@gmail.com",71397,Cheng,,Zheng,zzzzzccccc@gmail.com,,Zhejiang University,Hangzhou,,China,,,,,,58118,Caowei,,Zhang,415965428@qq.com,,Zhejiang University,Hangzhou,,China,,,,,,69501,Xuan,,Li,xuanli@zju.edu.cn,Zhejiang University,Hangzhou,,China,,,,,,,57374,Bing,,Li,happyice19870112@zju.edu.cn,"Zhejiang University, Hangzhou, China",Zhejiang University,Hangzhou,,China,,,,,,73641,Fan,,Zhang,zfan425@zju.edu.cn,"Zhejiang University, Hangzhou, China",Zhejiang University,Hangzhou,,China,,,,,,69502,Xin,,Liu,u_shin@126.com,,Zhejiang University,Hangzhou,,China,,,,,,59812,Cheng,,Yao,yaoch@zju.edu.cn,"Industrial Design,Zhejiang University",Computer Scienve and Technology,Hangzhou,Zhejiang,China,,,,,,74051,Yijun,,Zhao,zhaoyj@zju.edu.cn,"Industrial Design,Zhejiang University, Computer Scienve and Technology, Hangzhou, Zhejiang, China",Zhejiang University,Hangzhou,Zhejiang,China,,,,,,36206,Fangtian,,Ying,yingft@gmail.com,"Industrial Design,Zhejiang University",Computer Scienve and Technology,Hangzhou,Zhejiang,China,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Children with ASD (Autism Spectrum Disorder) have difficulties in expressing their feelings and needs, their teachers have to be very familiar with them to adjust teaching contents in related training lessons. In this paper, we present an adaptive training system with EEG (Electroencephalogram) devices for autistic children. We designed an EEG helmet to monitor children's attention levels, and a video chat system with virtual cartoon faces covered on teacher's face. Cartoon faces are synchronized with the performer's facial movements to help trainers express themselves in an exaggerated way. When the attention reduction is detected by the EEG helmet, cartoon face will adjust automatically, and try to draw their attention back through changing cartoon types, colors, brightness, etc. Each change and feedback from children will be traced by the helmet and analyzed for improvements. By continuous iterative learning, the system will become smarter in avoiding children's physical exhaustion. The system was introduced in the form of a specific training lesson to an ASD school, and preliminary experiment has indicated an encouraging result.",zzzzzccccc@gmail.com,"1. Lord, C., & Volkmar, F. (2002). Genetics of childhood disorders: XLII. Autism, part 1: Diagnosis and assessment in autistic spectrum disorders. Journal of the American Academy of Child and Adolescent Psychiatry, 41, 1134â€“1136. \ 2. Chawarska, K., &Volkmar, F. (2005). Autism in infancy and early childhood. In Volkmar, F. R., Paul, R., Klin, A., & Cohen, D. (Eds.), Handbook of autism and pervasive developmental disorders. Vol. 1 (pp.223â€“ 246). Hoboken, NJ: John Wiley & Sons. \ 3. Rosset, D. B., Rondan, C., Da Fonseca, D., Santos, A., Assouline, B., & Deruelle, C. (2008). Typical emotion processing for cartoon but not for real faces in children with autistic spectrum disorders. Journal of autism and developmental disorders, 38(5), 919-925. \ 4. Nicolas-Alonso, L. F., & Gomez-Gil, J. (2012). Brain Computer Interfaces, a Review. Sensors (Basel, Switzerland), 12(2), 1211â€“1279. http://doi.org/10.3390/s120201211 \ 5. Garzotto, F., Gelsomini, M., Pappalardo, A., Sanna, C., Stella, E., & Zanella, M. (2016, May). Using Brain Signals in Adaptive Smart Spaces for Disabled Children. In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems (pp. 1684-1690). ACM. \ 6. Zheng, C., Zhang, C. W., Li, X., Liu, X., Tang, C.Q., Wang, G.Y., Yao, C., Zhang, F., Xu, W.J., & Ying, F. T. (2017, July). Toon-Chat: A Cartoon-Masked Chat System for Children with Autism. ACM SIGGRAPH 2017 Posters. ACM. \ 7. Rebolledo-Mendez, G., Dunwell, I., MartÃ­nez-MirÃ³n, E. A., Vargas-CerdÃ¡n, M. D., De Freitas, S., Liarokapis, F., & GarcÃ­a-Gaona, A. R. (2009, July). Assessing neuroskyâ€™s usability to detect attention levels in an assessment exercise. In International Conference on Human-Computer Interaction (pp. 149-158). Springer, Berlin, Heidelberg. \",EEG; ASD; Children Education; Augmented Reality; Iterative Learning,H.5.2,uistpp0168-file1.doc,,,uistpp0168-file4.pdf,,This PDF is the poster file for exhibition.,It's difficult to communicate with ASD children for their defects in emotion perception and self expression. See what we have done in understanding these children by using augmented technologies and EEG portable devices.,,Caowei Zhang,Cheng Zheng,FormatComplete,2015BAF14B01,National Key Technologies R&D Program,State Key Program of National Natural Science of China,61332017,,,Aug 18 22:29,
uistpp169,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp169,A,Ani-Bot: A Mixed-Reality Modular Robotics System,Yuanzhi,Cao,cao158@purdue.edu,uistpp0169-paper.pdf,3,letter,,,"Yuanzhi Cao, Zhuangying Xu, Terrell Kendall Glenn, Ke Huo, Karthik Ramani","cao158@purdue.edu, xu970@purdue.edu, glenn3@purdue.edu, khuo@purdue.edu, ramani@purdue.edu",60234,Yuanzhi,,Cao,cao158@purdue.edu,School of Mechanical Engineering,Purdue University,west Lafayette,Indiana,United States,,,,,,71779,Zhuangying,,Xu,xu970@purdue.edu,School of Mechanical Engineering,Purdue University,west Lafayette,Indiana,United States,,,,,,71831,Terrell,Kendall,Glenn,glenn3@purdue.edu,Purdue University,West Lafayette,Indiana,United States,,,,,,,48741,Ke,,Huo,khuo@purdue.edu,School of Mechanical Engineering,Purdue University,West Lafayette,IN,USA,,,,,,29031,Karthik,,Ramani,ramani@purdue.edu,School of Mechanical Engineering,Purdue University,West Lafayette,Indiana,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present Ani-Bot, a modular robotics system that allows users to construct Do-It-Yourself (DIY) robots and use mixed-reality approach to interact with them. Ani-Bot enables novel user experience by embedding Mixed-Reality Interaction (MRI) in the three phases of interacting with a modular construction kit, namely, Creation, Tweaking, and Usage. In this paper, we first present the system design that allows users to instantly perform MRI once they finish assembling the robot. Further, we discuss the augmentations offered by MRI in the three phases in specific.",cao158@purdue.edu,"1. Ishii, H. Tangible bits: beyond pixels. In Proceedings of the 2nd international conference on Tangible and embedded interaction, ACM (2008), xvâ€“xxv. \ 2. Leong, J., Perteneder, F., Jetter, H.-C., and Haller, M. What a life!: Building a framework for constructive assemblies. In Tangible and Embedded Interaction (2017), 57â€“66. \ 3. Shaer, O., and Hornecker, E. Tangible user interfaces: past, present, and future directions. Foundations and Trends in Human-Computer Interaction 3, 1â€“2 (2010), 1â€“137. \",DIY robot kit; Mixed-reality interaction; User interface.,H.5.2.,uistpp0169-file1.zip,uistpp0169-file2.jpg,uistpp0169-file3.mp4,uistpp0169-file4.mp4,Ani-Bot: a modular robotics system that allows for DIY robot construction with intuitive mixed-reality interaction.,,"Ani-Bot is a modular robotics system with mixed-reality interaction for controlling and programming, which has strong potential of delivering novel user experiences for DIY robotics \",,Yuanzhi Cao,Zhuangying Xu,FormatComplete,1547134,NSF,NSF,1329979,NSF,1144843,Aug 7 18:03,
uistpp172,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp172,A,The Feedback Block Model for an Adaptive E-Book,Ja-Ryoung,Choi,j2arlove@gmail.com,uistpp0172-paper.pdf,2,letter,,,"Ja-Ryoung Choi, Suin Kim, Soon-Bum Lim","j2arlove@gmail.com, suin.kim@kaist.ac.kr, sblim@sookmyung.ac.kr",73647,Ja-Ryoung,,Choi,j2arlove@gmail.com,SookMyung Women's University,KAIST,Daejeon,,"Korea, Republic of",,,,,,19355,Suin,,Kim,suin.kim@kaist.ac.kr,,KAIST,Daejeon,,"Korea, Republic of",,,,,,51970,Soon-Bum,,Lim,sblim@sookmyung.ac.kr,Sookmyung Women's University,Seoul,,"Korea, Republic of",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The emergence of social reading services has enabled readers to participate actively in reading activities by means of sharing and feedback. Readers can state their opinion on a book by providing feedback. However, because current e-books are published with fixed, unchangeable content, it is difficult to reflect the reader's feedback on them. In this paper, we propose a system for an adaptive e-book that dynamically updates itself on user participation. To achieve this, we designed a Feedback Block Model and a Feedback Engine. In the Feedback Block Model, at the time of publication, the author defines the type of feedback expected from readers. After publication, the Feedback Engine collects and aggregates the readers' feedback. The Feedback Engine can be configured with drag-and-drop block programming, and hence, even authors inexperienced in programming can create an adaptive e-book.",j2arlove@gmail.com,"1. Kataoka, E., Amagasa, T., and Kitagawa, H. A system for social reading based on epub3. In Proceedings of International Conference on Information Integration and Web-based Applications & Services, ACM (2013), 72. \ 2. PÂ´erez-PÂ´erez, M., Glez-PeËœna, D., Fdez-Riverola, F., and LourencÂ¸o, A. Marky: A tool supporting annotation consistency in multi-user and iterative document annotation projects. Computer methods and programs in biomedicine 118, 2 (2015), 242â€“251. \",E-book; Feedback; Dynamic updates; Adaptive contents,H.5.4; H.5.3; H.5.2,uistpp0172-file1.zip,,,uistpp0172-file4.pdf,,This is a poster that describes the proposed e-book framework.,"We propose a system for an adaptive e-book that dynamically updates itself on user participation. The system can be configured with drag-and-drop block programming, thus even authors inexperienced in programming can create an adaptive e-book.",,Ja-Ryoung Choi,Suin Kim,FormatComplete,2017R1A6A3A11032211,National Research Foundation of Korea (NRF),,,,,Aug 11 9:31,
uistpp178,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp178,A,Contour: An Efficient Voice-enabled Workflow for Producing Text-to-Speech Content,Yuan-Yi,Fan,danny.fan@iamplus.com,uistpp0178-paper.pdf,3,letter,,,"Yuan-Yi Fan, Soyoung Shin, Vids Samanta","danny.fan@iamplus.com, soyoung.shin@iamplus.com, vids.samanta@iamplus.com",73555,Yuan-Yi,,Fan,danny.fan@iamplus.com,,I.AM+,Los Angeles,California,United States,,,,,,73660,Soyoung,,Shin,soyoung.shin@iamplus.com,,I.AM+,Los Angeles,California,United States,,,,,,73662,Vids,,Samanta,vids.samanta@iamplus.com,I.AM+,Los Angeles,California,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Voice assistant technology has expanded the design space for voice-activated consumer products and audio-centric user experience. To navigate this emerging design space, Speech Synthesis Markup Language (SSML) provides a standard to characterize synthetic speech based on parametric control of the prosody elements, i.e. pitch, rate, volume, contour, range, and duration. However, the existing voice assistants utilizing Text-to-Speech (TTS) lack expressiveness. The need of a new production workflow for more efficient and emotional audio content using TTS is discussed. A prototype that allows a user to produce TTS-based content in any emotional tone using voice input is presented. To evaluate the new workflow enabled by the prototype, an initial comparative study is conducted against the parametric approach. Preliminary quantitative and qualitative results suggest the new workflow is more efficient based on time to complete tasks and number of design iterations, while maintaining the same level of user preferred production quality.",danny.fan@iamplus.com,"1. AMFM decompy. . https://pypi.python.org/pypi/AMFM_decompy. \ 2. Baume, C., Plumbley, M. D., and Calic, J. Use of audio editors in radio production. In Audio Engineering Society Convention 138, Audio Engineering Society (2015). \ 3. Boersma, P. Praat: doing phonetics by computer. http://www. praat. org/ (2006). \ 4. Jin, Z., Mysore, G. J., DiVerdi, S., Lu, J., and Finkelstein, A. VoCo: Text-based insertion and replacement in audio narration. ACM Transactions on Graphics 36, 4 (July 2017), Article 96, 13 pages. \ 5. Mahrt, T. PraatIO. https://github.com/timmahrt/praatIO. \ 6. Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499 (2016). \ 7. Rubin, S., Berthouzoz, F., Mysore, G. J., Li, W., and Agrawala, M. Content-based tools for editing audio stories. In Proceedings of the 26th annual ACM symposium on User interface software and technology, ACM (2013), 113â€“122. \ 8. Zahorian, S. A., and Hu, H. A spectral/temporal method for robust fundamental frequency tracking. The Journal of the Acoustical Society of America 123, 6 (2008), 4559â€“4571. \",Voice User Interface; Text-to-Speech; Audio Production Workflow,D.2.2; H.5.2; H.5.2,uistpp0178-file1.zip,,,uistpp0178-file4.pdf,,A poster pdf file is included.,Contour: An Efficient Voice-enabled Workflow for Producing Text-to-Speech Content,,Soyoung Shin,Yuan-Yi Fan,FormatComplete,,,,,,,Aug 11 20:27,
uistpp181,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp181,A,Designing 3D-Printed Deformation Behaviors Using Spring-Based Structures: An Initial Investigation,Liang,He,lianghe@cs.umd.edu,uistpp0181-paper.pdf,3,letter,,,"Liang He, Huaishu Peng, Joshua Land, Mark D Fuge, Jon E Froehlich","lianghe@cs.umd.edu, hp356@cornell.edu, joshlumd@gmail.com, fuge@umd.edu, jonf@cs.umd.edu",55338,Liang,,He,lianghe@cs.umd.edu,,Cornell University,Ithaca,New York,United States,,,,,,13400,Huaishu,,Peng,hp356@cornell.edu,Information Science,Cornell University,Ithaca,New York,United States,,,,,,73659,Joshua,,Land,joshlumd@gmail.com,"University of Maryland, College Park",College Park,Maryland,United States,,,,,,,61442,Mark,D,Fuge,fuge@umd.edu,Mechanical Engineering,University of Maryland,College Park,Maryland,United States,,,,,,6165,Jon,E,Froehlich,jonf@cs.umd.edu,,"University of Maryland, College Park",College Park,Maryland,United States,,University of Washington,Washington,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Recent work in 3D printing has focused on tools and techniques to design deformation behaviors using mechanical structures such as joints and metamaterials. In this poster, we explore how to embed and control mechanical springs to create deformable 3D-printed objects. We propose an initial design space of 3D-printable spring-based structures to support a wide range of expressive behaviors, including stretch and compress, bend, twist, and all possible combinations. The poster concludes with a brief feasibility test and enumerates future work.",edigahe@gmail.com,"1. Richard G. Budynas and J.Keith Nisbett. Shigley's Mechanical Engineering Design (8th Edition). Mechanical Springs (Chapter 10). 2008. McGraw Hill Higher Education. \ 2. Mark D. Fuge, Greg Carmean, Jessica Cornelius, and Ryan Elder. The MechProcessor: Helping Novices Design Printable Mechanisms Across Different Printers. Journal of Mechanical Design 137, no. 11 (2015): 111415. \ 3. Liang He, Gierad Laput, Eric Brockmeyer, and Jon E. Froehlich. SqueezaPulse : Adding Interactive Input to Fabricated Objects Using Corrugated Tubes and Air \ Pulses. In Proceedings of Tangible and Embedded Interaction (TEI '17). 341â€“350. \ 4. Alexandra Ion, Johannes Frohnhofen, Ludwig Wall, Robert Kovacs, Mirela Alistar, Jack Lindsay, Pedro Lopes, Hsiang-Ting Chen, and Patrick Baudisch. 2016. Metamaterial Mechanisms. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). 529-539. \ 5. Miyu Iwafune, Taisuke Ohshima, and Yoichi Ochiai. 2016. Coded skeleton: programmable bodies for shape changing user interfaces. In ACM SIGGRAPH 2016 Posters (SIGGRAPH '16). \ 6. Ken Nakagaki, Luke Vink, Jared Counts, Daniel Windham, Daniel Leithinger, Sean Follmer, and Hiroshi Ishii. 2016. Materiable: Rendering Dynamic Material Properties in Response to Direct Physical Touch with Shape Changing Interfaces. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16). 2764-2772. \ 7. Huaishu Peng, FranÃ§ois GuimbretiÃ¨re, James McCann, and Scott E. Hudson. 2016. A 3D Printer for Interactive Electromagnetic Devices. In Proceedings of the 29th \ Annual Symposium on User Interface Software and Technology (UIST '16). 553-562. \ 8. Michael L. Rivera, Melissa Moukperian, Daniel Ashbrook, Jennifer Mankoff, and Scott E. Hudson. 2017. Stretching the Bounds of 3D Printing with Embedded Textiles. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems \ (CHI '17). 497-508.",3D Printing; Fabrication; Deformation Behaviors; Design Space; Mechanical Spring.,"H.5.2. Information interfaces and presentation (e.g., HCI): User Interfaces.",uistpp0181-file1.docx,uistpp0181-file2.jpg,uistpp0181-file3.mp4,uistpp0181-file4.pdf,The thumbnail image shows the Tigger example with three types of deformable tails: \ (a) sketch and compress only; (b) bend only; and (c) twist only (black marked dots show the rotation).,The auxiliary material includes a poster design of our poster paper.,"We contribute a design space of spring-based structures that constrain different types of printable deformation behaviors. We decouple spring behaviors into three individual categories: stretch and/or compress, bend, and twist by demonstrating a Tigger example.",,Liang He,Huaishu Peng,FormatComplete,,,,,,,Aug 11 1:00,
uistpp183,10/24,18,Posters,2:00:00 PM,4:00:00 PM,,,,,,,,,None,,uistpp183,A,Auditory Overview of Web Pages for Screen Reader Users,Tao,Wang,tao.wang@uci.edu,uistpp0183-paper.pdf,3,letter,,,"Tao Wang, David Redmiles","tao.wang@uci.edu, redmiles@ics.uci.edu",45171,Tao,,Wang,tao.wang@uci.edu,Department of Informatics,UC Irvine,Irvine,California,United States,,,,,,4467,David,,Redmiles,redmiles@ics.uci.edu,Informatics,UC Irvine,Irvine,California,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Blind users browse the web using screen readers. Screen readers read the content on a web page sequentially via synthesized speech. The linear nature of this process makes it difficult to obtain an overview of the web page, which creates navigation challenges. To alleviate this problem, we have developed ScreenTrack, a browser extension that summarizes a web page's accessibility features into a short, dynamically generated soundtrack. Users can quickly gain an overview of the presence of web elements useful for navigation on a web page. Here we describe ScreenTrack and discuss future research plans.",mr.taowang@gmail.com,"1. Marilyn Hughes Blackmon, Peter G. Polson, Muneo Kitajima, and Clayton Lewis. 2002. Cognitive Walkthrough for the Web. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI â€™02), 463â€“470. https://doi.org/10.1145/503376.503459 \ 2. Yevgen Borodin, Jeffrey P. Bigham, Glenn Dausch, and I. V. Ramakrishnan. 2010. More Than Meets the Eye: A Survey of Screen-reader Browsing Strategies. In Proceedings of the 2010 International Cross Disciplinary Conference on Web Accessibility (W4A) (W4A â€™10), 13:1â€“13:10. https://doi.org/10.1145/1805986.1806005 \ 3. Stuart K. Card, Peter Pirolli, Mija Van Der Wege, Julie B. Morrison, Robert W. Reeder, Pamela K. Schraedley, and Jenea Boshart. 2001. Information Scent As a Driver of Web Behavior Graphs: Results of a Protocol Analysis Method for Web Usability. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI â€™01), 498â€“505. https://doi.org/10.1145/365024.365331 \ 4. Caroline Jay, Robert Stevens, Mashhuda Glencross, Alan Chalmers, and Cathy Yang. 2007. How people use presentation to search for a link: expanding the understanding of accessibility on the Web. Universal Access in the Information Society 6, 3: 307â€“320. https://doi.org/10.1007/s10209-007-0089-5 \ 5. Johan Kildal and Stephen A. Brewster. 2006. Providing a size-independent overview of non-visual tables. In Proceedings of the 12th International Conference on Auditory Display (ICAD â€™06), 8â€“15. \ 6. Christos Kouroupetroglou, Michail Salampasis, and Athanasios Manitsaris. 2008. Analysis of navigation behaviour of blind users using Browsing Shortcuts. New Review of Hypermedia and Multimedia 14, 2: 199â€“228. https://doi.org/10.1080/13614560802624258 \ 7. Brian C. J. Moore. 2012. An Introduction to the Psychology of Hearing. BRILL. \ 8. Emma Murphy, Ravi Kuber, Philip Strain, Graham McAllister, and Wai Yu. 2007. Developing Sounds for a Multimodal Interface: Conveying Spatial Information to Visually Impaired Web Users. In Proceedings of the 13th International Conference on Auditory Display (ICAD â€™07), 348â€“355. \ 9. Peter Parente. 2004. Audio Enriched Links: Web Page Previews for Blind Users. In Proceedings of the 6th International ACM SIGACCESS Conference on Computers and Accessibility (Assets â€™04), 2â€“8. https://doi.org/10.1145/1028630.1028633 \ 10. Patrick Roth, Lori Petrucci, Thierry Pun, and AndrÃ© Assimacopoulos. 1999. Auditory Browser for Blind and Visually Impaired Users. In CHI â€™99 Extended Abstracts on Human Factors in Computing Systems (CHI EA â€™99), 218â€“219. https://doi.org/10.1145/632716.632852 \ 11. Barbara Shinn-Cunningham and Antje Ihlefeld. 2004. Selective and divided attention: Extracting information from simultaneous sound sources. In Proceedings of the 10th International Conference on Auditory Display (ICAD â€™04). \ 12. B. Shneiderman. 1996. The eyes have it: a task by data type taxonomy for information visualizations. In Proceedings 1996 IEEE Symposium on Visual Languages, 336â€“343. https://doi.org/10.1109/VL.1996.545307 \ 13. Shari Trewin, John Richards, Rachel Bellamy, Bonnie E. John, John Thomas, Cal Swart, and Jonathan Brezin. 2010. Toward Modeling Auditory Information Seeking Strategies on the Web. In CHI â€™10 Extended Abstracts on Human Factors in Computing Systems (CHI EA â€™10), 3973â€“3978. https://doi.org/10.1145/1753846.1754088 \ 14. Haixia Zhao, Catherine Plaisant, Ben Shneiderman, and Jonathan Lazar. 2008. Data Sonification for Users with Visual Impairment: A Case Study with Georeferenced Data. ACM Trans. Comput.-Hum. Interact. 15, 1: 4:1â€“ 4:28. https://doi.org/10.1145/1352782.1352786 \ 15. Web Audio API. Retrieved July 6, 2017 from https://www.w3.org/TR/webaudio/ \",Web accessibility; Web Audio API; spatial audio; 3D audio; auditory overview,H.5.m,uistpp0183-file1.docx,,,uistpp0183-file4.pdf,,The auxiliary material included is the poster itself that will be presented in UIST '17. It's a single PDF file. It can be opened with Adobe Acrobat Reader.,"For blind users, exploring unfamiliar web pages can be slow. To communicate key information faster and intuitively, we encode a web pageâ€™s accessibility features into a short, dynamically generated soundtrack.",,Tao Wang,David Redmiles,FormatComplete,,,,,,,Aug 11 21:53,
uistde102,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde102,A,GestAKey: Get More Done with Just-a-Key on a Keyboard,Yilei,Shi,yilei@ahlab.org,uistde0102-paper.pdf,3,letter,,,"Yilei Shi, TomÃ¡s Vega GÃ¡lvez, Haimo Zhang, Suranga Nanayakkara","yilei@ahlab.org, tomas.vega@berkeley.edu, zh.hammer@gmail.com, suranga@ahlab.org",60524,Yilei,,Shi,yilei@ahlab.org,Augmented Human Lab,Singapore University of Technology and Design,Singapore,Singapore,Singapore,,,,,,57960,Tomás,,Vega Gálvez,tomas.vega@berkeley.edu,Augmented Human Lab,Singapore University of Technology and Design,Singapore,,Singapore,,,,,,15812,Haimo,,Zhang,zh.hammer@gmail.com,Augmented Human Lab,Singapore University of Technology and Design,Singapore,,Singapore,,,,,,58387,Suranga,,Nanayakkara,suranga@ahlab.org,Augmented Human Lab,Singapore University of Technology and Design,Singapore,Singapore,Singapore,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The computer keyboard is a widely used input device to operate computers, such as text entry and command execution. Typically, keystrokes are detected as binary states (e.g. ""pressed"" vs. ""not pressed""). Due to this, more complex input commands need multiple key presses that could go up to pressing four keys at the same time, such as pressing ""Cmd + Shift + Opt + 4"" to take a screenshot to the clipboard on macOS. We present GestAKey, a technique to enable multifunctional keystrokes on a single key, providing new interaction possibilities on the familiar keyboards. The system consists of touch sensitive keycaps and a software backend that recognizes micro-gestures performed on individual keys to perform system commands or input special characters. In this demo, attendees will get the chance to interact with several GestAKey-enabled proof-of-concept applications.",yilei@ahlab.org,"1. Chan, L., Chen, M. Y., and Chen, W.-h. C. B.-y. FingerPad : Private and Subtle Interaction Using Fingertips. 255â€“260. \ 2. Dietz, P. H., Eidelson, B., Westhues, J., and Bathiche, S. A practical pressure sensitive computer keyboard. Uist (2009), 55. \ 3. Taylor, P., Hammond, J. M., Harvey, C. M., Koubek, R. J., Compton, W. D., and Darisipudi, A. Distributed Collaborative Design Teams : Media Effects on Design Processes Distributed Collaborative Design Teams : Media Effects on Design Processes. International Journal of Human-Computer Interaction 7318, June 2015 (2005), 37â€“41. \ 4. Taylor, S., Keskin, C., Hilliges, O., Izadi, S., and Helmes, J. Type-hover-swipe in 96 bytes. Proceedings of the 32nd annual ACM conference on Human factors in computing systems - CHI â€™14 (2014), 1695â€“1704. \ 5. Zhang, H., and Li, Y. GestKeyboard. Proceedings of the 32nd annual ACM conference on Human factors in computing systems - CHI â€™14 (2014), 1675â€“1684. \ 6. Zheng, J., and Vogel, D. Finger-Aware Shortcuts. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems - CHI â€™16 (2016), 4274â€“4285. \",Keyboard; Microgesture; Multifunctional Keystroke,H.5.2,uistde0102-file1.zip,uistde0102-file2.jpg,uistde0102-file3.m4v,,,,,,,,,,,,,,,,
uistde106,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde106,A,Mutual Human Actuation,Lung-Pan,Cheng,lung-pan.cheng@hpi.uni-potsdam.de,uistde0106-paper.pdf,9,letter,,incomplete,"Lung-Pan Cheng, Sebastian Marwecki, Patrick Baudisch","lung-pan.cheng@hpi.uni-potsdam.de, Sebastian.Marwecki@hpi.de, patrick.baudisch@hpi.uni-potsdam.de",17012,Lung-Pan,,Cheng,lung-pan.cheng@hpi.uni-potsdam.de,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,33875,Sebastian,,Marwecki,Sebastian.Marwecki@hpi.de,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,1437,Patrick,,Baudisch,patrick.baudisch@hpi.uni-potsdam.de,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Human actuation is the idea of using people to provide large-scale force feedback to users. The Haptic Turk system, for example, used four human actuators to lift and push a virtual reality user; TurkDeck used ten human actuators to place and animate props for a single user. While the experience of human actuators was decent, it was still inferior to the experience these people could have had, had they participated as a user. In this paper, we address this issue by making everyone a user. We introduce mutual human actuation, a version of human actuation that works without dedicated human actuators. The key idea is to run pairs of users at the same time and have them provide human actuation to each other. Our system, Mutual Turk, achieves this by (1) offering shared props through which users can exchange forces while obscuring the fact that there is a human on the other side, and (2) synchronizing the two users' timelines such that their way of manipulating the shared props is consistent across both virtual worlds. We demonstrate mutual human actuation with an example experience in which users pilot kites though storms, tug fish out of ponds, are pummeled by hail, battle monsters, hop across chasms, push loaded carts, and ride in moving vehicles. \",lung-pan.cheng@hpi.de,"\ WARNING: Reference 21 is very long. Please verify that it was extracted correctly. \ \ 1. Bergamasco, M. The GLAD-IN-ART Project. Virtual Reality SE-19. 251â€“258. \ 2. Lung-Pan Cheng, Patrick LÃ¼hne, Pedro Lopes, Christoph Sterz, and Patrick Baudisch. 2014. Haptic turk: a motion platform based on people. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '14). ACM, New York, NY, USA, 3463-3472. DOI=http://dx.doi.org/10.1145/2556288.2557101 \ 3. Lung-Pan Cheng, Thijs Roumen, Hannes Rantzsch, Sven KÃ¶hler, Patrick Schmidt, Robert Kovacs, Johannes Jasper, Jonas Kemper, and Patrick Baudisch. 2015. TurkDeck: Physical Virtual Reality Based on People. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST '15). ACM, New York, NY, USA, 417-426. DOI: https://doi.org/ 10.1145/2807442.2807463 \ 4. Fabien Danieau, Julien Fleureau, Philippe Guillotel, Nicolas Mollet, Anatole LÃ©cuyer, and Marc Christie. 2012. HapSeat: producing motion sensation with multiple force-feedback devices embedded in a seat. In Proceedings of the 18th ACM symposium on Virtual reality software and technology (VRST '12). ACM, New York, NY, USA,69-76. DOI=http://dx.doi.org/10.1145/2407336.2407350 \ 5. H. G. Hoffman, ""Physically touching virtual objects using tactile augmentation enhances the realism of virtual environments,"" Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180), Atlanta, GA, USA, 1998, pp. 59-63. DOI: 10.1109/VRAIS.1998.658423 \ 6. C. E. Hughes, C. B. Stapleton, D. E. Hughes and E. M. Smith, ""Mixed reality in education, entertainment, and training,"" in IEEE Computer Graphics and Applications, vol. 25, no. 6, pp. 24-30, Nov.-Dec. 2005. DOI: 10.1109/MCG.2005.139 \ 7. Insko, B.E. Passive haptics significantly enhances virtual environments. Dissertation at University of North Carolina at Chapel Hill, 2001. \ 8. Ivan E. Sutherland. 1965. The ultimate display. In Proceedings of the Congress of the International Federation of Information Processing (IFIP), 506â€“508. https://doi.org/10.1109/MC.2005.274 \ 9. Luv Kohli, Eric Burns, Dorian Miller, and Henry Fuchs. 2005. Combining passive haptics with redirected walking. In Proceedings of the 2005 international conference on Augmented tele-existence (ICAT '05). ACM, New York, NY, USA, 253-254. DOI=http://dx.doi.org/ \ 10. 1145/1152399.1152451 10.Kok-Lim Low, Greg Welch, Anselmo Lastra, and Henry Fuchs. 2001. Life-sized projector-based dioramas. In Proceedings of the ACM symposium on Virtual reality software and technology (VRST '01). ACM, New York, NY, USA, 93-101. DOI=http://dx.doi.org/10.1145/ 505008.505026 \ 11. W. A. McNeely, ""Robotic graphics: a new approach to force feedback for virtual reality,"" Proceedings of IEEE Virtual Reality Annual International Symposium, Seattle, WA, 1993, pp. 336-341. DOI: 10.1109/VRAIS. 1993.380761 \ 12. MotiveDirect, https://github.com/XmanLCH/MotiveDirect realism posibility toact qualityof interface posibilityto examine self-evaluation ofperformance sounds haptic Mutual Turk passive haptics \ 13. M. Ortega and S. Coquillart, ""Prop-based haptic interaction with co-location and immersion: an automotive application,"" IEEE International Workshop on Haptic Audio Visual Environments and their Applications, 2005, pp. 6. DOI: 10.1109/HAVE.2005.1545646 \ 14. J. Pair, U. Neumann, D. Piepol and B. Swartout, ""FlatWorld: combining Hollywood set-design techniques with VR,"" in IEEE Computer Graphics and Applications, vol. 23, no. 1, pp. 12-15, Jan/Feb 2003. DOI: 10.1109/MCG.2003.1159607 \ 15. Adalberto L. Simeone, Eduardo Velloso, and Hans Gellersen. 2015. Substitutional Reality: Using the Physical Environment to Design Virtual Reality Experiences. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15). ACM, New York, NY, USA, 3307-3316. DOI: https://doi.org/10.1145/2702123.2702389. \ 16. Rajinder Sodhi, Ivan Poupyrev, Matthew Glisson, and Ali Israr. 2013. AIREAL: interactive tactile experiences in free air. ACM Trans. Graph. 32, 4, Article 134 (July 2013), 10 pages. DOI=http://dx.doi.org/10.1145/2461912.2462007 \ 17. Stewart, D. A platform with six degrees of freedom. Proceedings of the institution of mechanical engineers 180, 1 (1965), 371â€“386. \ 18. Dzmitry Tsetserukou, Katsunari Sato, Alena Neviarouskaya, Naoki Kawakami, and Susumu Tachi. 2009. FlexTorque: innovative haptic interface for realistic physical interaction in virtual reality. In ACM SIGGRAPH ASIA 2009 Sketches (SIGGRAPH ASIA '09). ACM, New York, NY, USA, Article 10, 1 pages. DOI: https://doi.org/10.1145/1667146.1667159 \ 19. Martin Usoh, Kevin Arthur, Mary C. Whitton, Rui Bastos, Anthony Steed, Mel Slater, and Frederick P. Brooks, Jr.. 1999. Walking > walking-in-place > flying, in virtual environments. In Proceedings of the 26th annual conference on Computer graphics and interactive techniques (SIGGRAPH '99). ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 359-364. DOI=http://dx.doi.org/10.1145/311535.311589 \ 20. Wolfgang Reisig. 1991. Petri nets and algebraic specifications. Theoretical Computer Science 80, 1: 1â€“ 34. DOI=https://doi.org/http://dx.doi.org/10.1016/03043975(91)90203-E \ 21. Lining Yao, Sayamindu Dasgupta, Nadia Cheng, Jason Spingarn-Koff, Ostap Rudakevych, and Hiroshi Ishii. 2011. Rope Revolution: tangible and gestural rope interface for collaborative play. In Proceedings of the 8th International Conference on Advances in Computer Entertainment Technology (ACE '11), Teresa RomÃ£o, Nuno Correia, Masahiko Inami, Hirokasu Kato, Rui Prada, Tsutomu Terada, Eduardo Dias, and Teresa Chambel (Eds.). ACM, New York, NY, USA, Article 11, 8 pages. DOI=http://dx.doi.org/10.1145/2071423.2071437. \ 22. Bob G. Witmer, Christian J. Jerome, and Michael J. Singer. 2005. The factor structure of the presence questionnaire. Presence: Teleoper. Virtual Environ. 14, 3 (June 2005), 298-312. DOI=http://dx.doi.org/10.1162/105474605323384654 \",Virtual reality; haptics; immersion; Haptic Turk,H.5.2,uistde0106-file1.docx,uistde0106-file2.jpg,uistde0106-file3.mp4,,,,,,,,,,,,,,,,
uistde107,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde107,A,HaptI/O: Physical Node for the Internet of Haptics,Satoshi,Matsuzono,s.matsuzono@kmd.keio.ac.jp,uistde0107-paper.pdf,3,letter,Arial-BoldMT TimesNewRomanPS-BoldMT ArialMT TimesNewRomanPSMT TimesNewRomanPSMT,,"Satoshi Matsuzono, Haruki Nakamura, Daiya Kato, Roshan Peiris, Kouta Minamizawa","s.matsuzono@kmd.keio.ac.jp, h.nakamura@kmd.keio.ac.jp, i.mas.trunk@kmd.keio.ac.jp, roshan@kmd.keio.ac.jp, kouta@kmd.keio.ac.jp",72838,Satoshi,,Matsuzono,s.matsuzono@kmd.keio.ac.jp,Keio University Graduate School of Media Design,Graduate School,Yokohama,Kanagawa,Japan,,,,,,73584,Haruki,,Nakamura,h.nakamura@kmd.keio.ac.jp,Keio University Graduate School of Media Design,Graduate School,Yokohama,Kanagawa,Japan,,,,,,73585,Daiya,,Kato,i.mas.trunk@kmd.keio.ac.jp,Keio University Graduate School of Media Design,Graduate School,Yokohama,Kanagawa,Japan,,,,,,53252,Roshan,,Peiris,roshan@kmd.keio.ac.jp,Keio University Graduate School of Media Design,Graduate School,Yokohama,Kanagawa,Japan,,,,,,62190,Kouta,,Minamizawa,kouta@kmd.keio.ac.jp,Keio University Graduate School of Media Design,Graduate School,Yokohama,Kanagawa,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose the concept of the ""Internet of Haptics"" (IoH) that enables sharing experiences of others with the sense of touch. IoH allows to multicast haptic sensation from one Sensor-Node (Inter-Node) to multiple Actuator-Node (Ceive-Node) and to multicast from multiple Inter-Node to multiple Ceive-Node via the Internet. As a proof of concept, we developed the ""HaptI/O"" device which is a physical network node that can perform as a gateway to input or output the haptic information to/from the human body or tangible objects. We use the WebRTC as the baseline protocol for communication. Users can gain access IoH Web using a smartphone or PC and experience the haptic sensation by selecting the Inter-Node and Ceive-Node from a web browser. Multiple HaptI/O would be connected on the IoH server and transmit the haptic information from one node to multiple nodes as well as one to one mutual connection so that HaptI/O enables us to share our experiences with the sense of touch.",s.matsuzono@kmd.keio.ac.jp,"1. Minamizawa, K., Kakehi, Y., Nakatani, M., Mihara, S., & Tachi, S. (2012, August). TECHTILE toolkit: a prototyping tool for designing haptic media. In ACM SIGGRAPH 2012 Emerging Technologies (p. 22). ACM. \ 2. Maeda, T., Tsuchiya, K., Peiris, R., Tanaka, Y., & Minamizawa, K. (2017, March). HapticAid: Haptic Experiences System Using Mobile Platform. In Proceedings of the Tenth International Conference on Tangible, Embedded, and Embodied Interaction (pp. 397402). ACM. \ 3. Aijaz, A., Dohler, M., Aghvami, A. H., Friderikos, V., & Frodigh, M. (2017). Realizing the tactile internet: Haptic communications over next generation 5G cellular networks. IEEE Wireless Communications, 24(2), 82-89. \",Haptics; Internet of Things; Embodied Media,"C.2.5 Local and Wide-Area Networks: Internet (e.g., TCP/IP). H.5.2 User Interfaces (D.2.2, H.1.2, I.3.6): Haptic I/O.",uistde0107-file1.doc,,,,,,,,,,,,,,,,,,
uistde108,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde108,A,Tactile Element with Double-sided Inkjet Printing to Generate Electrostatic Forces and Electrostimuli,Kunihiro,Kato,kunihiro162@gmail.com,uistde0108-paper.pdf,3,letter,,,"Kunihiro Kato, Homei Miyashita, Hiroyuki Kajimoto, Hiroki Ishizuka","kunihiro162@gmail.com, homei@homei.com, kajimoto@kaji-lab.jp, hi1124@eng.kagawa-u.ac.jp",41251,Kunihiro,,Kato,kunihiro162@gmail.com,,Meiji University,Nakano,Tokyo,Japan,,,,,,36764,Homei,,Miyashita,homei@homei.com,Meiji University,"4-21-1,Nakano,",Nakano-ku,Tokyo,Japan,,,,,,14647,Hiroyuki,,Kajimoto,kajimoto@kaji-lab.jp,The University of Electro-communications/Kajimoto-Lab,University,Choufu,Choufugaoka,Japan,,,,,,73633,Hiroki,,Ishizuka,hi1124@eng.kagawa-u.ac.jp,,Kagawa University,Takamatsu,Kagawa,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a tactile element that can generate both an electrostatic force and an electrostimulus, and can be used to provide tactile feedback on a wide area of human skin such as the palm of the hand. Touching the flat surface through the our proposed tactile element allow the user to feel both uneven and rough textures. In addition, the element can be fabricated using double-sided inkjet printing with conductive ink. Use of a flexible substrate, such as a PET film or paper, allows the user to design a free-formed tactile element. In this demonstration, we describe the implementation of the proposed stimuli element and show examples of applications.",kunihiro162@gmail.com,"1. Lynette A. Jones and Susan J. Lederman. 2006. Human Hand Function. 1st ed. USA: Oxford University Press. \ 2. Takafumi Aoki, Hironori Mitake, Danial Keoki, Shoichi Hasegawa and Makoto Sato. 2009. Wearable Haptic Device to Present Contact Sensation Based on Cutaneous Sensation Using Thin Wire. In Proceedings of the International Conference on Advances in Computer Entertainment Technology (ACEâ€™09), pp.115â€“122. https://doi.org/10.1145/1690388.1690408 \ 3. Olivier Bau, Ivan Poupyrev, Ali Israr and Chris Harrison. 2010. TeslaTouch: Electrovibration for Touch Surfaces. In Proceedings of the 23rd annual ACM symposium on User Interface Software and Technology (UISTâ€™10), pp.283â€“292. https://doi.org/10.1145/1866029.1866074 \ 4. Takuma Hirotsu, Syoki Kitaguchi, Kunihiro Kato, Homei Miyashita, Hiroyuki Kajimoto and Hiroki Ishizuka. 2017. Paper-Based Tactile Display for Multiple Tactile Stimulation. IEEE World Haptics 2017 Demonstrations. \ 5. Hiroyuki Kajimoto. 2012. Design of Cylindrical Whole-Hand Haptic Interface Using Electrocutaneous Display. In Proceedings of the International Conference on Human Haptic Sensing and Touch Enabled Computer Applications (EuroHapticsâ€™12), https://doi.org/10.1007/978-3-642-31404-9_12, pp.67â€“72. \ 6. Laura Winï¬eld, John Glassmire and J. Edward Colgate. 2007. T-PaD: Tactile Pattern Display through Variable Friction Reduction. In Proceedings of EuroHaptics Conference, 2007 and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems. World Haptics 2007. Second Joint, pp.421â€“426. . https://doi.org/10.1109/WHC.2007.105 \ 7. Edward Mallinckrodt, A. L. Hughes and William Sleator. 1953. Perception by the Skin of Electrically Induced Vibrations. Journal of Science , Vol. 118, Issue 3062, pp. 277-278. https://doi.org/10.1126/science.118.3062.277 \ 8. Dongbum Pyo, Semin Ryu, Seung-Chan Kim and Dong-Soo Kwon. 2014. A New Surface Display for 3D Haptic Rendering. In Proceedings of the International Conference on Human Haptic Sensing and Touch Enabled Computer Applications (EuroHapticsâ€™14), pp.487â€“495. https://doi.org/10.1007/978-3-662-44193-0_61 \ 9. Qi Wang, V. Hayward. 2006. Compact, Portable, Modular, High-performance, Distributed Tactile Transducer Device Based on Lateral Skin Deformation. In Proceedings of Haptic Interfaces for Virtual Environment and Teleoperator Systems, pp.67â€“72. https://doi.org/10.1109/HAPTIC.2006.1627091 \ 10. Vibol Yem and Hiroyuki Kajimoto. 2017. Wearable Tactile Device using Mechanical and Electrical Stimulation for Fingertip Interaction with Virtual World. In Proceedings of IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VRâ€™17). https://doi.org/10.1109/VR.2017.7892236 \ 11. Andrew Y. J. Szeto, John Lyman and Ronald E. Prior. 1979. Electrocutaneous Pulse Rate and Pulse Width Psychometric Functions for Sensory Communications. Journal of the Human Factors and Ergonomics Society, Vol 21, Issue 2, pp.241â€“249. https://doi.org/10.1177/001872087902100212 \",Tactile Display; electrostatic force; electro stimulus; conductive ink; inkjet printer; flexible device,H.5.2.,uistde0108-file1.zip,uistde0108-file2.jpg,,,,,,,,,,,,,,,,,
uistde110,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde110,A,Printing System Reflecting User's Intent in Real Time Using a Handheld Printer,Yuya,Kitazawa,12.5-grdr@akane.waseda.jp,uistde0110-paper.pdf,3,letter,,,"Yuya Kitazawa, Tomoko Hashida","12.5-grdr@akane.waseda.jp, hashida@waseda.jp",72877,Yuya,,Kitazawa,12.5-grdr@akane.waseda.jp,school of Fundamental Science and Engineering/ Waseda university/ Tomoko Hashida Lab,Waseda University,Shinjuku-ku,Tokyo,Japan,,,,,,23557,Tomoko,,Hashida,hashida@waseda.jp,School of Fundamental Science and Engineering,Waseda University,"3-4-1 Okubo, Shinjuku-ku",Tokyo,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a new type of printing system that incorporates sensors in a handheld printer to reflect in real time user intent in the results of printing on paper. This system achieves two key functions: ""real-time embellishment"" for altering printed content by reading user hand movements by pressure and optical sensors, and ""local transcription"" for selecting content to be output by tracing existing content on paper with a linear camera. We performed experiments to measure the accuracy of both techniques and evaluate their usefulness.",drrtodr@gmail.com,"1. Endo T., et al., The Way of TypeTraceâ€”Maijo Otaro Edition (in Japanese) http://www.ntticc.or.jp/ja/feature/2012/Internet_Art_ Future/Works/work05_j.html (Accessed in January 2017). \ 2. Yoshida H., et al., Examination about the Input Method Using Pressure Sensor Supplementary to Select from Some Candidates, IPSJ SIG Technical Report, Human Computer Interaction (HCI), 2010HCI-140, 2010, pp. 1â€“8. (in Japanese) \ 3. Nakagaki K., et al., COMP*PASS: A Compass-based Drawing Interface. CHI EA '14: CHI '14 Extended Abstracts on Human Factors in Computing Systems. April 2014. \ 4. Hyunjung K., et al., Digital Rubbing: Playful and Intuitive Interaction Technique for Transferring a Graphic Image onto Paper with Pen-Based Computing. CHI EA '08: CHI '08 Extended Abstracts on Human Factors in Computing Systems. March 2008. \",printer; real-time embellishment; local transcription,H.5.2,uistde0110-file1.doc,uistde0110-file2.jpg,,,,,,,,,,,,,,,,,
uistde112,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde112,A,"CritiqueKit: A Mixed-Initiative, Real-Time Interface For Improving Feedback",C. Ailie,Fraser,cafraser@cs.ucsd.edu,uistde0112-paper.pdf,3,letter,,,"C. Ailie Fraser, Tricia Ngoon, Ariel Weingarten, Mira Dontcheva, Scott Klemmer","cafraser@cs.ucsd.edu, tngoon@ucsd.edu, aweingar@eng.ucsd.edu, mirad@adobe.com, srk@ucsd.edu",55131,C. Ailie,,Fraser,cafraser@cs.ucsd.edu,The Design Lab,UC San Diego,La Jolla,California,United States,,,,,,64295,Tricia,,Ngoon,tngoon@ucsd.edu,The Design Lab,UC San Diego,La Jolla,California,United States,,,,,,69198,Ariel,,Weingarten,aweingar@eng.ucsd.edu,Design Lab,"University of California, San Diego",La Jolla,California,United States,,,,,,5854,Mira,,Dontcheva,mirad@adobe.com,,Adobe Research,Seattle,Washington,United States,,,,,,1104,Scott,,Klemmer,srk@ucsd.edu,The Design Lab,UC San Diego,La Jolla,California,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present CritiqueKit, a mixed-initiative machine-learning system that helps students give better feedback to peers by reusing prior feedback, reducing it to be useful in a general context, and retraining the system about what is useful in real time. CritiqueKit exploits the fact that novices often make similar errors, leading reviewers to reuse the same feedback on many different submissions. It takes advantage of all prior feedback, and classifies feedback as the reviewer types it. CritiqueKit continually updates the corpus of feedback with new comments that are added, and it guides reviewers to improve their feedback, and thus the entire corpus, over time.",cafraser@cs.ucsd.edu,"1. 2017. Gradescope. (2017). https://gradescope.com \ 2. Michael D. Greenberg, Matthew W. Easterday, and Elizabeth M. Gerber. 2015. Critiki: A Scaffolded Approach to Gathering Design Feedback from Paid Crowdworkers. In Proceedings of the 2015 ACM SIGCHI Conference on Creativity and Cognition - C&C â€™15. 235â€“244. \ 3. J. Hattie and H. Timperley. 2007. The Power of Feedback. Review of Educational Research 77, 1 (mar 2007), 81â€“112. \ 4. David Kelley and Tom Kelley. 2013. Creative conï¬dence: Unleashing the creative potential within us all. Crown Publishing Group, New York. 288 pages. \ 5. Chinmay Kulkarni, Koh Pang Wei, Huy Le, Daniel Chia, Kathryn Papadopoulos, Justin Cheng, Daphne Koller, and Scott R. Klemmer. 2013. Peer and self assessment in massive online classes. ACM Transactions on Computer-Human Interaction 20, 6 (Dec 2013), 1â€“31. \ 6. Chinmay E. Kulkarni, Michael S. Bernstein, and Scott R. Klemmer. 2015. PeerStudio: Rapid Peer Feedback Emphasizes Revision and Improves Performance. In Proceedings of the Second (2015) ACM Conference on Learning @ Scale - L@S â€™15. 75â€“84. \ 7. Huy Nguyen, Wenting Xiong, and Diane Litman. 2016. Instant Feedback for Increasing the Presence of Solutions in Peer Reviews. HLT-NAACL Demos (2016), 6â€“10. \ 8. D. Royce Sadler. 1989. Formative assessment and the design of instructional systems. Instructional Science 18, 2 (Jun 1989), 119â€“144. \ 9. Donald A. SchÃ¶n. 1985. The design studio: An exploration of its traditions and potentials. RIBA Publications for RIBA Building Industry Trust. 99 pages. \ 10. W Xiong, D. Litman, and C. Schunn. 2012. Natural Language Processing techniques for researching and improving peer feedback. Journal of Writing Research 4, 2 (Nov 2012), 155â€“176. \ 11. Alvin Yuan, Kurt Luther, Markus Krause, Sophie Isabel Vennix, Steven P Dow, and BjÃ¶rn Hartmann. 2016. Almost an Expert: The Effects of Rubrics and Expertise on Perceived Value of Crowdsourced Design Critiques. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing - CSCW â€™16. 1003â€“1015. \",feedback; critique; real-time classification; educational technology,H.5.m,uistde0112-file1.zip,,uistde0112-file3.mp4,,,,,,,,,,,,,,,,
uistde115,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde115,A,shapeShift: A Mobile Tabletop Shape Display for Tangible and Haptic Interaction,Alexa,Siu,afsiu@stanford.edu,uistde0115-paper.pdf,3,letter,,,"Alexa Siu, Eric Gonzalez, Shenli Yuan, Jason Ginsberg, Allen Zhao, Sean Follmer","afsiu@stanford.edu, Ejgonz@stanford.edu, shenliy@stanford.edu, jasong2@stanford.edu, judule1@gmail.com, sfollmer@stanford.edu",69483,Alexa,,Siu,afsiu@stanford.edu,Mechanical Engineering,Stanford University,Stanford,California,USA,,,,,,69615,Eric,,Gonzalez,Ejgonz@stanford.edu,Mechanical Engineering,Stanford University,Stanford,California,United States,,,,,,69616,Shenli,,Yuan,shenliy@stanford.edu,Mechanical Engineering,Stanford University,Stanford,California,United States,,,,,,73529,Jason,,Ginsberg,jasong2@stanford.edu,,Stanford University,Stanford,California,United States of America,,,,,,73601,Allen,,Zhao,judule1@gmail.com,Symbolic Systems/SHAPE Lab,Stanford University,Stanford,California,United States,,,,,,15323,Sean,,Follmer,sfollmer@stanford.edu,Mechanical Engineering,Stanford University,Palo Alto,California,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"shapeShift is a compact, high-resolution (7mm pitch), mobile tabletop shape display. We explore potential interaction techniques in both passive and active mobile scenarios. In the passive case, the user is able to freely move and spin the display as it renders elements. We introduce use cases for rendering lateral I/O elements, exploring volumetric datasets, and grasping and manipulating objects. On an active omnidirectional-robot platform, shapeShift can display moving objects and provide both vertical and lateral force feedback. We use the active platform as an encounter-type haptic device combined with a head-mounted display to dynamically simulate the presence of virtual content.",afsiu@stanford.edu,"1. Benko, H., Holz, C., Sinclair, M., and Ofek, E. Normaltouch and texturetouch: High-ï¬delity 3d haptic shape rendering on handheld virtual reality controllers. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, ACM (2016), 717â€“728. \ 2. Follmer, S., Leithinger, D., Olwal, A., Hogge, A., and Ishii, H. inform: Dynamic physical affordances and constraints through shape and object actuation. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology, UIST â€™13, ACM (2013), 417â€“426. \ 3. Hirota, K., and Hirose, M. Simulation and presentation of curved surface in virtual reality environment through surface display. In Virtual Reality Annual International Symposium, 1995. Proceedings., IEEE (1995), 211â€“216. \ 4. Ikei, Y., and Shiratori, M. Textureexplorer: A tactile and force display for virtual textures. In Haptic Interfaces for Virtual Environment and Teleoperator Systems, 2002. HAPTICS 2002. Proceedings. 10th Symposium on, IEEE (2002), 327â€“334. \ 5. Iwata, H., Yano, H., Nakaizumi, F., and Kawamura, R. Project feelex: adding haptic surface to graphics. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, ACM (2001), 469â€“476. \ 6. Kyung, K.-U., Kim, S.-C., and Kwon, D.-S. Texture display mouse: vibrotactile pattern and roughness display. IEEE/ASME Transactions on Mechatronics 12, 3 (2007), 356â€“360. \ 7. Le Goc, M., Kim, L. H., Parsaei, A., Fekete, J.-D., Dragicevic, P., and Follmer, S. Zooids: Building blocks for swarm user interfaces. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, UIST â€™16, ACM (2016), 97â€“109. \ 8. Leithinger, D., Follmer, S., Olwal, A., and Ishii, H. Shape displays: Spatial interaction with dynamic physical form. IEEE computer graphics and applications 35, 5 (2015), 5â€“11. \ 9. McNeely, W. A. Robotic graphics: a new approach to force feedback for virtual reality. In Virtual Reality Annual International Symposium, 1993., 1993 IEEE, IEEE (1993), 336â€“341. \ 10. Poupyrev, I., Nashida, T., Maruyama, S., Rekimoto, J., and Yamaji, Y. Lumen: interactive visual and shape display for calm computing. In ACM SIGGRAPH 2004 Emerging technologies, ACM (2004), 17. \ 11. Seifert, J., Boring, S., Winkler, C., Schaub, F., Schwab, F., Herrdum, S., Maier, F., Mayer, D., and Rukzio, E. Hover pad: interacting with autonomous and self-actuated displays in space. In Proceedings of the 27th annual ACM symposium on User interface software and technology, ACM (2014), 139â€“147. \ 12. Steimle, J., Jordt, A., and Maes, P. Flexpad: highly ï¬‚exible bending interactions for projected handheld displays. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM (2013), 237â€“246. \ 13. Wagner, C. R., Lederman, S. J., and Howe, R. D. A tactile shape display using rc servomotors. In Haptic Interfaces for Virtual Environment and Teleoperator Systems, 2002. HAPTICS 2002. Proceedings. 10th Symposium on, IEEE (2002), 354â€“355. \ 14. Weiss, M., Schwarz, F., Jakubowski, S., and Borchers, J. Madgets: actuating widgets on interactive tabletops. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, ACM (2010), 293â€“302. \ 15. Yokokohji, Y., Hollis, R. L., and Kanade, T. What you can see is what you can feel-development of a visual/haptic interface to virtual environment. In Virtual Reality Annual International Symposium, 1996., Proceedings of the IEEE 1996, IEEE (1996), 46â€“53. \",Tangible User Interfaces; Shape Displays; Interactive Tabletop,H.5.2,uistde0115-file1.zip,uistde0115-file2.jpg,uistde0115-file3.mov,,,,,,,,,,,,,,,,
uistde118,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde118,A,JDLED: Towards Visio-Tactile Displays based on Electrochemical Locomotion of Liquid-Metal Janus Droplets,Deepak,Sahoo,D.R.Sahoo@swansea.ac.uk,uistde0118-paper.pdf,3,letter,,,"Deepak Sahoo, Timothy Neate, Yutaka Tokuda, Jennifer Pearson, Simon Robinson, Sriram Subramanian, Matt Jones","D.R.Sahoo@swansea.ac.uk, tdjneate@gmail.com, yutakamitsue@gmail.com, j.pearson@swan.ac.uk, s.n.w.robinson@swansea.ac.uk, sriram@sussex.ac.uk, always@acm.org",45808,Deepak,,Sahoo,D.R.Sahoo@swansea.ac.uk,Department of Computer Science,Swansea University,Swansea,,United Kingdom,,,,,,45783,Timothy,,Neate,tdjneate@gmail.com,Department of Computer Science,Swansea University,Swansea,,United Kingdom,,,,,,46249,Yutaka,,Tokuda,yutakamitsue@gmail.com,"Interact Lab, Engineering and Informatics",University of Sussex,Brighton,,United Kingdom,,,,,,14952,Jennifer,,Pearson,j.pearson@swan.ac.uk,,Swansea University,Swansea,,United Kingdom,,,,,,11354,Simon,,Robinson,s.n.w.robinson@swansea.ac.uk,,Swansea University,Swansea,Wales,United Kingdom,,,,,,2491,Sriram,,Subramanian,sriram@sussex.ac.uk,"Interact Lab, School of Engineering and Informatics",University of Sussex,Brighton,East Sussex,United Kingdom,,,,,,1008,Matt,,Jones,always@acm.org,,Swansea University,Swansea,Wales,United Kingdom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"An actuated shape-changing interface with faster response and smaller pixel size using a liquid material can provide real time tangible interaction with the digital world in physical space. To this end, we demonstrate an interface that displays user-defined patterns dynamically using liquid metal droplets as programmable micro robots on a flat surface. We built a prototype using an array of embedded electrodes and a switching circuit to control the jump of the droplets from electrode to electrode. The actuation and dynamics of the droplets under the finger provides mild tactile feedback to the user. Our demo is the first to show a planar visio-tactile display using liquid metal, and is a first step to make shape-changing physical ephemeral widgets on a tabletop interface.",deepakranjan@gmail.com,"WARNING: Reference 11 starts with a non-alphanumeric character. Please check it. \ \ 1. Choi, K., Ng, A. H., Fobel, R., and Wheeler, A. R. Digital microï¬‚uidics. Annual Review of Analytical Chemistry 5, 1 (2012), 413â€“440. PMID: 22524226. \ 2. Coelho, M., Ishii, H., and Maes, P. Surï¬‚ex: A programmable surface for the design of tangible interfaces. In CHI â€™08 Extended Abstracts on Human Factors in Computing Systems, CHI EA â€™08, ACM (New York, NY, USA, 2008), 3429â€“3434. \ 3. Follmer, S., Leithinger, D., Olwal, A., Hogge, A., and Ishii, H. inform: Dynamic physical affordances and constraints through shape and object actuation. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology, UIST â€™13, ACM (New York, NY, USA, 2013), 417â€“426. \ 4. Hardy, J., Weichel, C., Taher, F., Vidler, J., and Alexander, J. Shapeclip: Towards rapid prototyping with shape-changing displays for designers. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, CHI â€™15, ACM (New York, NY, USA, 2015), 19â€“28. \ 5. Harrison, C., and Hudson, S. E. Providing dynamically changeable physical buttons on a visual display. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI â€™09, ACM (New York, NY, USA, 2009), 299â€“308. \ 6. Hook, J., Taylor, S., Butler, A., Villar, N., and Izadi, S. A reconï¬gurable ferromagnetic input device. In Proceedings of the 22Nd Annual ACM Symposium on User Interface Software and Technology, UIST â€™09, ACM (New York, NY, USA, 2009), 51â€“54. \ 7. Ishii, H., Lakatos, D., Bonanni, L., and Labrune, J.-B. Radical atoms: Beyond tangible bits, toward transformable materials. interactions 19, 1 (Jan. 2012), 38â€“51. \ 8. Jansen, Y. Mudpad: Fluid haptics for multitouch surfaces. In CHI â€™10 Extended Abstracts on Human Factors in Computing Systems, CHI EA â€™10, ACM (New York, NY, USA, 2010), 4351â€“4356. \ 9. Leithinger, D., and Ishii, H. Relief: A scalable actuated shape display. In Proceedings of the Fourth International Conference on Tangible, Embedded, and Embodied Interaction, TEI â€™10, ACM (New York, NY, USA, 2010), 221â€“222. \ 10. Loget, G., Zigah, D., Boufï¬er, L., Sojic, N., and Kuhn, A. Bipolar electrochemistry: From materials science to motion and beyond. Accounts of Chemical Research 46, 11 (2013), 2513â€“2523. PMID: 23719628. \ 11. Lu, Q., Mao, C., Wang, L., and Mi, H. Lime: Liquid metal interfaces for non-rigid interaction. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, UIST â€™16, ACM (New York, NY, USA, 2016), 449â€“452. \ 12. Ochiai, Y., Hoshi, T., and Rekimoto, J. Pixie dust: Graphics generated by levitated and animated objects in computational acoustic-potential ï¬eld. ACM Trans. Graph. 33, 4 (July 2014), 85:1â€“85:13. \ 13. Ogata, M., and Fukumoto, M. Fluxpaper: Reinventing paper with dynamic actuation powered by magnetic ï¬‚ux. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, CHI â€™15, ACM (New York, NY, USA, 2015), 29â€“38. \ 14. Robinson, S., Coutrix, C., Pearson, J., Rosso, J., Torquato, M. F., Nigay, L., and Jones, M. Emergeables: Deformable displays for continuous eyes-free mobile interaction. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI â€™16, ACM (New York, NY, USA, 2016), 3793â€“3805. \ 15. Sahoo, D. R., HornbÃ¦k, K., and Subramanian, S. Tablehop: An actuated fabric display using transparent electrodes. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI â€™16, ACM (New York, NY, USA, 2016), 3767â€“3780. \ 16. Sahoo, D. R., Nakamura, T., Marzo, A., Omirou, T., Asakawa, M., and Subramanian, S. Joled: A mid-air display based on electrostatic rotation of levitated janus objects. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, UIST â€™16, ACM (New York, NY, USA, 2016), 437â€“448. \ 17. Wakita, A., Nakano, A., and Kobayashi, N. Programmable blobs: A rheologic interface for organic shape design. In Proceedings of the Fifth International Conference on Tangible, Embedded, and Embodied Interaction, TEI â€™11, ACM (New York, NY, USA, 2011), 273â€“276. \ 18. Yao, Y.-y., and Liu, J. A polarized liquid metal worm squeezing across a localized irregular gap. RSC Adv. 7 (2017), 11049â€“11056. \",Shape-changing Interface; Tactile Display; Liquid-metal; Gallium Indium Eutectic; Bipolar Electrochemistry; Electric-field Control; Janus Particle; Micro Robot,"H.5.2. Information Interfaces and Presentation (e.g. HCI):User Interfaces, Haptic I/O, Prototyping",uistde0118-file1.zip,uistde0118-file2.jpg,,,,,,,,,,,,,,,,,
uistde122,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde122,A,Multiplanes: Assisted Freehand VR Drawing,Mayra Donaji,Barrera Machuca,mbarrera@sfu.ca,uistde0122-paper.pdf,3,letter,,,"Mayra Donaji Barrera Machuca, Paul Asente, Jingwan Lu, Byungmoon Kim, Wolfgang Stuerzlinger","mbarrera@sfu.ca, asente@adobe.com, jingwan.lu.cynthia@gmail.com, bmkim@adobe.com, w.s@sfu.ca",62002,Mayra Donaji,,Barrera Machuca,mbarrera@sfu.ca,"School of Interactive Arts and Technology, Simon Fraser University, Vancouver, British Columbia, Canada",,,,,,,,,,51655,Paul,,Asente,asente@adobe.com,Adobe Research,Adobe,San Jose,California,United States,,,,,,40664,Jingwan,,Lu,jingwan.lu.cynthia@gmail.com,,Adobe Research,San Jose,California,United States,,,,,,54410,Byungmoon,,Kim,bmkim@adobe.com,Adobe Research,"Adobe Systems, Inc",San Jose,California,United States,,,,,,1316,Wolfgang,,Stuerzlinger,w.s@sfu.ca,School of Interactive Arts + Technology (SIAT),Simon Fraser University,Vancouver,British Columbia,Canada,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Multiplanes is a virtual reality (VR) drawing system that provides users with the flexibility of freehand drawing and the ability to draw perfect shapes. Through the combination of both beautified and 2D drawing, Multiplanes addresses challenges in creating 3D VR drawings. To achieve this, the system beautifies user's strokes based on the most probable, intended shapes while the user is drawing them. It also automatically generates snapping planes and beautification trigger points based on previous and current strokes and the current controller pose. Based on geometrical relationships to previous strokes, beautification trigger points act as guides inside the virtual environment. Users can hit these points to (explicitly) trigger a stroke beautification. In contrast to other systems, when using Multiplanes, users do not need to manually set or do any kind of special gesture to activate, such guides allowing the user to focus on the creative process.",mbarrera@sfu.ca,"1. Rahul Arora, Rubaiat H. Kazi, Fraser Anderson, Tovi Grossman, Karan Singh, and George Fitzmaurice. 2017. Experimental Evaluation of Sketching on Surfaces in VR. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17). ACM, New York, NY, USA, 5643-5654. DOI: https://doi.org/10.1145/3025453.3025474 \ 2. Seok-Hyung Bae, Ravin Balakrishnan, and Karan Singh. 2008. ILoveSketch: as-natural-as-possible sketching system for creating 3d curve models. Proceedings of the 21st annual ACM symposium on User interface software and technology (UISTâ€™2008). ACM, 151â€“ 160. \ 3. Eric. A. Bier. 1990. Snap-Dragging in three Dimensions. Computer Graphics, Symposium on Interactive 30 Graphics â€˜90, 24(2), March 1990, pp. 193-204. \ 4. Jonathan M. Cohen, Lee Markosian, Robert C. Zeleznik, John F. Hughes, and Ronen Barzel. 1999. An interface for sketching 3D curves. In Proceedings of the 1999 symposium on Interactive 3D graphics (I3D '99). ACM, New York, NY, USA, 17-21. \ 5. Jakub FiÅ¡er, Paul Asente, and Daniel SÃ½kora. 2015. ShipShape: a drawing beautification assistant. In Proceedings of the workshop on Sketch-Based Interfaces and Modeling (SBIM '15). Eurographics Association, Aire-laVille, Switzerland, Switzerland, 49-57. \ 6. Gravity Sketch, https://www.gravitysketch.com/ \ 7. Cindy Grimm and Pushkar Joshi. 2012. Just DrawIt: A 3D sketching system. In Proceedings of the International Symposium on Sketch-Based Interfaces and Modeling. Eurographics Association, Aire-la-Ville, Switzerland, 121â€“130. \ 8. Tovi Grossman, Ravin Balakrishnan, Gordon Kurtenbach, George W. Fitzmaurice, Azam Khan, and Bill Buxton. 2002. Creating principal 3D curves with digital tape drawing. ACM In Proceedings of the 2002 CHI Conference on Human Factors in Computing Systems. 121-128. \ 9. Tovi Grossman, Ravin Balakrishnan, Gordon Kurtenbach, George W. Fitzmaurice, Azam Khan, and Bill Buxton. 2001. Interaction Techniques for 3D Modeling on Large Displays. I3D, 17-23 \ 10. Johann H. Israel, Eva Wiese, Magdalena Mateescu, C. ZÃ¶llner, and Rainer Stark. 2009. Investigating three-dimensional sketching for early conceptual designâ€”Results from expert discussions and user studies. Computers & Graphics 33, 4: 462â€“473. http://doi.org/10.1016/j.cag.2009.05.005 \ 11. Bret Jackson and Daniel F. Keefe. 2016. Lift-Off: Using Reference Imagery and Freehand Sketching to Create 3D Models in VR. IEEE Transactions on Visualization and Computer Graphics 22, 4: 1442â€“1451. \ 12. Michele Fiorentino, Giuseppe Monno, Pietro A. Renzulli, and Antonio E. Uva. 2007. 3D Sketch Stroke Segmentation and Fitting in Virtual Reality. In IEEE Transactions on Visualization and Computer Graphics \ 13. Daniel F. Keefe, Robert C. Zeleznik, and David H. Laidlaw. 2007. Drawing on air: Input techniques for controlled 3d line illustration. IEEE Trans. Vis. Computer Graph., 13(5):1067â€“1081. \ 14. Dominik. Rausch, Ingo Assenmacher, and Torsten Kuhlen. 2010. 3D Sketch Recognition for Interaction in Virtual Environments. Workshop on Virtual Reality Interaction and Physical Simulation VRIPHYS (2010) \ 15. Ryan Schmidt, Azam Khan, Gord Kurtenbach, and Karan Singh. 2009. On Expert Performance in 3D Curve-drawing Tasks. Proceedings of the 6th Eurographics Symposium on Sketch-Based Interfaces and Modeling (SBIM â€™09), ACM, 133â€“140. \ 16. Steven Schkolne, Michael Pruett, and Peter Schroeder. 2001. Surface drawing: Creating organic 3D shapes with the hand and tangible tools. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI â€™01) ACM, 261-268. \ 17. Ryan Schmidt, Azam Khan, Karan Singh, and Gordon Kurtenbach. 2009. Analytic drawing of 3D scaffolds. ACM Transactions on Graphics, 28, 5. \ 18. Tilt brush https://www.tiltbrush.com/ \ 19. Steve Tsang, Ravin Balakrishnan, Karan Singh, and Abhishek Ranjan. 2004. A Suggestive Interface for Image Guided 3D Sketching. In Proceedings of the Conference on Human Factors in Computing Systems (CHIâ€™04), ACM, 591-598. \ 20. Gerold Wesche and Hans-Peter Seidel. 2001. FreeDrawer: a free-form sketching system on the responsive workbench. ACM Press, 167. http://doi.org/10.1145/505008.505041 \",Virtual Reality Drawing; Sketching; 3D User Interfaces,H.5.2,uistde0122-file1.docx,uistde0122-file2.jpg,,,,,,,,,,,,,,,,,
uistde124,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde124,A,PhyShare: Sharing Physical Interaction in Virtual Reality,zhenyi,he,snowymohehe@gmail.com,uistde0124-paper.pdf,3,letter,,,"Zhenyi He, Fengyuan Zhu, Ken Perlin","snowymohehe@gmail.com, zhufyaxel@gmail.com, ken.perlin@gmail.com",45561,Zhenyi,,He,snowymohehe@gmail.com,,New York University,New York,New York,United States,,,,,,46313,Fengyuan,,Zhu,zhufyaxel@gmail.com,Interactive Telecommunication Program,New York University,New York,New York,United States,,,,,,5774,Ken,,Perlin,ken.perlin@gmail.com,,New York University,New York City,New York,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present PhyShare, a new haptic user interface based on actuated robots. Virtual reality has recently been gaining wide adoption, and an effective haptic feedback in these scenarios can strongly support user's sensory in bridging virtual and physical world. Since participants do not directly observe these robotic proxies, we investigate the multiple mappings between physical robots and virtual proxies that can utilize the resources needed to provide a well rounded VR experience. PhyShare bots can act either as directly touchable objects or invisible carriers of physical objects, depending on different scenarios. They also support distributed collaboration, allowing remotely located VR collaborators to share the same physical feedback.",snowymohehe@gmail.com,"1. Araujo, B., Jota, R., Perumal, V., Yao, J. X., Singh, K., and Wigdor, D. Snake charmer: Physically enabling virtual objects. In Proceedings of the TEIâ€™16: Tenth International Conference on Tangible, Embedded, and Embodied Interaction, ACM (2016), 218â€“226. \ 2. Benko, H., Holz, C., Sinclair, M., and Ofek, E. Normaltouch and texturetouch: High-ï¬delity 3d haptic shape rendering on handheld virtual reality controllers. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, ACM (2016), 717â€“728. \ 3. Brave, S., Ishii, H., and Dahley, A. Tangible interfaces for remote collaboration and communication. In Proceedings of the 1998 ACM conference on Computer supported cooperative work, ACM (1998), 169â€“178. \ 4. Cheng, L.-P., Roumen, T., Rantzsch, H., KÂ¨ohler, S., Schmidt, P., Kovacs, R., Jasper, J., Kemper, J., and Baudisch, P. Turkdeck: Physical virtual reality based on people. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology, ACM (2015), 417â€“426. \ 5. Follmer, S., Leithinger, D., Olwal, A., Hogge, A., and Ishii, H. inform: dynamic physical affordances and constraints through shape and object actuation. In Uist, vol. 13 (2013), 417â€“426. \ 6. Gauglitz, S., Nuernberger, B., Turk, M., and HÂ¨ollerer, T. World-stabilized annotations and virtual scene navigation for remote collaboration. In Proceedings of the 27th annual ACM symposium on User interface software and technology, ACM (2014), 449â€“459. \ 7. Ishii, H. Tangible bits: beyond pixels. In Proceedings of the 2nd international conference on Tangible and embedded interaction, ACM (2008), xvâ€“xxv. \ 8. Ishii, H., and Ullmer, B. Tangible bits: towards seamless interfaces between people, bits and atoms. In Proceedings of the ACM SIGCHI Conference on Human factors in computing systems, ACM (1997), 234â€“241. \ 9. Iwata, H., Yano, H., Fukushima, H., and Noma, H. Circulaï¬‚oor [locomotion interface]. IEEE Computer Graphics and Applications 25, 1 (2005), 64â€“67. \ 10. Kajita, H., Koizumi, N., and Naemura, T. Skyanchor: Optical design for anchoring mid-air images onto physical objects. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, ACM (2016), 415â€“423. \ 11. Le Goc, M., Kim, L. H., Parsaei, A., Fekete, J.-D., Dragicevic, P., and Follmer, S. Zooids: Building blocks for swarm user interfaces. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, ACM (2016), 97â€“109. \ 12. Leithinger, D., Follmer, S., Olwal, A., and Ishii, H. Physical telepresence: shape capture and display for embodied, computer-mediated remote collaboration. In Proceedings of the 27th annual ACM symposium on User interface software and technology, ACM (2014), 461â€“470. \ 13. Leithinger, D., Follmer, S., Olwal, A., and Ishii, H. Shape displays: Spatial interaction with dynamic physical form. IEEE computer graphics and applications 35, 5 (2015), 5â€“11. \ 14. McNeely, W. A. Robotic graphics: A new approach to force feedback for virtual reality. In Virtual Reality Annual International Symposium, 1993., 1993 IEEE, IEEE (1993), 336â€“341. \ 15. Orts-Escolano, S., Rhemann, C., Fanello, S., Chang, W., Kowdle, A., Degtyarev, Y., Kim, D., Davidson, P. L., Khamis, S., Dou, M., et al. Holoportation: Virtual 3d teleportation in real-time. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, ACM (2016), 741â€“754. \ 16. Otsuki, M., Sugihara, K., Kimura, A., Shibata, F., and Tamura, H. Mai painting brush: an interactive device that realizes the feeling of real painting. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, ACM (2010), 97â€“100. \ 17. Pedersen, E. W., and HornbÃ¦k, K. Tangible bots: interaction with active tangibles in tabletop interfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM (2011), 2975â€“2984. \ 18. Riedenklau, E., Hermann, T., and Ritter, H. An integrated multi-modal actuated tangible user interface for distributed collaborative planning. In Proceedings of the Sixth International Conference on Tangible, Embedded and Embodied Interaction, ACM (2012), 169â€“174. \ 19. Rosenfeld, D., Zawadzki, M., Sudol, J., and Perlin, K. Physical objects as bidirectional user interface elements. IEEE Computer Graphics and Applications 24, 1 (2004), 44â€“49. \ 20. Sra, M. Asymmetric design approach and collision avoidance techniques for room-scale multiplayer virtual reality. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, ACM (2016), 29â€“32. \ 21. Sra, M., Jain, D., Caetano, A. P., Calvo, A., Hilton, E., and Schmandt, C. Resolving spatial variation and allowing spectator participation in multiplayer vr. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, ACM (2016), 221â€“222. \ 22. Sra, M., and Schmandt, C. Metaspace: Full-body tracking for immersive multiperson virtual reality. In Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology, ACM (2015), 47â€“48. \ 23. Sugihara, K., Otsuki, M., Kimura, A., Shibata, F., and Tamura, H. Mai painting brush++: Augmenting the feeling of painting with new visual and tactile feedback mechanisms. In Proceedings of the 24th annual ACM symposium adjunct on User interface software and technology, ACM (2011), 13â€“14. \",Virtual Reality; Haptic User Interfaces; Robots;,H.5.m,uistde0124-file1.zip,uistde0124-file2.jpg,,,,,,,,,,,,,,,,,
uistde125,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde125,A,SkinBot: A Wearable Skin Climbing Robot,Artem,Dementyev,artemd@mit.edu,uistde0125-paper.pdf,3,letter,,,"Artem Dementyev, Javier Hernandez, Sean Follmer, Inrak Choi, Joseph Paradiso","artemd@mit.edu, javierhr@mit.edu, sfollmer@stanford.edu, irchoi@stanford.edu, joep@media.mit.edu",40824,Artem,,Dementyev,artemd@mit.edu,Media Lab/Responsive Environments,MIT,Cambridge,MA,USA,,,,,,19858,Javier,,Hernandez,javierhr@mit.edu,Media Lab,Massachusetts Institute of Technology,Cambridge,Massachusetts,United States,,,,,,15323,Sean,,Follmer,sfollmer@stanford.edu,Mechanical Engineering,Stanford University,Palo Alto,California,United States,,,,,,60335,Inrak,,Choi,irchoi@stanford.edu,Mechanical Engineering / Stanford University,Stanford University,Stanford,CA,USA,,,,,,4140,Joseph,,Paradiso,joep@media.mit.edu,MIT Media Lab,Massachusetts Institute of Technology,Cambridge,Massachusetts,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We introduce SkinBot; a lightweight robot that moves over the skin surface with a two-legged suction-based locomotion mechanism and that captures a wide range of body parameters with an exchangeable multipurpose sensing module. We believe that robots that live on our skin such as SkinBot will enable a more systematic study of the human body and will offer great opportunities to advance many areas such as telemedicine, human-computer interfaces, body care, and fashion.",artemd@mit.edu,"1. Birkmeyer, P., Gillies, A. G., and Fearing, R. S. Clash: Climbing vertical loose cloth. In Intelligent Robots and Systems (IROS), IEEE (2011), 5087â€“5093. \ 2. Briones, L., Bustamante, P., and Serna, M. A. Wall-climbing robot for inspection in nuclear power plants. In Proc. of Robotics and Automation, IEEE (1994), 1409â€“1414. \ 3. Chen, G., Liu, Y., Fu, R., Sun, J., Wu, X., and Xu, Y. Rubbot: Rubbing on ï¬‚exible loose surfaces. In Intelligent Robots and Systems, IEEE (2013), 2303â€“2308. \ 4. Daltorio, K. A., Horchler, A. D., Gorb, S., Ritzmann, R. E., and Quinn, R. D. A small wall-walking robot with compliant, adhesive feet. In Intelligent Robots and Systems, IEEE (2005), 3648â€“3653. \ 5. Dementyev, A., Kao, H.-L. C., Choi, I., Ajilo, D., Xu, M., Paradiso, J. A., Schmandt, C., and Follmer, S. Rovables: Miniature on-body robots as mobile wearables. In Symposium on User Interface Software and Technology, ACM (2016), 111â€“120. \ 6. Eich, M., and VÂ¨ogele, T. Design and control of a lightweight magnetic climbing robot for vessel inspection. In Control & Automation (MED), IEEE (2011), 1200â€“1205. \ 7. Hernandez, J., McDuff, D. J., and Picard, R. W. Biophone: Physiology monitoring from peripheral smartphone motions. In Int. Conf. of the IEEE Eng. in Med. and Biol. Soc. (2015), 7180â€“7183. \ 8. Kim, S., Spenko, M., Trujillo, S., Heyneman, B., Santos, D., and Cutkosky, M. R. Smooth vertical surface climbing with directional adhesion. IEEE Transactions on robotics 24, 1 (2008), 65â€“74. \ 9. Liu, Y., Wu, X., Qian, H., Zheng, D., Sun, J., and Xu, Y. System and design of clothbot: A robot for ï¬‚exible clothes climbing. In Robotics and Automation, IEEE (2012), 1200â€“1205. \ 10. Menon, C., Murphy, M., and Sitti, M. Gecko inspired surface climbing robots. In Robotics and Biomimetics, IEEE (2004), 431â€“436. \",Skin; robotics; wearable devices; telemedicine,H.5.m.,uistde0125-file1.zip,,uistde0125-file3.mp4,,,,,,,,,,,,,,,,
uistde127,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde127,A,Filum: A Sewing Technique to Alter Textile Shapes,Tomomi,KONO,icstomo@gmail.com,uistde0127-paper.pdf,3,letter,,,"Tomomi Kono, Keita Watanabe","icstomo@gmail.com, watanabe@gmail.com",73542,Tomomi,,Kono,icstomo@gmail.com,Interaction Design Lab,Meiji University,Nakano,Tokyo,Japan,,,,,,17727,Keita,,Watanabe,watanabe@gmail.com,,Meiji University,Nakano,Tokyo,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a novel shape-changing technique called Filum, which makes it possible to alter the shapes of textiles to better suit the requirements of people and the environment. Using strings and various sewing methods, ordinary textiles can be automatically shortened or shrunk into curved shapes. We demonstrate a series of novel interactions between people and textiles via three applications.",icstomo@gmail.com,"1. ANREALAGE, http://www.anrealage.com/ \ 2. Bern, J.M., Chang, K.H. and Coros, S. Interactive design of animated plushies. ACM Transactions on Graphics (TOG) 2017, 36(4), 80. \ 3. Coelho, M., Ishii, H. and Maes, P. Surflex: a programmable surface for the design of tangible interfaces. Ext. Abstracts CHI 2008, 3429-3434. \ 4. Custodio, V., Herrera, F. J., LÃ³pez, G. and Moreno, J. I. A review on architectures and communications technologies for wearable health-monitoring systems. Sensors 12, 10 (2012), 13907-13946. \ 5. Jovanov, E., Milenkovic, A., Otto, C. and De Groen, P. C. A wireless body area network of intelligent motion sensors for computer assisted physical rehabilitation. Journal of NeuroEngineering and rehabilitation 2, 1(2005), 6. \ 6. Niiyama, R., Sun, X., Yao, L., Ishii, H., Rus, D. and Kim, S. Sticky actuator: Free-form planar actuators for animated objects. In: Proc. TEI 2015, 77-84. \ 7. Otto, C., Milenkovic, A., Sanders, C. and Jovanov, E. System architecture of a wireless body area sensor network for ubiquitous health monitoring. Journal of mobile multimedia 1, 4(2006), 307-326. \ 8. Poupyrev, I., Gong, N.W., Fukuhara, S., Karagozler, M.E., Schwesig, C. and Robinson, K.E. Project Jacquard: interactive digital textiles at scale. In: Proc. CHI 2016, 4216-4227. \ 9. Rivera, M. L., Moukperian, M., Ashbrook, D., Mankoff, J. and Hudson, S. E. Stretching the Bounds of 3D Printing with Embedded Textiles. In: Proc. CHI 2017, 497-508. \ 10. Roudaut, A., Karnik, A., LÃ¶chtefeld, M. and Subramanian, S. Morphees: toward high shape resolution in self-actuated flexible mobile devices. In: Proc. CHI 2013, 593-602. \ 11. Sareen, H., Umapathi, U., Shin, P., Kakehi, Y., Ou, J. and Ishii, H. Printflatables: Printing Human-Scale, Functional and Dynamic Inflatable Objects. In: Proc. CHI 2017, 3669-3680. \ 12. Sugiura, Y., Lee, C., Ogata, M., Withana, A., Makino, Y., Sakamoto, D. and Igarashi, T. PINOKY: a ring that animates your plush toys. In Proc. CHI 2012, 725â€“734. \ 13. Vogl, A., Parzer, P., Babic, T., Leong, J., Olwal, A. and Haller, M. StretchEBand: Enabling Fabric-based Interactions through Rapid Fabrication of Textile Stretch Sensors. In: Proc. CHI 2017, 2617-2627. \ 14. Yao, L., Niiyama, R., Ou, J., Follmer, S., Della Silva, C. and Ishii, H. PneUI: pneumatically actuated soft composite materials for shape changing interfaces. In: Proc. UIST 2013, 13-22. \ 15. Yao, L., Ou, J., Cheng, C. Y., Steiner, H., Wang, W., Wang, G. and Ishii, H. BioLogic: natto cells as nanoactuators for shape changing interfaces. In: Proc. CHI 2015, 1-10. \ 16. Yao, L., Steiner, H., Wang, W., Wang, G., Cheng, C. Y., Ou, J. and Ishii, H. Second Skin: Biological Garment Powered by and Adapting to Body in Motion. Ext. Abstracts CHI 2016, 13. \",Smart string; interactive string; digital textile.,H.5.m.,uistde0127-file1.zip,uistde0127-file2.jpg,,,,,,,,,,,,,,,,,
uistde128,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde128,A,Playful Interactions with Body Channel Communication: Conquer it!,Virag,Varga,virag.varga@disneyresearch.com,uistde0128-paper.pdf,2,letter,,,"Virag Varga, Gergely Vakulya, Alanson Sample, Thomas Gross","virag.varga@disneyresearch.com, vakulyag@inf.ethz.ch, alanson.sample@disneyresearch.com, trg@inf.ethz.ch",63393,Virag,,Varga,virag.varga@disneyresearch.com,Department of Computer Science,ETH Zurich,Zurich,,Switzerland,,Disney Research,Zurich,,Switzerland,64955,Gergely,,Vakulya,vakulyag@inf.ethz.ch,Department of Computer Science,ETH Zurich,Zurich,,Switzerland,,Disney Research,Zurich,,Switzerland,46817,Alanson,,Sample,alanson.sample@disneyresearch.com,,Disney Research,Pittsburgh,Pennsylvania,United States,,,,,,46599,Thomas,,Gross,trg@inf.ethz.ch,Department of Computer Science,ETH Zurich,Zurich,,Switzerland,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Conquer it! is a lightweight proof-of-concept exertion game that demonstrates Body Channel Communication (BCC) in a smart environment. BCC employs the human body as communication medium to transfer digital data between physical objects by using electric fields that are coupled to the body. During the game participants are provided with BCC wearables, each of which represents a specific RGB color. When the user stands, walks on, or touches with a hand the BCC tiles, communication is automatically established: the corresponding sensor area decodes the message (RGB value) originating from the wearable and lights up according to that color for two seconds. The goal of the game is to try to light up as many tile cells simultaneously as possible. Participants can try to keep alive the colors by continuously moving around on the tiles. In the multiuser version, by stepping on or touching a blinking cell, users can immediately claim the area and overwrite the color of that subtile.",virag.varga@disneyresearch.com,"1. Varga, V., Vakulya, G., Sample, A., and Gross, T. R. TouchCom: a Practical Body Channel Communication Platform for Touch-Based Interactions. In Tech Report, Disney Research (2017). \ 2. Zimmerman, T. G. Personal Area Networks: Near-ï¬eld Intrabody Communication. IBM Syst. J. 35, 3-4 (Sept. 1996), 609â€“617. \",Body Channel Communication; Capacitive Coupling; Human-Computer Interaction; Wearable; Smart Floor,H.5.m.,uistde0128-file1.zip,uistde0128-file2.jpg,,,,,,,,,,,,,,,,,
uistde129,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde129,A,MagTics: Flexible and Thin Form Factor Magnetic Actuators for Dynamic and Wearable Haptic Feedback,,,,,,,,,"Juan Zarate, Fabrizio Pece, Velko Vechev, Nadine Besse, Olexandr Gudozhnik, Ronan Hinchet, Herbert Shea, Otmar Hilliges","juan.zarate@epfl.ch, fabrizio.pece@inf.ethz.ch, velko@student.chalmers.se, nadine.besse@epfl.ch, olexandr.gudozhnik@epfl.ch, ronan.hinchet@epfl.ch, herbert.shea@epfl.ch, otmar.hilliges@inf.ethz.ch",68289,Juan,,Zarate,juan.zarate@epfl.ch,LMTS,École Polytechnique Fédérale de Lausanne (EPFL),NeuchÃ¢tel,,Switzerland,,,,,,43359,Fabrizio,,Pece,fabrizio.pece@inf.ethz.ch,Advanced Interactive Technologies Lab,ETH Zurich,Zurich,,Switzerland,,,,,,50851,Velko,,Vechev,velko@student.chalmers.se,Dept. of Applied IT,Chalmers University of Technology,Gothenburg,,Sweden,,,,,,71780,Nadine,,Besse,nadine.besse@epfl.ch,LMTS,École Polytechnique Fédérale de Lausanne (EPFL),NeuchÃ¢tel,,Switzerland,,,,,,68323,Olexandr,,Gudozhnik,olexandr.gudozhnik@epfl.ch,LMTS,École Polytechnique Fédérale de Lausanne (EPFL),NeuchÃ¢tel,,Switzerland,,,,,,73579,Ronan,,Hinchet,ronan.hinchet@epfl.ch,LMTS,École Polytechnique Fédérale de Lausanne (EPFL),NeuchÃ¢tel,,Switzerland,,,,,,68325,Herbert,,Shea,herbert.shea@epfl.ch,LMTS,École Polytechnique Fédérale de Lausanne (EPFL),NeuchÃ¢tel,,Switzerland,,,,,,6754,Otmar,,Hilliges,otmar.hilliges@inf.ethz.ch,Advanced Interactive Technologies Lab,ETH Zurich,Zurich,,Switzerland,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present MagTics, a novel flexible and wearable haptic interface based on magnetically actuated bi-directional tactile pixels (taxels). MagTics' thin form-factor and flexibility allows for rich haptic feedback in mobile settings. \ We propose a novel actuation mechanism based on bi-stable electromagnetic latching that combines high framerate and holding force with low energy consumption and a soft and flexible form-factor. \ We overcome limitations of traditional soft-actuators by placing several hard actuation cells, driven by flexible printed electronics, in a soft 3D printed case. A novel EM-shielding prevents magnet-magnet interactions and allows for high actuator densities. A prototypical implementation comprising of 4 actuated pins on a 1.7 cm pitch, with 2 mm travel, and generating 160 mN to 200 mN of latching force is used to implement a number of compelling application scenarios.",,,,,,,,,,,,,,,,,,,,,,,
uistde130,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde130,A,Designing Vibrotactile Widgets with Printed Actuators and Sensors,Christian,Frisson,christian.frisson@inria.fr,uistde0130-paper.pdf,3,letter,,,"Christian Frisson, Julien Decaudin, Thomas Pietrzak, Alexander Ng, Pauline Poncet, Fabrice Casset, Antoine Latour, Stephen Brewster","christian.frisson@inria.fr, julien.decaudin@inria.fr, thomas.pietrzak@univ-lille1.fr, alexander.ng@glasgow.ac.uk, pauline.poncet@cea.fr, fabrice.casset@cea.fr, antoine.latour@cea.fr, stephen.brewster@glasgow.ac.uk",43188,Christian,,Frisson,christian.frisson@inria.fr,,Inria,Lille,,France,,,,,,73557,Julien,,Decaudin,julien.decaudin@inria.fr,,Inria,Lille,,France,,,,,,14172,Thomas,,Pietrzak,thomas.pietrzak@univ-lille1.fr,,Univ. Lille,Lille,,France,,,,,,29580,Alexander,,Ng,alexander.ng@glasgow.ac.uk,School of Computing Science,University of Glasgow,Glasgow,,United Kingdom,,,,,,73558,Pauline,,Poncet,pauline.poncet@cea.fr,"LETI, MINATEC Campus",CEA,Grenoble,,France,,,,,,73559,Fabrice,,Casset,fabrice.casset@cea.fr,"LETI, MINATEC Campus",CEA,Grenoble,,France,,,,,,73560,Antoine,,Latour,antoine.latour@cea.fr,LITEN,CEA,Grenoble,,France,,,,,,1374,Stephen,,Brewster,stephen.brewster@glasgow.ac.uk,School of Computing Science,University of Glasgow,Glasgow,,United Kingdom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Physical controls are fabricated through complicated assembly of parts requiring expensive machinery and are prone to mechanical wear. One solution is to embed controls directly in interactive surfaces, but the proprioceptive part of gestural interaction that makes physical controls discoverable and usable solely by hand gestures is lost and has to be compensated, by vibrotactile feedback for instance. Vibrotactile actuators face the same aforementioned issues as for physical controls. We propose printed vibrotactile actuators and sensors. They are printed on plastic sheets, with piezoelectric ink for actuation, and with silver ink for conductive elements, such as wires and capacitive sensors. These printed actuators and sensors make it possible to design vibrotactile widgets on curved surfaces, without complicated mechanical assembly.",christian.frisson@inria.fr,"1. Ivica Ico Bukvic, Jonathan Wilkes, and Albert GrÃ¤f. 2016. Latest developments with Pd-L2Ork and its Development Branch Purr-Data. In Proceedings of the 5th International Pure Data Convention (PdCon16). 9. \ 2. Christian Frisson, Thomas Pietrzak, Siyan Zhao, and Ali Israr. 2016. WebAudioHaptics: Tutorial on Haptics with Web Audio. In 2nd Web Audio Conference (WACâ€™16). 1. \ 3. Masaaki Fukumoto and Toshiaki Sugimura. 2001. Active Click: Tactile Feedback for Touch Panels. In CHI â€™01 Extended Abstracts on Human Factors in Computing Systems (CHI EA â€™01). ACM, New York, NY, USA, 121â€“122. DOI:http://dx.doi.org/10.1145/634067.634141 \ 4. Aakar Gupta, Thomas Pietrzak, Nicolas Roussel, and Ravin Balakrishnan. 2016. Direct Manipulation in Tactile Displays. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI â€™16). ACM, New York, NY, USA, 3683â€“3693. DOI: http://dx.doi.org/10.1145/2858036.2858161 \ 5. Eve Hoggan, Stephen A. Brewster, and Jody Johnston. 2008. Investigating the Effectiveness of Tactile Feedback for Mobile Touchscreens. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI â€™08). ACM, New York, NY, USA, 1573â€“1582. DOI: http://dx.doi.org/10.1145/1357054.1357300 \ 6. Ali Israr, Siyan Zhao, Kyna McIntosh, Zachary Schwemler, Adam Fritz, John Mars, Job Bedford, Christian Frisson, Ivan Huerta, Maggie Kosek, Babis Koniaris, and Kenny Mitchell. 2016. Stereohaptics: A Haptic Interaction Toolkit for Tangible Virtual Experiences. In ACM SIGGRAPH 2016 Studio (SIGGRAPHâ€™16). ACM, 13:1â€“13:57. DOI: http://dx.doi.org/10.1145/2929484.2970273 \ 7. Sunjun Kim and Geehyuk Lee. 2013. Haptic Feedback Design for a Virtual Button Along Force-displacement Curves. In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (UIST â€™13). ACM, New York, NY, USA, 91â€“96. DOI: http://dx.doi.org/10.1145/2501988.2502041 \ 8. Jani Lylykangas, Veikko Surakka, Katri Salminen, Jukka Raisamo, Pauli Laitinen, Kasper RÃ¶nning, and Roope Raisamo. 2011. Designing Tactile Feedback for Piezo Buttons. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI â€™11). ACM, New York, NY, USA, 3281â€“3284. DOI: http://dx.doi.org/10.1145/1978942.1979428 \ 9. Pauline Poncet, Fabrice Casset, Antoine Latour, Fabrice Domingues Dos Santos, SÃ©bastien Pawlak, Romain Gwoziecki, Arnaud Devos, Patrick Emery, and StÃ©phane Fanget. 2017. Static and Dynamic Studies of Electro-Active Polymer Actuators and Integration in a Demonstrator. Actuators 6, 2 (2017). DOI: http://dx.doi.org/10.3390/act6020018 \ 10. Pauline Poncet, Fabrice Casset, Antoine Latour, Fabrice Domingues Dos Santos, SÃ©bastien Pawlak, Romain Gwoziecki, and StÃ©phane Fanget. 2016. Design and realization of electroactive polymer actuators for transparent and ï¬‚exible haptic feedback interfaces. In Proc. EuroSimE 2016. 1â€“5. DOI: http://dx.doi.org/10.1109/EuroSimE.2016.7463310 \",Vibrotactile feedback; printed electronics; printed actuators; printed sensors; widgets; piezoelectric ink; thin-film actuators,H.5.2,uistde0130-file1.zip,,uistde0130-file3.mp4,,,,,,,,,,,,,,,,
uistde131,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde131,A,Ultrasonic Cuisine: Proposal of ultrasonic non-contact stirring methods,Yuta,Sato,s.yutassie@gmail.com,uistde0131-paper.pdf,2,letter,,,"Yuta Sato, Kensuke Abe, Kotaro Omomo, Ryota Kawamura, Kazuki Takazawa, Yoichi Ochiai","s.yutassie@gmail.com, abekensuke.1011@gmail.com, momotap.huwa@gmail.com, s1721663@s.tsukuba.ac.jp, 1220kazu1412@gmail.com, wizard@slis.tsukuba.ac.jp",73573,Yuta,,Sato,s.yutassie@gmail.com,Digital Nature Group,University of Tsukuba,Japan,Ibaraki,Japan,,,,,,73622,Kensuke,,Abe,abekensuke.1011@gmail.com,Digital Nature Group,University of Tsukuba,Tsukuba city,Ibaraki,Japan,,,,,,60070,Kotaro,,Omomo,momotap.huwa@gmail.com,Digital Nature Group,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,73613,Ryota,,Kawamura,s1721663@s.tsukuba.ac.jp,Graduate School of Library Information and Media Studies in University of Tsukuba,University,Tsukuba-shi,Ibaraki-ken,Japan,,,,,,69562,Kazuki,,Takazawa,1220kazu1412@gmail.com,Digital Nature Group,UNIVERSITY OF TSUKUBA,TSUKUBA,,Japan,,,,,,53649,Yoichi,,Ochiai,wizard@slis.tsukuba.ac.jp,Digital Nature Group,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this paper we propose a method of non-contact stirring. \ Ultrasonic waves have been studied for various applications. \ However, devices using ultrasound have been devised only to \ specialize in one role up to now. In recent years, we aim at \ generalization of aerial ultrasonic equipment used for various \ applications such as tactile presentation and super directional \ speaker, and propose applications closely our real life.",s.yutassie@gmail.com,"1. Harada, S., and Itsumi, A. A basic study of cooking utilizing ultra-sonic waves : Hydration and extraction of dry ingredients. Bulletin of Toyama Collage 41 (2006), 39â€“45. \ 2. Hoshi, T., Takahashi, M., Iwamoto, T., and Shinoda, H. Noncontact tactile display based on radiation pressure of airborne ultrasound. IEEE Transactions on Haptics 3, 3 (July 2010), 155â€“165. \ 3. KATOU, H., MIYAKE, R., and TERAYAMA, T. Non-contact micro-liquid mixing method using ultrasound. JSME International Journal Series B Fluids and Thermal Engineering 48, 2 (2005), 350â€“355. \ 4. Ochiai, Y., Kumagai, K., Hoshi, T., Hasegawa, S., and Hayasaki, Y. Cross-ï¬eld aerial haptics: Rendering haptic feedback in air with light and acoustic ï¬elds. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI â€™16, ACM (New York, NY, USA, 2016), 3238â€“3247. \ 5. Uragami, T., Osumi, A., and Ito, Y. Behavior of liquid in a vessel irradiated by high-intensity aerial ultrasonic waves. Proceedings of Symposium on Ultrasonic Electronics 36 (2015). \",Ultrasonic; Ultrasound; stirring,J.4.,uistde0131-file1.zip,uistde0131-file2.jpg,,,,,,,,,,,,,,,,,
uistde133,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde133,A,Demonstrating TrussFab's Editor: Designing Sturdy Large-Scale Structures,Robert,Kovacs,robert.kovacs@hpi.de,uistde0133-paper.pdf,3,letter,,,"Robert Kovacs, Ludwig Wilhelm Wall, Anna Seufert, Hsiang-Ting Chen, Willi MÃ¼ller, Florian Meinel, Yannis Kommana, Thomas BlÃ¤sius, Thijs Jan Roumen, Oliver S. Schneider, Patrick Baudisch","robert.kovacs@hpi.de, ludwig.wall@student.hpi.uni-potsdam.de, anna.m.seufert@gmail.com, ht.timchen@gmail.com, willi.mueller@student.hpi.uni-potsdam.de, florian.meinel@student.hpi.uni-potsdam.de, Yannis.Kommana@student.hpi.uni-potsdam.de, thomas.blaesius@hpi.de, thijs.roumen@hpi.de, oliver.schneider@hpi.de, patrick.baudisch@hpi.uni-potsdam.de",45709,Robert,,Kovacs,robert.kovacs@hpi.de,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,51237,Ludwig,Wilhelm,Wall,ludwig.wall@student.hpi.uni-potsdam.de,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,51238,Anna,,Seufert,anna.m.seufert@gmail.com,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,27038,Hsiang-Ting,,Chen,ht.timchen@gmail.com,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,43296,Willi,,Müller,willi.mueller@student.hpi.uni-potsdam.de,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,45703,Florian,,Meinel,florian.meinel@student.hpi.uni-potsdam.de,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,64682,Yannis,,Kommana,Yannis.Kommana@student.hpi.uni-potsdam.de,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,64647,Thomas,,Bläsius,thomas.blaesius@hpi.de,Hasso Plattner Institute,,Potsdam,,Germany,,,,,,42878,Thijs,Jan,Roumen,thijs.roumen@hpi.de,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,21068,Oliver,S.,Schneider,oliver.schneider@hpi.de,"Hasso Plattner Institute, Potsdam, Germany",,,,,,,,,,1437,Patrick,,Baudisch,patrick.baudisch@hpi.uni-potsdam.de,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We demonstrate TrussFab's editor for creating large-scale structures that are sturdy enough to carry human weight. TrussFab achieves the large scale by using plastic bottles as beams that form structurally sound node-link structures, also known as trusses, allowing it to handle the forces resulting from scale and load. During this hands-on demo at UIST, attendees will use the TrussFab software to design their own structures, validate their design using integrated structural analysis, and export their designs for 3D printing.",robert.kovacs@hpi.de,"1. Robert Kovacs, Anna Seufert, Ludwig Wall, HsiangTing Chen, Florian Meinel, Willi MÃ¼ller, Si-jing You, Maximilian Brehm, Jonathan Striebel, Yannis Kommana, Alexander Popiak, Thomas BlÃ¤sius, Patrick Baudisch. 2017. TrussFab: Fabricating Sturdy LargeScale Structures on Desktop 3D Printers. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI â€™17). ACM, New York, NY, USA, 2606-2616. http://dx.doi.org/10.1145/3025453.3026016 \ 2. Harshit Agrawal, Udayan Umapathi, Robert Kovacs, Johannes Frohnhofen, Hsiang-Ting Chen, Stefanie Mueller, and Patrick Baudisch. 2015. Protopiper: Physically Sketching Room-Sized Objects at Actual Scale. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST â€™15), ACM, New York, NY, USA, 427â€“436. http://doi.org/10.1145/2807442.2807505 \ 3. Wai-Fah Chen and Eric M Lui. 2005. Handbook of structural engineering. CRC press. \ 4. Manfred Lau, Akira Ohgawara, Jun Mitani, and Takeo Igarashi. 2011. Converting 3D Furniture Models to Fabricatable Parts and Connectors. ACM Transactions on Graphics 30, 212: 1â€“6. http://doi.org/10.1145/1964921.1964980 \ 5. Linjie Luo, Ilya Baran, Szymon Rusinkiewicz, and Wojciech Matusik. 2012. Chopper: Partitioning Models into 3D-Printable Parts. ACM Transactions on Graphics 31, 6: 1. http://doi.org/10.1145/2366145.2366148 \ 6. Homes Made from Plastic Bottles. Retrieved September 15, 2016 from http://www.inspirationgreen.com/plastic-bottle-homes \",Fabrication; 3D printing; truss structure,H5.2,uistde0133-file1.docx,uistde0133-file2.jpg,uistde0133-file3.mp4,,,,,,,,,,,,,,,,
uistde135,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde135,A,Mobile Brain-Computer Interface for Dance and Somatic Practice,Naoto,Hieda,mail@naotohieda.com,uistde0135-paper.pdf,2,letter,,,Naoto Hieda,mail@naotohieda.com,73587,Naoto,,Hieda,mail@naotohieda.com,Topological Media Lab,Concordia University,Montreal,Quebec,Canada,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Sensor technologies have been adapted to performing arts, and owing to the recent advancement of low-cost mobile electroencephalography devices, brain-computer interface (BCI) is integrated to dance performances as well. Nevertheless, BCI is less accessible to artists compared to other sensors because signal processing and machine learning are required. This paper proposes a work-in-progress example of BCI applications for performances that has been designed in collaboration with contemporary dancers. Its contribution is that the piece is not an add-on to a performance, but the implementation reflects practices of contemporary dance. \",mail@naotohieda.com,"1. Eaton, J., Jin, W., and Miranda, E. The space between us: a live performance with musical score generated via affective correlates measured in eeg of one performer and an audience member. In NIME14 International Conference on New Interfaces for Musical Expression (2014), 593â€“596. \ 2. Lucier, A. Statement on: music for solo performer. Biofeedback and the Arts, Results of Early Experiments. Vancouver: Aesthetic Research Center of Canada Publications (1976), 60â€“61. \ 3. Nicolas-Alonso, L. F., and Gomez-Gil, J. Brain computer interfaces, a review. Sensors 12, 2 (2012), 1211â€“1279. \ 4. Pearlman, E. Brain opera. PAJ: A Journal of Performance and Art 39, 2 (2017), 79â€“85. \",Brain-computer interface; media art; contemporary dance,H.5.m.,uistde0135-file1.zip,,,,,,,,,,,,,,,,,,
uistde137,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde137,A,Creating Haptic Illusion of Compliance for Tangential Force Input using Vibrotactile Actuator,Seongkook,Heo,seongkook@kaist.ac.kr,uistde0137-paper.pdf,3,letter,,,"Seongkook Heo, Geehyuk Lee","seongkook@dgp.toronto.edu, geehyuk@gmail.com",73648,Seongkook,,Heo,seongkook@dgp.toronto.edu,"University of Toronto, Toronto, Ontario, Canada",,,,,,,,,,5474,Geehyuk,,Lee,geehyuk@gmail.com,,KAIST,Daejeon,,"Korea, Republic of",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We demonstrate a haptic feedback method that generates a compliance illusion on a rigid surface with a tangential force sensor and a vibrotactile actuator. The method assumes a conceptual model where a virtual object is placed on a textured surface and stringed to four walls with four springs. A two dimensional tangential force vector measured from the rigid surface is mapped to the virtual position of the virtual object on the textured surface. By playing vibration patterns that simulate the friction-induced vibrations made from the movement of the virtual object, we could make an illusion that the rigid surface feels like moving. We also demonstrate that the perceptual properties of the illusion, such as the stiffness of the virtual spring and the maximum travel distance of the virtual object, can be programmatically controlled.",seongkook@dgp.toronto.edu,"[1] Harrison, C. and Hudson, S. Using Shear As a Supplemental Two-dimensional Input Channel for Rich Touchscreen Interaction. In Proc. CHI 2012, ACM Press (2012), 3149â€“3152. \ [2] Heo, S. and Lee, G. Force Gestures: Augmenting Touch Screen Gestures with Normal and Tangential Forces. In Proc. UIST 2011, ACM Press (2011), 621â€“626. \ [3] Heo, S. and Lee, G. Vibrotactile Compliance Feedback for Tangential Force Interaction. IEEE Transactions on Haptics. IEEE (2017). \ [4] Herot, C.F. and Weinzapfel, G. One-point Touch Input of Vector Information for Computer Displays. In Proc. SIGGRAPH '78, ACM Press (1978), 210â€“216. \ [5] Kildal, J. 3D-press: Haptic Illusion of Compliance when Pressing on a Rigid Surface. In Proc. ICMI-MLMI'10, ACM Press (2010), 21:1â€“21:8. \ [6] Minsky, M.R. Manipulating Simulated Objects with Real-world Gestures Using a Force and Position Sensitive Screen. In Proc. SIGGRAPH '84, 195â€“203. \ [7] Rutledge, J.D. and Selker, T. Force-to-motion Functions for Pointing. In Proc. IFIP TC13 Third International Conference on Human-Computer Interaction (1990), 701â€“706. \ [8] Srinivasan, M.A. and LaMotte, R.H. Tactual discrimination of softness. Journal of Neurophysiology. 73, 1 (Jan. 1995), 88â€“101. \ [9] Weigel, M. and Steimle, J. DeformWear: Deformation Input on Tiny Wearable Devices. In Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1, 2 (Jun. 2017), 28:1â€“28:23.",Force input; tangential force; compliance illusion; haptic illusion; tactile display.,H.5.2. Information Interfaces and Presentation: Haptic I/O,uistde0137-file1.doc,uistde0137-file2.jpg,,,,,,,,,,,,,,,,,
uistde138,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde138,A,Hand Development Kit: Soft Robotic Fingers as Prosthetic Augmentation of the Hand,Sang-won,Leigh,sangwon@mit.edu,uistde0138-paper.pdf,3,letter,,,"Yuhan Hu, Sang-won Leigh, Pattie Maes","yuhanhu@mit.edu, sangwon@mit.edu, pattie@media.mit.edu",73602,Yuhan,,Hu,yuhanhu@mit.edu,Media Lab,MIT,Somerville,Massachusetts,United States,,,,,,36740,Sang-won,,Leigh,sangwon@mit.edu,Media Lab,MIT,Cambridge,Massachusetts,United States,,,,,,3707,Pattie,,Maes,pattie@media.mit.edu,,MIT Media Lab,Cambridge,Massachusetts,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Recent developments in wearable robots and human augmentation open up new possibilities of designing computational interfaces integrated to the body. Particularly, supernumerary robot is a recently established field of research that investigates a radical idea of adding robotic limbs to users. Such augmentations, however, pose a limit in how much we can add to the body due to weight or interference with other body parts. To address that, we explore the use of soft robots as supernumerary robotic fingers. We present a pair of soft robotic fingers driven by cables and servomotors, and applications using the robotic fingers in various contexts. \",sangwon@mit.edu,"WARNING: Reference 1 is very long. Please verify that it was extracted correctly. \ \ 1. Bonilla, B. L.; Asada, H. H. A Robot on the Shoulder: Coordinated Human-Wearable Robot Control Using Coloured Petri Nets and Partial Least Squares Predictions. In 2014 IEEE International Conference on Robotics and Automation (ICRA); IEEE, 2014; pp 119â€“ 125. https://doi.org/10.1109/ICRA.2014.6906598 \ 2. Bretan, M.; Gopinath, D.; Mullins, P.; Weinberg, G. A Robotic Prosthesis for an Amputee Drummer. 2016. http://arxiv.org/abs/1612.04391 \ 3. Herr, H. Exoskeletons and Orthoses: Classification, Design Challenges and Future Directions. J. Neuroeng. Rehabil. 2009, 6 (1), 21. https://doi.org/10.1186/17430003-6-21 \ 4. Hussain, I.; Salvietti, G.; Prattichizzo, D. On Control Interfaces for the Robotic Sixth Finger. In Proceedings of the 7th Augmented Human International Conference 2016 on - AH â€™16; ACM Press: New York, New York, USA, 2016; pp 1â€“2. https://doi.org/10.1145/2875194.2875243 \ 5. Leigh, S.; Maes, P. Body Integrated Programmable Joints Interface. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems - CHI â€™16; ACM Press: New York, New York, USA, 2016; pp 6053â€“6057. https://doi.org/10.1145/2858036.2858538 \ 6. Leigh, S.; Parekh, K.; Denton, T.; Peebles, W. S.; Johnson, M. H.; Maes, P. Morphology Extension Kit: A Modular Robotic Platform for Customizable and Physically Capable Wearables. In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems - CHI EA â€™17; ACM Press: New York, New York, USA, 2017; pp 397â€“400. https://doi.org/10.1145/3027063.3052969 \ 7. Wu, F.Y.; Asada, H. Bio-Artificial Synergies for Grasp Posture Control of Supernumerary Robotic Fingers. In Robotics: Science and Systems; University of California: Berkeley, CA, USA, 2014. http://hdl.handle.net/1721.1/88457 \ 8. Wu, F.Y.; Asada, H.H. â€œHold-and-manipulateâ€ with a Single Hand Being Assisted by Wearable Extra Fingers. In Proceedings of the 2015 IEEE International Conference on Robotics and Automation, ICRA â€™15, Seattle, WA, USA, 26â€“30 May 2015; pp. 6205â€“6212. https://doi.org/10.1109/ICRA.2015.7140070 \",Synergistic Interaction; Human Augmentation; Wearable Robotics,H.5.2,uistde0138-file1.docx,uistde0138-file2.jpg,uistde0138-file3.mp4,,,,,,,,,,,,,,,,
uistde139,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde139,A,Demonstrating Interactive Systems based on Electrical Muscle Stimulation,Pedro,Lopes,Pedro.Lopes@hpi.uni-potsdam.de,uistde0139-paper.pdf,3,letter,,,"Pedro Lopes, Patrick Baudisch","pedro.lopes@hpi.de, patrick.baudisch@hpi.uni-potsdam.de",70430,Pedro,,Lopes,pedro.lopes@hpi.de,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,1437,Patrick,,Baudisch,patrick.baudisch@hpi.uni-potsdam.de,,Hasso Plattner Institute,Potsdam,,Germany,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We provide a hands-on demonstration of the potential of interactive systems based on electrical muscle stimulation (EMS). These wearable devices allow attendees, for example, to physically learn how to manipulate objects they never seen before, feel walls and forces in virtual reality, and so forth. In our demo we plan to not only demonstrate several of these EMS-based prototypes but also to provide instructions and free hardware for people to conduct their first projects using EMS.",pedro.lopes@hpi.de,"1. Farbiz, F., Yu, Z. H., Manders, C., and Ahmad, W. An electrical muscle stimulation haptic feedback for mixed reality tennis game. In ACM SIGGRAPH 2007 Posters, SIGGRAPH â€™07, ACM (New York, NY, USA, 2007). \ 2. Lopes, P. openEMSstim. https://github.com/PedroLopes/openEMSstim, 2016. [Online; accessed 10-July-2017]. \ 3. Lopes, P., and Baudisch, P. Muscle-propelled force feedback: Bringing force feedback to mobile devices. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI â€™13, ACM (New York, NY, USA, 2013), 2577â€“2580. \ 4. Lopes, P., Ion, A., and Baudisch, P. Impacto: Simulating physical impact by combining tactile stimulation with electrical muscle stimulation. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology, UIST â€™15, ACM (New York, NY, USA, 2015), 11â€“19. \ 5. Lopes, P., Ion, A., Mueller, W., Hoffmann, D., Jonell, P., and Baudisch, P. Proprioceptive interaction. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, CHI â€™15, ACM (New York, NY, USA, 2015), 939â€“948. \ 6. Lopes, P., Jonell, P., and Baudisch, P. Affordance++: Allowing objects to communicate dynamic use. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, CHI â€™15, ACM (New York, NY, USA, 2015), 2515â€“2524. \ 7. Lopes, P., You, S., Cheng, L.-P., Marwecki, S., and Baudisch, P. Providing haptics to walls & heavy objects in virtual reality by means of electrical muscle stimulation. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, CHI â€™17, ACM (New York, NY, USA, 2017), 1471â€“1482. \ 8. Lopes, P., YÂ¨uksel, D., Guimbreti`ere, F., and Baudisch, P. Muscle-plotter: An interactive system based on electrical muscle stimulation that produces spatial output. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, UIST â€™16, ACM (New York, NY, USA, 2016), 207â€“217. \ 9. Moe, J., and Post, H. Functional electrical stimulation for ambulation in hemiplegia. In Lancet Journal, vol. 82 (1962), 285288. \ 10. Pfeiffer, M. letyourbodymove. https://bitbucket. org/MaxPfeiffer/letyourbodymove/wiki/Home, 2016. [Online; accessed 10-January-2017]. \ 11. Tamaki, E., Miyaki, T., and Rekimoto, J. Possessedhand: Techniques for controlling human hands using electrical muscles stimuli. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI â€™11, ACM (New York, NY, USA, 2011), 543â€“552. \",Electrical Muscle Stimulation; wearable; haptics; body I/O,H.5.2;,uistde0139-file1.zip,uistde0139-file2.jpg,uistde0139-file3.mp4,,,,,,,,,,,,,,,,
uistde147,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde147,A,Sonoliards: Rendering audible sound spots by reflecting the ultrasound beams,Naoya,Muramatsu,sh.mn.nat@gmail.com,uistde0147-paper.pdf,3,letter,"Arial,Bold Times_New_Roman,Bold Times_New_Roman Times_New_Roman,Italic Helvetica,Bold",,"Naoya Muramatsu, Kazuki Ohshima, Ryota Kawamura, Yoichi Ochiai, Yuta Sato, CHUN WEI OOI","sh.mn.nat@gmail.com, s1613108@u.tsukuba.ac.jp, s1721663@s.tsukuba.ac.jp, wizard@slis.tsukuba.ac.jp, s.yutassie@gmail.com, made3102@gmail.com",73619,Naoya,,Muramatsu,sh.mn.nat@gmail.com,Digital Nature Group,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,64449,Kazuki,,Ohshima,s1613108@u.tsukuba.ac.jp,Digital Nature Group,University of Tsukuba,Tsukuba-shi,Ibaraki-ken,Japan,,,,,,73613,Ryota,,Kawamura,s1721663@s.tsukuba.ac.jp,Graduate School of Library Information and Media Studies in University of Tsukuba,University,Tsukuba-shi,Ibaraki-ken,Japan,,,,,,53649,Yoichi,,Ochiai,wizard@slis.tsukuba.ac.jp,Digital Nature Group,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,73573,Yuta,,Sato,s.yutassie@gmail.com,Digital Nature Group,University of Tsukuba,Japan,Ibaraki,Japan,,,,,,73630,CHUN WEI,,OOI,made3102@gmail.com,"Graduate school of Library,Information and Media studies",University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This paper proposes a dynamic acoustic field generation system for a spot audio towards particular person indoors. Spot audio techniques have been explored by generating the ultrasound beams toward the target person in certain area however everyone in this area can hear the sound. Our system recognizes the positions of each person indoor using motion capture and 3D model data of the room. After that we control direction of parametric speaker in real-time so that sound reach only particular person by calculating the reflection of sound on surfaces such as wall and ceiling. We calculate direction of parametric speaker using a beam tracing method. We present generating methods of dynamic acoustic field in our system and conducted the experiments on human factor to evaluate performance of proposed system.,sh.mn.nat@gmail.com,"1. Tung, Y.-C., and Shin, K. G. Expansion of human-phone interface by sensing structure-borne sound propagation. In Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services, MobiSys â€™16, ACM (New York, NY, USA, 2016), 277â€“ 289. \ 2. Hong, D., Lee, T.-H., Joo, Y., and Park, W.-C. Real-time sound propagation hardware accelerator for immersive virtual reality 3d audio. In Proceedings of the 21st ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D â€™17, ACM (New York, NY, USA, 2017), 20:1â€“20:2. \",Ultrasound beams; Parametric Speaker; Acoustic Field Generation; Acoustic Simulation; Ray Tracing Method.,H.5.1,uistde0147-file1.doc,,,,,,,,,,,,,,,,,,
uistde149,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde149,A,AccelTag: A Passive Smart ID Tag with Acceleration Sensor for Interactive Applications,Kazuya,Oharada,oharada@iplab.cs.tsukuba.ac.jp,uistde0149-paper.pdf,2,letter,,,"Kazuya Oharada, Buntarou Shizuki, Shin Takahashi","oharada@iplab.cs.tsukuba.ac.jp, shizuki@cs.tsukuba.ac.jp, shin@cs.tsukuba.ac.jp",73603,Kazuya,,Oharada,oharada@iplab.cs.tsukuba.ac.jp,,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,26480,Buntarou,,Shizuki,shizuki@cs.tsukuba.ac.jp,,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,6407,Shin,,Takahashi,shin@cs.tsukuba.ac.jp,,University of Tsukuba,Tsukuba,Ibaraki,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"There are many everyday situations in which users need to enter their user identification (user ID), such as logging in to computer systems and entering secure offices. In such situations, contactless passive IC cards are convenient because users can input their user ID simply by passing the card over a reader. However, these cards cannot be used for successive interactions. To address this issue, we propose AccelTag, a contactless IC card equipped with an acceleration sensor and a liquid crystal display (LCD). AccelTag utilizes high-function RFID technology so that the acceleration sensor and the LCD can also be driven by a wireless power supply. With its built-in acceleration sensor, AccelTag can acquire its direction and movement when it is waved over the reader. We demonstrate several applications using AccelTag, such as displaying several types of information in the card depending on the user's requirements.",oharada@iplab.cs.tsukuba.ac.jp,"1. Shu, Y., Chen, J., Jiang, F., Gu, Y., Dai, Z., and He, T. Demo: WISP-based Access Control Combining Electronic and Mechanical Authentication. In Proceedings of the 9th ACM Conference on Embedded Networked Sensor Systems, SenSys â€™11, ACM (New York, NY, USA, 2011), 433â€“434. \ 2. Zhao, Y., Smith, J. R., and Sample, A. NFC-WISP: A sensing and computationally enhanced near-ï¬eld RFID platform. In 2015 IEEE International Conference on RFID (RFID), IEEE (New York, NY, USA, 2015), 174â€“181. \",RFID; High-function RFID; contactless IC card; battery free; gesture,H.5.2,uistde0149-file1.zip,uistde0149-file2.jpg,uistde0149-file3.mp4,,,,,,,,,,,,,,,,
uistde151,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde151,A,Eye Tracking Using Built-in Camera for Smartphone-based HMD,Hiroyuki,Hakoda,hakoch0712@gmail.com,uistde0151-paper.pdf,2,letter,,,"Hiroyuki Hakoda, Wataru Yamada, Hiroyuki Manabe","hiroyukihakoda@acm.org, wataruyamada@acm.org, manabehiroyuki@acm.org",73646,Hiroyuki,,Hakoda,hiroyukihakoda@acm.org,Research Labs,NTT DOCOMO,Yokosuka,Kanagawa,Japan,,,,,,45181,Wataru,,Yamada,wataruyamada@acm.org,Research Labs,NTT DOCOMO,Yokosuka,Kanagawa,Japan,,,,,,36743,Hiroyuki,,Manabe,manabehiroyuki@acm.org,Research Labs,NTT DOCOMO,Yokosuka,Kanagawa,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Virtual reality (VR) using head-mounted displays (HMDs) is becoming popular. \ Smartphone-based HMDs (SbHMDs) are so low cost that users can easily experience VR. \ Unfortunately, their input modality is quite limited. \ We propose a real-time eye tracking technique that uses the built-in front facing camera to capture the user's eye. \ It realizes stand-alone pointing functionality without any additional device.",hakoch0712@gmail.com,"1. Greenwald, S. W., Loreti, L., Funk, M., Zilberman, R., and Maes, P. Eye gaze tracking with Google Cardboard using purkinje images. In Proc. VRST â€™16, 19â€“22, 2016. \ 2. Kato, K., and Miyashita, H. Creating a mobile head-mounted display with proprietary controllers for interactive virtual reality content. In Adjunct Proc. UIST â€™15 Adjunct, 35â€“36, 2015. \ 3. Smus, B., and Riederer, C. Magnetic input for mobile virtual reality. In Proc. ISWC â€™15, 43â€“44, 2015. \",Mobile HMD; Eye Tracking; Virtual Reality;,H.5.2,uistde0151-file1.tex,uistde0151-file2.jpg,,,,,,,,,,,,,,,,,
uistde152,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde152,A,Attaching Objects to Smartphones Back Side for a Modular Interface,Nobutaka,Matsushima,nob.matsushima@acm.org,uistde0152-paper.pdf,2,letter,,,"Nobutaka Matsushima, Wataru Yamada, Hiroyuki Manabe","nob.matsushima@acm.org, wataruyamada@acm.org, manabehiroyuki@acm.org",73645,Nobutaka,,Matsushima,nob.matsushima@acm.org,,"NTT DOCOMO, INC.",Tokyo,,Japan,,,,,,45181,Wataru,,Yamada,wataruyamada@acm.org,Research Labs,NTT DOCOMO,Yokosuka,Kanagawa,Japan,,,,,,36743,Hiroyuki,,Manabe,manabehiroyuki@acm.org,Research Labs,NTT DOCOMO,Yokosuka,Kanagawa,Japan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper proposes a new approach to enhancing interaction with general applications on smartphones. \ Physical objects held on back surface of the smartphone, which can be captured by the rear camera with a mirror, work as input devices or controllers. \ It does not require any additional electronic devices but offers tactile feedback. \ The occlusion problem does not occur when using smartphone's back side, in terms of both display and camera viewing. \ We implemented on an Android smartphone and confirmed that it provides richer interaction and low latency (100 ms).",nob.matsushima@acm.org,"1. Baudisch, P., and Chu, G. Back-of-device interaction allows creating very small touch devices. In Proc. CHI â€™09 (2009), 1923â€“1932. \ 2. Corsten, C., Avellino, I., MÂ¨ollers, M., and Borchers, J. Instant User Interfaces: Repurposing everyday objects as input devices. In Proc. ITS â€™13 (2013), 71â€“80. \ 3. Wong, P. C., Fu, H., and Zhu, K. Back-Mirror: Back-of-device one-handed interaction on smartphones. In Proc. SIGGRAPH ASIA â€™16 (2016), 10:1â€“10:5. \",Smartphone; Back-of-device; Physical input; Mirror,H.5.2.,uistde0152-file1.zip,uistde0152-file2.jpg,,,,,,,,,,,,,,,,,
uistde153,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde153,A,Feeling Fireworks,Paul,Beardsley,pab@disneyresearch.com,uistde0153-paper.pdf,3,letter,,,"Dorothea Reusser, Dorothea Reusser, Espen Knoop, Roland Siegwart, Paul Beardsley","dorothea.reusser@gmail.com, dorothea.reusser@gmail.com, espen.knoop@bristol.ac.uk, rsiegwart@ethz.ch, pab@disneyresearch.com",73582,Dorothea,,Reusser,dorothea.reusser@gmail.com,,Disney Research Zurich,Zurich,Zurich,Switzerland,,,,,,73582,Dorothea,,Reusser,dorothea.reusser@gmail.com,,ETH Zurich,Zurich,Zurich,Switzerland,,,,,,49047,Espen,,Knoop,espen.knoop@bristol.ac.uk,,Disney Research,Zurich,Zurich,Switzerland,,,,,,73650,Roland,,Siegwart,rsiegwart@ethz.ch,,ETH Zurich,Zurich,Zurich,Switzerland,,,,,,43491,Paul,,Beardsley,pab@disneyresearch.com,,Disney Research Zurich,Zurich,ZH,Switzerland,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present Feeling Fireworks, a tactile firework show. Feeling Fireworks is aimed at making fireworks more inclusive for blind and visually impaired users, in a novel experience that can be shared by all. Tactile effects are created using directable water jets that spray onto the rear of a flexible screen, with different nozzles for different firework effects. Our approach is low-cost and scales well, and allows for dynamic tactile effects to be rendered with high spatial resolution. A user study demonstrated that the tactile effects are meaningful analogs to the visual fireworks that they represent, with sighted users able to label the correct correspondence of tactile-to-visual effects by a large margin over chance. Beyond the specific application, the technology represents a novel and cost-effective approach for making large scalable tactile displays, with the potential for wider use.",pab@disneyresearch.com,"1. Bau, O., Poupyrev, I., Israr, A., and Harrison, C. Tesla Touch: Electrovibration for touch surfaces. UISTâ€™10 Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology (2010), 283â€“292. \ 2. Follmer, F., Leithinger, D., Olwal, A., Hogge, A., and Ishii, H. inFORM: Dynamic physical affordances and constraints through shape and object actuation. UISTâ€™13 Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (2013), 417â€“426. \ 3. Israr, A., and Poupyrev, I. Tactile Brush: Drawing on skin with a tactile grid display. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (2011). \ 4. Kingsley, P. eTable: A haptic elastic table for 3D multi-touch interactions. Masterâ€™s thesis, University of Bristol, 2012. \ 5. Long, B., Seah, S., Carter, T., and Subramanian, S. Rendering volumetric haptic shapes in mid-air using ultrasound. ACM Transactions on Graphics 33, 6 (2014). \ 6. Nature News. Deaf people use â€˜mindâ€™s earâ€™ to process vibrations. http://www.nature.com/news/2001/ 011127/full/news011129-10.html, 2001. \ 7. Sahoo, D. R., HornbÃ¦k, K., and Subramanian, S. TableHop: An actuated fabric display using transparent electrodes. CHIâ€™16 - 34th Annual ACM Conference on Human Factors in Computing Systems (2016), 3767â€“3780. \",Haptic I/O; Interaction styles,H.5.2,uistde0153-file1.zip,uistde0153-file2.jpg,uistde0153-file3.mp4,,,,,,,,,,,,,,,,
uistde155,10/23,19,Demos,6:00:00 PM,9:00:00 PM,,,,,,,,,,,uistde155,A,Atypical: A Type System for Live Performances,Gabriel,Nunes,gabriel.nunes@nyu.edu,uistde0155-paper.pdf,2,letter,,,"Gabriel Barbosa Nunes, Ken Perlin","gabriel.nunes@nyu.edu, perlin@nyu.edu",73578,Gabriel,Barbosa,Nunes,gabriel.nunes@nyu.edu,,New York University,New York City,New York,United States,,,,,,5774,Ken,,Perlin,perlin@nyu.edu,,New York University,New York City,New York,United States,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Chalktalk is a computer-based visual language based around real-time interaction with virtual objects in a blackboard-style environment. Its aim is to be a presentation and communication tool, using animation and interactivity to allow easy visualization of complex ideas and concepts. This demonstration will show Chalktalk in action, with focus on its ability to link these objects together to send data between them, as well as the flexible type system, named Atypical, that underpins this feature.",gabriel.nunes@nyu.edu,"1. Nunes, G. B. Atypical: a type system for live performances. Masterâ€™s thesis, New York University, May 2017. \ 2. Perlin, K. Future reality: How emerging technologies will change language itself. IEEE Computer Graphics and Applications 36, 3 (May 2016), 84â€“89. \",Information visualization; types; type systems,H.5.1;D.3.3;D.1.7,uistde0155-file1.zip,uistde0155-file2.jpg,uistde0155-file3.mp4,,,,,,,,,,,,,,,,
np_reg0,10/22,,Registration,5:00:00 PM,9:00:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_welcome,10/22,,Welcome Reception,6:00:00 PM,9:00:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_reg1,10/23,,Registration,8:00:00 AM,5:00:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_opening,10/23,,Opening Remarks,9:00:00 AM,9:30:00 AM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_keynote1,10/23,,Opening Keynote: Prof. Gabriella Coleman,9:30:00 AM,10:30:00 AM,,,,,,,,,,,,,,,,,,,,,,Gabriella Coleman,,,Gabriella,,Coleman,,,McGill University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_awards,10/23,,Best Paper and Lasting Impact Awards,10:30:00 AM,11:00:00 AM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_break1,10/23,,Break,11:00:00 AM,11:40:00 AM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_lunch1,10/23,,Lunch,1:00:00 PM,2:30:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_break2,10/23,,Break,3:50:00 PM,4:20:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_demorec,10/23,,Demo Reception,6:00:00 PM,9:00:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_reg2,10/24,,Registration,9:00:00 AM,5:00:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_break3,10/24,,Break,10:30:00 AM,11:00:00 AM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_lunch2,10/24,,Lunch,12:10:00 PM,2:00:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_break4,10/24,,Poster session & Coffee Break,3:30:00 PM,5:00:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_townhall,10/24,,Town Hall,5:00:00 PM,5:45:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_banquet,10/24,,Banquet,6:00:00 PM,9:00:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_scommittee,10/24,,Steering Committee Meeting,9:30:00 PM,11:00:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_reg3,10/25,,Registration,9:00:00 AM,5:00:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_hcompwelcome,10/25,,Joint HCOMP/UIST Welcome,8:45:00 AM,9:00:00 AM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_keynote2,10/25,,Keynote: Prof. Niki Kittur,9:00:00 AM,10:00:00 AM,,,,,,,,,,,,,,,,,,,,,,Niki Kittur,,,Niki,,Kittur,,,Carnegie Mellon University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_break5,10/25,,Break,10:00:00 AM,10:40:00 AM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_lunch2,10/25,,Lunch,12:00:00 PM,1:30:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_break6,10/25,,Break,3:00:00 PM,3:40:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
np_closing,10/25,,UIST Closing,4:50:00 PM,5:30:00 PM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,