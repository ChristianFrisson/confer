Paper ID,Paper Title,Track Name,Abstract,Author Names,Author Emails,Subject Areas,Conflict Reasons,Files,Supplementary File,Paper has been anonymized (double blind)921,Transaction Healing: Scaling Optimistic Concurrency Control on Multicores,"Research, November 2015 Revision","Today's main-memory databases can support very high transaction rate for OLTP applications. However, when massive amounts of concurrent transactions contend on the same data records, the system performance can deteriorate significantly. This is especially the case when scaling transaction processing with optimistic concurrency control (OCC) on multicore machines. In this paper, we propose a new concurrency-control mechanism, called transaction healing, that exploits program semantics to scale the conventional OCC towards dozens of cores even under highly contended workloads. Transaction healing captures the dependencies across operations within a transaction prior to its execution. Instead of blindly rejecting a transaction once its validation fails, the proposed mechanism judiciously restores any non-serializable operation and heals inconsistent transaction states as well as query results according to the extracted dependencies. Transaction healing can partially update the membership of read/write sets when processing dependent transactions. Such overhead, however, is largely reduced by carefully avoiding false aborts and rearranging validation orders. We implemented the idea of transaction healing in THEDB, a main-memory database prototype that provides full ACID guarantee with a scalable commit protocol. By evaluating THEDB on a 48-core machine with two widely-used benchmarks, we confirm that transaction healing can scale near-linearly, yielding significantly higher transaction rate than the state-of-the-art OCC implementations.","Yingjun Wu*, National University of Singapo; Chee Yong Chan, National University of Singapore; Kian-Lee Tan, National University of Singapore",yingjun@comp.nus.edu.sg; chancy@comp.nus.edu.sg; tankl@comp.nus.edu.sg,Transaction processing*,"Kian-Lee Tan, National University of Singapore(tankl@comp.nus.edu.sg) This PC member is a co-author of the paper.; Stéphane Bressan, NUS(steph@nus.edu.sg) This PC member has been a co-worker in the same company or university within the past two years.;  Beng Chin Ooi, National University of Singapore(ooibc@comp.nus.edu.sg) This PC member has been a co-worker in the same company or university within the past two years.; Andy Pavlo, Carnegie Mellon University(pavlo@cs.cmu.edu) This PC member has been a co-worker in the same company or university within the past two years.; Anthony Tung, National University of Singapore(anthony@comp.nus.edu.sg) This PC member has been a co-worker in the same company or university within the past two years.","submission.pdf (963,828 bytes)",,I Agree922,A Hybrid Approach to Functional Dependency Discovery,"Research, November 2015 Revision","Functional dependencies are structural metadata that can be used for schema normalization, data integration, data cleansing, and many other data management tasks. Despite their importance, the functional dependencies of a specific dataset are usually unknown and almost impossible to discover manually. For this reason, database research has proposed various algorithms for functional dependency discovery. None, however, are able to process datasets of typical real-world size, e.g., datasets with more than 50 attributes and a million records. We present a hybrid discovery algorithm called HyFD, which combines fast approximation techniques with efficient validation techniques in order to find all minimal functional dependencies in a given dataset. While operating on compact data structures, HyFD not only outperforms all existing approaches, it also scales to much larger datasets.","Thorsten Papenbrock*, Hasso Plattner Institute; Felix Naumann, Hasso Plattner Institute",thorsten.papenbrock@hpi.de; Felix.Naumann@hpi.de,"Schema matching, data integration, and data cleaning*; Knowledge discovery, clustering, data mining","Felix Naumann, Hasso Plattner Institute(Felix.Naumann@hpi.de) This PC member is a co-author of the paper.; Jorge Quiané-Ruiz, Qatar Computing Research Institute(jquianeruiz@qf.org.qa) This PC member has been a collaborator within the past two years.; Lukasz Golab, University of Waterloo(lgolab@uwaterloo.ca) This PC member has been a collaborator within the past two years.; Paolo Papotti, QCRI(papotti@gmail.com) This PC member has been a collaborator within the past two years.; Ihab Ilyas, University of Waterloo(ilyas@uwaterloo.ca) This PC member has been a collaborator within the past two years.; Sergey Melnik, Google(melnik@google.com) This PC member is a relative or close personal friend of an author.","922.pdf (1,779,325 bytes)",,I Agree924,GTS: A Fast and Scalable Graph Processing Method based on Streaming Topology to GPUs,"Research, November 2015 Revision","A fast and scalable graph processing method becomes increasingly important as graphs become popular in a wide range of applications and their sizes are growing rapidly. Most of distributed graph processing methods require a lot of machines equipped with a total of thousands of GPU cores and a few terabyte main memory for handling billion-scale graphs. Meanwhile, GPUs could be a promising direction toward fast processing of large-scale graphs by exploiting thousands of GPU cores. All of the existing methods using GPUs, however, fail to process large-scale graphs that do not fit in main memory of a single machine. Here, we propose a fast and scalable graph processing method \GS\ that handles even RMAT32\,(64 billion edges) very efficiently only by using a single machine. The proposed method stores graphs in PCI-E SSDs and executes a graph algorithm using thousands of GPU cores while streaming topology data of graphs to GPUs via PCI-E interface. \GS\ is fast due to no communication overhead and scalable due to no data duplication from graph partitioning among machines. Through extensive experiments, we show that \GS\ consistently and significantly outperforms the major distributed graph processing methods, GraphX, Giraph, and PowerGraph, and the state-of-the-art GPU-based method TOTEM.","Min-Soo Kim*, DGIST; Kyu-Hyeon An, DGIST; Himchan Park, DGIST; Hyunseok Seo, DGIST; Jinwook Kim, DGIST",mskim@dgist.ac.kr; khan@dgist.ac.kr; chan150@dgist.ac.kr; hsseo@dgist.ac.kr; bm010515@dgist.ac.kr,"Graph data management, RDF, social networks*; Databases for emerging hardware","Wook-Shin Han, POSTECH(wshan.postech@gmail.com) This PC member is a co-author of the paper.; Jiawei Han, UIUC(hanj@cs.uiuc.edu) This PC member has been a co-worker in the same company or university within the past two years.; Stratos Idreos, Harvard University(stratos@seas.harvard.edu) This PC member has been a co-worker in the same company or university within the past two years.; Vijayshankar Raman, IBM Research - Almaden(ravijay@us.ibm.com) This PC member has been a co-worker in the same company or university within the past two years.","924.pdf (488,273 bytes)",,I Agree925,Fast Multi-column Sorting in Main-Memory Column-Stores,"Research, November 2015 Revision","Sorting is a crucial operation that could be used to implement SQL operators such as GROUP BY, ORDER BY, and SQL:2003 PARTITION BY. Queries with multiple attributes in those clauses are common in real workloads. When executing queries of that kind, state-of-the-art main-memory column-stores require one round of sorting per input column. With the advent of recent fast scans and denormalization techniques, that kind of multi-column sorting could become a bottleneck. In this paper, we propose a new technique called “code massaging”, which manipulates the bits across the columns so that the overall sorting time can be reduced by eliminating some rounds of sorting and/or by improving data parallelism. Empirical results show that a main-memory column-store with code massaging can achieve speedup of up to 4.7X, 4.7X, 2.8X, and 3.2X on TPC-H, TPC-H skew, TPC-DS, and real workload, respectively.","Wenjian Xu*, Hong Kong Polytechnic University; ziqiang Feng, the Hong Kong Polytechnic University; Eric Lo, Polytecnic University of Hong Kong",cswxu@comp.polyu.edu.hk; cszqfeng@comp.polyu.edu.hk; ericlo@comp.polyu.edu.hk,Databases for emerging hardware*; Distributed and parallel databases,"Eric Lo, Polytecnic University of Hong Kong(ericlo@comp.polyu.edu.hk) This PC member is a co-author of the paper.","451_revision.pdf (2,002,383 bytes)",,I Agree926,RDFind: Scalable Conditional Inclusion Dependency Discovery in RDF Datasets,"Research, November 2015 Revision","Inclusion dependencies (INDs) form an important integrity constraint on relational databases, supporting data management tasks, such as join path discovery and query optimization. Conditional inclusion dependencies (CINDs), which define including and included data in terms of conditions, allow to transfer these capabilities to RDF data. However, CIND discovery is computationally much more complex than IND discovery and the number of CINDs even on small RDF datasets are intractable. To cope with both problems, we first introduce the notion of pertinent CINDs with an adjustable relevance criterion to filter and rank CINDs based on their extent and implications among each other. Second, we present RDFind, a distributed system to efficiently discover all pertinent CINDs in RDF datasets. RDFind employs a lazy pruning strategy to drastically reduce the CIND search space. Also, its exhaustive parallelization strategy as well as robust data structures make it highly scalable. In our experimental evaluation, we show that RDFind is up to 419 times faster than the state-of-the-art, while considering a more general class of CINDs. Furthermore, it is capable of processing a very large dataset consisting of billions of triples, which was entirely infeasible before.","Sebastian Kruse*, Hasso Plattner Institute; Anja Jentzsch, Hasso Plattner Institute; Thorsten Papenbrock, Hasso Plattner Institute; Zoi Kaoudi, Qatar Computing Research Institute; Jorge Quiané-Ruiz, Qatar Computing Research Institute; Felix Naumann, Hasso Plattner Institute",sebastian.kruse@hpi.de; anja.jentzsch@hpi.de; thorsten.papenbrock@hpi.de; zkaoudi@qf.org.qa; jquianeruiz@qf.org.qa; Felix.Naumann@hpi.de,"Knowledge discovery, clustering, data mining*; Distributed and parallel databases; Graph data management, RDF, social networks; Query processing and optimization","Felix Naumann, Hasso Plattner Institute(Felix.Naumann@hpi.de) This PC member is a co-author of the paper.; Jorge Quiané-Ruiz, Qatar Computing Research Institute(jquianeruiz@qf.org.qa) This PC member is a co-author of the paper.; Ashraf Aboulnaga, Qatar Computing Research Institute(aaboulnaga@qf.org.qa) This PC member has been a co-worker in the same company or university within the past two years.; Essam Mansour, Qatar Reasearch and Computing Institute(emansour@qf.org.qa) This PC member has been a co-worker in the same company or university within the past two years.; Felix Naumann, Hasso Plattner Institute(Felix.Naumann@hpi.de) This PC member has been a co-worker in the same company or university within the past two years.; Paolo Papotti, QCRI(papotti@gmail.com) This PC member has been a co-worker in the same company or university within the past two years.; Samuel Madden, MIT(madden@csail.mit.edu) This PC member has been a collaborator within the past two years.; Jens Dittrich, Saarland University(jens.dittrich@cs.uni-saarland.de) This PC member has been a collaborator within the past two years.; Aaron Elmore, University of Chicago(aelmore@cs.uchicago.edu) This PC member has been a collaborator within the past two years.; Felix Naumann, Hasso Plattner Institute(Felix.Naumann@hpi.de) This PC member has been a collaborator within the past two years.; Lukasz Golab, University of Waterloo(lgolab@uwaterloo.ca) This PC member has been a collaborator within the past two years.; Alekh Jindal, MIT(alekh@csail.mit.edu) This PC member has been a collaborator within the past two years.; Paolo Papotti, QCRI(papotti@gmail.com) This PC member has been a collaborator within the past two years.; Ihab Ilyas, University of Waterloo(ilyas@uwaterloo.ca) This PC member has been a collaborator within the past two years.; Felix Naumann, Hasso Plattner Institute(Felix.Naumann@hpi.de) This PC member is or was an author's primary thesis advisor, no matter how long ago.; Sergey Melnik, Google(melnik@google.com) This PC member is a relative or close personal friend of an author.","926.pdf (2,657,183 bytes)",,I Agree927,Stop-and-Stare: Optimal Sampling Algorithms for Viral Marketing in Billion-scale Networks,"Research, November 2015 Revision","Influence Maximization (IM), that seeks a small set of key users who spread the influence widely into the network, is a core problem in multiple domains. It finds applications in viral marketing, epidemic control, and assessing cascading failures within complex systems. Despite the huge amount of effort, IM in billion-scale networks such as Facebook, Twitter, and World Wide Web has not been satisfactorily solved. Even the state-of-the-art methods such as TIM+ and IMM may take days on those networks.  In this paper, we propose SSA and D-SSA, two novel sampling frameworks for IM-based viral marketing problems. SSA and D-SSA are up to 1000 times faster than the SIGMOD'15 best method, IMM, while providing the same (1-1/e-\epsilon)-approximation guarantee. Underlying our frameworks is an innovative Stop-and-Stare strategy in which they stop at exponential check points to verify (stare) if there is adequate statistical evidence on the solution quality. Theoretically, we prove that SSA and D-SSA are the first approximation algorithms that use (asymptotically) minimum numbers of samples, meeting strict theoretical thresholds characterized for IM. The absolute superiority of SSA and D-SSA are confirmed through extensive experiments on real network data for IM and an another topic-aware viral marketing marketing problem, named TVM.","Hung Nguyen, Virginia Commonwealth Uni; My Thai, University of Florida; Thang Dinh*, VCU",hungnt@vcu.edu; mythai@cise.ufl.edu; tndinh@vcu.edu,"Graph data management, RDF, social networks*; Knowledge discovery, clustering, data mining",,"927.pdf (1,767,979 bytes)",,I Agree928,Adaptive Logging: Optimizing Logging and Recovery Costs in Distributed In-memory Databases,"Research, November 2015 Revision","By maintaining the data in main memory, in-memory databases dramatically reduce the I/O cost of transaction processing. However, for recovery purposes, in-memory systems still need to flush the log to disk, which incurs a substantial number of I/Os. Recently, command logging has been employed to replace the traditional data log (e.g., ARIES logging) in in-memory databases. Instead of recording how the tuples are updated, command logging only tracks the transactions that are being executed, thereby effectively reducing the size of the log and improving the performance. However, when there is a failure, all the transactions in the log after the last checkpoint must be redone sequentially and this significantly increases the cost of recovery.  In this paper, we first extend the command logging technique to a distributed system, where all the nodes can perform their recovery in parallel. We show that in a distributed system, the only bottleneck of recovery caused by command logging is the synchronization process that attempts to resolve the data dependency among the transactions. We then propose an adaptive logging approach by combining data logging and command logging. The percentage of data logging versus command logging becomes a tuning knob between the performance of transaction processing and recovery to meet different OLTP requirements, and a model is proposed to guide such tuning. Our experimental study compares the performance of our proposed adaptive logging, ARIES-style data logging and command logging on top of H-Store. The results show that adaptive logging can achieve a 10x boost for recovery and a transaction throughput that is comparable to that of command logging.","Chang Yao, NUS; Divyakant Agrawal*, ; Gang Chen, Zhejiang University;  Beng Chin Ooi, National University of Singapore; Sai Wu, Zhejiang University",yaochang@comp.nus.edu.sg; agrawal@cs.ucsb.edu; cg@zju.edu.cn; ooibc@comp.nus.edu.sg; wusai@zju.edu.cn,Distributed and parallel databases*; Transaction processing," Beng Chin Ooi, National University of Singapore(ooibc@comp.nus.edu.sg) This PC member is a co-author of the paper.; Stéphane Bressan, NUS(steph@nus.edu.sg) This PC member has been a co-worker in the same company or university within the past two years.; Jorge Quiané-Ruiz, Qatar Computing Research Institute(jquianeruiz@qf.org.qa) This PC member has been a co-worker in the same company or university within the past two years.; Kian-Lee Tan, National University of Singapore(tankl@comp.nus.edu.sg) This PC member has been a co-worker in the same company or university within the past two years.; Anthony Tung, National University of Singapore(anthony@comp.nus.edu.sg) This PC member has been a co-worker in the same company or university within the past two years.; Paolo Papotti, QCRI(papotti@gmail.com) This PC member has been a co-worker in the same company or university within the past two years.; Samuel Madden, MIT(madden@csail.mit.edu) This PC member has been a collaborator within the past two years.; Aaron Elmore, University of Chicago(aelmore@cs.uchicago.edu) This PC member has been a collaborator within the past two years.; Guoliang Li, Tsinghua University(liguoliang@tsinghua.edu.cn) This PC member has been a collaborator within the past two years.; Andy Pavlo, Carnegie Mellon University(pavlo@cs.cmu.edu) This PC member has been a collaborator within the past two years.; Alekh Jindal, MIT(alekh@csail.mit.edu) This PC member has been a collaborator within the past two years.; Bin Cui, Peking U.(bin.cui@pku.edu.cn) An author is or was the PC member's primary thesis advisor, no matter how long ago.; Sudipto Das, Microsoft Research(sudiptod@microsoft.com) An author is or was the PC member's primary thesis advisor, no matter how long ago.","928.pdf (962,858 bytes)",,I Agree929,DualSim: Parallel Subgraph Enumeration in a Massive Graph on a Single Machine,"Research, November 2015 Revision","Subgraph enumeration is important for many applications such as subgraph frequencies, network motif discovery, graphlet kernel computation, and studying the evolution of social networks. Most earlier work on subgraph enumeration assumes that graphs are resident in memory, which encounters serious scalability problems. Recently, efforts to enumerate all subgraphs in a large-scale graph have seemed to enjoy some success by partitioning the data graph and exploiting the distributed frameworks such as MapReduce and distributed graph engines. However, we notice that all existing distributed approaches have serious performance problems for subgraph enumeration due to the explosive number of partial results. In this paper, we design and implement a disk-based, single machine parallel subgraph enumeration solution called DualSim that can handle massive graphs without maintaining exponential numbers of partial results. Specifically, we propose a novel concept of the dual approach for subgraph enumeration. The dual approach swaps the roles of the data graph and the query graph. Specifically, instead of fixing the matching order in the query and then matching data vertices, it fixes the data vertices by fixing a set of disk pages and then finds all subgraph matchings in these pages. This enables us to significantly reduce the number of disk reads. We conduct extensive experiments with various real-world graphs to systematically demonstrate the superiority of DualSim over state-of-the-art distributed subgraph enumeration methods. DualSim outperforms the state-of-the-art methods by up to orders of magnitude, while the state-of-the-art methods fail for many queries due to explosive intermediate results.","Hyeonji Kim, DB Lab, POSTECH; JuneYoung Lee, ; Sourav S Bhowmick, Nanyang Technological University; Wook-Shin Han*, POSTECH; Jeong Hoon Lee, ; Seongyun Ko, POSTECH; Moath Jarrah, POSTECH",hjkim@dblab.postech.ac.kr; jylee@dblab.postech.ac.kr; assourav@ntu.edu.sg; wshan.postech@gmail.com; jhlee@dblab.postech.ac.kr; syko@dblab.postech.ac.kr; jarrah@dblab.postech.ac.kr,"Graph data management, RDF, social networks*; Knowledge discovery, clustering, data mining","Wook-Shin Han, POSTECH(wshan.postech@gmail.com) This PC member is a co-author of the paper.; James Cheng, CUHK(jcheng@cse.cuhk.edu.hk) This PC member has been a co-worker in the same company or university within the past two years.; Bingsheng He, Nanyang Technological University(bshe@ntu.edu.sg) This PC member has been a co-worker in the same company or university within the past two years.; Xiaokui Xiao, Nanyang Technological University(xkxiao@ntu.edu.sg) This PC member has been a co-worker in the same company or university within the past two years.; Curtis Dyreson, Utah State(Curtis.Dyreson@usu.edu) This PC member has been a collaborator within the past two years.; Shuai Ma, Beihang University, Beijing, China(shuai.ma@gmail.com) This PC member has been a collaborator within the past two years.; Stéphane Bressan, NUS(steph@nus.edu.sg) This PC member is a relative or close personal friend of an author.","929.pdf (2,962,072 bytes)",,I Agree930,Expressive Query Construction through Direct Manipulation of Nested Relational Results,"Research, November 2015 Revision","Despite extensive research on visual query systems, the standard way to interact with relational databases remains to be through SQL queries and tailored form interfaces. We consider three requirements to be essential to a successful alternative: (1)~query specification through direct manipulation of results, (2)~the ability to view and modify any part of the current query without departing from the direct manipulation interface, and (3)~SQL-like expressiveness. This paper presents the first visual query system to meet all three requirements in a single design. By directly manipulating nested relational results, and using spreadsheet idioms such as formulas and filters, the user can express a relationally complete set of query operators plus calculation, aggregation, outer joins, sorting, and nesting, while always remaining able to track and modify the state of the complete query. Our prototype gives the user an experience of responsive, incremental query building while pushing all actual query processing to the database layer. We evaluate our system with a formative and a controlled user study on 26 spreadsheet users; the controlled study shows our system significantly outperforming Microsoft Access on the System Usability Scale (SUS).","Eirik Bakke*, MIT; David Karger, MIT CSAIL",ebakke@mit.edu; karger@mit.edu,"Database usability*; Data models, semantics, query languages","Samuel Madden, MIT(madden@csail.mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Alvin Cheung, University of Washington(akcheung@cs.washington.edu) This PC member has been a co-worker in the same company or university within the past two years.; Jennie Duggan, Northwestern Univeristy(jennie.duggan@northwestern.edu) This PC member has been a co-worker in the same company or university within the past two years.; Aaron Elmore, University of Chicago(aelmore@cs.uchicago.edu) This PC member has been a co-worker in the same company or university within the past two years.; Stavros Papadopoulos, Intel Labs and MIT(stavrosp@csail.mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Holger Pirk, MIT(holger@csail.mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Nesime Tatbul, Intel Labs and MIT(tatbul@csail.mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Matei Zaharia, MIT(matei@mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Alekh Jindal, MIT(alekh@csail.mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Samuel Madden, MIT(madden@csail.mit.edu) This PC member has been a collaborator within the past two years.","paper_sigmod.pdf (7,006,438 bytes)",,I Agree931,Local Similarity Search for Unstructured Text,"Research, November 2015 Revision","With the growing popularity of electronic documents, replication can occur for many reasons. People may copy text segments from various sources and make modifications. In this paper, we study the problem of local similarity search to find partially replicated text. Unlike existing studies on similarity search which find entirely duplicated documents, our target is to identify documents that approximately share a pair of sliding windows which differ by no more than $\tau$ tokens. Our problem is technically challenging because for sliding windows the tokens to be indexed are less selective than entire documents, rendering set similarity join-based algorithms less efficient. Our proposed method is based on enumerating token combinations to obtain signatures with high selectivity. In order to strike a balance between signature and candidate generation, we partition the token universe and for different partitions we generate combinations composed of different numbers of tokens. A cost-aware algorithm is devised to find a good partitioning of the token universe. We also propose to leverage the overlap between adjacent windows to share computation and thus speed up query processing. In addition, we develop the techniques to support the large thresholds. Experiments on real datasets demonstrate the efficiency of our method against alternative solutions.","Pei Wang, Nagoya University; Chuan Xiao*, Nagoya University; Jianbin Qin, The University of New South Wales; Wei Wang, The University of New South Wales; Xiaoyang Zhang, The University of New South Wales; Yoshiharu Ishikawa, Nagoya University",wang@db.ss.is.nagoya-u.ac.jp; chuanx@nagoya-u.jp; jqin@cse.unsw.edu.au; weiw@cse.unsw.edu.au; Xiaoyang.Zhang@amadeus.com; y-ishikawa@nagoya-u.jp,"Schema matching, data integration, and data cleaning*; Query processing and optimization","Kaushik Chakrabarti, Microsoft Research(kaushik@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Xuemin Lin, University of New South Wales(lxue@cse.unsw.edu.au) This PC member has been a collaborator within the past two years.; Xiaochun Yang, Northeastern University, China(yangxc@mail.neu.edu.cn) This PC member has been a collaborator within the past two years.; Chengfei Liu, Swinburne University of Technology(cliu@swin.edu.au) This PC member has been a collaborator within the past two years.; Jiaheng Lu, University of Helsinki(jiahenglu@ruc.edu.cn) This PC member has been a collaborator within the past two years.; Xiaofeng Meng, ""Renmin University of China, China""(xfmeng@ruc.edu.cn) This PC member has been a collaborator within the past two years.; Mohamed Mokbel, Northeastern University(mokbel@cs.umn.edu) This PC member has been a collaborator within the past two years.; Xuemin Lin, University of New South Wales(lxue@cse.unsw.edu.au) This PC member is or was an author's primary thesis advisor, no matter how long ago.","SIGMOD2016-KWise-Revised.pdf (956,550 bytes)",,I Agree932,Enabling Incremental Query Re-Optimization,"Research, November 2015 Revision","As declarative query processing techniques expand in scope --- to the Web, data streams, network routers, and cloud platforms --- there is an increasing need for \emph{adaptive} query processing techniques that can re-plan in the presence of failures or unanticipated performance changes.  New information on data or the environment may affect which query plan we would prefer to run. Such adaptive techniques require innovation both in terms of the \emph{algorithms used to estimate costs} (e.g., using control theory, alternative plans, filter ordering, probing using multi-armed bandit strategies) as well as in terms of the \emph{planning algorithm} (i.e., redesigning the query optimizer to support continuous re-estimation of the best plan).  Existing cost-based query optimizers are not incremental in nature, and must be run ``from scratch'' upon each status or cost update.  Hence, they are not effective building blocks for rapid adaptivity.  An open question has been whether it is possible to build a \emph{cost-based re-optimizer} for adaptive query processing in a streaming or repeated-query execution environment, e.g., by \emph{incrementally} updating optimizer state given new cost information, much as a stream engine constantly updates its outputs given new data.  We show that this can be achieved beneficially, especially for stream processing workloads.  This lays the foundations upon which a variety of novel adaptive optimization \emph{algorithms} can be built.  Our techniques build upon the recently proposed approach of formulating query plan enumeration as a set of \emph{recursive datalog queries}; we develop a variety of novel optimization approaches to ensure effective pruning in both static and incremental cases. We implement our solution within an existing research query processing system, and show that it effectively supports cost-based initial optimization as well as frequent adaptivity. ","Mengmeng Liu*, UPENN; Zachary Ives, University of Pensylvania; Boon Loo, UPENN",liumengmeng@gmail.com; zives@cis.upenn.edu; boonloo@cis.upenn.edu,"Streams, sensor networks, complex event processing*; Query processing and optimization","Zachary Ives, University of Pensylvania(zives@cis.upenn.edu) This PC member is a co-author of the paper.; Todd Green, LogicBlox(todd.green@logicblox.com) This PC member has been a co-worker in the same company or university within the past two years.; Zhuowei Bao, Facebook Inc.(baozhuowei@gmail.com) This PC member has been a co-worker in the same company or university within the past two years.; Cong Yu, Google Research(congyu@google.com) This PC member has been a collaborator within the past two years.; Todd Green, LogicBlox(todd.green@logicblox.com) An author is or was the PC member's primary thesis advisor, no matter how long ago.","dec-adp-cover.pdf (5,022,717 bytes)",,I Agree933,System-X: A Relational Engine for Graph Processing,"Research, November 2015 Revision","There are two types of high-performance graph processing engines: low- and high-level engines. Low-level engines (Galois, PowerGraph, Snap) provide optimized data structures and computation models but require users to write low-level imperative code; hence ensuring that efficiency is the burden of the user. In high-level engines users write in query languages like datalog (SociaLite) or SQL (Grail). High-level engines are easier to use but are orders of magnitude slower than the low-level graph engines. We present System-X, a high-level engine that supports a rich datalog-like query language and achieves performance comparable to that of low-level engines. At the core of System-X's design is a new class of join algorithms that satisfy strong theoretical guarantees, but have thus far not achieved  performance comparable to that of specialized graph processing engines. To achieve high performance, System-X introduces a new join engine architecture, including a novel query optimizer and data layouts that leverage single-instruction multiple data (SIMD) parallelism. With this architecture, System-X outperforms high-level approaches by up to three orders of magnitude on graph pattern queries, PageRank, and Single-Source Shortest Paths (SSSP), and is an order of magnitude faster than many standard low-level baselines. We validate that System-X competes with the best-of-breed low-level engine (Galois), achieving comparable performance on PageRank and at most 3x worse performance on SSSP.","Christopher Aberger*, Stanford University; Susan Tu, Stanford University; Kunle Olukotun, Stanford University; Christopher Re, Stanford University",craberger@gmail.com; sctu@stanford.edu; kunle@stanford.edu; chrismre@cs.stanford.edu,"Graph data management, RDF, social networks*; Databases for emerging hardware; Query processing and optimization","Peter Bailis, Stanford University(pbailis@cs.berkeley.edu) This PC member has been a co-worker in the same company or university within the past two years.; Spyros Blanas, Ohio State(blanas.2@osu.edu) This PC member has been a co-worker in the same company or university within the past two years.; Michael Cafarella, University of Michigan(michjc@umich.edu) This PC member has been a co-worker in the same company or university within the past two years.; Foto Afrati, NTU Athens(afrati@softlab.ece.ntua.gr) This PC member has been a collaborator within the past two years.; Ihab Ilyas, University of Waterloo(ilyas@uwaterloo.ca) This PC member is a relative or close personal friend of an author.","933.pdf (1,013,006 bytes)",,I Agree934,Robust and Noise Resistant Wrapper Induction,"Research, November 2015 Revision","Wrapper induction is the problem of automatically inferring a query from annotated web pages of the same template. This query should not only select the annotated content accurately but also other content following the same template. Beyond accurately matching the template, we consider two additional requirements: (1) wrappers should be robust against a large class of changes to the web pages, and (2) the induction process should be noise resistant, i.e., tolerate slightly erroneous (e.g., machine generated) samples. Key to our approach is a query language that is powerful enough to permit accurate selection, but limited enough to force noisy samples to be generalized into wrappers that select the likely intended items. We introduce such a language as subset of XPATH and show that even for such a restricted language, inducing optimal queries according to a suitable scoring is infeasible. Nevertheless, our wrapper induction framework infers highly robust and noise resistant queries. We evaluate the queries on snapshots from web pages that change over time as provided by the Internet Archive, and show that the induced queries are as robust as the human-made queries. The queries often survive hundreds sometimes thousands of days, with many changes to the relative position of the selected nodes (including changes on template level). This is due to the few and discriminative anchor (intermediately selected) nodes of the generated queries. The queries are highly resistant against positive noise (up to 50%) and negative noise (up to 20%).","Tim Furche*, Oxford University; Jinsong Guo, Oxford University; Sebastian Maneth, ; christian Schallhart, ",tim.furche@gmail.com; jinsong.guo@cs.ox.ac.uk; smaneth@inf.ed.ac.uk; christian.schallhart@gmail.com,Information extraction*; Semi-structured data,"Tim Furche, Oxford(tim@furche.net) This PC member is a co-author of the paper.; Dan Olteanu, Oxford University & LogicBlox(dan.olteanu@cs.ox.ac.uk) This PC member has been a co-worker in the same company or university within the past two years.; Floris Geerts, University of Antwerp(floris.geerts@ua.ac.be) This PC member is a relative or close personal friend of an author.","p934_revised-with-cover.pdf (1,094,425 bytes)",,I Agree935,Publishing Attributed Social Graphs with Formal Privacy Guarantees,"Research, November 2015 Revision","Many data analysis tasks rely on the abstract of a graph to represent relations between entities, with attributes on the nodes and edges. Since the relationships encoded are often sensitive, we seek effective ways to release representative graphs which nevertheless protect the privacy of the data subjects. Prior work in this direction has focused primarily on the graph structure in isolation, and has not provided ways to handle richer graphs with correlated attributes.   We introduce an approach to release such graphs under the strong guarantee of differential privacy. We adapt existing graph models, and introduce a new one, and show how to augment them with meaningful privacy. This provides a complete workflow, where the input is a sensitive graph, and the output is a realistic synthetic graph. Our experimental study demonstrates that our process produces useful, accurate graph data. ","Zachary Jorgensen*, North Carolina State Universit; Ting Yu, QCRI; Graham Cormode, Warwick University",zjorgen@ncsu.edu; tyu@qf.org.qa; g.cormode@warwick.ac.uk,"Database security, privacy, access control*; Graph data management, RDF, social networks","Ashraf Aboulnaga, Qatar Computing Research Institute(aaboulnaga@qf.org.qa) This PC member has been a co-worker in the same company or university within the past two years.; Ashwin Machanavajjhala, Duke U.(ashwin@cs.duke.edu) This PC member has been a collaborator within the past two years.; Stavros Papadopoulos, Intel Labs and MIT(stavrosp@csail.mit.edu) This PC member has been a collaborator within the past two years.; Xiaokui Xiao, Nanyang Technological University(xkxiao@ntu.edu.sg) This PC member has been a collaborator within the past two years.; Lei Chen, Hong Kong University of Science and Technology(leichen@cse.ust.hk) This PC member has been a collaborator within the past two years.; Antonios Deligiannakis, Technical University of Crete(adeli@softnet.tuc.gr) This PC member has been a collaborator within the past two years.","paper_935_revised.pdf (826,648 bytes)",,I Agree936,Diversified Subgraph Querying in a Large Graph,"Research, November 2015 Revision","Subgraph querying in a large data graph is interesting for different applications. A recent study shows that top-k diversified results are useful since the number of matching subgraphs can be very large. In this work, we study the problem of top-k diversified subgraph querying which asks for a set of up to k subgraphs isomorphic to a given query graph, and which covers the most number of vertices. We propose a novel level-based algorithm for this problem which supports early termination and has a theoretical approximation guarantee. From experiments, most of our results on the tested real datasets are near optimal with a query time within 10ms on a commodity machine.","Yang Zhengwei*, CUHK; Ada Wai-Chee Fu, The Chinese University of Hong Kong; Ruifeng Liu, CUHK",yangzw1993@gmail.com; adafu@cse.cuhk.edu.hk; rfliu@cse.cuhk.edu.hk,"Graph data management, RDF, social networks*","James Cheng, CUHK(jcheng@cse.cuhk.edu.hk) This PC member has been a co-worker in the same company or university within the past two years.; James Cheng, CUHK(jcheng@cse.cuhk.edu.hk) This PC member has been a collaborator within the past two years.","936.pdf (820,332 bytes)",,I Agree937,Efficient and Progressive Group Steiner Tree Search,"Research, November 2015 Revision","The Group Steiner Tree (GST) problem is a fundamental problem in database area that has been successfully applied to keyword search in relational databases and team search in social networks. The state-of-the-art algorithm for the GST problem is a parameterized dynamic programming (DP) algorithm, which finds the optimal tree in $O(3^kn+2^k(n\log n + m))$ time, where $k$ is the number of given groups, $m$ and $n$ are the number of the edges and nodes of the graph respectively. The major limitations of the parameterized DP algorithm are twofold: (i) it is intractable even for very small values of $k$ (e.g., $k=8$) in large graphs due to its exponential complexity, and (ii) it cannot generate a solution until the algorithm has completed its entire execution. To overcome these limitations, we propose an efficient and progressive GST algorithm in this paper, called PrunedDP. It is based on newly-developed optimal-tree decomposition and conditional tree merging techniques. The proposed algorithm not only drastically reduces the search space of the parameterized DP algorithm, but it also produces progressively-refined feasible solutions during algorithm execution. To further speed up the PrunedDP algorithm, we propose a progressive $A^*$-search algorithm, based on several carefully-designed lower-bounding techniques. We conduct extensive experiments to evaluate our algorithms on several large scale real-world graphs. The results show that our best algorithm is not only able to generate progressively-refined feasible solutions, but it also finds the optimal solution with at least two orders of magnitude acceleration over the state-of-the-art algorithm, using much less memory.","Rong-Hua LI*, Shenzhen University, China; Lu Qin, University of Technology Sydne; Jeffrey Xu Yu, CUHK; Rui Mao, Shenzhen University",rhli@szu.edu.cn; lu.qin@uts.edu.au; yu@se.cuhk.edu.hk; mao@szu.edu.cn,"Graph data management, RDF, social networks*; Knowledge discovery, clustering, data mining; Query processing and optimization","James Cheng, CUHK(jcheng@cse.cuhk.edu.hk) This PC member has been a co-worker in the same company or university within the past two years.; Xuemin Lin, University of New South Wales(lxue@cse.unsw.edu.au) This PC member has been a collaborator within the past two years.; Jian Pei, Simon Fraser University(jpei@cs.sfu.ca) This PC member has been a collaborator within the past two years.; Jiaheng Lu, University of Helsinki(jiahenglu@ruc.edu.cn) This PC member is a relative or close personal friend of an author.","GroupSTreeSIGMODNew.pdf (406,566 bytes)",,I Agree940,DBSherlock: A Performance Diagnostic Tool for Transactional Databases,"Research, November 2015 Revision","Running an online transaction processing (OLTP) system is one of the most daunting tasks required of database administrators (DBAs). As businesses rely on OLTP databases to support their mission-critical and real-time applications, poor database performance directly impacts their revenue and user experience. As a result, DBAs constantly monitor, diagnose, and rectify any performance decays.  Unfortunately, the manual process of debugging and diagnosing OLTP performance problems is extremely tedious and non-trivial. Rather than being caused by a single slow query, performance problems in OLTP databases are often due to a large number of concurrent and competing transactions adding up to compounded, non-linear effects that are difficult to isolate. Sudden changes in request volume, transactional patterns, network traffic, or data distribution can cause previously abundant resources to become scarce, and the performance to plummet.  This paper presents a practical tool for assisting DBAs in quickly and reliably diagnosing performance problems in an OLTP database. By analyzing hundreds of statistics and configurations collected over the lifetime of the system, our algorithm quickly identifies a small set of potential causes and presents them to the DBA. The root-cause established by the DBA is  reincorporated into our algorithm as a new causal model to improve future diagnoses. Our experiments show that this algorithm is substantially more accurate than the state-of-the-art algorithm in finding correct explanations. ","Dong Young Yoon*, University of Michigan, Ann Arbor; Ning Niu, ; Barzan Mozafari, University of Michigan",dyoon@umich.edu; nniu@umich.edu; mozafari@umich.edu,Database monitoring and tuning*; Benchmarking and performance evaluation; Database usability,"Barzan Mozafari, University of Michigan(mozafari@umich.edu) This PC member is a co-author of the paper.; Michael Cafarella, University of Michigan(michjc@umich.edu) This PC member has been a co-worker in the same company or university within the past two years.; Sameer Agarwal, Databricks, Inc(sameerag@cs.berkeley.edu) This PC member has been a collaborator within the past two years.; Ion Stoica, UC Berkeley(istoica@cs.berkeley.edu) This PC member has been a collaborator within the past two years.; Kai Zeng, Microsoft(kaizeng@microsoft.com) This PC member has been a collaborator within the past two years.; Samuel Madden, MIT(madden@csail.mit.edu) This PC member is or was an author's primary thesis advisor, no matter how long ago.","explanation-sigmod2016_rev.pdf (1,620,903 bytes)",,I Agree943,GeckoFTL: Scalable Flash Translation Techniques For Very Large Flash Devices,"Research, November 2015 Revision","The amount of metadata needed for a flash translation layer is proportional to the storage capacity of a flash device. Ideally, this metadata should be stored in integrated RAM to enable fast access. However, as flash devices scale to terabytes, the space requirements of state-of-the-art flash translation layers (FTLs) are exceeding the available integrated RAM. Moreover, recovery time after power failure, which is proportional to the size of the metadata, is becoming impractical.   In this paper, we identify a key component of the metadata called the Page Validity Bitmap (PVB) as the bottleneck. PVB is used by the garbage-collectors of state-of-the-art FTLs to keep track of which physical pages in the device are invalid. PVB constitutes 95% of the FTL’s overall metadata space requirement, and recreating it when power fails takes most of recovery time. To solve this problem, we propose a novel page-associative FTL called GeckoFTL, whose central innovation is replacing PVB with a new data structure called Logarithmic Gecko. Logarithmic Gecko is similar to LSM-trees in that it logs updates about page validity metadata and to later reorganize them in flash to ensure fast and scalable access time. Relative to the baseline of storing PVB in flash, Logarithmic Gecko enables cheaper updates at the cost of slightly more expensive garbage-collection queries. We show that this is a good trade-off because (1) updates are intrinsically more frequent than garbage-collection queries to page validity metadata, and (2) flash writes are more expensive than flash reads. We demonstrate analytically and empirically that GeckoFTL achieves a 95% reduction in space requirements and a 51% reduction in recovery time by storing page validity metadata in flash while keeping the contribution to internal IO overheads 98% lower than the baseline. ","Niv Dayan*, Harvard University; Philippe Bonnet, IT University of Copenhagen; Stratos Idreos, Harvard University",dayan@g.harvard.edu; phbo@itu.dk; stratos@seas.harvard.edu,"Storage, indexing, and physical database design*; Databases for emerging hardware","Stratos Idreos, Harvard University(stratos@seas.harvard.edu) This PC member is a co-author of the paper.; Philippe Bonnet, IT University of Copenhagen(phbo@itu.dk) This PC member is a co-author of the paper.; Peter Boncz, CWI Amsterdam(p.boncz@cwi.nl) This PC member has been a co-worker in the same company or university within the past two years.; Holger Pirk, MIT(holger@csail.mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Hannes Mühleisen, CWI Amsterdam(hannes.muehleisen@cwi.nl) This PC member has been a co-worker in the same company or university within the past two years.; Samuel Madden, MIT(madden@csail.mit.edu) This PC member has been a collaborator within the past two years.; Erietta Liarou, EPFL(erietta@gmail.com) This PC member has been a collaborator within the past two years.; Panagiotis Karras, Skoltech(karras@skoltech.ru) This PC member has been a collaborator within the past two years.; Martin Kersten, CWI(martin.kersten@cwi.nl) This PC member is or was an author's primary thesis advisor, no matter how long ago.","review.pdf (871,918 bytes)",,I Agree945,Speedup Graph Processing by Graph Ordering,"Research, November 2015 Revision","The CPU cache performance is one of the key issues to efficiency in database systems. It is reported that cache miss latency takes a half of the execution time in database systems. To improve the CPU cache performance, there are studies to support searching including cache-oblivious, and cache-conscious trees.  In this paper, we focus on CPU speedup for graph computing in general by reducing the CPU cache miss ratio for different graph algorithms. The approaches dealing with trees are not applicable to graphs which are complex in nature. In this paper, we explore a general approach to speed up CPU computing, in order to further enhance the efficiency of the graph algorithms without changing the graph algorithms (implementations) and the data structures used. That is, we aim at designing a general solution that is not for a specific graph algorithm, neither for a specific data structure. The approach studied in this work is graph ordering, which is to find the optimal permutation among all nodes in a given graph by keeping nodes that will be frequently accessed together locally, to minimize the CPU cache miss ratio. We prove the graph ordering problem is NP-hard, and give a basic algorithm with a bounded approximation. To improve the time complexity of the basic algorithm, we further propose a new algorithm to reduce the time complexity and improve the efficiency with new optimization techniques based on a new data structure. We conducted extensive experiments to evaluate our approach in comparison with other 9 possible graph orderings (such as the one obtained by METIS) using 8 large real graphs and 9 representative graph algorithms. We confirm that our approach can achieve high performance by reducing the CPU cache miss ratios significantly.","Hao Wei*, Chinese University of Hong Kong; Jeffrey Xu Yu, CUHK; Can LU, CUHK; Xuemin Lin, University of New South Wales",hwei@se.cuhk.edu.hk; yu@se.cuhk.edu.hk; lucan@se.cuhk.edu.hk; lxue@cse.unsw.edu.au,"Graph data management, RDF, social networks*","Xuemin Lin, University of New South Wales(lxue@cse.unsw.edu.au) This PC member is a co-author of the paper.; James Cheng, CUHK(jcheng@cse.cuhk.edu.hk) This PC member has been a co-worker in the same company or university within the past two years.; Jian Pei, Simon Fraser University(jpei@cs.sfu.ca) This PC member has been a collaborator within the past two years.","feedback-and-paper.pdf (917,339 bytes)",,I Agree946,Generating Preview Tables for Entity Graphs,"Research, November 2015 Revision","We witness an unprecedented proliferation of big, complex entity graphs that capture many types of entities and their relationships. Users are tapping into big, complex entity graphs for many applications. It is challenging to select entity graphs for a particular need, given abundant datasets from many sources and the oftentimes scarce information for them.  We propose methods to produce preview tables for compact presentation of important entity types and relationships in entity graphs. The preview tables assist users in attaining a quick and rough preview of the data.  They can be shown in a limited display space for a user to browse and explore, before she decides to spend time and resources to fetch and investigate the complete dataset. We propose scoring functions for measuring the goodness of previews. Based on the scoring measures, We formulate several optimization problems that look for previews with the highest scores according to intuitive goodness measures, under various constraints on preview size and distance between preview tables. The optimization problem under distance constraint is NP-hard.  We design a dynamic-programming algorithm and an Apriori-style algorithm for finding optimal previews. Our experiments, comparison with related work and user studies demonstrated the scoring measures' accuracy and the discovery algorithms' efficiency.","Ning Yan, Huawei; Sona Hasani*, University of Texas Arlington; Abolfazl Asudeh, University of Texas-Arlington; Chengkai Li, UT Arlington",Yan.NingYan@huawei.com; sona.hasani@mavs.uta.edu; ab.asudeh@mavs.uta.edu; cli@uta.edu,"Database usability*; Graph data management, RDF, social networks; Knowledge discovery, clustering, data mining","Chengkai Li, UT Arlington(cli@uta.edu) This PC member is a co-author of the paper.; Mahashweta Das, HP Labs(mahashweta@gmail.com) This PC member has been a co-worker in the same company or university within the past two years.; Cong Yu, Google Research(congyu@google.com) This PC member has been a collaborator within the past two years.; Shimin Chen, ICT, Chinese Academy of Science(chensm@ict.ac.cn) This PC member has been a collaborator within the past two years.; Kevin Chang, University of Illinois at Urbana-Champaign(kcchang@illinois.edu) This PC member is or was an author's primary thesis advisor, no matter how long ago.","tabview_revision.pdf (725,437 bytes)",,I Agree947,Reducing the Storage Overhead of Main-Memory OLTP Databases with Hybrid Indexes,"Research, November 2015 Revision","Using indexes for query execution is crucial for achieving high performance in modern on-line transaction processing databases. For a main-memory database, however, these indexes consume a large fraction of the total memory available and are thus a major source of storage overhead of in-memory databases. To reduce this overhead, we propose using a two-stage index: The first stage ingests all incoming entries and is kept small for fast read and write operations. The index periodically migrates entries from the first stage to the second, which uses a more compact, read-optimized data structure. Our first contribution is hybrid index, a dual-stage index architecture that achieves both space efficiency and high performance. Our second contribution is Dual-Stage Transformation (DST), a general method for converting any order-preserving index structure into a hybrid index. Our third contribution is applying DST to three popular order-preserving index structures and evaluating them in both standalone microbenchmarks and a full in-memory DBMS using several transaction processing workloads. Our results show that hybrid indexes provide comparable throughput to the original ones while reducing the memory overhead by up to 70%.","Huanchen Zhang*, Carnegie Mellon University; David Andersen, Carnegie Mellon University; Andy Pavlo, Carnegie Mellon University; Michael Kaminsky, Intel Labs; Lin Ma, Carnegie Mellon University; Rui Shen, Carnegie Mellon University",huanche1@cs.cmu.edu; dga@cs.cmu.edu; pavlo@cs.cmu.edu; michael.e.kaminsky@intel.com; lin.ma@cs.cmu.edu; ruis@andrew.cmu.edu,"Storage, indexing, and physical database design*; Query processing and optimization; Transaction processing","Andy Pavlo, Carnegie Mellon University(pavlo@cs.cmu.edu) This PC member is a co-author of the paper.; Wolfgang Gatterbauer, CMU Tepper School of Business(gatt@cmu.edu) This PC member has been a co-worker in the same company or university within the past two years.; Jayant Madhavan, Google(jayant@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Christopher Olston, Google(olston@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Cong Yu, Google Research(congyu@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Luna Dong, Google(lunadong@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Anastasios Kementsietsidis, Google Inc(akement@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Hongrae Lee, Google(hrlee@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Sergey Melnik, Google(melnik@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Samuel Madden, MIT(madden@csail.mit.edu) This PC member has been a collaborator within the past two years.; Ashraf Aboulnaga, Qatar Computing Research Institute(aaboulnaga@qf.org.qa) This PC member has been a collaborator within the past two years.; Peter Bailis, Stanford University(pbailis@cs.berkeley.edu) This PC member has been a collaborator within the past two years.; Jennie Duggan, Northwestern Univeristy(jennie.duggan@northwestern.edu) This PC member has been a collaborator within the past two years.; Aaron Elmore, University of Chicago(aelmore@cs.uchicago.edu) This PC member has been a collaborator within the past two years.; Essam Mansour, Qatar Reasearch and Computing Institute(emansour@qf.org.qa) This PC member has been a collaborator within the past two years.; Ippokratis Pandis, Amazon Web Services(ippokratis@gmail.com) This PC member has been a collaborator within the past two years.; Nesime Tatbul, Intel Labs and MIT(tatbul@csail.mit.edu) This PC member has been a collaborator within the past two years.; Sudipto Das, Microsoft Research(sudiptod@microsoft.com) This PC member has been a collaborator within the past two years.; Philippe Cudre-Mauroux, University of Fribourg(phil@exascale.info) This PC member has been a collaborator within the past two years.; Justin Levandoski, Microsoft(justin.levandoski@microsoft.com) This PC member has been a collaborator within the past two years.","paper_response.pdf (1,170,125 bytes)",,I Agree949,Graph Stream Summarization: From Big Bang to Big Crunch,"Research, November 2015 Revision","A graph stream, which refers to the graph with edges being updated sequentially in a form of a stream, has important applications in cyber security and social networks. Due to the sheer volume and highly dynamic nature of graph streams, the practical way of handling them is by summarization. Given a graph stream G, directed or undirected, the problem of graph stream summarization is to summarize G as SG with a much smaller (sublinear) space, linear construction time and constant maintenance cost for each edge update, such that SG allows many queries over G to be approximately conducted efficiently. The widely used practice of summarizing data streams is to treat each stream element independently by e.g., hashing- or sampling-based methods, without maintaining the connections between elements. Hence, existing methods can only solve ad-hoc problems, without supporting diversified and complicated analytics over graph streams. We present B3C, a generalized graph stream summary. Given an incoming edge, it summarizes both node and edge information in constant time. Consequently, the summary forms a graphical sketch where edges capture the connections inside elements, and nodes maintain relationships across elements. We discuss a wide range of supported queries and establish some error bounds. In addition, we experimentally show that B3C can effectively and efficiently support analytics over graph streams.","Nan Tang*, QCRI; Qing Chen, QCRI; Prasenjit Mitra, QCRI",ntang@qf.org.qa; qchen@qf.org.qa; pmitra@qf.org.qa,"Graph data management, RDF, social networks*; Streams, sensor networks, complex event processing","Ashraf Aboulnaga, Qatar Computing Research Institute(aaboulnaga@qf.org.qa) This PC member has been a co-worker in the same company or university within the past two years.; Jorge Quiané-Ruiz, Qatar Computing Research Institute(jquianeruiz@qf.org.qa) This PC member has been a co-worker in the same company or university within the past two years.; Daniel Kifer, Penn State(dkifer@cse.psu.edu) This PC member has been a co-worker in the same company or university within the past two years.; Paolo Papotti, QCRI(papotti@gmail.com) This PC member has been a co-worker in the same company or university within the past two years.; Samuel Madden, MIT(madden@csail.mit.edu) This PC member has been a collaborator within the past two years.; Guoliang Li, Tsinghua University(liguoliang@tsinghua.edu.cn) This PC member has been a collaborator within the past two years.; Shuai Ma, Beihang University, Beijing, China(shuai.ma@gmail.com) This PC member has been a collaborator within the past two years.; Floris Geerts, University of Antwerp(floris.geerts@ua.ac.be) This PC member has been a collaborator within the past two years.; Jiannan Wang, UC Berkeley(jnwang@berkeley.edu) This PC member has been a collaborator within the past two years.; Ihab Ilyas, University of Waterloo(ilyas@uwaterloo.ca) This PC member has been a collaborator within the past two years.","graph.stream.summarization.pdf (1,235,603 bytes)",,I Agree950,GPL: A GPU-based Pipelined Query Processing Engine,"Research, November 2015 Revision","Graphics Processing Units (GPUs) have evolved as a powerful query co-processor for main memory On-Line Analytical Processing (OLAP) databases. However, existing GPU-based query processors adopt a kernel-based execution approach which optimizes individual kernels for resource utilization and executes the GPU kernels involved in the query plan one by one. Such a kernel-based approach cannot utilize all GPU resources efficiently due to the resource underutilization of individual kernels and excessive memory accesses across kernel executions. In this paper, we propose GPL, a novel pipelined query execution engine to improve the resource utilization of query co-processing on the GPU. Different from the existing kernel-based execution, GPL takes advantage of hardware features of new-generation GPUs including concurrent kernel execution and efficient data communication channel between kernels. We use the tiling technique to logically partition the input data into smaller data tiles so that the pipelined query plan can be adapted in a cost-based manner. We further develop an analytical model to guide the generation of the optimal pipelined query plan. We evaluate GPL with TPC-H queries on both AMD and NVIDIA GPUs. The experimental results show that 1) the analytical model is able to guide determining the suitable parameter values in pipelined query execution plan, and 2) GPL is able to significantly outperform the state-of-the-art kernel-based query processing approaches, with improvement by 41% and 50% on AMD and NVIDIA GPUs, respectively.","Jiong He, NTU; Paul Johns, Nanyang Technological University; Bingsheng He*, Nanyang Technological University",hejiongsg@gmail.com; JOHNS001@e.ntu.edu.sg; bshe@ntu.edu.sg,Databases for emerging hardware*; Distributed and parallel databases; Query processing and optimization,"Bingsheng He, Nanyang Technological University(bshe@ntu.edu.sg) This PC member is a co-author of the paper.; Bingsheng He, Nanyang Technological University(bshe@ntu.edu.sg) This PC member has been a co-worker in the same company or university within the past two years.; Bingsheng He, Nanyang Technological University(bshe@ntu.edu.sg) This PC member has been a collaborator within the past two years.; Qiong Luo, Hong Kong University of Science and Technology(luo@cse.ust.hk) This PC member has been a collaborator within the past two years.; Bingsheng He, Nanyang Technological University(bshe@ntu.edu.sg) This PC member is or was an author's primary thesis advisor, no matter how long ago.; Qiong Luo, Hong Kong University of Science and Technology(luo@cse.ust.hk) An author is or was the PC member's primary thesis advisor, no matter how long ago.; Qiong Luo, Hong Kong University of Science and Technology(luo@cse.ust.hk) This PC member is a relative or close personal friend of an author.; Xiaokui Xiao, Nanyang Technological University(xkxiao@ntu.edu.sg) This PC member is a relative or close personal friend of an author.","revision-final.pdf (3,220,166 bytes)",,I Agree954,Distributed Set Reachability,"Research, November 2015 Revision","In this paper, we focus on the efficient and scalable processing of set-reachability queries over a distributed, directed data graph. A {\em set-reachability query} is a generalized form of a graph-reachability query, in which we consider two sets $S$ and $T$ of source and target vertices, respectively, to be given as the query. The result of a set-reachability query are all pairs of source and target vertices $(s, t)$, with $s \in S$ and $t \in S$, where $s$ is reachable to $t$ (denoted as $S \leadsto T$). In case the data graph is partitioned into multiple, edge-disjoint subgraphs (e.g., when distributed across multiple compute nodes in a cluster), we refer to the resulting set-reachability problem as a {\em distributed set-reachability query}. The key goal in processing a distributed set-reachability query over a partitioned data graph both efficiently and in a scalable manner is (1) to avoid redundant computations within the local compute nodes as much as possible, (2) to partially evaluate the local components of a reachability query $S \leadsto T$ among all compute nodes in parallel, and (3) to minimize both the size and number of iteratively exchanged messages among the compute nodes.   The distributed set reachability problem has a plethora of applications in graph analytics and query-processing tasks. The current W3C recommendation for SPARQL 1.1, for example, introduces a notion of labeled {\em property paths}, which resolves to processing a form of generalized graph-patterns queries with set-reachability predicates. Moreover, analyzing dependencies among {\em social-network communities} inherently involves reachability checks between large sets of source and target vertices.Our experiments confirm very significant performance gains of our approach in comparison to state-of-art graph engines such as Giraph++, and over a variety of graph collections with up to one billion edges.","Sairam Gurajada*, MPI Informatics; Martin  Theobald, University of Ulm",sairam@cse.iitm.ac.in; martin.theobald@uni-ulm.de,"Graph data management, RDF, social networks*; Distributed and parallel databases","Gerhard Weikum, Max-Planck-Institut für Informatik(weikum@mpi-inf.mpg.de) This PC member has been a co-worker in the same company or university within the past two years.; Denilson Barbosa, University of Alberta(denilson@ualberta.ca) This PC member has been a co-worker in the same company or university within the past two years.; Floris Geerts, University of Antwerp(floris.geerts@ua.ac.be) This PC member has been a co-worker in the same company or university within the past two years.; Rainer Gemulla, Max-Planck-Institut für Informatik(rgemulla@mpi-inf.mpg.de) This PC member has been a co-worker in the same company or university within the past two years.; Katja Hose, Aalborg University(khose@cs.aau.dk) This PC member has been a co-worker in the same company or university within the past two years.; Srikanta Bedathur, IBM Research(sbedathur@in.ibm.com) This PC member has been a collaborator within the past two years.; Thomas Neumann, TU Munich(neumann@in.tum.de) This PC member has been a collaborator within the past two years.","954.pdf (859,804 bytes)",,I Agree955,T-Part: Partitioning of Transactions for Onward-Pushing in Deterministic Database Systems,"Research, November 2015 Revision","The deterministic database systems have recently been shown to yield high throughput on a cluster of commodity machines while ensuring the strong consistency between replicas, provided that the data can be well-partitioned on these machines. However, data partitioning can be suboptimal for many reasons in real-world applications. In this paper, we present T-Part, a transaction execution engine that partitions transactions in a deterministic database system to deal with the unforeseeable workloads or workloads whose data are hard to partition. By modeling the dependency between transactions as a T-graph and continuously partitioning that graph, T-Part allows each transaction to know which later transactions on other machines will read its writes so that it can push onward the writes to those latter transactions immediately after committing. This onward pushing reduces the chance that the latter transactions stall due to the unavailability of remote data. T-Part also opens up numerous opportunities for sophisticated optimization of data movement and transaction execution, such as deciding which transactions to replicate/reorder to achieve better transaction partitioning. We implement a prototype for T-Part. Extensive experiments are conducted and the results demonstrate the effectiveness of T-Part. ","Shan-Hung Wu*, National Tsing Hua University; Tsai-Yu Feng, National Tsing Hua University; Meng-Kai Liao, National Tsing Hua University; Shao-Kan Pi, National Tsing Hua University; Yu-Shan Lin, National Tsing Hua University",shwu@cs.nthu.edu.tw; tyfeng@netdb.cs.nthu.edu.tw; mkliao@netdb.cs.nthu.edu.tw; skpi@netdb.cs.nthu.edu.tw; yslin@netdb.cs.nthu.edu.tw,Transaction processing*,,"merged_document_6.pdf (1,074,978 bytes)",,I Agree956,Matrix Sketching Over Sliding Windows,"Research, November 2015 Revision"," Large-scale matrix computation becomes essential for many data data applications, and hence the problem of sketching matrix with small space and high precision has received extensive study for the past few years. This problem is often considered in the row-update  streaming model, where the data set is a matrix $A \in \mathbb{R}^{n \times d}$, and the processor receives a row ($1\times d$) of $A$ at each timestamp. The goal is to maintain a  smaller matrix (termed approximation matrix, or simply approximation) $B\in \mathbb{R}^{\ell\times d}$ as an approximation to $A$, such that the covariance error $\|A^T A - B^TB\|$ is small and $\ell \ll n$.  This paper studies continuous tracking approximations to the matrix defined by a sliding window of most recent rows. We consider both sequence-based and time-based window. We show that maintaining $A^TA$ exactly requires linear space in the sliding window model, as opposed to $O(d^2)$ space in the streaming model. With this observation, we present three general frameworks for matrix sketching on sliding windows. The sampling techniques give random samples of the rows in the window according to their squared norms. The \textsf{Logarithmic Method} converts a {\em mergeable} streaming matrix sketch into a matrix sketch on time-based sliding windows. The \textsf{Dyadic Interval} framework converts arbitrary streaming matrix sketch into a matrix sketch on sequence-based sliding windows.  In addition to proving all algorithmic properties theoretically, we also conduct extensive empirical study with real datasets to demonstrate the efficiency of these algorithms.","Zhewei Wei*, Renmin University of China; Xuancheng Liu, Johns Hopkins University; Feifei Li, University of Utah; Shuo Shang, China University of Petroleum, Beijing; Xiaoyong Du, Renmin Univ. China; Ji-Rong Wen, Renmin University of China",zhewei@ruc.edu.cn; shinysuccessliu@126.com; lifeifei@cs.utah.edu; jedi.shang@gmail.com; duyong@ruc.edu.cn; jirong.wen@gmail.com,"Streams, sensor networks, complex event processing*","Feifei Li, University of Utah(lifeifei@cs.utah.edu) This PC member is a co-author of the paper.; Jiaheng Lu, University of Helsinki(jiahenglu@ruc.edu.cn) This PC member has been a co-worker in the same company or university within the past two years.; Xiaofeng Meng, ""Renmin University of China, China""(xfmeng@ruc.edu.cn) This PC member has been a co-worker in the same company or university within the past two years.; Jiawei Han, UIUC(hanj@cs.uiuc.edu) This PC member has been a collaborator within the past two years.;  Beng Chin Ooi, National University of Singapore(ooibc@comp.nus.edu.sg) This PC member has been a collaborator within the past two years.; Kevin Zheng, University of Queensland(kevinz@itee.uq.edu.au) This PC member has been a collaborator within the past two years.; Jian Pei, Simon Fraser University(jpei@cs.sfu.ca) This PC member has been a collaborator within the past two years.; George Kollios, Boston University(gkollios@cs.bu.edu) This PC member is or was an author's primary thesis advisor, no matter how long ago.","956.pdf (824,916 bytes)",,I Agree959,Distributed Wavelet Thresholding for Maximum Error Metrics,"Research, November 2015 Revision","Modern data analytics involve simple and complex computations over enormous numbers of data records. The volume of data and the increasingly stringent response-time requirements place increasing emphasis on the efficiency of approximate query processing. A major challenge over the past years has been the efficient construction of fixed-space synopses that provide a deterministic quality guarantee, often expressed in terms of a maximum error metric. For data reduction, wavelet decomposition has proved to be a very effective tool, as it can successfully approximate sharp discontinuities and provide accurate answers to queries. However, existing polynomial time wavelet thresholding schemes that minimize maximum error metrics are constrained with impractical time and space complexities for large datasets. In order to provide a practical solution to the problem, we develop parallel algorithms that take advantage of key-properties of the wavelet decomposition and allocate tasks to multiple workers. To that end, we present i) a general framework for the parallelization of existing dynamic programming algorithms, ii) a parallel version of one such DP-based algorithm and iii) a new parallel greedy algorithm for the problem. To the best of our knowledge, this is the first attempt to scale algorithms for wavelet thresholding for maximum error metrics via a state-of-the-art distributed runtime. Our extensive experiments on both real and synthetic datasets over Hadoop show that the proposed algorithms achieve linear scalability and superior running-time performance compared to their centralized counterparts. Furthermore, our distributed greedy algorithm outperforms the distributed version of the current state-of-the-art dynamic programming algorithm by 2--4 times, while its performance gain does not compromise the quality of results.","IOANNIS MYTILINIS*, NTUA; DIMITRIOS TSOUMAKOS, ; NECTARIOS KOZIRIS, ",gmytil@cslab.ece.ntua.gr; dtsouma@ionio.gr; nkoziris@cslab.ece.ntua.gr,"Uncertain, probabilistic, and approximate databases*; Distributed and parallel databases","Panagiotis Karras, Skoltech(karras@skoltech.ru) This PC member has been a collaborator within the past two years.; Evaggelia Pitoura, University of Ioannina(pitoura@cs.uoi.gr) This PC member has been a collaborator within the past two years.","959.pdf (1,081,675 bytes)",,I Agree961,Bridging the Archipelago between Row-Stores and Column-Stores for Hybrid Workloads,"Research, November 2015 Revision","Data-intensive applications seek to obtain new insights in real-time by analyzing a combination of historical data sets alongside recently collected data. This means that to support such hybrid workloads, database management systems (DBMSs) need to handle both fast ACID transactions and complex analytical queries on the same database. But the current trend is to use specialized systems that are optimized for only one of these workloads, and thus require an organization to maintain separate copies of the database. This adds additional cost to deploying a database application in terms of both storage and administration overhead.  To overcome this barrier, we present a hybrid DBMS architecture that efficiently supports varied workloads on the same database. Our approach differs from previous methods in that we use a single execution engine that is oblivious to the storage layout of data without sacrificing the performance benefits of the specialized systems. This obviates the need to maintain separate copies of the database in multiple independent systems. We also present a technique to continuously evolve the database’s physical storage layout by analyzing the queries’ access patterns and choosing the optimal layout for different segments of data within the same table. To evaluate this work, we implemented our architecture in an in-memory DBMS. Our results show that our approach delivers up to 3_ higher through- put compared to static storage layouts across different workloads. We also demonstrate that our continuous adaptation mechanism allows the DBMS to achieve a near-optimal layout for an arbitrary workload without requiring any manual tuning.","Joy Arulraj*, Carnegie Mellon University; Andy Pavlo, Carnegie Mellon University; Prashanth Menon, Carnegie Mellon University",jarulraj@cs.cmu.edu; pavlo@cs.cmu.edu; pmenon@cs.cmu.edu,"Query processing and optimization*; Data warehousing, OLAP, SQL Analytics; Transaction processing","Andy Pavlo, Carnegie Mellon University(pavlo@cs.cmu.edu) This PC member is a co-author of the paper.; Wolfgang Gatterbauer, CMU Tepper School of Business(gatt@cmu.edu) This PC member has been a co-worker in the same company or university within the past two years.; Samuel Madden, MIT(madden@csail.mit.edu) This PC member has been a collaborator within the past two years.; Ashraf Aboulnaga, Qatar Computing Research Institute(aaboulnaga@qf.org.qa) This PC member has been a collaborator within the past two years.; Peter Bailis, Stanford University(pbailis@cs.berkeley.edu) This PC member has been a collaborator within the past two years.; Jennie Duggan, Northwestern Univeristy(jennie.duggan@northwestern.edu) This PC member has been a collaborator within the past two years.; Aaron Elmore, University of Chicago(aelmore@cs.uchicago.edu) This PC member has been a collaborator within the past two years.; Essam Mansour, Qatar Reasearch and Computing Institute(emansour@qf.org.qa) This PC member has been a collaborator within the past two years.; Ippokratis Pandis, Amazon Web Services(ippokratis@gmail.com) This PC member has been a collaborator within the past two years.; Nesime Tatbul, Intel Labs and MIT(tatbul@csail.mit.edu) This PC member has been a collaborator within the past two years.; Sudipto Das, Microsoft Research(sudiptod@microsoft.com) This PC member has been a collaborator within the past two years.; Philippe Cudre-Mauroux, University of Fribourg(phil@exascale.info) This PC member has been a collaborator within the past two years.; Justin Levandoski, Microsoft(justin.levandoski@microsoft.com) This PC member has been a collaborator within the past two years.","complete.pdf (1,170,527 bytes)",,I Agree962,Functional Dependencies for Graphs,"Research, November 2015 Revision","We propose a class of functional dependencies for graphs, referred to as GFDs. GFDs capture both attribute-value dependencies and topological structures of entities, and subsume conditional functional dependencies (CFDs) as a special case. We show that the satisfiability and implication problems for GFDs are coNP-complete and NP-complete, respectively, no worse than their CFD counterparts. We also show that the validation problem for GFDs is coNP-complete. Despite the intractability, we develop parallel scalable algorithms for catching violations of GFDs in large-scale graphs. Using real-life and synthetic data, we experimentally verify that \GFDs provide an effective approach to detecting inconsistencies in knowledge and social graphs. ","Wenfei Fan, Univsersity of Edinburgh; YINGHUI WU*, Washington State University; Jingbo Xu, University of Edinburgh",wenfei@inf.ed.ac.uk; yinghui@eecs.wsu.edu; jingbo.xu@ed.ac.uk,"Graph data management, RDF, social networks*; Semi-structured data","Floris Geerts, University of Antwerp(floris.geerts@ua.ac.be) This PC member has been a co-worker in the same company or university within the past two years.; Jiawei Han, UIUC(hanj@cs.uiuc.edu) This PC member has been a collaborator within the past two years.; Luna Dong, Google(lunadong@google.com) This PC member has been a collaborator within the past two years.; Shuai Ma, Beihang University, Beijing, China(shuai.ma@gmail.com) An author is or was the PC member's primary thesis advisor, no matter how long ago.; Carmem Hara, Universidade Federal do Parana(carmem@inf.ufpr.br) An author is or was the PC member's primary thesis advisor, no matter how long ago.; Anastasios Kementsietsidis, Google Inc(akement@google.com) An author is or was the PC member's primary thesis advisor, no matter how long ago.; Christoph Koch, EPFL(christoph.koch@epfl.ch) This PC member is a relative or close personal friend of an author.;  Beng Chin Ooi, National University of Singapore(ooibc@comp.nus.edu.sg) This PC member is a relative or close personal friend of an author.; Anthony Tung, National University of Singapore(anthony@comp.nus.edu.sg) This PC member is a relative or close personal friend of an author.","paper.pdf (474,129 bytes)",,I Agree963,Sequential Data Cleaning: A Statistical Approach,"Research, November 2015 Revision","Errors are prevalent in data sequences, such as GPS trajec- tories or sensor readings. Existing methods on cleaning se- quential data employ a constraint on value changing speeds and perform constraint-based repairing. While such speed constraints are effective in identifying large spike errors, the small errors that do not significantly deviate from the truth and indeed satisfy the speed constraints can hardly be iden- tified and repaired. To handle such small errors, in this pa- per, we propose a statistical based cleaning method. Rather than declaring a broad constraint of max/min speeds, we model the probability distribution of speed changes. The repairing problem is thus to maximize the likelihood of the sequence w.r.t. the probability of speed changes. We for- malize the likelihood-based cleaning problem, show its np- hardness, devise exact algorithms, and propose several ap- proximate/heuristic methods to trade off effectiveness for efficiency. Experiments on real data sets (in various appli- cations) demonstrate the superiority of our proposal.","Aoqian Zhang, ; Shaoxu Song*, Tsinghua University; Jianmin Wang, Tsinghua University",zaq13@mails.tsinghua.edu.cn; sxsong@tsinghua.edu.cn; jimwang@tsinghua.edu.cn,"Schema matching, data integration, and data cleaning*","Guoliang Li, Tsinghua University(liguoliang@tsinghua.edu.cn) This PC member has been a co-worker in the same company or university within the past two years.; Jiannan Wang, UC Berkeley(jnwang@berkeley.edu) This PC member has been a co-worker in the same company or university within the past two years.; Xuemin Lin, University of New South Wales(lxue@cse.unsw.edu.au) This PC member has been a collaborator within the past two years.; Lei Chen, Hong Kong University of Science and Technology(leichen@cse.ust.hk) This PC member has been a collaborator within the past two years.; Jian Pei, Simon Fraser University(jpei@cs.sfu.ca) This PC member has been a collaborator within the past two years.","963.pdf (605,667 bytes)",,I Agree964,Range-based Obstructed Nearest Neighbor Queries,"Research, November 2015 Revision","In this paper, we study a novel variant of obstructed nearest neighbor queries, namely, range-based obstructed nearest neighbor (RONN) search. A natural generalization of continuous obstructed nearest-neighbor (CONN), an RONN query retrieves the obstructed nearest neighbor for every point in a specified range. To process RONN, we first propose a CONN-Based (CONNB) algorithm as our baseline, which reduces the RONN query into a range query and four CONN queries processed using an R-tree. To address the shortcomings of the CONNB algorithm, we then propose a new RONN by R-tree Filtering (RONN-RF) algorithm, which explores effective filtering, also using R-tree. Next, we propose a new index, called O-tree, dedicated for indexing objects in the obstructed space. The novelty of O-tree lies in the idea of dividing the obstructed space into non-obstructed space, aiming to efficiently retrieve highly qualified candidates for RONN processing. We develop an O-tree construction algorithm and propose a space division scheme, called optimal obstacle balance (OOB) scheme, to address the tree balance problem. Accordingly, we propose an efficient algorithm, called RONN by O-tree Acceleration (RONN-OA), which exploits O-tree to accelerate query processing of RONN. In addition, we extend O-tree for indexing polygons. At last, we conduct a comprehensive performance evaluation using both real and synthetic datasets to validate our ideas and the proposed algorithms. The experimental result shows that the RONN-OA algorithm outperforms the two R-tree based algorithms significantly. Moreover, we show that the OOB scheme achieves the best tree balance in O-tree and outperforms two baseline schemes.","Huaijie Zhu*, Northeastern University, China; Xiaochun Yang, Northeastern University, China; bin wang, ; Wang-Chien Lee, Pennsylvania State University, USA",huaijie@stumail.neu.edu.cn; yangxc@mail.neu.edu.cn; binwang@mail.neu.edu.cn; wlee@cse.psu.edu,Spatio-temporal databases*,"Xiaochun Yang, Northeastern University, China(yangxc@mail.neu.edu.cn) This PC member is a co-author of the paper.","sigmod-response-revision-huaijie.pdf (755,522 bytes)",,I Agree965,Wander Join: Online Aggregation via Random Walks,"Research, November 2015 Revision","Joins are expensive, and online aggregation over joins was proposed to mitigate the cost, which offers users a nice and flexible tradeoff between query efficiency and accuracy in a continuous, online fashion. However, the state-of-the-art approach, in both internal and external memory, is based on ripple join, which is still very expensive and even needs unrealistic assumptions (e.g., tuples in a table are stored in random order). This paper proposes a new approach the wander join algorithm, to the online aggregation problem by performing random walks over the underlying join graph.  Wander join produces independent but non-uniform samples, which gives huge performance gains over the uniform but non-independent samples returned by ripple join. We also design an optimizer that chooses the optimal plan for conducting the random walks without having to collect any statistics a priori. Wander join works for different types of joins including chain, acyclic, cyclic joins, with selection predicates and group-by clauses. It easily extends to $\theta$-joins as well.  Extensive experiments using the TPC-H benchmark have demonstrated the superior performance of wander join over ripple join in both internal and external memory. We have also implemented and tested wander join in the latest version of PostgreSQL; the results show its excellent efficiency and effectiveness in a full fledged, commercial level DBMS.","Feifei Li, University of Utah; Bin Wu, ; Ke Yi*, HKUST; Zhuoyue Zhao, ",lifeifei@cs.utah.edu; bwuac@cse.ust.hk; yike@cse.ust.hk; zzy7896321@sjtu.edu.cn,"Query processing and optimization*; Data warehousing, OLAP, SQL Analytics","Feifei Li, University of Utah(lifeifei@cs.utah.edu) This PC member is a co-author of the paper.; Qiong Luo, Hong Kong University of Science and Technology(luo@cse.ust.hk) This PC member has been a co-worker in the same company or university within the past two years.; Lei Chen, Hong Kong University of Science and Technology(leichen@cse.ust.hk) This PC member has been a co-worker in the same company or university within the past two years.; George Kollios, Boston University(gkollios@cs.bu.edu) This PC member is or was an author's primary thesis advisor, no matter how long ago.","965.pdf (692,502 bytes)",,I Agree967,Efficient Subgraph Matching by Postponing Cartesian Products,"Research, November 2015 Revision","In this paper, we study the problem of subgraph matching that extracts all subgraph isomorphic embeddings of a query graph $q$ in a large data graph $G$. The existing algorithms for subgraph matching follow Ullmann's backtracking approach; that is, iteratively map query vertices to data vertices by following a matching order of query vertices. It has been shown that the matching order of query vertices is a very important aspect to the efficiency of a subgraph  matching algorithm. Recently, many advanced techniques, such as enforcing connectivity and merging similar vertices in query or data graphs, have been proposed to provide an effective matching order with the aim to reduce unpromising intermediate results especially the ones caused by redundant Cartesian products. In this paper, for the first time we address the issue of unpromising results by Cartesian products from  ``dissimilar'' vertices. We propose a new framework by postponing the Cartesian products based on the structure of a query to minimize the redundant Cartesian products. Our second contribution is proposing a new path-based auxiliary data structure, with the size $O(|E(G)|\times |V(q)|)$, to generate a matching order and conduct subgraph matching, which significantly reduces the exponential size $O(|V(G)|^{|V(q)|-1})$ of the existing path-based auxiliary data structure, where $V (G)$ and $E (G)$ are the vertex and edge sets of a data graph $G$, respectively, and $V (q)$ is the vertex set of a query $q$. Extensive empirical studies on real and synthetic graphs demonstrate that our techniques outperform the state-of-the-art algorithms by up to $3$ orders of magnitude.","Fei Bi*, University of New South Wales; Lijun Chang, University of New South Wales; Xuemin Lin, University of New South Wales; Lu Qin, University of Technology Sydne; Wenjie Zhang, ",f.bi@student.unsw.edu.au; ljchang@cse.unsw.edu.au; lxue@cse.unsw.edu.au; lu.qin@uts.edu.au; zhangw@cse.unsw.edu.au,"Graph data management, RDF, social networks*; Query processing and optimization","Xuemin Lin, University of New South Wales(lxue@cse.unsw.edu.au) This PC member is a co-author of the paper.; Zhou Minqi, East China Normal University(mqzhou@sei.ecnu.edu.cn) This PC member has been a co-worker in the same company or university within the past two years.; Jian Pei, Simon Fraser University(jpei@cs.sfu.ca) This PC member has been a collaborator within the past two years.","Paper967.pdf (2,954,444 bytes)",,I Agree968,Simba: Efficient In-Memory Spatial Analytics,"Research, November 2015 Revision","Large spatial data becomes ubiquitous. As a result, it is critical to provide fast, scalable, and high-throughput spatial queries and analytics for numerous applications in location-based services (LBS). Traditional spatial databases and spatial analytics systems are disk-based and optimized for IO efficiency. But increasingly, data are stored and processed in memory to achieve low latency, and CPU time becomes the new bottleneck. We present the Simba (Spatial In-Memory Big data Analytics) system that offers scalable and efficient in-memory spatial query processing and analytics for big spatial data. Simba is based on Spark and runs over a cluster of commodity machines. In particular, Simba extends the Spark SQL engine to support rich spatial queries and analytics through both SQL and the DataFrame API. It introduces the concept and construction of indexes over RDDs in order to work with big spatial data and complex spatial operations. Lastly, Simba implements an effective query optimizer, which leverages its indexes and novel spatial-aware optimizations, to achieve both low latency and high throughput. Extensive experiments over large data sets demonstrate Simba’s superior performance compared against other spatial analytics system.","Dong Xie, University of Utah; Feifei Li*, University of Utah; Bin Yao, Shanghai JIao Tong University; Gefei Li, Shanghai JIao Tong University; Liang Zhou, Shanghai JIao Tong University; Minyi Guo, Shanghai JIao Tong University",dongx@cs.utah.edu; lifeifei@cs.utah.edu; yaobin@cs.sjtu.edu.cn; oizz01@sjtu.edu.cn; nichozl@sjtu.edu.cn; guo-my@cs.sjtu.edu.cn,Distributed and parallel databases*; Spatio-temporal databases,"Feifei Li, University of Utah(lifeifei@cs.utah.edu) This PC member is a co-author of the paper.; George Kollios, Boston University(gkollios@cs.bu.edu) This PC member is or was an author's primary thesis advisor, no matter how long ago.","968.pdf (2,122,907 bytes)",,I Agree970,Graph Analytics Through Fine-Grained Parallelism,"Research, November 2015 Revision","Large graphs are getting increasingly popular and even indispensable in many applications, for example, in social media data, large networks, and knowledge bases. Efficient graph analytics thus becomes an important subject of study. To increase efficiency and scalability, in-memory computation and parallelism have been explored extensively to speed up various graph analytical workloads. In many graph analytical engines (e.g., Pregel, Neo4j, GraphLab), parallelism is achieved via one of the three concurrency control models, namely, bulk synchronization processing (BSP), asynchronous processing, and synchronous processing. Among them, synchronous processing has the potential to achieve the best performance due to fine-grained parallelism, while ensuring the correctness and the convergence of the computation, if an effective concurrency control scheme is used. This paper explores the topological properties of the underlying graph to design and implement a highly effective concurrency control scheme for efficient synchronous processing in an in-memory graph analytical engine. Our design uses a novel hybrid approach that combines 2PL (two-phase locking) with OCC (optimistic concurrency control), for high degree and low degree vertices in a graph respectively. Our results show that the proposed hybrid synchronous scheduler has significantly outperformed other synchronous schedulers in existing graph analytical engines, as well as BSP and asynchronous schedulers.","Zechao Shang*, CUHK; Feifei Li, University of Utah; Jeffrey Xu Yu, CUHK; Zhiwei Zhang, CUHK; Hong Cheng, ",zcshang@se.cuhk.edu.hk; lifeifei@cs.utah.edu; yu@se.cuhk.edu.hk; zwzhang@se.cuhk.edu.hk; hcheng@se.cuhk.edu.hk,"Distributed and parallel databases*; Graph data management, RDF, social networks","Feifei Li, University of Utah(lifeifei@cs.utah.edu) This PC member is a co-author of the paper.; James Cheng, CUHK(jcheng@cse.cuhk.edu.hk) This PC member has been a co-worker in the same company or university within the past two years.; Xuemin Lin, University of New South Wales(lxue@cse.unsw.edu.au) This PC member has been a collaborator within the past two years.; Lei Chen, Hong Kong University of Science and Technology(leichen@cse.ust.hk) This PC member has been a collaborator within the past two years.; Jian Pei, Simon Fraser University(jpei@cs.sfu.ca) This PC member has been a collaborator within the past two years.; Jiawei Han, UIUC(hanj@cs.uiuc.edu) This PC member is or was an author's primary thesis advisor, no matter how long ago.; George Kollios, Boston University(gkollios@cs.bu.edu) This PC member is or was an author's primary thesis advisor, no matter how long ago.","970.pdf (565,048 bytes)",,I Agree971,SQL Schema Design,"Research, November 2015 Revision","Normalization helps us find a database schema at design time that can process the most frequent updates efficiently at run time. Unfortunately, relational normalization only works for idealized database instances in which duplicates and null markers are not present. On one hand, these features occur frequently in real-world data compliant with the industry standard SQL, and especially in modern application domains. On the other hand, the features impose challenges that have made it impossible so far to extend the existing forty year old normalization framework to SQL. We introduce a new class of functional dependencies and show that they provide the right notion for SQL schema design. Axiomatic and linear-time algorithmic characterizations of the associated implication problem are established. These foundations enable us to propose a Boyce-Codd normal form for SQL. Indeed, we justify the normal form by showing that it permits precisely those SQL instances which are free from data redundancy. Unlike the relational case, there are SQL schemata that cannot be converted into Boyce-Codd normal form. Nevertheless, for an expressive sub-class of our functional dependencies we establish a normalization algorithm that always produces a schema in Value-Redundancy free normal form. This normal form permits precisely those instances which are free from any redundant data value occurrences other than the null marker. Experiments show that our functional dependencies occur frequently in real-world data and that they are effective in eliminating redundant values from these data sets without loss of information.","Henning Koehler, Massey University; Sebastian Link*, The University of Auckland",h.koehler@massey.ac.nz; s.link@auckland.ac.nz,"Database monitoring and tuning*; Data models, semantics, query languages","Floris Geerts, University of Antwerp(floris.geerts@ua.ac.be) This PC member is a relative or close personal friend of an author.","submission_971.pdf (701,558 bytes)",,I Agree972,iOLAP: Incremental OLAP for Interactive Analysis on Big Data,"Research, November 2015 Revision","The size of data and the complexity of analytics continue to grow along with the need for timely and cost-effective analysis. However, the growth of computation power cannot keep up with the growth of data. This calls for a paradigm shift from traditional batch OLAP processing model to an incremental OLAP processing model. In this paper, we propose iOLAP, an incremental OLAP query engine that provides a smooth trade-off between query accuracy and latency, and fulfills a full spectrum of user requirements from approximate but timely query execution to a more traditional  accurate query execution. iOLAP enables interactive incremental query processing using a novel mini-batch execution model---given an OLAP query, iOLAP first randomly partitions the input dataset into smaller sets (mini-batches) and then incrementally processes through these partitions by executing a delta update query on each mini-batch,  where each subsequent delta update query computes an update based on the output of the previous one.  The key idea behind iOLAP is a novel delta update algorithm that models delta processing as an uncertainty propagation problem, and minimizes the recomputation during each subsequent delta update by minimizing the uncertainties in the partial (including intermediate) query results. We implement iOLAP on top of Apache Spark and have successfully demonstrated it at scale on over $100$ machines. Extensive experiments on a multitude of queries and datasets demonstrate that iOLAP can deliver approximate query answers for complex OLAP queries orders of magnitude faster than a traditional OLAP engine,  while continuously delivering updates every few seconds.","Kai Zeng*, Microsoft; Sameer Agarwal, Databricks Inc.; Ion Stoica, UC Berkeley",kaizeng@microsoft.com; sameer@databricks.com; istoica@cs.berkeley.edu,"Data warehousing, OLAP, SQL Analytics*; Query processing and optimization; Uncertain, probabilistic, and approximate databases","Sameer Agarwal, Databricks, Inc(sameerag@cs.berkeley.edu) This PC member is a co-author of the paper.; Ion Stoica, UC Berkeley(istoica@cs.berkeley.edu) This PC member is a co-author of the paper.; Kai Zeng, Microsoft(kaizeng@microsoft.com) This PC member is a co-author of the paper.; Peter Alvaro, UC Berkeley(palvaro@gmail.com) This PC member has been a co-worker in the same company or university within the past two years.; Peter Bailis, Stanford University(pbailis@cs.berkeley.edu) This PC member has been a co-worker in the same company or university within the past two years.; Matei Zaharia, MIT(matei@mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Jiannan Wang, UC Berkeley(jnwang@berkeley.edu) This PC member has been a co-worker in the same company or university within the past two years.; Samuel Madden, MIT(madden@csail.mit.edu) This PC member has been a collaborator within the past two years.; Barzan Mozafari, University of Michigan(mozafari@umich.edu) This PC member has been a collaborator within the past two years.","response+972.pdf (1,406,719 bytes)",,I Agree974,Optimization of Nested Queries using the NF2 Algebra,"Research, November 2015 Revision","A key promise of SQL is that the optimizer will find the most efficient execution plan, regardless of how the query is formulated. In general, query optimizers of modern database systems are able to keep this promise, with the notable exception of nested queries. While several optimization techniques for nested queries have been proposed, their adoption in practice has been limited. In this paper, we argue that the NF2 (non-first normal form) algebra, which was originally designed to process nested tables, is a better approach to nested query optimization as it fulfills two key requirements. First, the NF2 algebra can represent all types of nested queries as well as both existing and novel optimization techniques based on its equivalences. Second, performance benefits can be achieved with little changes to existing transformation-based query optimizers as the NF2 algebra is an extension of the relational algebra. ","Jürgen Hölsch*, University of Konstanz; Michael Grossniklaus, University of Konstanz; Marc Scholl, University of Konstanz",juergen.hoelsch@uni-konstanz.de; michael.grossniklaus@uni-konstanz.de; marc.scholl@uni-konstanz.de,Query processing and optimization*; Benchmarking and performance evaluation,"Torsten Grust, U Tübingen(torsten.grust@uni-tuebingen.de) An author is or was the PC member's primary thesis advisor, no matter how long ago.","SIGMOD2016.pdf (940,262 bytes)",,I Agree975,Interactive and Deterministic Data Cleaning,"Research, November 2015 Revision","We present Falcon, an interactive, deterministic, and declarative data cleaning system, which uses SQL update (SQLU) queries as the language to repair data. Falcon does not rely on the existence of a set of pre-defined data quality rules. On the contrary, it encourages users to explore the data, identify possible problems, and make updates to fix them. Bootstrapped by one user update, Falcon guesses a set of possible sql update queries that can be used to repair the data. The main technical challenge addressed in this paper consists in finding a set of sql update queries that is minimal in size and at the same time fixes the largest number of errors in the data. We formalize this problem as a search in a lattice-shaped space. To guarantee that the chosen updates are semantically correct, Falcon navigates the lattice by interacting with users to gradually validate the set of sql update queries. Besides using traditional one-hop based traverse algorithms (e.g., BFS or DFS), we describe novel multi-hop search algorithms such that Falcon can dive over the lattice and conduct the search efficiently. Our novel search strategy is coupled with a number of optimization techniques to further prune the search space and efficiently maintain the lattice. We have conducted extensive experiments using both real-world and synthetic datasets to show that Falcon can effectively communicate with users in data repairing.","Jian He, ; Enzo Veltri, ; Donatello Santoro, ; Guoliang Li, Tsinghua University; Gianni Mecca, Università della Basilicata; Paolo Papotti, QCRI; Nan Tang*, QCRI",jianhe25@gmail.com; enzo.veltri@gmail.com; donatello.santoro@gmail.com; liguoliang@tsinghua.edu.cn; giansalvatore.mecca@gmail.com; papotti@gmail.com; ntang@qf.org.qa,"Schema matching, data integration, and data cleaning*","Guoliang Li, Tsinghua University(liguoliang@tsinghua.edu.cn) This PC member is a co-author of the paper.; Gianni Mecca, Università della Basilicata(giansalvatore.mecca@gmail.com) This PC member is a co-author of the paper.; Paolo Papotti, QCRI(papotti@gmail.com) This PC member is a co-author of the paper.; Ashraf Aboulnaga, Qatar Computing Research Institute(aaboulnaga@qf.org.qa) This PC member has been a co-worker in the same company or university within the past two years.; Essam Mansour, Qatar Reasearch and Computing Institute(emansour@qf.org.qa) This PC member has been a co-worker in the same company or university within the past two years.; Jorge Quiané-Ruiz, Qatar Computing Research Institute(jquianeruiz@qf.org.qa) This PC member has been a co-worker in the same company or university within the past two years.; K. Selcuk Candan, Arizona State University(candan@asu.edu) This PC member has been a co-worker in the same company or university within the past two years.; Samuel Madden, MIT(madden@csail.mit.edu) This PC member has been a collaborator within the past two years.; Shuai Ma, Beihang University, Beijing, China(shuai.ma@gmail.com) This PC member has been a collaborator within the past two years.; Felix Naumann, Hasso Plattner Institute(Felix.Naumann@hpi.de) This PC member has been a collaborator within the past two years.;  Beng Chin Ooi, National University of Singapore(ooibc@comp.nus.edu.sg) This PC member has been a collaborator within the past two years.; Kian-Lee Tan, National University of Singapore(tankl@comp.nus.edu.sg) This PC member has been a collaborator within the past two years.; Floris Geerts, University of Antwerp(floris.geerts@ua.ac.be) This PC member has been a collaborator within the past two years.; Boris Glavic, IIT Chicago(bglavic@iit.edu) This PC member has been a collaborator within the past two years.; Alekh Jindal, MIT(alekh@csail.mit.edu) This PC member has been a collaborator within the past two years.; Jiannan Wang, UC Berkeley(jnwang@berkeley.edu) This PC member has been a collaborator within the past two years.; Ihab Ilyas, University of Waterloo(ilyas@uwaterloo.ca) This PC member has been a collaborator within the past two years.","interactive.and.deterministic.data.cleaning.pdf (1,510,178 bytes)",,I Agree976,SLING: A Near-Optimal Index Structure for SimRank,"Research, November 2015 Revision","SimRank is a similarity measure for graph nodes that has numerous applications in practice. Scalable SimRank computation has been the subject of extensive research for more than a decade, and yet, none of the existing solutions can efficiently derive SimRank scores on large graphs with provable accuracy guarantees. In particular, the state-of-the-art solution requires up to a few seconds to compute a SimRank score in million-node graphs, and does not offer any worst-case assurance in terms of the query error.  This paper presents SLING, an efficient index structure for SimRank computation. SLING guarantees that each SimRank score returned has at most $\epsilon$ additive error, and it answers any single-pair and single-source SimRank queries in $O(1/\epsilon)$ and $O(n/\epsilon)$ time, respectively. These time complexities are near-optimal, and are significantly better than the asymptotic bounds of the states of the art. Furthermore, SLING requires only $O(n/\epsilon)$ space (which is also near-optimal) and $O(m/\epsilon + n\log \frac{n}{\delta}/\epsilon^2)$ pre-computation time, where $\delta$ is the failure probability of the preprocessing algorithm. We experimentally evaluate SLING with a variety of real-world graphs with up to several millions of nodes. Our results demonstrate that SLING is up to 20000 times (resp. 80 times) faster than competing methods for single-pair (resp. single-source) SimRank queries.","Boyu Tian*, Shanghai Jiao Tong University; Xiaokui Xiao, Nanyang Technological University",anshantby@sjtu.edu.cn; xkxiao@ntu.edu.sg,"Graph data management, RDF, social networks*; Knowledge discovery, clustering, data mining","Xiaokui Xiao, Nanyang Technological University(xkxiao@ntu.edu.sg) This PC member is a co-author of the paper.; Bingsheng He, Nanyang Technological University(bshe@ntu.edu.sg) This PC member has been a co-worker in the same company or university within the past two years.; James Cheng, CUHK(jcheng@cse.cuhk.edu.hk) This PC member has been a collaborator within the past two years.; Stavros Papadopoulos, Intel Labs and MIT(stavrosp@csail.mit.edu) This PC member has been a collaborator within the past two years.; Kevin Zheng, University of Queensland(kevinz@itee.uq.edu.au) This PC member has been a collaborator within the past two years.; Jiaheng Lu, University of Helsinki(jiahenglu@ruc.edu.cn) This PC member has been a collaborator within the past two years.","731.pdf (662,157 bytes)",,I Agree977,How to Architect a Query Compiler,"Research, November 2015 Revision","This paper studies architecting query compilers. The state of the art in query compiler construction is lagging behind that in the compilers field. We attempt to remedy this by exploring the key causes of technical challenges in need of well founded solutions, and by gathering the most relevant ideas and approaches from the PL and compilers communities for easy digestion by database researchers. All query compilers known to us are more or less monolithic template expanders that do the bulk of the compilation task in one large leap. Such systems are hard to build and maintain. We propose to use a stack of multiple DSLs on different levels of abstraction with lowering in multiple steps to make query compilers easier to build and extend, ultimately allowing us to create more convincing and sustainable compiler-based data management systems. We attempt to derive our advice for creating such DSL stacks from widely acceptable principles. We have also re-created a well-known query compiler following these ideas and report on this effort.","Amir Shaikhha*, EPFL; Yannis Klonatos, EPFL; lionel Parreaux, EPFL; Lewis Brown, ; Mohammad Dashti, EPFL; Christoph Koch, EPFL",amir.shaikhha@epfl.ch; yannis.klonatos@epfl.ch; lionel.parreaux@epfl.ch; lewis.brown@epfl.ch; mohammad.dashti@epfl.ch; christoph.koch@epfl.ch,Query processing and optimization*,"Christoph Koch, EPFL(christoph.koch@epfl.ch) This PC member is a co-author of the paper.; Ippokratis Pandis, Amazon Web Services(ippokratis@gmail.com) This PC member has been a co-worker in the same company or university within the past two years.; Peter Boncz, CWI Amsterdam(p.boncz@cwi.nl) This PC member has been a collaborator within the past two years.; Thomas Neumann, TU Munich(neumann@in.tum.de) This PC member has been a collaborator within the past two years.; Oliver Kennedy, University at Buffalo(okennedy@buffalo.edu) An author is or was the PC member's primary thesis advisor, no matter how long ago.; Yanif Ahmad, Johns Hopkins University(yanif@cs.jhu.edu) This PC member is a relative or close personal friend of an author.","977-architect-query-compiler.pdf (2,020,238 bytes)",,I Agree982,Data Polygamy: The Many-Many Relationships among Urban Spatio-Temporal Data Sets,"Research, November 2015 Revision","The increasing ability to collect data from urban environments, coupled with a push towards openness by governments, has resulted in the availability of numerous spatio-temporal data sets covering diverse aspects of a city. Discovering relationships between these data sets can produce new insights by enabling domain experts to not only test but also generate hypotheses. However, discovering these relationships is difficult. First, a relationship between two data sets may occur only at certain locations and/or time periods. Second, the sheer number and size of the data sets, coupled with the diverse spatial and temporal scales at which the data is available, presents computational challenges on all fronts, from indexing and querying to analyzing them. Finally, it is non-trivial to differentiate between meaningful and spurious relationships. To address these challenges, we propose Data Polygamy, a scalable topology-based framework that allows users to query for statistically significant relationships between spatio-temporal data sets. We have performed an experimental evaluation using over 300 spatial-temporal urban data sets which shows that our approach is scalable and effective at identifying interesting relationships.","Fernando Chirigati*, NYU; Harish Doraiswamy, New York University; Theodoros Damoulas, University of Warwick; Juliana Freire, NYU",fchirigati@nyu.edu; harishd@nyu.edu; damoulas@warwick.ac.uk; juliana.freire@nyu.edu,Spatio-temporal databases*,"Alberto Lerner, New York University(alberto.lerner@gmail.com) This PC member has been a co-worker in the same company or university within the past two years.","paper-with-response.pdf (2,764,194 bytes)",,I Agree984,How to win a hot dog eating contest,"Research, November 2015 Revision","In the quest for valuable information, modern big data applications continuously monitor streams of data. These applications demand low latency stream processing even when faced with high volume and velocity of incoming changes and the user's desire to ask complex queries. In this paper, we study low-latency incremental computation of complex SQL queries in both local and distributed streaming environments. We develop a technique for the efficient incrementalization of queries with nested aggregates for batch updates. We identify the cases in which batch processing can boost the performance of incremental view maintenance but also demonstrate that tuple-at-a-time processing often can achieve better performance in local mode. Batch updates are essential for enabling distributed incremental view maintenance and amortizing the cost of network communication and synchronization. We show how to derive incremental programs optimized for running on large-scale processing platforms. Our implementation of distributed incremental view maintenance can process tens of million of tuples with few-second latency on a scale of up to 1,000 nodes. ","Milos Nikolic*, EPFL; Christoph Koch, EPFL; Mohammad Dashti, EPFL",milos.nikolic@epfl.ch; christoph.koch@epfl.ch; mohammad.dashti@epfl.ch,"Streams, sensor networks, complex event processing*; Distributed and parallel databases; Query processing and optimization","Christoph Koch, EPFL(christoph.koch@epfl.ch) This PC member is a co-author of the paper.; Oliver Kennedy, University at Buffalo(okennedy@buffalo.edu) This PC member has been a co-worker in the same company or university within the past two years.; Yanif Ahmad, Johns Hopkins University(yanif@cs.jhu.edu) This PC member has been a collaborator within the past two years.; Peter Boncz, CWI Amsterdam(p.boncz@cwi.nl) This PC member has been a collaborator within the past two years.; Oliver Kennedy, University at Buffalo(okennedy@buffalo.edu) This PC member has been a collaborator within the past two years.; Thomas Neumann, TU Munich(neumann@in.tum.de) This PC member has been a collaborator within the past two years.; Oliver Kennedy, University at Buffalo(okennedy@buffalo.edu) This PC member is or was an author's primary thesis advisor, no matter how long ago.","main.pdf (2,139,044 bytes)",,I Agree985,Similarity Join over Array Data,"Research, November 2015 Revision","Scientific applications are generating an ever-increasing volume of multi-dimensional data that are largely processed inside distributed array databases and frameworks. Similarity join is a fundamental operation across scientific workloads that requires complex processing over an unbounded number of pairs of multi-dimensional points. In this paper, we introduce a novel distributed similarity join operator for multi-dimensional arrays. Unlike immediate extensions to array join and relational similarity join, the proposed operator minimizes the overall data transfer and network congestion while providing load-balancing, without completely repartitioning and replicating the input arrays. We define formally array similarity join and present the design, optimization strategies, and evaluation of the first array similarity join operator.","Weijie Zhao, UC Merced; Florin Rusu*, UC Merced; Bin Dong, LBNL; John Wu, LBNL",wzhao23@ucmerced.edu; frusu@ucmerced.edu; dbin@lbl.gov; kwu@lbl.gov,Query processing and optimization*; Distributed and parallel databases; Scientific databases,"Florin Rusu, UC Merced(frusu@ucmerced.edu) This PC member is a co-author of the paper.; Spyros Blanas, Ohio State(blanas.2@osu.edu) This PC member has been a collaborator within the past two years.","revision.pdf (840,964 bytes)",,I Agree987,PrivateClean: Data Cleaning and Differential Privacy,"Research, November 2015 Revision","Recent advances in differential privacy make it possible to guarantee user privacy while preserving the main characteristics of the data. However, consistent result estimation in after privacy is based on the assumption that the underlying data are accurate. Raw data are often dirty, and this places the burden of potentially expensive data cleaning on the data provider. This paper explores the link between data cleaning and differential privacy in a framework we call PrivateClean. We extend the well-studied randomized response model to show that it is compatible with subsequent extraction, transformation, and merging operations. PrivateClean includes a technique for creating private datasets of numerical and discrete-valued attributes, a formalism for privacy-preserving data cleaning, and techniques for answering \sumfunc, \countfunc, and \avgfunc queries after cleaning. We show: (1) how the degree of privacy affects subsequent aggregate query accuracy, (2) how privacy potentially amplifies errors in a dataset, and (3) this analysis can be used to tune the degree of privacy. We validate these results on three real and synthetic datasets. ","Sanjay Krishnan*, UC Berkeley; Jiannan Wang, UC Berkeley; Tim Kraska, Brown University; Ken Goldberg, ; Michael Franklin, UC Berkeley",sanjay@eecs.berkeley.edu; jnwang@berkeley.edu; tim_kraska@brown.edu; goldberg@berkeley.edu; franklin@berkeley.edu,"Schema matching, data integration, and data cleaning*; Uncertain, probabilistic, and approximate databases","Tim Kraska, Brown University(tim_kraska@brown.edu) This PC member is a co-author of the paper.; Jiannan Wang, UC Berkeley(jnwang@berkeley.edu) This PC member is a co-author of the paper.; Peter Alvaro, UC Berkeley(palvaro@gmail.com) This PC member has been a co-worker in the same company or university within the past two years.; Peter Bailis, Stanford University(pbailis@cs.berkeley.edu) This PC member has been a co-worker in the same company or university within the past two years.; Eric Lo, Polytecnic University of Hong Kong(ericlo@comp.polyu.edu.hk) This PC member has been a co-worker in the same company or university within the past two years.; Matei Zaharia, MIT(matei@mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Kai Zeng, Microsoft(kaizeng@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Jian Pei, Simon Fraser University(jpei@cs.sfu.ca) This PC member has been a co-worker in the same company or university within the past two years.; Carsten Binnig, Brown University(carsten.binnig@dhbw-mannheim.de) This PC member has been a co-worker in the same company or university within the past two years.; Jennie Duggan, Northwestern Univeristy(jennie.duggan@northwestern.edu) This PC member has been a collaborator within the past two years.; Aaron Elmore, University of Chicago(aelmore@cs.uchicago.edu) This PC member has been a collaborator within the past two years.; Wolfgang Gatterbauer, CMU Tepper School of Business(gatt@cmu.edu) This PC member has been a collaborator within the past two years.; Ashwin Machanavajjhala, Duke U.(ashwin@cs.duke.edu) This PC member has been a collaborator within the past two years.; Stavros Papadopoulos, Intel Labs and MIT(stavrosp@csail.mit.edu) This PC member has been a collaborator within the past two years.; Olga Papaemmanouil, Brandeis University(olga@cs.brandeis.edu) This PC member has been a collaborator within the past two years.; Andy Pavlo, Carnegie Mellon University(pavlo@cs.cmu.edu) This PC member has been a collaborator within the past two years.; Ion Stoica, UC Berkeley(istoica@cs.berkeley.edu) This PC member has been a collaborator within the past two years.; Nesime Tatbul, Intel Labs and MIT(tatbul@csail.mit.edu) This PC member has been a collaborator within the past two years.; Matei Zaharia, MIT(matei@mit.edu) This PC member has been a collaborator within the past two years.; Alexandra Meliou, UMass Amherst(ameli@cs.umass.edu) This PC member has been a collaborator within the past two years.; Tim Kraska, Brown University(tim_kraska@brown.edu) This PC member has been a collaborator within the past two years.; Jiannan Wang, UC Berkeley(jnwang@berkeley.edu) This PC member has been a collaborator within the past two years.; Carsten Binnig, Brown University(carsten.binnig@dhbw-mannheim.de) This PC member has been a collaborator within the past two years.; Daisy Zhe Wang, University of Florida(daisyw@cise.ufl.edu) This PC member has been a collaborator within the past two years.; Guoliang Li, Tsinghua University(liguoliang@tsinghua.edu.cn) This PC member is or was an author's primary thesis advisor, no matter how long ago.; Peter Fischer, Universität Freiburg(peter.fischer@informatik.uni-freiburg.de) This PC member is or was an author's primary thesis advisor, no matter how long ago.; Samuel Madden, MIT(madden@csail.mit.edu) An author is or was the PC member's primary thesis advisor, no matter how long ago.; Ashwin Machanavajjhala, Duke U.(ashwin@cs.duke.edu) An author is or was the PC member's primary thesis advisor, no matter how long ago.; Christoph Koch, EPFL(christoph.koch@epfl.ch) This PC member is a relative or close personal friend of an author.; Kai Zeng, Microsoft(kaizeng@microsoft.com) This PC member is a relative or close personal friend of an author.; Ihab Ilyas, University of Waterloo(ilyas@uwaterloo.ca) This PC member is a relative or close personal friend of an author.","datacleaning_privacy.pdf (1,406,126 bytes)",,I Agree988,Sample + Seek: Approximating Aggregates with Distribution Precision Guarantee,"Research, November 2015 Revision","Users demand interactive query response time in modern decision support applications even though data volumes are exponentially growing over time. As small error is usually tolerable in such applications, traditional approximate query processing (AQP) tackles this challenge by providing fast confidence-interval based estimations for groups in the answer to an aggregation query. However, we argue that the semantics of confidence-interval guarantees falls short when there are many groups in the answer. Instead, we propose that an aggregation query’s answer be normalized as a distribution over groups, and introduce the notion of distribution precision of an estimated answer. This notion measures accuracy with the L2 distance between distributions of the estimated answer and the exact one. In this paper, we study how to provide fast approximate answers to aggregation queries with distribution precision guaranteed. We propose a novel sampling scheme called measure-biased sampling. We introduce how to use pre-drawn (measure-biased and uniform) samples with size sub-linear to the database size to answer aggregation queries. For queries with highly selective predicates, we propose two new on-disk indexes, measure-augmented inverted index and low-frequency group index, to aid in-memory samples. Those indexes have sizes linear to the database size, but we only need a constant number of index seeks to answer an aggregation query under the distribution-precision requirement. In addition to deriving rigorous theoretical guarantees, we conduct experimental study to compare our system with alternatives in both synthetic and real enterprise datasets. In our experiments, compared to a commercial database product with support for column-store, our system provides a median speed-up of 100x with 5% distribution error in answers.","Bolin Ding*, Microsoft; Silu Huang, University of Illinois, Urbana-Champaign, IL; Surajit Chaudhuri, Microsoft; Kaushik Chakrabarti, Microsoft Research; Chi Wang, Microsoft Research, Redmond, WA",bolind@microsoft.com; shuang86@illinois.edu; surajitc@microsoft.com; kaushik@microsoft.com; chiw@microsoft.com,"Data warehousing, OLAP, SQL Analytics*; Query processing and optimization","Kaushik Chakrabarti, Microsoft Research(kaushik@microsoft.com) This PC member is a co-author of the paper.; Arvind Arasu, Microsoft Research(arvinda@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Ravi Ramamurthy, Microsoft Research(ravirama@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Badrish Chandramouli, Microsoft Research(badrishc@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Kevin Chang, University of Illinois at Urbana-Champaign(kcchang@illinois.edu) This PC member has been a co-worker in the same company or university within the past two years.; Sudipto Das, Microsoft Research(sudiptod@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Vivek Narasayya, Microsoft Research(viveknar@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Rimma Nehme, Microsoft(rimman@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Justin Levandoski, Microsoft(justin.levandoski@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Aaron Elmore, University of Chicago(aelmore@cs.uchicago.edu) This PC member has been a collaborator within the past two years.; Ashwin Machanavajjhala, Duke U.(ashwin@cs.duke.edu) This PC member has been a collaborator within the past two years.; Kevin Chang, University of Illinois at Urbana-Champaign(kcchang@illinois.edu) This PC member has been a collaborator within the past two years.; James Cheng, CUHK(jcheng@cse.cuhk.edu.hk) This PC member is or was an author's primary thesis advisor, no matter how long ago.; Jiawei Han, UIUC(hanj@cs.uiuc.edu) This PC member is or was an author's primary thesis advisor, no matter how long ago.","paper (revision).pdf (1,110,556 bytes)",,I Agree989,Spheres of Influence for More Effective Viral Marketing,"Research, November 2015 Revision","Given a social network, a probabilistic contagion model, and one node $s$, what is the set of nodes that would get infected if $s$ get infected? We call this the \emph{sphere of influence} of $s$. Due to the stochastic nature of the contagion model we need to define a notion of ``expected'' or ``typical'' cascade: this is a set of nodes which is the closest to all the possible cascades starting from $s$.  We formalize the Typical Cascade problem which requires, for a given source node $s$, to find the set of nodes minimizing the expected Jaccard distance to all the possible cascades from $s$. The expected cost of a typical cascade also provides us a measure of the \emph{stability} of cascade propagation, i.e., how much random cascades from a source node $s$ deviate from the ``typical'' cascade. In this sense source nodes with lower expected costs are more reliable.  We characterize the hardness of our problem, but show that a method based on $(1)$ sampling random cascades and $(2)$ computing their Jaccard Median, can obtain a multiplicative approximation with only $O(\log n)$ samples. We thus devise an index that allows to efficiently retrieve the sphere of influence for any node in the network.  Finally, we propose an approach to the influence maximization problem based on the spheres of influence and set cover. Trough an exhaustive evaluation using real-world networks and different methods of assigning the influence probability to each edge, we show that our approach \emph{outperforms in quality} the theoretically optimal greedy algorithm.","Yasir Mehmood, ; Francesco Bonchi*, Yahoo Labs, Barcelona; David Garcia-Soriano, Eurecat",yasir.mehmood@gmail.com; bonchi@gmail.com; david.garcia@eurecat.org,"Graph data management, RDF, social networks*; Knowledge discovery, clustering, data mining; Uncertain, probabilistic, and approximate databases","Francesco Bonchi, Yahoo Labs, Barcelona(bonchi@gmail.com) This PC member is a co-author of the paper.; Sihem Amer-Yahia, CNRS/LIG(sihem.amer-yahia@imag.fr) This PC member has been a co-worker in the same company or university within the past two years.","sigmod16_rev_letter.pdf (5,403,212 bytes)",,I Agree990,FPTree: A Hybrid SCM-DRAM Persistent and Concurrent B-Tree for Storage Class Memory,"Research, November 2015 Revision","The advent of Storage Class Memory (SCM) is driving a rethink of storage systems towards a single-level architecture where memory and storage are merged. In this context, several works have investigated how to design persistent trees in SCM as a fundamental building block for these novel systems. However, these trees are significantly slower than \dram-based counterparts since trees are latency-sensitive and SCM exhibits higher latencies than \dram. In this paper we propose a novel hybrid SCM-DRAM persistent and concurrent B+-Tree, named Fingerprinting Persistent Tree (FPTree) that achieves similar performance to DRAM-based counterparts. In this novel design, leaf nodes are persisted in SCM while inner nodes are placed in DRAM and rebuilt upon recovery. The FPTree uses Fingerprinting, a technique that limits the expected number of in-leaf probed keys to one. In addition, we propose a hybrid concurrency scheme for the FPTree that is partially based on Hardware Transactional Memory. We conduct a thorough performance evaluation and show that the FPTree outperforms state-of-the-art persistent trees with different SCM latencies by up to a factor of 8.2. Moreover, we show that the FPTree scales very well on a machine with 88 logical cores. Finally, we integrate the evaluated trees in memcached and a prototype database. We show that the FPTree incurs an almost negligible performance overhead over using fully transient data structures, while significantly outperforming other persistent trees.","Ismail Oukid*, TU Dresden; Johan Lasperas, TU Berlin & Ecole Centrale Paris; Anisoara Nica, SAP SE; Thomas Willhalm, Intel GmbH; Wolfgang Lehner, TU Dresden",i.oukid@sap.com; johan.lasperas@gmail.com; anisoara.nica@sap.com; thomas.willhalm@intel.com; wolfgang.lehner@tu-dresden.de,"Databases for emerging hardware*; Storage, indexing, and physical database design","Wolfgang Lehner, TU Dresden(wolfgang.lehner@tu-dresden.de) This PC member is a co-author of the paper.","990.pdf (775,903 bytes)",,I Agree991,Cost-Effective Crowdsourced Entity Resolution: A Partial-Order Approach,"Research, November 2015 Revision","Crowdsourced entity resolution has recently attracted a significant attention because it can harness the wisdom of crowds to improve the quality of entity resolution. However existing techniques either cannot achieve perfect quality or incur huge monetary costs. To address these problems, we propose a cost-effective crowdsourced entity resolution framework, which can significantly reduce the monetary cost while keeping high quality. We first define a partial order on the pairs of records. Then we select a pair as a question and ask the crowd to check whether the records in the pair refer to the same entity.  After getting the answer of this pair, we infer the answers of other pairs based on the partial order.  Next we iteratively select pairs without answers to ask until all pairs have answers. We devise effective algorithms to judiciously select the pairs to ask in order to reduce the number of asked pairs. To further reduce the cost, we propose a grouping technique to group the pairs such that we only ask one pair instead of all pairs in each group. We develop error-tolerant techniques to tolerate the errors introduced by the partial order and the crowd. Experimental results show that our method reduces the cost to 1.25\% of existing approaches (or existing approaches take more than 80 times money of our method) while not sacrificing the quality. ","Chengliang Chai, T; Guoliang Li*, Tsinghua University; Jian Li, Tsinghua University; Dong Deng, Tsinghua University; Jianhua Feng, Tsinghua University",chaicl15@mails.tsinghua.edu.cn; liguoliang@tsinghua.edu.cn; lijian83@mail.tsinghua.edu.cn; dd11@mails.tsinghua.edu.cn; fengjh@tsinghua.edu.cn,Crowd sourcing*; Query processing and optimization,"Guoliang Li, Tsinghua University(liguoliang@tsinghua.edu.cn) This PC member is a co-author of the paper.;  Beng Chin Ooi, National University of Singapore(ooibc@comp.nus.edu.sg) This PC member has been a co-worker in the same company or university within the past two years.; Jiannan Wang, UC Berkeley(jnwang@berkeley.edu) This PC member has been a co-worker in the same company or university within the past two years.; Amol Deshpande, University of Maryland(amol@cs.umd.edu) This PC member is or was an author's primary thesis advisor, no matter how long ago.","main.pdf (1,219,731 bytes)",,I Agree429,iBFS: Concurrent Breadth-First Graph Traversal on GPUs,Research Subtrack 8 Revision,"Breadth-First Search (BFS) is a key graph algorithm with many important applications. In this work, we focus on a special class of graph traversal algorithm - concurrent BFS - where multiple breadth-first traversals are performed si- multaneously on the same graph. We have designed and developed a new approach called iBFS that is able to run i concurrent BFSes from i distinct source vertices, very effi- ciently on Graphics Processing Units (GPUs). iBFS consists of three novel designs. First, iBFS develops a single GPU kernel for joint traversal of concurrent BFS to take advan- tage of shared frontiers across different instances. Second, outdegree-based GroupBy rules enables iBFS to selectively run a group of BFS instances which further maximizes the frontier sharing within such a group. Third, iBFS brings ad- ditional performance benefit by utilizing highly optimized bitwise operations on GPUs, which allows a single GPU thread to inspect a vertex for concurrent BFS instances. We evaluate iBFS on a wide spectrum of graph benchmarks and show that iBFS on a single GPU runs up to 30_ faster than executing different BFS instances sequentially, and on 112 GPUs achieves near linear speedup with the maximum performance of 57,267 billion traversed edges per second (TEPS).","Hang Liu, ; Howie Huang*, GWU; Yang Hu, ",asherliu@gwu.edu; howie@gwu.edu; huyang@gwu.edu,"Graph data management, RDF, social networks*","Guoliang Li, Tsinghua University(liguoliang@tsinghua.edu.cn) This PC member has been a co-worker in the same company or university within the past two years.","429.pdf (3,592,288 bytes)",,I Agree430,Towards Best Region Search with Submodular Monotone Aggregate Function,Research Subtrack 8 Revision,"The increasing popularity and growth of mobile devices and location-based services enable us to utilize large-scale geo-tagged data to support novel location-based applications. This paper introduces a novel problem called the \textit{best region search} (\emph{BRS}) problem and provides efficient solutions to it. Given a set $O$ of spatial objects, a submodular monotone aggregate score function, and the size $a\times b$ of a query rectangle, the \emph{BRS} problem aims to find $a\times b$ rectangular region such that the \textit{aggregate score} of the spatial objects inside the region is maximized. This problem is fundamental to support several real-world applications such as \textit{most influential region} search (\eg the best location for a billboard to attract most audience) and \textit{most diversified region} search (\eg region with most diverse facilities). We propose an efficient algorithm called \emph{SliceBRS} to find the exact answer to the \emph{BRS} problem. Furthermore, we propose an approximate solution called \emph{CoverBRS} and prove that the answer found by it is bounded by a constant. Our experimental study with real-world datasets and applications demonstrates the effectiveness and superiority of our proposed algorithms.","Kaiyu Feng*, Nanyang Technological Universi; Gao Cong, NTU; Sourav S Bhowmick, Nanyang Technological University; Wen-Chih Peng, ; Chunyan Miao, Nanyang Technological University",kfeng002@e.ntu.edu.sg; gaocong@ntu.edu.sg; assourav@ntu.edu.sg; wcpeng@cs.nctu.edu.tw; ASCYMiao@ntu.edu.sg,Spatio-temporal databases*,"Stéphane Bressan, NUS(steph@nus.edu.sg) This PC member has been a collaborator within the past two years.","BRS-sigmod.pdf (912,206 bytes)",,I Agree432,Continuous Influence Maximization: What Discounts Should We Offer to Social Network Users?,Research Subtrack 8 Revision,"Imagine we are introducing a new product through a social network, where for each user in the network the purchase probability curve with respect to discount can be assumed, what discount should we offer to those social network users so that the adoption of the product is maximized in expectation under a predefined budget? Surprisingly, this appealing problem cannot be answered by the existing influence maximization methods. To tackle the problem, we formulate the general continuous influence maximization problem, investigate the essential properties, and develop a general coordinate descent algorithm as well as the engineering techniques for practical implementation. Our investigation remains general and does not assume any specific influence model. At the same time, we demonstrate that more efficient methods are feasible for specific influence models using the most popularly used independent influence model as a concrete example. Our extensive empirical study on four benchmark real world networks with synthesized purchase probability curves clearly illustrates that continuous influence maximization can improve influence spread significantly with very moderate extra running time comparing to the classical influence maximization methods.","Yu Yang*, Simon Fraser University; Xiangbo Mao, Simon Fraser University; Jian Pei, Simon Fraser University; Xiaofei He, Zhejiang University",yya119@sfu.ca; xiangbom@sfu.ca; jpei@cs.sfu.ca; xiaofeihe@gmail.com,"Graph data management, RDF, social networks*; Uncertain, probabilistic, and approximate databases","Xuemin Lin, University of New South Wales(lxue@cse.unsw.edu.au) This PC member has been a collaborator within the past two years.","CIM(1).pdf (585,096 bytes)",response.pdf (109982 bytes),I Agree434,AT-GIS: Highly Parallel Spatial Query Processing with Associative Transducers,Research Subtrack 8 Revision,"The rise of crowd-sourcing projects, such as OpenStreetMap, and high-resolution sensing capabilities have led to spatial datasets of increasing size and complexity. Users in many applications domains, including urban planning, transportation, and environmental science, therefore want to execute analytical queries over these continuously updated spatial datasets. Current solutions for large-scale spatial query processing either rely on extensions to RDBMS, which entails expensive loading and indexing phases whenever the data changes, or distributed map/reduce frameworks, which require expensive compute clusters. Both solutions struggle to manage the sequential bottleneck of parsing complex, hierarchical spatial data formats, which typically dominates query execution time. Our goal is to fully exploit the parallelism offered by modern multi-core CPUs during parsing and query execution, thus providing the performance of a cluster with the resources of a single machine.  We describe AT-GIS, a highly-parallel spatial query processing system that scales linearly to a large number of CPU cores. AT-GIS integrates the parsing and querying of spatial data using a new computational abstraction called Associative Transducers (ATs). ATs can form a single data-parallel pipeline for computation without requiring the spatial input data to be split into logically independent blocks.  Using ATs, AT-GIS can execute, in parallel, spatial query operators on the raw input data in multiple formats, without any pre-processing. On a single 64-core machine, AT-GIS provides 3x the performance of an 8-node Hadoop cluster with 192 cores for containment queries, and 10x for aggregation queries.  We also show that AT-GIS scales regardless of the size or the skew of the input data. ","Peter Ogden*, Imperial College London; Peter Pietzuch, Imperial College London; David Thomas, ",p.ogden12@imperial.ac.uk; prp@doc.ic.ac.uk; d.thomas1@imperial.ac.uk,"Spatio-temporal databases*; Distributed and parallel databases; Semi-structured data; Streams, sensor networks, complex event processing",,"p434-revised.pdf (387,042 bytes)",,I Agree435,Set-based Similarity Search for Time Series,Research Subtrack 8 Revision,"A fundamental problem of time series is the k nearest neighbor (k-NN) query processing. Existing methods are not fast enough for large dataset. In this paper, we propose a novel approach, STS3, processing k-NN queries by transforming time series to a set and measuring similarity under Jaccard metric. Our approach is faster than most of the existing methods due to the efficient similarity search for sets. Besides, we developed index, a pruning and an approximation technique to speed up the k-NN query processing. As shown in the experimental results, all of them could accelerate the query processing effectively.","Jinglin Peng, ; Hongzhi Wang*, Harbin Institute of Technology; Jianzhong Li, Harbin Institute of Technology; Hong Gao, ",jlpengcs@gmail.com; wangzh@hit.edu.cn; lijzh@hit.edu.cn; honggao@hit.edu.cn,"Spatio-temporal databases*; Query processing and optimization; Uncertain, probabilistic, and approximate databases",,"revision-Set-based Similarity Search for Time Series.pdf (1,148,404 bytes)",,I Agree436,ROLL: Fast In-Memory Generation of Gigantic Scale-free Networks,Research Subtrack 8 Revision,"Real-world graphs are not always publicly available or sometimes do not meet specific research requirements. These challenges call for generating synthetic networks that follow properties of the real-world networks. Barab\'{a}si-Albert (BA) is a well-known model for generating scale-free graphs, i.e graphs with power-law degree distribution. In BA model, the network is generated through an iterative stochastic process called preferential attachment. Although BA is highly demanded, due to the inherent complexity of the preferential attachment, this model cannot be scaled to generate billion-node graphs.  In this paper, we propose ROLL-tree, a fast in-memory roulette wheel data structure that accelerates the BA network generation process by exploiting the statistical behaviors of the underlying growth model. Our proposed method has the following properties: (a) Fast: It performs +1000 times faster than the state-of-the-art on a single node PC; (b) Exact: It strictly follows the BA model, using an efficient data structure instead of approximation techniques; (c) Generalizable: It can be adapted for other ''rich-get-richer'' stochastic growth models. Our extensive experiments prove that ROLL-tree can effectively accelerate graph-generation through the preferential attachment process. On a commodity single processor machine, for example, ROLL-tree generates a scale-free graph of 1.1 billion nodes and 6.6 billion edges (the size of Yahoo's Webgraph) in 62 minutes while the state-of-the-art (SA) takes about four years on the same machine.","Ali Hadian, Iran University of Sci & Tech.; Sadegh Nobari*, Innopolis University; Behrouz Minaei-Bidgoli, Iran University of Science and Technology; Qiang Qu, Innopolis University ",ali.hadian@gmail.com; nobari@innopolis.ru; b_minaei@iust.ac.ir; qu@innopolis.ru,"Graph data management, RDF, social networks*; Benchmarking and performance evaluation","Stéphane Bressan, NUS(steph@nus.edu.sg) This PC member is or was an author's primary thesis advisor, no matter how long ago.","Cover letter and revised paper 436 (352 before revision)-ROLL.pdf (643,158 bytes)",,I Agree404,Lazily Approximating Complex AdHoc Queries in Big Data Clusters,Research Subtrack 6 Revision,"Experience with queries in a large production big data cluster reveals that apriori samples are untenable. Accesses to input files are heavy tailed. And, queries often join multiple inputs or have complex filters. Consequently, systems like BlinkDB which store differently stratified samples of the inputs have small coverage and limited performance gains even when offered several times the size of the inputs to store samples.   This paper explores lazy on-the-fly sampling. We assume no pre-existing samples, indices, materialized views or foreknowledge of the queries.   When a query arrives, our system carefully injects sampler operators into the query execution graph. Even though all the input data has to be read at least once, the gains from such on-the-fly sampling can be substantial because big data queries typically perform many passes over the input-- a general join has 2-to-3 reads and 1 network shuffle.   In addition, key new techniques include: (1) a new sampling operator that effectively mimics sampling of the join output by sampling the inputs of the join instead and (2) a new query optimizer that automatically inserts appropriate samplers at appropriate locations in the query graph. Our system uses three different samplers all of which function in a ""streaming"" mode, ie, in one pass over data, with sub-linear memory footprint and in a partitionable manner. We offer transformation rules that carefully move these samplers ever closer to the raw inputs where they have higher perf gains while ensuring that the err remains in check. An implementation on a cluster with O(10^4) machines shows that up to 60\% of the queries in the TPC-DS benchmark speed-up by a median of 2X with negligible error, both of which are substantially higher than BlinkDB. ","Srikanth Kandula*, Microsoft; Anil Shanbhag, Microsoft/ MIT; Aleksandar Vitorovic, EPFL; Matt Olma, Microsoft; Robert Grandl, Microsoft; Surajit Chaudhuri, Microsoft; Bolin Ding, Microsoft",srikanth@microsoft.com; anilshanbhag@outlook.com; aleksandar.vitorovic@epfl.ch; t-maolm@microsoft.com; t-roberg@microsoft.com; surajitc@microsoft.com; bolind@microsoft.com,"Distributed and parallel databases*; Data warehousing, OLAP, SQL Analytics; Query processing and optimization; Uncertain, probabilistic, and approximate databases","Erietta Liarou, EPFL(erietta@gmail.com) This PC member has been a co-worker in the same company or university within the past two years.; Alekh Jindal, MIT(alekh@csail.mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Ion Stoica, UC Berkeley(istoica@cs.berkeley.edu) This PC member has been a collaborator within the past two years.; Erietta Liarou, EPFL(erietta@gmail.com) This PC member is or was an author's primary thesis advisor, no matter how long ago.","paper404.pdf (1,390,281 bytes)",review_response.pdf (426612 bytes),I Agree406,Top-k Relevant Semantic Place Retrieval on Spatial RDF Data,Research Subtrack 6 Revision,"RDF data are traditionally accessed using structured query languages, such as SPARQL. However, this requires users to understand the language as well as the RDF schema. Keyword search on RDF data aims at relieving the user from these requirements; the user only inputs a set of keywords and the goal is to find small RDF subgraphs which contain all keywords. At the same time, popular RDF knowledge bases also include spatial semantics, which opens the road to location-based search operations. In this work, we propose and study a novel location-based keyword search query on RDF data. The objective of top-k relevant semantic places (kSP) retrieval is to find RDF subgraphs which contain the query keywords and are rooted at spatial entities close to the query location. The novelty of kSP queries is that they are location-aware and that they do not rely on the use of structured query languages. We design a basic method for the processing of kSP queries. To further accelerate kSP retrieval, two pruning approaches and a data preprocessing technique are proposed. Extensive empirical studies on two real datasets demonstrate the superior and robust performance of our proposals compared to the basic method.","Jieming Shi*, THE UNIVERSITY OF HONG KONG; Dingming Wu, The University of Hong Kong; Nikos Mamoulis, U. Hong Kong",jmshi@cs.hku.hk; dmwu@cs.hku.hk; nikos@cs.hku.hk,"Graph data management, RDF, social networks*","Silviu Maniu, Université Paris-Sud(silviu.maniu@lri.fr) This PC member has been a co-worker in the same company or university within the past two years.","rdf-with-response.pdf (2,984,290 bytes)",,I Agree407,An Effective Syntax for Bounded Relational Queries,Research Subtrack 6 Revision,"A query Q is boundedly evaluable under a form of access constraints \A if for all datasets D that satisfy \A, there exists a subset D_Q of D such that Q(D) = Q(D_Q), and the size of D_Q and time for identifying D_Q are both independent of the size |D| of D. That is, we can compute Q(D) by accessing a bounded amount of data no matter how big D grows. However, while desirable, it is undecidable to determine whether a relational algebra (\RA) query is bounded under \A. In light of the undecidability, this paper develops an effective syntax for bounded \RA queries.  We identify a class of covered \RA queries such that under \A, (a) every boundedly evaluable \RA query is equivalent to a covered query, (b) every covered \RA query is boundedly evaluable, and (c) it takes \PTIME in |Q| and |\A| to check whether Q is covered by \A. We provide quadratic-time algorithms to check the coverage of Q, and to generate a bounded query plan for covered Q.  We also study a new optimization problem for answering covered queries with minimum access constraints, and develop its complexity and algorithms. These provide both fundamental results and practical algorithms for making use of bounded evaluability.  Using real-life data, we experimentally verify that a large number of \RA queries in practice are covered, and that bounded query plans improve \RA query evaluation by orders of magnitude.  ","Yang Cao*, University of Edinburgh; Wenfei Fan, Univsersity of Edinburgh",yang.cao@ed.ac.uk; wenfei@inf.ed.ac.uk,"Query processing and optimization*; Data models, semantics, query languages; Data warehousing, OLAP, SQL Analytics",,"paper.pdf (654,422 bytes)",,I Agree408,THEMIS: Fairness in Federated Stream Processing under Overload,Research Subtrack 6 Revision,"  Federated stream processing systems, which utilise nodes from multiple   independent domains, can be found increasingly in multi-provider cloud   deployments, internet of things systems, collaborative sensing applications    and large-scale grid systems. To pool resources from several sites and take advantage of   local processing, submitted queries are split into query fragments, which are   executed collaboratively by different sites. When supporting many concurrent   users, however, queries may exhaust available processing resources, thus   requiring constant load shedding. Given that individual sites have autonomy   over how they allocate query fragments on their nodes, it is an open   challenge how to ensure global fairness on processing quality   experienced by queries in a federated scenario.     We describe THEMIS, a federated stream processing system for resource-starved,   multi-site deployments. It executes queries in a globally fair   fashion and provides users with constant feedback on the experienced   processing quality for their queries. THEMIS associates stream data with its   Source Information Content (SIC), a metric that quantifies the   contribution of that data towards the query result, based on the amount of   source data used to generate it. We provide the BALANCE-SIC distributed load shedding   algorithm that aims to balance the SIC values of result data. Our extensive    evaluation validates that the BALANCE-SIC algorithm yields balanced SIC values across   queries as measured with the Jain's index fairness metric. Our approach also    incurs a low execution time overhead.","Evangelia Kalyvianaki*, City University London; Marco Fiscato, ; Theodoros Salonidis, IBM; Peter Pietzuch, ",evangelia.kalyvianaki.1@city.ac.uk; mfiscato@doc.ic.ac.uk; tsaloni@us.ibm.com; prp@imperial.ac.uk,"Streams, sensor networks, complex event processing*",,"SigmodThemis.pdf (722,932 bytes)",CoverLetter16ThemisSIGMOD.pdf (106212 bytes),I Agree410,Query Planning for Evaluating SPARQL Property Paths,Research Subtrack 6 Revision,"The extension of SPARQL in version 1.1 with property paths offers a type of regular path query for RDF graph databases. Such queries are difficult to optimize and evaluate efficiently, however. We have embarked on a project, Waveguide, to build a cost-based optimizer for SPARQL queries with property paths. Waveguide builds a query plan—a waveguide plan (WGP)—which guides the query evaluation. There are numerous choices in the construction of a plan, and a number of optimization methods, meaning the space of plans for a query can be quite large. Execution costs of plans for the same query can vary by orders of magnitude. We illustrate the types of optimizations this approach affords and the performance gains that can be obtained. A WGP’s costs can be estimated, which opens the way to cost-based optimization.","Nikolay Yakovets*, York University; Parke Godfrey, York University; Jarek Gryz, York University",hush@cse.yorku.ca; godfrey@cse.yorku.ca; jarek@cse.yorku.ca,"Query processing and optimization*; Graph data management, RDF, social networks",,"paper.pdf (2,081,611 bytes)",,I Agree411,FluxQuery: An Execution Framework for Highly Interactive Query Workloads,Research Subtrack 6 Revision,"Modern computing devices and user interfaces have necessitated highly interactive querying. Some of these interfaces issue a large number of dynamically changing and continuous queries to the backend. In others, users expect to inspect results during the query formulation process, in order to guide or help them towards specifying a full-fledged query. Thus, users end up issuing a fast-changing workload to the underlying database. In such situations, user's query intent can be thought of as being in flux.   In this paper, we show that the traditional query execution engines are not well-suited for this new class of highly interactive workloads. We propose a novel model to interpret the variability of likely queries in a workload. We implemented a cyclic scan-based approach to process queries from such workloads in an efficient and practical manner while reducing the overall system load. We evaluate and compare our methods with traditional systems and demonstrate the scalability of our approach, enabling thousands of queries to run simultaneously within interactive response times given low memory and CPU requirements.","Roee Ebenstein*, The Ohio State University; Niranjan Kamat, The Ohio State University; Arnab Nandi, Ohio State University",ebenstei@cse.ohio-state.edu; kamatn@cse.ohio-state.edu; arnab@cse.ohio-state.edu,Query processing and optimization*; Database usability,"Arnab Nandi, Ohio State University(arnab@cse.ohio-state.edu) This PC member is a co-author of the paper.","ToUpload-Final.pdf (1,097,519 bytes)",,I Agree412,Time Adaptive Sketches (Ada-Sketches) for Summarizing Data Streams,Research Subtrack 6 Revision,"Obtaining frequency information of data streams, in limited space, is a well-recognized problem in literature.  A number of recent practical applications (such as those in computational advertising) require temporally-aware solutions:  obtaining historical count statistics for both time-points as well as time-ranges. In these scenarios, accuracy of estimates is typically more important for recent instances than for older ones; we call this desirable property as ``\emph{Time Adaptiveness}"". With this observation,~\cite{Proc:Matusevych_UAI12} introduced the ``Hokusai"" technique based on count-min sketches for estimating frequencies of any given item at any given time.   The proposed approach is problematic in practice as its memory requirements grow linearly with time and it produces discontinuities in the estimation accuracy.  In this work, we describe a new method,``\emph{Time Adaptive Sketches}"" (Ada-sketches), that overcomes these limitations, while extending and providing a strict generalization of several existing popular sketching algorithms.  The core idea of our method is inspired by the well-known digital Dolby noise reduction procedure that dates back to 1960s.  Theoretical analysis presented could be of independent interest in itself, as it provides clear results for the time-adaptive nature of the errors. Experimental evaluation on real streaming datasets demonstrates the superiority of the described method over Hokusai in estimating point and range queries over time.  The method is simple to implement and offers a variety of design choices for future extensions. The simplicity of the procedure and the method's generalization of classic sketching techniques give hope for wide applicability of Ada-sketches in practice.  ","Anshumali Shrivastava*, Rice University; Christian Konig, Microsoft Research; Mikhail Bilenko, Microsoft Research",anshumali@rice.edu; chrisko@microsoft.com; mbilenko@microsoft.com,"Streams, sensor networks, complex event processing*; Information retrieval and text mining; Knowledge discovery, clustering, data mining; Storage, indexing, and physical database design; Uncertain, probabilistic, and approximate databases",,"RevisionMaterial_final3.pdf (1,696,811 bytes)",,I Agree413,Scalable Pattern Sharing over Event Streams,Research Subtrack 6 Revision,"Complex Event Processing (CEP) has emerged as a technology of choice for high performance event analytics in time-critical decision-making applications. It is becoming increasingly difficult to support high-performance event processing queries due to the combination of the rising number and complexity of event pattern queries and the increasing velocity of event streams. In this work we design the SPASS (Scalable PAttern Sharing over event Streams) framework that successfully tackles these demanding CEP workloads. Our SPASS optimizer identifies effective sharing opportunities among CEP queries by leveraging time-based event correlations among queries.  The problem of pattern sharing is shown to be NP-hard by reducing the Minimum Substring Cover problem to the pattern sharing problem.  Thereafter, the SPASS optimizer is designed that finds a shared pattern plan in polynomial-time covering all sequence patterns while still guaranteeing an optimality bound.  To execute this shared pattern plan, the SPASS runtime employs stream transactions that assure concurrent shared maintenance and re-use of sub-patterns across queries.  Our SPASS framework is shown to achieve over 16 fold performance improvement for a wide range of experiments compared to the state-of-the-art solution.","Medhabi Ray*, 1984; Chuan Lei, WPI; Elke Rundensteiner, Worcester Polytechnic Institute (WPI).",medhabi@cs.wpi.edu; chuanlei@cs.wpi.edu; rundenst@cs.wpi.edu,"Streams, sensor networks, complex event processing*",,"p336_revision.pdf (798,766 bytes)",p336_rebuttal.pdf (296155 bytes),I Agree394,Graph Indexing for Shortest-Path Finding over Dynamic Sub-Graphs,Research Subtrack 5 Revision,"A variety of applications spanning various domains, e.g., social networks, transportation, and bioinformatics, have graphs as first-class citizens. These applications share a vital operation, namely, finding the shortest path between two nodes. In many scenarios, users are interested in filtering the graph before finding the shortest path. For example, in social networks, one may need to compute the shortest path between two persons on a sub-graph containing only family relationships. This paper focuses on dynamic graphs with labeled edges, where the target is to find a shortest path after filtering some edges based on user-specified query labels. This problem is termed the Edge-Constrained Shortest Path query (or ECSP, for short). This paper introduces Edge-Disjoint Partitioning (EDP, for short), a new technique for efficiently answering ECSP queries over dynamic graphs. EDP has two main components: a dynamic index that is based on graph partitioning, and a traversal algorithm that exploits the regular patterns of the answers of ECSP queries. The main idea of EDP is to partition the graph based on the labels of the edges. On demand, EDP computes specific sub-paths within each partition and updates its index. The computed sub-paths act as pre-computations that can be leveraged by future queries. To answer an ECSP query, EDP connects sub-paths from different partitions using its efficient traversal algorithm. EDP can dynamically handle various types of graph updates, e.g., label, edge, and node updates. The index entries that are potentially affected by graph updates are invalidated and re-computed on demand. EDP is evaluated using real graph datasets from various domains. Experimental results demonstrate that EDP can achieve query performance gains of up to four orders of magnitude in comparison to state of the art techniques.","Mohamed Hassan*, Purdue University; Walid Aref, Purdue University; Ahmed Aly, Purdue University",msaberab@cs.purdue.edu; aref@cs.purdue.edu; aaly@cs.purdue.edu,"Graph data management, RDF, social networks*; Query processing and optimization; Storage, indexing, and physical database design","Ihab Ilyas, University of Waterloo(ilyas@uwaterloo.ca) An author is or was the PC member's primary thesis advisor, no matter how long ago.","394.pdf (1,386,768 bytes)",,I Agree395,Unleashing Parallelism in Multi-core Databases via Dependency Tracking,Research Subtrack 5 Revision,"Multicore in-memory databases rely on traditional concur- rency control schemes such as 2-phase-locking (2PL) or op- timistic concurrency control (OCC). Unfortunately, when the workload exhibits a non-trivial amount of contention, both 2PL and OCC sacrifice multi-core parallelism. In this paper, we describe a new concurrency control scheme, inter- leaving constrained concurrency control (IC3), which pro- vides serializability while allowing for parallel execution of conflicting transactions. Given a user transaction consisting of multiple data accesses, IC3 selectively synchronizes some of the accesses to preclude unsafe interleaving according to an offline analysis of the transaction workload. Evaluations on a 64-core machine using a set of OLTP workloads like TPC-C, TPC-E and SEATS show that IC3 outperforms tradi- tional concurrency control schemes. The performance gains are particularly pronounced when the workload exhibits high contention.","Zhaoguo Wang*, New York University ; Yang Cui, New York University; Han Yi, Shanghai Jiao Tong University; Shuai Mu, New York University; haibo Chen, Shanghai Jiao Tong University; Jinyang Li, New York University",zhaoguo@nyu.edu; cuiyang1125@gmail.com; ken.yihan1990@gmail.com; msmummy@gmail.com; oldseawave@gmail.com; jinyang@cs.nyu.edu,Transaction processing*; Distributed and parallel databases,,"icdb.pdf (426,323 bytes)",,I Agree397,TicToc: Time-Traveling Optimistic Concurrency Control,Research Subtrack 5 Revision,"Concurrency control for on-line transaction processing (OLTP) data- base management systems (DBMSs) is a nasty game. Achieving higher performance on emerging many-core systems is becoming increasingly difficult. Previous research has shown that timestamp allocation and management in a DBMS’s concurrency control al- gorithm is the key bottleneck that prevents the system from scaling up to support a larger number of simultaneous transactions. In this paper we present TicToc, a new optimistic concurrency control algorithm that avoids the scalability and concurrency bot- tlenecks of prior T/O schemes. TicToc relies on a novel and prov- ably correct data-driven timestamp management protocol. Instead of assigning timestamps to transactions, this protocol assigns read and write timestamps to data items and uses them to lazily com- pute a valid commit timestamp for each transaction. TicToc re- moves the need for centralized timestamp allocation, and commits transactions that would be aborted by conventional T/O schemes. We implemented TicToc along with three other concurrency con- trol algorithms in an in-memory, shared-everything OLTP DBMS and compared their performance on different workloads. Our re- sults show that TicToc achieves up to 92% better throughput while reducing the abort rate by 3.3_ than these previous algorithms.","Xiangyao Yu*, MIT; Andy Pavlo, Carnegie Mellon University; Daniel Sanchez, MIT; Srinivas Devadas, MIT",yxy@mit.edu; pavlo@cs.cmu.edu; sanchez@mit.edu; devadas@mit.edu,Transaction processing*; Distributed and parallel databases,"Holger Pirk, MIT(holger@csail.mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Ippokratis Pandis, Amazon Web Services(ippokratis@gmail.com) This PC member has been a collaborator within the past two years.","revision.pdf (2,087,785 bytes)",,I Agree399,Robust Query Processing in Co-Processor-accelerated Databases,Research Subtrack 5 Revision,"Technology limitations are making the use of heterogeneous computing devices much more than an academic curiosity. In fact, the use of such devices is widely acknowledged to be the only promising way to achieve application-speedups that users urgently need and expect. However, building a robust and efficient query engine for heterogeneous co-processor environments is still a significant challenge.  In this paper, we identify two effects that limit performance in case co-processor resources become scarce. Cache thrashing occurs when the working set of queries does not fit into the co-processor’s data cache, resulting in performance degradations up to a factor of 24. Heap contention occurs when multiple operators run in parallel on a co-processor and when their accumulated memory footprint exceeds the main memory capacity of the co-processor, slowing down query execution by up to a factor of six.  We propose solutions for both effects. Data-driven operator placement avoids data movements when they might be harmful; query chopping limits co-processor memory usage and thus avoids contention. The combined approach—data-driven query chopping—achieves robust and scalable performance on co-processors. We validate our proposal with an open-source GPU-accelerated database engine and the popular star schema and TPC-H benchmarks.","Sebastian Breß*, TU Dortmund University; Henning Funke, TU Dortmund University; Jens Teubner, TU Dortmund University",sebastian.bress@tu-dortmund.de; henning.funke@tu-dortmund.de; jens.teubner@cs.tu-dortmund.de,"Databases for emerging hardware*; Data warehousing, OLAP, SQL Analytics; Query processing and optimization; Storage, indexing, and physical database design","Gustavo Alonso, ETH Zurich(alonso@inf.ethz.ch) This PC member has been a collaborator within the past two years.","p399_revised.pdf (543,167 bytes)",,I Agree400,Adding Counting Quantifiers to Graph Patterns,Research Subtrack 5 Revision,"This paper proposes quantified graph patterns (QGPs), an extension of graph patterns by supporting simple counting quantifiers on edges. We show that QGPs naturally express universal and existential quantification, numeric and ratio aggregates, as well as negation. Better still, the increased expressive power does not come with a much higher price. We show that quantified matching, i.e., graph pattern matching with QGPs, remains \NP-complete in the absence of negation, and is DP-complete for general QGPs. We show how quantified matching can be conducted by incorporating quantifier check into conventional subgraph isomorphism methods. We also develop parallel scalable algorithms for quantified matching. As an application of QGPs, we introduce quantified graph association rules defined with QGPs, to identify potential customers in social media marketing. Using real-life and synthetic graphs, we experimentally verify the effectiveness of QGPs and the scalability of our algorithms.","Wenfei Fan, Univsersity of Edinburgh; YINGHUI WU*, Washington State University; Jingbo Xu, University of Edinburgh",wenfei@inf.ed.ac.uk; yinghui@eecs.wsu.edu; jingbo.xu@ed.ac.uk,"Graph data management, RDF, social networks*; Knowledge discovery, clustering, data mining; Semi-structured data","Jiawei Han, UIUC(hanj@cs.uiuc.edu) This PC member has been a co-worker in the same company or university within the past two years.","paper.pdf (590,678 bytes)",,I Agree401,Hybrid Pulling/Pushing for I/O-Efficient Distributed and Iterative Graph Computing,Research Subtrack 5 Revision,"Billion-node graphs are rapidly growing in size in many applications such as online social networks. Most graph algorithms generate a large number of messages during iterative computations. Vertex-centric distributed systems usually store graph data and message data on disk to improve scalability. Currently, distributed systems with disk-resident data take a push-based approach to handle messages. This works well if few messages reside on disk. Otherwise, it is I/O-inefficient due to expensive random writes. The existing memory-resident pull-based approach individually pulls messages for each vertex on demand. In disk scenarios, although this avoids disk operations of messages, expensive I/O costs are incurred by random and frequent access to vertices.  This paper proposes a hybrid solution to support switching between push and pull adaptively, to obtain optimal performance for distributed systems with disk-resident data in different scenarios. We first employ a block-centric technique (b-pull) to improve the I/O-performance of pulling messages, although the iterative computation is vertex-centric. I/O costs of data accesses are shifted from the receiver side where messages are written/read by push to the sender side where graph data are read by b-pull. Graph data are organized by clustering vertices and edges to achieve high I/O-efficiency. Second, we design a seamless switching mechanism and a prominent performance prediction method to guarantee efficiency when switching between push and b-pull. We conduct extensive performance studies to confirm the effectiveness of our proposals over existing up-to-date solutions using a broad spectrum of real-world graphs.","Zhigang Wang*, Northeastern University; Yu Gu, Northeastern University (China); Yubin Bao, Northeastern University (China); Ge Yu, ; Jeffrey Xu Yu, CUHK",wangzhiganglab@gmail.com; guyu@ise.neu.edu.cn; baoyubin@ise.neu.edu.cn; yuge@mail.neu.edu.cn; yu@se.cuhk.edu.hk,"Distributed and parallel databases*; Graph data management, RDF, social networks",,"response and revision paper.pdf (683,230 bytes)",,I Agree402,Learning Linear Regression Models over Factorized Joins,Research Subtrack 5 Revision,"  We investigate the problem of building least squares regression   models over training datasets defined by arbitrary join queries on   database tables. Our key observation is that joins entail a high   degree of redundancy in both computation and data representation,   which is not required for the end-to-end solution to learning over   joins.    We propose a variant of batch gradient descent called F that   can learn the parameters of a linear regression function in two   passes over factorized representations of the datasets, even though   they can be exponentially smaller than the flat relational datasets.   We consider factorizations of asymptotically optimal sizes, which   are governed by well-understood properties of the hypergraph of the   join query Q.     F exploits the factorization structure, a rewriting of the   regression objective function that decouples the computation of   cofactors of model parameters from their convergence, and the   commutativity of cofactor computation with relational union and   projection. The join intermixed with F's cofactor computation   is expressible as one SQL query.    In experiments with real-world and synthetic datasets, F    outperforms the state-of-the-art systems R, Python StatsModels,    and MADlib, by up to three orders of magnitude. ","Dan Olteanu*, Oxford University & LogicBlox; Maximilian Schleich, University of Oxford; Radu Ciucanu, University of Oxford",dan.olteanu@cs.ox.ac.uk; mjschleich@gmail.com; radu.ciucanu@cs.ox.ac.uk,"Data warehousing, OLAP, SQL Analytics*; Knowledge discovery, clustering, data mining; Uncertain, probabilistic, and approximate databases","Tim Furche, Oxford(tim@furche.net) This PC member has been a co-worker in the same company or university within the past two years.; Tim Furche, Oxford(tim@furche.net) This PC member is a relative or close personal friend of an author.","paper-with-response.pdf (4,239,421 bytes)",,I Agree388,PrivTree: A Differentially Private Algorithm for Hierarchical Decompositions,Research Subtrack 4 Revision,"Given a set D of tuples defined on a domain Omega, we study differentially private algorithms for constructing a histogram over Omega to approximate the tuple distribution in D. Existing solutions for the problem mostly adopt a hierarchical decomposition approach, which recursively splits Omega into sub-domains and computes a noisy tuple count for each sub-domain, until all noisy counts are below a certain threshold. This approach, however, requires that we (i) impose a limit h on the recursion depth in the splitting of Omega and (ii) set the noise in each count to be proportional to h. The choice of h is a serious dilemma: a small h makes the resulting histogram too coarse-grained, while a large h leads to excessive noise in the tuple counts used in deciding whether sub-domains should be split. Furthermore, h cannot be directly tuned based on D; otherwise, the choice of h itself reveals private information and violates differential privacy.  To remedy the deficiency of existing solutions, we present PrivTree, a histogram construction algorithm that adopts hierarchical decomposition but completely eliminates the dependency on a pre-defined h. The core of PrivTree is a novel mechanism that (i) exploits a new analysis on the Laplace distribution and (ii) enables us to use only a constant amount of noise in deciding whether a sub-domain should be split, without worrying about the recursion depth of splitting. We demonstrate the application of PrivTree in modelling spatial data, and show that it can be extended to handle sequence data (where the decision in sub-domain splitting is not based on tuple counts but a more sophisticated measure). Our experiments on a variety of real datasets show that PrivTree considerably outperforms the states of the art in terms of data utility.","Jun Zhang, NTU Singapore; Xiaokui Xiao*, Nanyang Technological University; Xing Xie, Microsoft Research Asia",jzhang027@ntu.edu.sg; xkxiao@ntu.edu.sg; xing.xie@microsoft.com,"Database security, privacy, access control*","Bin Cui, Peking U.(bin.cui@pku.edu.cn) This PC member has been a collaborator within the past two years.; Stavros Papadopoulos, Intel Labs and MIT(stavrosp@csail.mit.edu) This PC member has been a collaborator within the past two years.","388.pdf (1,632,431 bytes)",,I Agree390,Holistic Influence Maximization: Combining Scalability and Efficiency with Opinion-Aware Models,Research Subtrack 4 Revision,"The steady growth of graph data from social networks has resulted in wide-spread research in finding solutions to the influence maximization problem. In this paper, we propose a holistic solution to the influence maximization (IM) problem. (1) We introduce an opinion-cum-interaction (OI) model that closely mirrors the real-world scenarios. Under the OI model, we introduce a novel problem of Maximizing the Effective Opinion (MEO) of influenced users. We prove that the MEO problem is NP-hard and cannot be approximated within any finite ratio unless P=NP. (2) We propose a heuristic algorithm OSIM to efficiently solve the MEO problem. To better explain the OSIM heuristic, we first introduce EaSyIM – the opinion-oblivious version of OSIM, a scalable algorithm capable of running within practical compute times on commodity hardware. In addition to serving as a fundamental building block for OSIM, EaSyIM is capable of addressing the scalability aspect – memory consumption and running time, of the IM problem as well.  Empirically, our algorithms are capable of maintaining the deviation in the spread always within 5% of the best known methods in the literature. In addition, our experiments show that both OSIM and EaSyIM are effective, efficient, scalable and significantly enhance the ability to analyze real datasets.","Sainyam Galhotra, XRCI; Akhil Arora*, XRCI; Shourya Roy, XRCI",sainyamgalhotra@gmail.com; akhil.arora@xerox.com; shourya.roy@xerox.com,"Graph data management, RDF, social networks*; Knowledge discovery, clustering, data mining",,"390.pdf (901,708 bytes)",,I Agree391,LazyLSH: Approximate Nearest Neighbor Search for Multiple Distance Functions with a Single Index,Research Subtrack 4 Revision,"Due to the ``curse of dimensionality"" problem, it is very expensive to process the nearest neighbor (NN) query in high-dimensional spaces; and hence, approximate approaches, such as Locality-Sensitive Hashing (LSH), are widely used for their theoretical guarantees and empirical performance. Current LSH-based approaches target at the L1 and L2 spaces, while as shown in previous work, the fractional distance metrics (Lp metrics with 0 < p < 1) can provide more insightful results than the usual L1 and L2 metrics for data mining and multimedia applications. However, none of the existing work can support multiple fractional distance metrics using one index. In this paper, we propose LazyLSH that answers approximate nearest neighbor queries for multiple Lp metrics with theoretical guarantees. Different from previous LSH approaches which need to build one dedicated index for every query space, LazyLSH uses a single base index to support the computations in multiple Lp spaces, significantly reducing the maintenance overhead. Extensive experiments show that LazyLSH provides more accurate results for approximate kNN search under fractional distance metrics.","Yuxin Zheng*, NUS; Qi Guo, NUS; Anthony Tung, National University of Singapore; Sai Wu, Zhejiang University",yuxin@comp.nus.edu.sg; qiguo@comp.nus.edu.sg; anthony@comp.nus.edu.sg; wusai@zju.edu.cn,"Storage, indexing, and physical database design*; Knowledge discovery, clustering, data mining; Query processing and optimization"," Beng Chin Ooi, National University of Singapore(ooibc@comp.nus.edu.sg) This PC member has been a co-worker in the same company or university within the past two years.; Kian-Lee Tan, National University of Singapore(tankl@comp.nus.edu.sg) This PC member has been a co-worker in the same company or university within the past two years.","LazyLSH_revision.pdf (1,864,745 bytes)",,I Agree392,Principled evaluation of differentially private algorithms using DPBench,Research Subtrack 4 Revision,"Differential privacy has become the dominant standard in the research community for strong privacy protection.  There has been a flood of research into query answering algorithms that meet this standard.  Algorithms are becoming increasingly complex, and in particular, the performance of many emerging algorithms is data dependent, meaning the distribution of the noise added to query answers may change depending on the input data.  Theoretical analysis typically only considers the worst case, making empirical study of average case performance increasingly important.  In this paper we propose a set of evaluation principles which we argue are essential for sound evaluation.  Based on these principles we propose DPBench, a novel evaluation framework for  standardized evaluation of privacy algorithms.  We then apply our benchmark to evaluate algorithms for answering 1- and 2-dimensional range queries. The result is a thorough empirical study of 15 published algorithms on a total of 27 datasets that offers new insights into algorithm behavior---in particular the influence of dataset scale and shape---and a more complete characterization of the state of the art.  Our methodology is able to resolve inconsistencies in prior empirical studies and place algorithm performance in context through comparison to simple baselines.  Finally, we pose open research questions  which we hope will guide future algorithm design.  ","Michael Hay*, Colgate; Ashwin Machanavajjhala, Duke U.; Gerome Miklau, University of Massachusetts Amherst; Yan Chen, Duke University; Dan Zhang, University of Massachusetts Amherst",mhay@colgate.edu; ashwin@cs.duke.edu; miklau@cs.umass.edu; yanchen@cs.duke.edu; dzhang@cs.umass.edu,"Database security, privacy, access control*; Benchmarking and performance evaluation; Knowledge discovery, clustering, data mining","Ashwin Machanavajjhala, Duke U.(ashwin@cs.duke.edu) This PC member is a co-author of the paper.; Alexandra Meliou, UMass Amherst(ameli@cs.umass.edu) This PC member has been a co-worker in the same company or university within the past two years.; Gerhard Weikum, Max-Planck-Institut für Informatik(weikum@mpi-inf.mpg.de) This PC member has been a collaborator within the past two years.","paper.pdf (498,341 bytes)",,I Agree393,Adaptive Indexing over Encrypted Data,Research Subtrack 4 Revision,"Today, outsourcing heavy query processing tasks to remote servers becomes a viable option; such outsourcing calls for encrypting data so as to render it secure against adversaries and/or untrusted servers. At the same time, to be efficiently managed, outsourced data should be indexed, while current trends call for adaptive indexing as side-effect of query processing. Yet current trends in encryption propose computationally heavy encryption schemes that render such outsourcing unattractive albeit secure; an alternative, Order-Preserving Encryption Scheme (OPES), intentionally preserves and reveals the order in the data, hence is unattractive from the security viewpoint. In this paper, we propose and analyze a lightweight and indexable encryption scheme, which provides higher security than OPES, while it fits with the philosophy of adaptive indexing. Our scheme is based on algebraic operations and allows for equality and range queries to be efficiently evaluated over encrypted data; thereby it enables incremental adaptive indexing, without leaking order information and without prohibitive overhead, as our extensive experimental study demonstrates.","Panagiotis Karras*, Skoltech; Saad Malhotra, ; Rudrika Bhatt, ; Artyom Nikitin, ; Denis Antyukhov, ; Stratos Idreos, Harvard University",karras@skoltech.ru; muhammad.saad@skolkovotech.ru; rudrikaa@gmail.com; artem.nikitin@skolkovotech.ru; denis.antyukhov@skolkovotech.ru; stratos@seas.harvard.edu,"Database security, privacy, access control*; Query processing and optimization; Storage, indexing, and physical database design","Panagiotis Karras, Skoltech(karras@skoltech.ru) This PC member is a co-author of the paper.; Stavros Papadopoulos, Intel Labs and MIT(stavrosp@csail.mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Georgios Kellaris, Harvard(gkellaris@ust.hk) This PC member has been a co-worker in the same company or university within the past two years.; Nikos Mamoulis, U. Hong Kong(nikos@cs.hku.hk) This PC member is or was an author's primary thesis advisor, no matter how long ago.; Kyriakos MOURATIDIS, Singapore Management University(kyriakos@smu.edu.sg) This PC member is a relative or close personal friend of an author.","revision.pdf (3,429,285 bytes)",,I Agree380,Topic Exploration in Spatio-Temporal Document Collections,Research Subtrack 3 Revision,"Huge amounts of data with both spatial and temporal information (e.g., geo-tagged tweets) are being generated. Such data can be modeled as spatio-temporal documents and they have been widely used to share and spread personal updates, spontaneous ideas, and breaking news. Consequently, it is of great interest to explore topics in a collection of spatio-temporal documents.  In this paper, we study the problem of mining topics from spatio-temporal documents within a user specified bounded region and timespan, to provide users with insights about events, trends, and public concerns within the specified region and time period. We propose a novel algorithm that is able to efficiently combine two pre-trained topic models learnt from two document sets within a bounded error, based on which we develop an efficient approach to mining topics from a large number of spatio-temporal documents within a region and a timespan. Our experimental results show that our proposal is able to improve the runtime by at least an order of magnitude compared with the baselines. Meanwhile, the effectiveness of our proposed method is close to the baselines.","Kaiqi Zhao*, Nanyang Technological Universi; Lisi Chen, NTU; Gao Cong, NTU",kzhao002@e.ntu.edu.sg; lchen012@e.ntu.edu.sg; gaocong@ntu.edu.sg,"Information retrieval and text mining*; Knowledge discovery, clustering, data mining",,"revision.pdf (1,484,195 bytes)",,I Agree381,An Efficient MapReduce Cube Algorithm for Varied Data Distributions,Research Subtrack 3 Revision,"Data cubes allow users to discover insights from their data and are commonly used in data analysis. While very useful, the data cube is expensive to compute, in particular when the input relation is very large. To address this problem, we consider cube computation in MapReduce, the popular paradigm for distributed big data processing, and present an efficient algorithm for computing cubes over large data sets. We show that our new algorithm consistently performs better than the previous solutions. In particular, existing techniques for cube computation in MapReduce suffer from sensitivity to the distribution of the input data and their performance heavily depends on whether or not, and how exactly, the data is skewed. In contrast, the cube algorithm that we present here is resilient and significantly outperforms previous solutions for varying data distributions. At the core of our solution is a dedicated data structure called the Skews and Partitions Sketch (SP-Sketch for short). The SP-Sketch is compact in size and fast to compute, and records all needed information for identifying skews and effectively partitioning the workload between the machines. Our algorithm uses the sketch to speed up computation and minimize communication overhead. Our theoretical analysis and thorough experimental study demonstrate the feasibility and efficiency of our solution, including comparisons to state of the art tools for big data processing such as Pig and Hive.","Eyal Altshuler*, Tel-Aviv University; Tova Milo, Tel Aviv University",eyal.altshuler@gmail.com; milo@cs.tau.ac.il,"Distributed and parallel databases*; Data warehousing, OLAP, SQL Analytics; Query processing and optimization",,"revision.pdf (2,597,092 bytes)",,I Agree382,Towards a Non-2PC Transaction Management in Distributed Database Systems,Research Subtrack 3 Revision,"Shared-nothing architecture has been widely used in distributed databases to achieve good scalability. While it offers superior performance for single-partition transactions (i.e., transactions whose data are hosted on a single data partition), the overhead of processing distributed transactions (i.e., transactions whose data are stored over multiple data partitions) can degrade the system performance significantly. The key contributor to the degradation is due to the expensive two-phase commit (2PC) protocol for preserving the atomicity of distributed transaction processing. In this paper, we propose a transaction management scheme called LEAP to eliminate the 2PC protocol within distributed transaction processing. Instead of distributing the transaction, LEAP converts any distributed transaction into a local transaction through gathering the ownership of its required data at the tuple granularity. This benefits the processing locality and facilitates adaptive data repartitioning with respect to the alteration of data access pattern. Based on LEAP we develop an online transaction processing (OLTP) system and compare it with the state-of-the-art distributed in-memory OLTP system, H-Store, which relies on the 2PC protocol for distributed transaction processing, and H^L-store, a H-store that makes use of LEAP. Results of an extensive experimental evaluation show that LEAP outperforms 2PC by a wide margin, especially for workloads characteristic of certain locality of cross-partition data access.","Qian Lin, NUS; Pengfei Chang, NUS; Gang Chen, ZJU;  Beng Chin Ooi*, National University of Singapore; Kian-Lee Tan, National University of Singapore; Zhengkui Wang, NUS",linqian@comp.nus.edu.sg; changpeng3336@gmail.com; cg@cs.zju.edu.cn; ooibc@comp.nus.edu.sg; tankl@comp.nus.edu.sg; wangzhengkui@comp.nus.edu.sg,Distributed and parallel databases*; Transaction processing,,"382.pdf (918,684 bytes)",,I Agree383,Automatic Generation of Normalized Relational Schemas from Key-Value Data,Research Subtrack 3 Revision,"Self-describing key-value data formats such as JSON are becoming increasingly popular as application developers choose to avoid the rigidity imposed by the relational model. Database systems designed for these self-describing formats, such as MongoDB, encourage users to use denormalized, heavily nested data models so that relationships across records and other schema information need not be predefined or standardized. Such data models contribute to long-term development complexity, as their lack of explicit entity and relationship tracking burdens new developers unfamiliar with the dataset. Furthermore, the large amount of data repetition present in such data layouts can introduce update anomalies and poor scan performance, which reduce both the quality and performance of analytics over the data.   In this paper we present an algorithm that automatically transforms the denormalized, nested data commonly found in NoSQL systems into traditional relational data that can be stored in a standard RDBMS. This process includes a schema generation algorithm that discovers relationships across the attributes of the denormalized datasets in order to organize those attributes into relational tables. It further includes a matching algorithm that discovers sets of attributes that represent overlapping entities and merges those sets together. Combined, these algorithms reduce data repetition, enable the end-user to use traditional data analysis tools designed to work with relational data, accelerate scan-based analysis algorithms over the data, and help users gain a semantic understanding of complex, nested datasets.","Daniel Abadi*, Yale University; Michael DiScala, ",dna@cs.yale.edu; mike.discala@gmail.com,"Semi-structured data*; Knowledge discovery, clustering, data mining","Peter Boncz, CWI Amsterdam(p.boncz@cwi.nl) This PC member is a co-author of the paper.","schemagen.pdf (921,986 bytes)",,I Agree384,Vectorized Access to Hot and Cold Data in Compiling Query Engines,Research Subtrack 3 Revision,"This work aims at reducing the main-memory footprint in high performance hybrid OLTP&OLAP databases, while retaining high query performance and transactional throughput. For this purpose an innovative compressed columnar storage format for cold data, called Data Blocks is intro- duced. Data Blocks, further incorporate a new light-weight index structure called Positional SMA, that narrows scan ranges within Data Blocks even if the entire block cannot be ruled out. To achieve highest OLTP performance the compression schemes of Data Blocks are very light-weight, such that OLTP transactions can still quickly access individual tuples – Setting this storage scheme apart from those used in specialized analytical databases, where data must usually be bit-unpacked. So far high-performance analytical systems use either vectorized query execution or “just-in-time” (JIT) query compilation. The fine-grained adaptivity of Data Blocks necessitates the integration of the best features of each approach by an interpreted vectorized scan sub-system feeding into JIT-compiled query pipelines. Experimental evaluation of our full-fledged hybrid OLTP&OLAP database system shows that Data Blocks accelerate performance on a variety of query workloads while retaining high transaction throughput.","Tobias Muehlbauer*, TU Munich; Harald Lang, TU Munich; Florian Funke, Snowflake Computing; Peter Boncz, CWI Amsterdam; Thomas Neumann, TU Munich; Alfons Kemper, TU Munich",muehlbau@in.tum.de; harald.lang@in.tum.de; funkef@in.tum.de; p.boncz@cwi.nl; neumann@in.tum.de; kemper@in.tum.de,"Query processing and optimization*; Data warehousing, OLAP, SQL Analytics; Databases for emerging hardware; Storage, indexing, and physical database design","Peter Boncz, CWI Amsterdam(p.boncz@cwi.nl) This PC member is a co-author of the paper.","datablocks_revision.pdf (1,492,203 bytes)",,I Agree386,Extracting Equivalent SQL From Imperative Code in Database Applications,Research Subtrack 3 Revision,"Optimizing the performance of database applications is an area of practical importance, and has received significant attention in recent years. In this paper we present an approach to this problem which is based on extracting a concise algebraic representation of (parts of) an application, which may include imperative code as well as SQL queries. The algebraic representation can then be translated into SQL to improve application performance by reducing the volume of data transferred, as well as reducing latency by minimizing the number of network round trips.  Our techniques can be used for performing optimizations of database applications that techniques proposed earlier cannot perform. The algebraic representations can also be used for other purposes such as extracting equivalent queries for keyword search on form results.  Our experiments indicate that the techniques we present are widely applicable to real world database applications, in terms of successfully extracting algebraic representations of application behavior, as well as in terms of providing performance benefits when used for optimization.","Venkatesh Emani*, IIT Bombay; Karthik Ramachandra, Microsoft; S. Sudarshan, IIT Bombay; Subhro Bhattacharya, Citrix",venkateshek@cse.iitb.ac.in; karthik.s.ramachandra@gmail.com; sudarsha@cse.iitb.ac.in; subhro.bhattacharya@gmail.com,Query processing and optimization*,"Ravi Ramamurthy, Microsoft Research(ravirama@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Rimma Nehme, Microsoft(rimman@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.","ardb.pdf (1,102,887 bytes)",,I Agree387,A Fast Randomized Algorithm for Multi-Objective Query Optimization,Research Subtrack 3 Revision,"Query plans are compared according to multiple cost metrics in multi-objective query optimization. The goal is to find the set of Pareto plans realizing optimal cost tradeoffs for a given query. So far, only algorithms with exponential time complexity have been proposed for multi-objective query optimization. In this work we present the first polynomial time heuristic for multi-objective query optimization.  Our algorithm is randomized and iterative. It improves query plans via a multi-objective version of hill climbing that applies multiple transformations in each climbing step for maximal efficiency. Based on a locally optimal plan, we approximate the Pareto plan set within the restricted space of plans with similar join orders. We maintain a cache of Pareto-optimal plans for each potentially useful intermediate results to share partial plans that were discovered in different iterations. We show that each iteration of our algorithm performs in expected polynomial time based on an analysis of the expected path length between a random plan and local optima reached by hill climbing. We experimentally show that our algorithm can optimize queries with hundreds of tables and outperforms other randomized algorithms such as the NSGA-II genetic algorithm over a wide range of scenarios. ","Immanuel Trummer*, EPFL; Christoph Koch, EPFL",immanuel.trummer@epfl.ch; christoph.koch@epfl.ch,Query processing and optimization*,"Peter Boncz, CWI Amsterdam(p.boncz@cwi.nl) This PC member has been a collaborator within the past two years.","answersAndRevision.pdf (733,054 bytes)",,I Agree370,Sampling-Based Query Re-Optimization,Research Subtrack 2 Revision,"Despite of decades of work, query optimizers still make mistakes on ""difficult"" queries because of bad cardinality estimates, often due to the interaction of multiple predicates and correlations in the data. In this paper, we propose a low-cost post-processing step that can take a plan produced by the optimizer, detect when it is likely to have made such a mistake, and take steps to fix it. Specifically, our solution is a sampling-based iterative procedure that requires almost no changes to the original query optimizer or query evaluation mechanism of the system. We show that this indeed imposes low overhead and catches cases where three widely used optimizers (PostgreSQL and two commercial systems) make large errors. ","Wentao Wu*, University of Wisconsin-Madison; Jeffrey Naughton, University of Wisconsin-Madison; Harneet Singh, ",wentaowu@cs.wisc.edu; naughton@cs.wisc.edu; harneet@cs.wisc.edu,Query processing and optimization*,"Arvind Arasu, Microsoft Research(arvinda@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Kaushik Chakrabarti, Microsoft Research(kaushik@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Badrish Chandramouli, Microsoft Research(badrishc@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Sudipto Das, Microsoft Research(sudiptod@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Justin Levandoski, Microsoft(justin.levandoski@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Ashraf Aboulnaga, Qatar Computing Research Institute(aaboulnaga@qf.org.qa) An author is or was the PC member's primary thesis advisor, no matter how long ago.","paper56-revised.pdf (1,029,907 bytes)",,I Agree373,Streaming Algorithms for Robust Distinct Elements,Research Subtrack 2 Revision,"We study the problem of estimating distinct elements in the data stream model, which has a central role in traffic monitoring, query optimization, data mining and data integration. Different from all previous work, we study the problem in the noisy data setting, where two different looking items in the stream may reference the same entity (determined by a distance function and a threshold value), and the goal is to estimate the number of distinct entities in the stream. In this paper, we formalize the problem of robust distinct elements, and develop space and time-efficient streaming algorithms for datasets in the Euclidean space, using a novel technique we call bucket sampling.  We also extend our algorithmic framework to other metric spaces by establishing a connection between bucket sampling and the theory of locality sensitive hashing. Moreover, we formally prove that our algorithms are still effective under small distinct elements ambiguity. Our experiments demonstrate the practicality of our algorithms.","Di Chen*, HKUST; Qin Zhang, Indiana University Bloomington",dchenad@cse.ust.hk; qzhangcs@indiana.edu,"Streams, sensor networks, complex event processing*; Knowledge discovery, clustering, data mining","Vijayshankar Raman, IBM Research - Almaden(ravijay@us.ibm.com) This PC member has been a co-worker in the same company or university within the past two years.","paper-letter.pdf (668,551 bytes)",,I Agree375,Micro-architectural Analysis of In-memory OLTP,Research Subtrack 2 Revision,"Micro-architectural behavior of traditional disk-based online transaction processing (OLTP) systems has been investigated extensively over the past couple of decades. Results show that traditional OLTP mostly underutilize the available micro-architectural resources. In-memory OLTP systems, on the other hand, process all the data in main-memory, and therefore, can omit the buffer pool. In addition, they usually adopt more lightweight concurrency control mechanisms, cache-conscious data structures and a cleaner codebase since they are usually designed from scratch. Hence, we expect significant differences in micro-architectural behavior when running OLTP on in-memory as opposed to disk-based database systems. In particular,  we expect that in-memory systems exploit micro architectural features such as instruction and data caches significantly better than disk-based systems. Surprisingly, the differences are not as pronounced as expected. This paper sheds light on the micro-architectural behaviour of in-memory database systems by analyzing and contrasting it to the behaviour of disk-based systems when running OLTP workloads. The results show that despite all the design changes in-memory OLTP exhibits very similar micro-architectural behavior to disk-based OLTP systems: more than half of the execution time goes to memory stalls where L1 instruction misses and the long-latency data misses from the last-level cache are the dominant factors in the overall stall time. Even though aggressive compilation optimizations can almost eliminate instruction misses, the reduction in instruction stalls amplifies the impact of last-level cache data misses. As a result, the number of instructions retired per cycle barely reaches one on machines that are able to retire up to four for both traditional disk-based and new generation in-memory transaction processing systems. ","Utku Sirin*, EPFL; Pinar Tozun, IBM Research; Danica Porobic, EPFL; Anastasia Ailamaki, EPFL",utku.sirin@epfl.ch; ptozun@us.ibm.com; danica.porobic@epfl.ch; anastasia.ailamaki@epfl.ch,Transaction processing*,"Vijayshankar Raman, IBM Research - Almaden(ravijay@us.ibm.com) This PC member has been a co-worker in the same company or university within the past two years.; Stratos Idreos, Harvard University(stratos@seas.harvard.edu) This PC member has been a collaborator within the past two years.; Ippokratis Pandis, Amazon Web Services(ippokratis@gmail.com) An author is or was the PC member's primary thesis advisor, no matter how long ago.","SIGMOD2016_RevisedSubmission_Sirin.pdf (2,911,368 bytes)",,I Agree376,An Experimental Comparison of Thirteen Relational Equi-Joins in Main Memory,Research Subtrack 2 Revision,"Relational equi-joins are at the heart of almost every query plan. They have been studied, improved, and reexamined on a regular basis since the existence of the database community. In the past four years several new join algorithms have been proposed and experimentally evaluated. Some of those papers contradict each other in their experimental findings. This makes it surprisingly hard to answer a very simple question: what is the fastest join algorithm in 2015?  In this paper we will try to develop an answer. We start with an end-to-end black box comparison of the most important methods. Afterwards, we inspect the internals of these algorithms in a white box comparison. We derive improved variants of state-of-the-art join algorithms by applying optimizations like~software-write combine buffers, various hash table implementations, as well as NUMA-awareness in terms of data placement and scheduling. We also inspect various radix partitioning strategies. Eventually, we are in the position to perform a comprehensive comparison of thirteen different join algorithms. We factor in scaling effects in terms of size of the input datasets, the number of threads, different page sizes, and data distributions. Furthermore, we analyze the impact of various joins on an (unchanged) TPC-H query.  Finally, we conclude with a list of major lessons learned from our study and a guideline for practitioners implementing massive main-memory joins. As is the case with almost all algorithms in databases, we will learn that there is no single best join algorithm. Each algorithm has its strength and weaknesses and shines in different areas of the parameter space. ","Stefan Schuh*, Saarland University; Jens Dittrich, Saarland University; Xiao Chen, Saarland University",schuh@cs.uni-saarland.de; jens.dittrich@cs.uni-saarland.de; s9xochen@stud.uni-saarland.de,Benchmarking and performance evaluation*; Distributed and parallel databases; Query processing and optimization,,"376.pdf (975,998 bytes)",,I Agree377,A Hybrid B+-Tree as Solution for In-Memory Indexing on CPU-GPU Heterogeneous Computing Platforms,Research Subtrack 2 Revision,"An in-memory indexing tree is a critical component of many databases. Modern many-core processors, such as GPUs, are offering tremendous amounts of computing power which makes them an attractive choice for accelerating indexing. However, the memory available to the accelerating co-processor is rather limited and expensive in comparison to the memory available to the CPU. This drawback is a barrier to exploit the computing power of co-processors for arbitrarily large index trees. In this paper, we propose a novel design for a B+-tree based on the heterogeneous computing platform and the hybrid memory architecture found in GPUs. Furthermore, we propose a hybrid CPU-GPU B+-tree, – HB+-tree, – which targets high search throughput use cases. Unique to our design is the joint and simultaneous use of computing and memory resources of CPU-GPU systems. Our experiments show that our HB+-tree can perform up to 240 million index queries per second, which is 2.4X higher than our CPU optimized solution.","Amirhesam Shahvarani*, Technische Universität München; Hans-Arno Jacobsen, Technical University Munich",shahvara@in.tum.de; jacobsen@in.tum.de,"Databases for emerging hardware*; Data warehousing, OLAP, SQL Analytics; Storage, indexing, and physical database design","Thomas Neumann, TU Munich(neumann@in.tum.de) This PC member has been a co-worker in the same company or university within the past two years.","HBTree.pdf (1,009,555 bytes)",,I Agree378,Dynamic Prefetching of Data Tiles for Interactive Visualization,Research Subtrack 2 Revision,"In this paper, we present ForeCache, a general-purpose tool for exploratory browsing of large datasets. ForeCache utilizes a client-server architecture, where the user interacts with a lightweight client-side interface to browse datasets, and the data to be browsed is retrieved from a DBMS running on a back-end server. We assume a detail-on-demand browsing paradigm, and optimize the back-end support for this paradigm by inserting a separate middleware layer in front of the DBMS. To improve response times, the middleware layer fetches data ahead of the user as she explores a dataset.  We consider two different mechanisms for prefetching: (a) learning what to fetch from the user’s recent movements, and (b) using data characteristics (e.g., histograms) to find data similar to what the user has viewed in the past. We incorporate these mechanisms into a single prediction engine that adjusts its prediction strategies over time, based on changes in the user’s behavior. We evaluated our prediction engine with a user study, and found that our dynamic prefetching strategy provides: (1) significant improvements in overall latency when compared with non-prefetching systems (430% improvement); and (2) substantial improvements in both prediction accuracy (25% improvement) and latency (88% improvement) relative to existing prefetching techniques.","Leilani Battle*, MIT; Michael Stonebraker, MIT; Remco Chang, Tufts University",leibatt@mit.edu; stonebraker@csail.mit.edu; remco@cs.tufts.edu,Data visualization*,"Alvin Cheung, University of Washington(akcheung@cs.washington.edu) This PC member has been a co-worker in the same company or university within the past two years.; Jennie Duggan, Northwestern Univeristy(jennie.duggan@northwestern.edu) This PC member has been a co-worker in the same company or university within the past two years.; Stavros Papadopoulos, Intel Labs and MIT(stavrosp@csail.mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Matei Zaharia, MIT(matei@mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Badrish Chandramouli, Microsoft Research(badrishc@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Magda Balazinska, Unversity of Washington(magda@cs.washington.edu) This PC member is or was an author's primary thesis advisor, no matter how long ago.; Jennie Duggan, Northwestern Univeristy(jennie.duggan@northwestern.edu) This PC member is a relative or close personal friend of an author.","forecache_sigmod2016revision.pdf (6,039,031 bytes)",,I Agree379,Sharing-Aware Outlier Analytics over High-Volume Data Streams,Research Subtrack 2 Revision,"Real-time analytics of anomalous phenomena on streaming data typically relies on processing a large variety of continuous outlier detection requests, each configured with different parameter settings. The processing of such complex outlier analytics workloads is resource consuming due to the algorithmic complexity of the outlier mining process. In this work we propose a sharing-aware multi-query execution strategy for outlier detection on data streams called SOP. A key insight of SOP is to transform the problem of handling a \emph{multi-query outlier} analytics workload into a \emph{single-query skyline} computation problem. We prove that the output of the skyline computation process corresponds to the minimal information needed for determining the outlier status of any point in the stream. Based on this new formulation, we design a customized skyline algorithm called K-SKY that leverages the \textit{domination relationships} among the streaming data points to minimize the number of data points that must be evaluated for supporting multi-query outlier detection. Based on this K-SKY algorithm, our SOP solution achieves minimal utilization of both computational and memory resources for the processing of these complex outlier analytics workload. Our experimental study demonstrates that SOP consistently outperforms the state-of-art solutions by three orders of magnitude in CPU time, while only consuming 5\% of their memory footprint $-$ a clear win-win. Furthermore, SOP is shown to scale to large workloads composed of thousands of parameterized queries. ","Lei Cao*, Worcester Polytechnic Inst; Jiayuan Wang, Worcester Polytechnic Institute; Elke Rundensteiner, Worcester Polytechnic Institute (WPI).",lcao@cs.wpi.edu; jwang1@wpi.edu; rundenst@cs.wpi.edu,"Streams, sensor networks, complex event processing*",,"SOPRevision.pdf (477,362 bytes)",,I Agree363,A Study of Sorting Algorithms on Approximate Memory,Research Subtrack 1 Revision,"Hardware evolution has been one of the driving factors for the redesign of database systems. Recently, approximate storage emerges in the area of computer architecture. It trades o_ precision for better performance or energy consumption, or both. Previous studies have demonstrated the beneﬁts of approximate storage on applications that are tolerant to imprecision such as image processing. However, it is still an open problem whether and how approximate storage can be used for applications that do not expose such intrinsic tolerance. In this paper, we study one of the most basic operation in database–sorting on a hybrid storage system with both precise storage and approximate storage. Particularly, we start with a study of three common sorting algorithms on approximate storage. Experimental results show that a 95% sorted sequence can be obtained with up to 40% reduction in total write latencies. Thus, we exploit a novel approx-reﬁne execution mechanism to improve the performance of sorting algorithms on the hybrid storage system to produce precise results. Our optimization gains the performance beneﬁts by o_oading the sort operation to approximate storage, followed by an e_cient reﬁnement to resolve the unsortedness on the output of the approximate storage. Our experiments show that our approx-reﬁne can reduce the total memory access time by up to 10%. These studies shed light on the potential of approximate hardware for improving the performance of applications with the requirement of precise results.","Bingsheng He*, Nanyang Technological University; Shunning Jiang, Nanyang Technological University; Shuang Chen, Nanyang Technological University; Xueyan Tang, Nanyang Technological University",bshe@ntu.edu.sg; Shunning.Chen@gmail.com; Shuang.Chen@gmail.com; asxytang@ntu.edu.sg,Databases for emerging hardware*,"Xiaokui Xiao, Nanyang Technological University(xkxiao@ntu.edu.sg) This PC member has been a co-worker in the same company or university within the past two years.","SIGMOD2016-id38.pdf (1,776,534 bytes)",,I Agree364,Estimating the Impact of Unknown Unknowns on Aggregate Query Results,Research Subtrack 1 Revision,"It is common practice for data scientists to acquire and integrate disparate data sources to achieve higher quality results. But even with a perfectly cleaned and merged data set, two fundamental questions remain: (1) is the integrated data set complete and (2) what is the impact of any unknown (i.e., unobserved) data on query results?  In this work, we develop and analyze techniques to estimate the impact of the unknown data (a.k.a., unknown unknowns) on simple aggregate queries. The key idea is that the overlap between different data sources enables us to estimate the number and values of the missing data items. Our main techniques are parameter-free and do not assume prior knowledge about the distribution. Through a series of experiments, we show that estimating the impact of unknown unknowns is invaluable to better assess the results of aggregate queries over integrated data sources. ","Yeounoh Chung*, Brown University; Tim Kraska, Brown University; Carsten Binnig, Brown University; Michael Lind Mortensen, AarhusUniversity",yeounoh_chung@brown.edu; tim_kraska@brown.edu; carsten_binnig@brown.edu; illio@cs.au.dk,"Uncertain, probabilistic, and approximate databases*; Crowd sourcing","Samuel Madden, MIT(madden@csail.mit.edu) This PC member has been a co-worker in the same company or university within the past two years.","364.pdf (1,493,242 bytes)",,I Agree365,SHARE Interface in Flash Storage for Databases,Research Subtrack 1 Revision,"In the era of all-flash data centers in cloud computing, it is urgent to make open source database engines including tranditional relational DBMSs (e.g. MySQL/InnoDB) and emerging NoSQL storage engines (e.g. Couchbase) to be flash-optimized. For guaranteering the write atomicity which is critical for database consistency, many modern database storage engines are resorting to either redundant write or copy-on-write approach, both of which su_er from excessive write amplification, causing tardy performance and faster wear-out in flash storages. In order to tackle this write amplification problem in flash storage, this paper proposes a novel solution called SHARE. By calling SHARE, a new flash storage interface, database storage engines can explicitly remap the address mapping inside FTL so that they can, in combination with their original out-of-place update approach, achieve the write atomicity of data pages without any write ampfilication. By exploiting the address mapping available in every modern FTLs, the SHARE interface can make almost free the overhead of write amplification in database storage engine, which is a legacy of harddisk-based design principles. In addition, with SHARE, the compaction process, which is inevitable and time-consuming in NoSQL storage engines, can be completed in Couchbase without copying any data pages in flash storage. Our preliminary experimental results of SHARE-based MySQL/InnoDB and Couchbase on a real SSD development board show that it can significantly boost the database performance. In particular, the performance of compaction can be accelerated by more than an order of magnitude in terms of write time.","GIHWAN OH, Sungkyunkwan Univ.; Chiyoung Seo, Couchbase Inc.; Ravi Mayuram, Couchbase; Yang-Suk Kee, Samsung Semiconductor Inc.; Sang-Won Lee*, Sungkyunkwan University",wurikiji@skku.edu; chiyoung@couchbase.com; Ravi@couchbase.com; yangseok.ki@samsung.com; swlee@skku.edu,"Storage, indexing, and physical database design*; Databases for emerging hardware; Transaction processing",,"share.pdf (451,901 bytes)",,I Agree367,Practical Private Range Search Revisited,Research Subtrack 1 Revision,"We consider a data owner that outsources its dataset to an untrusted server. The owner wishes to enable the server to answer range queries on a single attribute, without compromising the privacy of the data and the queries. There are several schemes on ""practical"" private range search (mainly in Databases venues) that attempt to strike a trade-off between efficiency and security. Nevertheless, these methods either lack provable security guarantees, or permit unacceptable privacy leakages. In this paper, we take an interdisciplinary approach, which combines the rigor of Security formulations and proofs with efficient Data Management techniques. We construct a wide set of novel schemes with realistic security/performance trade-offs, adopting the notion of Searchable Symmetric Encryption (SSE) primarily proposed for keyword search. Specifically, we reduce range search to multi-keyword search using range covering techniques with tree-like indexes. We demonstrate that, given any secure SSE scheme, the challenge boils down to (i) formulating leakages that arise from the index structure, and (ii) minimizing false positives incurred by some schemes under heavy data skew. We analytically detail the superiority of our proposals over prior work and experimentally confirm their practicality. Our paper is self-contained and accessible to both Security and Data Management practitioners.","Ioannis Demertzis*, Technical University of Crete; Minos Garofalakis, ; Antonios Deligiannakis, Technical University of Crete; Stavros Papadopoulos, Intel Labs and MIT; Odysseas Papapetrou, ",idemertzis@softnet.tuc.gr; minos@softnet.tuc.gr; adeli@softnet.tuc.gr; stavrosp@csail.mit.edu; papapetrou@softnet.tuc.gr,"Database security, privacy, access control*","Samuel Madden, MIT(madden@csail.mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Dimitris Papadopoulos, Boston University(dipapado@bu.edu) This PC member has been a collaborator within the past two years.","RSSE_revised.pdf (786,543 bytes)",,I Agree368,Distributed Top-k Temporal Joins,Research Subtrack 1 Revision,"We study a particular kind of join, coined Ranked Temporal Join (RTJ), featuring predicates that compare time intervals and a scoring function associated with each predicate to quantify how well it is satisfied. RTJ queries are prevalent in a variety of applications such as network traffic monitoring, task scheduling, and tweet analysis. RTJ queries are often best interpreted as top-k queries where only the best matches are returned. We show how to exploit the nature of temporal predicates and the properties of their associated scoring semantics to design TKIJ, an efficient query evaluation approach on a distributed Map-Reduce architecture. TKIJ relies on an offline statistics computation that, given a time partitioning into granules, computes the distribution of intervals’ endpoints in each granule, and an online computation that generates query-dependent score bounds. Those statistics are used for workload assignment to reducers. This aims at reducing data replication, to limit I/O cost. Additionally, high-scoring results are distributed evenly to enable each reducer to prune unnecessary results. Our extensive experiments on synthetic and real datasets show that TKIJ outperforms state-of-the-art competitors and provides very good performance for n-ary RTJ queries on temporal data.","Julien Pilourdault*, Univ. Grenoble Alpes LIG,CNRS; Sihem Amer-Yahia, CNRS/LIG; Vincent Leroy, Univ. Grenoble Alpes, LIG, CNRS",julien.pilourdault@imag.fr; sihem.amer-yahia@imag.fr; vincent.leroy@imag.fr,Query processing and optimization*; Spatio-temporal databases,"Sihem Amer-Yahia, CNRS/LIG(sihem.amer-yahia@imag.fr) This PC member is a co-author of the paper.","368.pdf (775,590 bytes)",,I Agree369,Probabilistic Truss Decomposition: Concepts and Algorithms,Research Subtrack 1 Revision,"A key operation in network analysis is the discovery of cohesive subgraphs. The notion of k-truss has gained considerable popularity in this regard, based on its rich structure and efficient computability. However, many complex networks such as social, biological and communication networks feature uncertainty, best modeled using probabilities. Unfortunately the problem of discovering k-trusses in probabilistic graphs has received little attention to date.  In this paper, given a probabilistic graph G, number k and parameter _ _ (0, 1], we define a (k, _)-truss as a maximal connected subgraph H _ G, in which for each edge, the probability that it is contained in at least (k _ 2) triangles is at least _. We develop an efficient algorithm for decomposing a probabilistic graph into such maximal (k, _)-trusses. The above definition is local in that the “witness” graphs that has the (k _ 2) triangles containing an edge in H may be quite different for distinct edges. To mitigate this, we also explore a “global” notion: a global (k, _)-truss, in addition to being a local (k, _)-truss, has to satisfy the condition that the probability that H contains a k-truss is at least _. We show that unlike local (k,_)-truss, the global (k,_)-truss decomposition of a probabilistic graph is intractable. We propose a novel sampling scheme which enables approximate discovery of global (k,_)-trusses with high probability. Our extensive experiments on real and synthetic datasets demonstrate the efficiency and effectiveness of our proposed approach and the usefulness of the notions of local and global (k,_)-truss.","Xin Huang*, UBC; Wei Lu, University of British Columbia; Laks Lakshmanan, University of British Columbia",xin0@cs.ubc.ca; welu@cs.ubc.ca; laks@cs.ubc.ca,"Graph data management, RDF, social networks*; Uncertain, probabilistic, and approximate databases","Sihem Amer-Yahia, CNRS/LIG(sihem.amer-yahia@imag.fr) This PC member has been a collaborator within the past two years.","paper.pdf (540,962 bytes)",response-final.pdf (144772 bytes),I Agree504,UpBit: Scalable In-Memory Updatable Bitmap Indexing,"Research, November 2015","Bitmap indexes are widely used in both scientific and commercial databases. They bring fast read performance for specific types of queries, such as equality and selective range queries, by binning values. A major drawback, however, is that updating a bitmap index is particularly costly because it requires both decoding and encoding a bitvector (bitvectors are always kept compressed to minimize storage footprint). Today, more and more applications need support for both reads and writes, blurring the boundaries between analytical processing and transaction processing. This requires new system designs and access methods that support general updates and offer competitive read performance.  In this paper, we propose scalable in-memory Updatable Bitmap indexing (UpBit), which offers efficient updates, without hurting read performance. UpBit relies on two design points. First, in addition to the main bitvector for each domain value, UpBit maintains an update bitvector, to keep track of updated values. Effectively, every update can now be directed to a highly-compressible, easy-to-update bitvector. While update bitvectors double the amount of uncompressed data, they are sparse, and as a result their compressed size is kept small. Second, we introduce fence pointers in all update bitvectors which allow for efficient retrieval of a value at an arbitrary position. Using both synthetic and real-life data, we demonstrate that UpBit significantly outperforms state-of-the-art bitmap indexes for workloads that contain both reads and updates. In particular, compared to update-optimized bitmap index designs UpBit is 15-29x faster in terms of update time and 2.7x faster in terms of read performance. In addition, compared to read-optimized bitmap index designs UpBit achieves efficient and scalable updates (51-115x lower update latency), while allowing for comparable read performance, having up to 8% overhead.","Manos Athanassoulis*, Harvard University; Zheng Yan, University of Maryland; Stratos Idreos, Harvard University",manos@seas.harvard.edu; zhengyan@cs.umd.edu; stratos@seas.harvard.edu,"Storage, indexing, and physical database design*; Benchmarking and performance evaluation; Data warehousing, OLAP, SQL Analytics; Databases for emerging hardware; Query processing and optimization","Stratos Idreos, Harvard University(stratos@seas.harvard.edu) This PC member is a co-author of the paper.; Peter Boncz, CWI Amsterdam(p.boncz@cwi.nl) This PC member has been a co-worker in the same company or university within the past two years.; Christoph Koch, EPFL(christoph.koch@epfl.ch) This PC member has been a co-worker in the same company or university within the past two years.; Holger Pirk, MIT(holger@csail.mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Hannes Mühleisen, CWI Amsterdam(hannes.muehleisen@cwi.nl) This PC member has been a co-worker in the same company or university within the past two years.; Amol Deshpande, University of Maryland(amol@cs.umd.edu) This PC member has been a co-worker in the same company or university within the past two years.; Samuel Madden, MIT(madden@csail.mit.edu) This PC member has been a collaborator within the past two years.; Olga Papaemmanouil, Brandeis University(olga@cs.brandeis.edu) This PC member has been a collaborator within the past two years.; Philippe Bonnet, IT University of Copenhagen(phbo@itu.dk) This PC member has been a collaborator within the past two years.; Shimin Chen, ICT, Chinese Academy of Science(chensm@ict.ac.cn) This PC member has been a collaborator within the past two years.; Panagiotis Karras, Skoltech(karras@skoltech.ru) This PC member has been a collaborator within the past two years.; Martin Kersten, CWI(martin.kersten@cwi.nl) This PC member is or was an author's primary thesis advisor, no matter how long ago.; Erietta Liarou, EPFL(erietta@gmail.com) This PC member is a relative or close personal friend of an author.; Ippokratis Pandis, Amazon Web Services(ippokratis@gmail.com) This PC member is a relative or close personal friend of an author.","updateable-bitmaps-1924.pdf (431,223 bytes)",,I Agree692,Range Thresholding on Streams,"Research, November 2015","This paper studies a type of continuous queries called {\em range thresholding on streams} (RTS). Imagine the stream as an unbounded sequence of elements each of which is a real value. A query registers an interval, and must be notified as soon as a certain number of incoming elements fall into the interval. The system needs to support multiple queries simultaneously, and aims to minimize the space consumption and computation time.             Currently, all the solutions to this problem entail quadratic time $O(nm)$ to process $n$ stream elements and $m$ queries, which severely limits their applicability to only a small number of queries. We propose the first algorithm that breaks the quadratic barrier, by reducing the computation cost dramatically to $O(n + m)$, subject only to a polylogarithmic factor. The algorithm is general enough to guarantee the same on weighted versions of the queries even in $d$-dimensional space of any constant $d$. Its vast advantage over the previous methods in practical environments has been confirmed through extensive experimentation.  ","Miao Qiao, ADSC Singapore; Junhao Gan, Chinese University of Hong Kong; Yufei Tao*, University of Queensland",mqiaocuhk@gmail.com; jhgan@cse.cuhk.edu.hk; cstaoyf@gmail.com,"Streams, sensor networks, complex event processing*; Query processing and optimization","James Cheng, CUHK(jcheng@cse.cuhk.edu.hk) This PC member has been a co-worker in the same company or university within the past two years.; Feifei Li, University of Utah(lifeifei@cs.utah.edu) This PC member has been a collaborator within the past two years.; Jian Pei, Simon Fraser University(jpei@cs.sfu.ca) This PC member has been a collaborator within the past two years.; Xiaokui Xiao, Nanyang Technological University(xkxiao@ntu.edu.sg) An author is or was the PC member's primary thesis advisor, no matter how long ago.","rts.pdf (430,305 bytes)",,I Agree839,Low-Overhead Asynchronous Checkpointing in Main-Memory Database Systems,"Research, November 2015","As it becomes increasingly common for transaction processing systems to operate on datasets that fit within the main memory of a single machine or a cluster of commodity machines, traditional mechanisms for guaranteeing transaction durability---which typically involve synchronous log flushes---incur increasingly unappealing costs to otherwise lightweight transactions. Many applications have turned to periodically checkpointing full database state. However, existing checkpointing methods---even those which avoid freezing the storage layer---often come with significant costs to operation throughput, end-to-end latency, and total memory usage.         This paper presents Checkpointing Asynchronously using Logical Consistency (CALC), a lightweight, asynchronous technique for capturing database snapshots that  does not require a physical point of consistency to create a checkpoint, and avoids conspicuous latency spikes incurred by other database snapshotting schemes. Our experiments show that CALC can capture frequent checkpoints across a variety of transactional workloads with extremely small cost to transactional throughput and low additional memory usage compared to other state-of-the-art checkpointing systems.","Kun Ren*, Yale University; thaddeus Diamond, Yale University; Daniel Abadi, Yale University; Alexander Thomson, Google",kun.ren@yale.edu; thaddeus.diamond@aya.yale.edu; dna@cs.yale.edu; agt@google.com,Transaction processing*; Databases for emerging hardware,"Jayant Madhavan, Google(jayant@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Christopher Olston, Google(olston@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Cong Yu, Google Research(congyu@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Luna Dong, Google(lunadong@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Anastasios Kementsietsidis, Google Inc(akement@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Hongrae Lee, Google(hrlee@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Sergey Melnik, Google(melnik@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Samuel Madden, MIT(madden@csail.mit.edu) This PC member is or was an author's primary thesis advisor, no matter how long ago.; Samuel Madden, MIT(madden@csail.mit.edu) An author is or was the PC member's primary thesis advisor, no matter how long ago.","paper.pdf (426,235 bytes)",,I Agree309, Learning-Aided Cleansing for Indoor RFID Data,Research Subtrack 8,"RFID is widely used for object tracking in indoor environments, e.g., airport baggage tracking. Analyzing RFID data generated in such scenarios offers insight into the underlying tracking systems and enables prediction of system behaviors. However, the uncertain characteristics of RFID data, including noises (cross readings) and incompleteness (missing readings), pose challenges to high level RFID data querying and analysis. In this paper, we address such challenges by a learning-aided data cleansing approach that requires no detailed prior knowledge about the spatio-temporal properties of the indoor space and the RFID reader deployment. Requiring minimal information about RFID deployment, the approach learns relevant knowledge from raw RFID data and uses it to cleanse the data. In particular, we model raw RFID readings as time series that are quite sparse because the indoor space is only partly covered by a limited number of RFID readers. Essentially, we use an Indoor RFID Multi-variate Hidden Markov Model (IR-MHMM) to capture the uncertainties of indoor RFID data as well as the correlation of moving object locations and object's RFID reading. We propose for IR-MHMM three state space design methods that enable the learning of parameters while contending with raw RFID data time series. We solely use raw uncleaned RFID data for the learning of model parameters, and no special labeled data or ground truth data is required. The resulting IR-MHMM aided RFID data cleansing approach is able to recover missing readings and reduce cross readings with high effectiveness and efficiency, as demonstrated by the extensive experimental studies with both synthetic and real data. With sufficient indoor RFID data for learning, the proposed approach achieves data cleansing accuracy that is comparable to and even better than state-of-the-art techniques requiring very detailed prior knowledge, making our solution superior in terms of both effectiveness and employability.","Asif iqbal Baba*, Aalborg University; Manfred Jaeger, Aalborg University; Hua Lu, Aalborg University; Torben Bach Pedersen, Aalborg University; Wei-Shinn Ku, Auburn University; Xike Xie, Aalborg University",asif@cs.aau.dk; manfred@cs.aau.dk; luhua@cs.aau.dk; tbp@cs.aau.dk; weishinn@auburn.edu; xkxie@cs.aau.dk,"Schema matching, data integration, and data cleaning*; Spatio-temporal databases; Streams, sensor networks, complex event processing; Uncertain, probabilistic, and approximate databases","Torben Bach Pedersen, Aalborg University(tbp@cs.aau.dk) This PC member is a co-author of the paper.; Katja Hose, Aalborg University(khose@cs.aau.dk) This PC member has been a co-worker in the same company or university within the past two years.; Bin Cui, Peking U.(bin.cui@pku.edu.cn) This PC member has been a collaborator within the past two years.; Curtis Dyreson, Utah State(Curtis.Dyreson@usu.edu) This PC member has been a collaborator within the past two years.; Felix Naumann, Hasso Plattner Institute(Felix.Naumann@hpi.de) This PC member has been a collaborator within the past two years.; Wolfgang Lehner, TU Dresden(wolfgang.lehner@tu-dresden.de) This PC member has been a collaborator within the past two years.; Jiaheng Lu, University of Helsinki(jiahenglu@ruc.edu.cn) This PC member has been a collaborator within the past two years.","309.pdf (6,877,646 bytes)",,I Agree195,SABER: Window-Based Hybrid Stream Processing for Heterogeneous Architectures,Research Subtrack 6,"Modern servers have become heterogeneous, often combining multicore CPUs with many-core GPGPUs. Such heterogeneous architectures have the potential to improve the performance of data-intensive stream processing applications, but they are not supported by current relational stream processing engines. For an engine to exploit a heterogeneous architecture, it must execute streaming SQL queries with sufficient data-parallelism to fully utilise all available heterogeneous processors, and decide how to use each in the most effective way. It must do this while respecting the semantics of streaming SQL queries, in particular with regard to window handling.  We describe SABER, a hybrid high-performance relational stream processing engine for CPUs and GPGPUs. SABER executes window-based streaming SQL queries in a data-parallel fashion using all available CPU and GPGPU cores. Instead of statically assigning query operators to heterogeneous processors, SABER employs a new adaptive heterogeneous lookahead scheduling strategy, which increases the share of queries executing on the processor that yields the highest performance. To hide data movement costs, SABER pipelines the transfer of stream data between different memory types and the CPU/GPGPU. Our experimental comparison against state-of-the-art engines shows that SABER increases processing throughput while maintaining low latency for a wide range of streaming SQL queries with small and large windows sizes. ","Alexandros Koliousis, ; Matthias Weidlich*, Humboldt-Universität zu Berlin; Raul Castro Fernandez, ; Paolo Costa, ; Alexander Wolf, ; Peter Pietzuch, Imperial College London",a.koliousis@imperial.ac.uk; matthias.weidlich@hu-berlin.de; r.castro-fernandez11@imperial.ac.uk; Paolo.Costa@microsoft.com; a.wolf@imperial.ac.uk; prp@doc.ic.ac.uk,"Streams, sensor networks, complex event processing*; Query processing and optimization","Peter Pietzuch, Imperial College London(prp@doc.ic.ac.uk) This PC member is a co-author of the paper.; Arvind Arasu, Microsoft Research(arvinda@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Ravi Ramamurthy, Microsoft Research(ravirama@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Kaushik Chakrabarti, Microsoft Research(kaushik@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Badrish Chandramouli, Microsoft Research(badrishc@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Sudipto Das, Microsoft Research(sudiptod@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Vivek Narasayya, Microsoft Research(viveknar@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Rimma Nehme, Microsoft(rimman@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Justin Levandoski, Microsoft(justin.levandoski@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Avigdor Gal, Technion(avigal@ie.technion.ac.il) This PC member has been a collaborator within the past two years.","saber-sigmod.pdf (1,783,913 bytes)",,I Agree102,Elastic Pipelining In an In-Memory Database Cluster,Research Subtrack 3,"An in-memory database cluster consists of multiple interconnected nodes with a large capacity of RAM and modern multi-core CPUs. As a conventional query processing strategy, pipelining remains a promising solution for in-memory parallel database systems, as it avoids expensive intermediate result materialization and parallelizes the data processing among nodes. However, to fully unleash the power of pipelining in a cluster with multi-core nodes, it is crucial for the query optimizer to generate good query plans with appropriate intra-node parallelism, in order to maximize CPU and network bandwidth utilization. A suboptimal plan, on the contrary, causes load imbalance in the pipelines and consequently degrades the query performance. Parallelism assignment optimization at compile time is nearly impossible, as the workload in each node is affected by numerous factors and is highly dynamic during query evaluation. To tackle this problem, we propose elastic pipelining, which makes it possible to optimize intra-node parallelism assignments in the pipelines based on the actual workload at runtime. It is achieved with the adoption of new elastic iterator model and a fully optimized dynamic scheduler. The elastic iterator model generally upgrades traditional iterator model with new dynamic multi-core execution adjustment capability. And the dynamic scheduler efficiently provisions CPU cores to query execution segments in the pipelines based on the light-weight measurements on the operators. Extensive experiments on real and synthetic (TPC-H) data show that our proposal achieves almost full CPU utilization on typical decision-making analytical queries, outperforming state-of-the-art open-source systems by a huge margin.","Li Wang*, ; Zhou Minqi, East China Normal University; Zhenjie Zhang, ADSC; Yin Yang, ; Aoying Zhou, ",wangli1426@gmail.com; mqzhou@sei.ecnu.edu.cn; zhenjie@adsc.com.sg; yin@yang.net; ayzhou@sei.ecnu.edu.cn,Distributed and parallel databases*; Database monitoring and tuning; Query processing and optimization,"Zhou Minqi, East China Normal University(mqzhou@sei.ecnu.edu.cn) This PC member is a co-author of the paper.; Zhou Minqi, East China Normal University(mqzhou@sei.ecnu.edu.cn) This PC member has been a co-worker in the same company or university within the past two years.; Xiaokui Xiao, Nanyang Technological University(xkxiao@ntu.edu.sg) This PC member has been a collaborator within the past two years.; Anthony Tung, National University of Singapore(anthony@comp.nus.edu.sg) This PC member is or was an author's primary thesis advisor, no matter how long ago.","Elastic Pipelining.pdf (704,394 bytes)",,I Agree173,ERMIA: Fast memory-optimized database system for heterogeneous workloads,Research Subtrack 2,"Large main memories and massively parallel processors have triggered not only a resurgence of high-performance memory- and multicore-optimized transaction processing systems, but also an increasing demand for processing heterogeneous workloads that include read-mostly transactions. Many modern transaction processing systems adopt a lightweight optimistic concurrency control (OCC) scheme to leverage its low overhead in low contention workloads. However,weobservethatthelightweightOCCisnotsuitable for heterogeneous workloads, causing signiﬁcant starvation of readmostly transactions and overall performance degradation. In this paper, we present ERMIA, a memory-optimized database system built from scratch to cater the need of handling heterogeneous workloads. ERMIA adopts snapshot isolation concurrency control to coordinate heterogeneous transactions and provides serializability when desired. Its physical layer supports the concurrency control schemes in a scalable way. Experimental results show that ERMIA delivers comparable or superior performance and nearlinear scalability in a variety of workloads, compared to a recent lightweight OCC-based system. At the same time, ERMIA maintains high throughput on read-mostly transactions when the performance of the OCC-based system drops by orders of magnitude. ","Kangnyeon Kim*, University of Toronto; Tianzheng Wang, University of Toronto; Ryan Johnson, LogicBlox; Ippokratis Pandis, Cloudera",knkim@cs.toronto.edu; tzwang@cs.toronto.edu; ryan.johnson@logicblox.com; ippokratis@cloudera.com,Transaction processing*,"Ippokratis Pandis, Amazon Web Services(ippokratis@gmail.com) This PC member is a co-author of the paper.; Todd Green, LogicBlox(todd.green@logicblox.com) This PC member has been a co-worker in the same company or university within the past two years.; Nick Koudas, University of Toronto(koudas@cs.toronto.edu) This PC member has been a co-worker in the same company or university within the past two years.; Vijayshankar Raman, IBM Research - Almaden(ravijay@us.ibm.com) This PC member has been a co-worker in the same company or university within the past two years.; Srikanta Bedathur, IBM Research(sbedathur@in.ibm.com) This PC member has been a co-worker in the same company or university within the past two years.; Mahashweta Das, HP Labs(mahashweta@gmail.com) This PC member has been a co-worker in the same company or university within the past two years.; Avrilia Floratou, IBM Almaden(aflorat@us.ibm.com) This PC member has been a co-worker in the same company or university within the past two years.; Oktie Hassanzadeh, IBM Research(hassanzadeh@us.ibm.com) This PC member has been a co-worker in the same company or university within the past two years.; Eser Kandogan, IBM Research - Almaden(eser@us.ibm.com) This PC member has been a co-worker in the same company or university within the past two years.; Umar Farooq Minhas, IBM Research(ufminhas@us.ibm.com) This PC member has been a co-worker in the same company or university within the past two years.; Yunyao Li, IBM Research - Almaden(yunyaoli@us.ibm.com) This PC member has been a co-worker in the same company or university within the past two years.","ermia.pdf (469,174 bytes)",,I Agree870,Operator and Query Progress Estimation in Microsoft SQL Server Live Query Statistics,Industrial,We describe the design and implementation of the new Live Query Statistics feature in Microsoft SQL Server 2016. The functionality includes  overall query progress as well as progress of  individual operators in the query execution plan. Estimating overall progress of execution for a SQL query can be valuable to help decide whether the query should be terminated or allowed to run to completion.  Estimating progress of individual operators in the plan is often helpful in troubleshooting query performance. We build upon the extensive prior work on the query progress estimation problem and introduce new refinements that significantly improve accuracy of progress estimation. We report results of experiments on synthetic benchmarks and real customer workloads that demonstrate the importance of our new techniques and reveals difficult cases that remain open.,"Kukjin Lee, Microsoft Research; Christian Konig*, Microsoft Research; Vivek Narasayya, Microsoft Research; Bolin Ding, Microsoft; Surajit Chaudhuri, Microsoft; Brent Ellwein, Microsoft SQL Server; Alexey Eksarevskiy, Microsoft SQL Server; Manbeen  Kohli, Microsoft; Praneeta Prakash, Microsoft; Jacob Wyant, Microsoft; Jiexing Li, University of Wisconsin, Madison; Rimma Nehme, Microsoft; Jeffrey Naughton, University of Wisconsin-Madison",kulee@microsoft.com; chrisko@microsoft.com; viveknar@microsoft.com; bolind@microsoft.com; surajitc@microsoft.com; brellwei@microsoft.com; alexek@microsoft.com; makohli@microsoft.com; pprakash@microsoft.com; jacobwy@microsoft.com; jxli@cs.wisc.edu; rimman@microsoft.com; naughton@cs.wisc.edu,Database monitoring and tuning*; Query processing and optimization,"Philip Bernstein, Microsoft Research(phil.bernstein@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Nico Bruno, ""Google,""(nicolas@cs.columbia.edu) This PC member has been a co-worker in the same company or university within the past two years.; Jingren Zhou, Alibaba(jingren.zhou@alibaba-inc.com) This PC member has been a co-worker in the same company or university within the past two years.; Karthik Ramachandra, Microsoft Jim Gray Systems Lab(karam@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.","qprog.pdf (3,832,590 bytes)",,872,Page As You Go: Piecewise Columnar Access In SAP HANA,Industrial,"In-memory columnar databases such as SAP HANA achieve extreme performance by means of vector processing over logical units of main memory resident columns. The core in-memory algorithms can be challenged when the working set of an application does not fit into main memory. To deal with memory pressure, most in-memory columnar databases evict candidate columns (or tables) using a set of heuristics gleaned from recent workload. As an alternative approach, we propose to reduce the unit of load and eviction from column to a contiguous portion of the in-memory columnar representation, which we call a page. In this paper, we adapt the core algorithms to be able to operate with partially loaded columns while preserving the performance benefits of vector processing. Our approach has two key advantages. First, partial column loading reduces the mandatory memory footprint for each column, making more memory available for other purposes. Second, partial eviction extends the in-memory lifetime of partially loaded column. We present a new in-memory columnar implementation for our approach, that we term page loadable column. We design a new persistency layout and access algorithms for the encoded data vector of the column, the order-preserving dictionary, and the inverted index. We compare the performance attributes of page loadable columns with those of regular in-memory columns and present a use-case for page loadable columns for cold data in data aging scenarios. Page loadable columns are completely integrated in SAP HANA, and we present extensive experimental results that quantify the performance overhead and the resource consumption when these columns are deployed. ","Reza Sherkat*, SAP ; Colin Florendo, SAP SE; Mihnea Andrei, SAP; Anil Goel, SAP; Anisoara Nica, SAP SE; Peter Bumbulis, SAP; Ivan Schreter, SAP SE; Guenter Radestock, SAP SE; Christian  Bensberg, SAP SE; Daniel Booss, SAP SE; Heiko Gerwens, SAP SE",reza.sherkat@sap.com; colin.florendo@sap.com; mihnea.andrei@sap.com; anil.goel@sap.com; anisoara.nica@sap.com; peter.bumbulis@sap.com; ivan.schreter@sap.com; guenter.radestock@sap.com; christian.bensberg@sap.com; daniel.booss@sap.com; heiko.gerwens@sap.com,"Data warehousing, OLAP, SQL Analytics*; Query processing and optimization","Shel Finkelstein, UCSC(shelfin68@yahoo.com) This PC member has been a co-worker in the same company or university within the past two years.","pa_paper.pdf (795,450 bytes)",,873,Goods: Organizing Google's Datasets,Industrial,"Enterprises increasingly rely on structured datasets to run their businesses. These datasets take a variety of forms, such as structured files, databases, spreadsheets, or even services that provide access to the data.  The datasets often reside in different storage systems, may vary in their formats, may change every day.  In this paper, we present \goods, a project to rethink how we organize structured datasets at scale, in a setting where teams use diverse and often idiosyncratic ways to produce the datasets and where there is no centralized system for storing and querying them.  \goods\ extracts metadata ranging from salient information about each dataset (owners, timestamps, schema) to relationships among datasets, such as similarity and provenance.  It then exposes this metadata through services that allow engineers to find datasets within the company, to monitor datasets, to annotate them in order to enable others to use their datasets, and to analyze relationships between them.  We discuss the technical challenges that we had to overcome in order to crawl and infer the metadata for billions of datasets, to maintain the consistency of our metadata catalog at scale, and to expose the metadata to users. We believe that many of the lessons that we learned are applicable to building large-scale enterprise-level data-management systems in general.","Sudip Roy*, Google Inc.; Neoklis Polyzotis, Google Research; Natasha Noy, Google Research; Steven Whang, Google Research; Christopher Olston, Google; Alon Halevy, ; Flip Korn, ",sudipr@google.com; npolyzotis@google.com; noy@google.com; swhang@google.com; olston@google.com; alon.halevy@recruitusainc.com; flip@google.com,None of the above*,"Brian Cooper, ""Google,""(brianfrankcooper@gmail.com) This PC member has been a co-worker in the same company or university within the past two years.; Nico Bruno, ""Google,""(nicolas@cs.columbia.edu) This PC member has been a co-worker in the same company or university within the past two years.","main.pdf (295,385 bytes)",,874,Closing the Schema and Performance Gap between SQL and NoSQL ,Industrial,"Oracle release 12cR1 supports JSON data management that enables users to store, index and query JSON data along with relational data. The integration of the JSON data model into the RDBMS allows a new paradigm of data management where data is storable, indexable and queryable without upfront schema definition. We call this new paradigm Flexible Schema Data Management (FSDM). In this paper, we present enhancements to Oracle's JSON data management in the upcoming 12cR2 release. We present JSON DataGuide, an auto-computed dynamic soft schema for JSON collections that closes the functional gap between the fixed-schema SQL world and the schema-less NoSQL world.  We present a self-contained query friendly binary format for encoding JSON (OSON) to close the query performance gap between schema-encoded relational data and schema free JSON textual data. The addition of these new features makes the Oracle RDBMS well suited to both fixed-schema SQL and flexible-schema NoSQL use cases, and allows users to freely mix the two paradigms in a single data management system.","Zhen Hua Liu*, Oracle; Beda Hammerschmidt, oracle; doug. Mcmahon, ; ying Lu, ; hui Chang, ",zhen.liu@oracle.com; beda.hammerschmidt@oracle.com; doug.mcmahon@oracle.com; ying.lu@oracle.com; hui.x.zhang@oracle.com,None of the above*,"Till Westmann, Oracle(till.westmann@oracle.com) This PC member has been a co-worker in the same company or university within the past two years.; Guy Lohman, IBM Research - Almaden(lohmang@us.ibm.com) This PC member is a relative or close personal friend of an author.","close-sql-nosql-zliu.pdf (640,079 bytes)",,882,ParTime: Parallel Temporal Aggregation,Industrial,"This paper presents ParTime, a parallel algorithm for temporal aggregation. Temporal aggregation is one of the most important, yet most complex temporal query operators.    It has been extensively studied in the past, but so far there has only been one attempt to parallelize this operator. ParTime supports data parallelism and has a number of additional   advantages: It supports the full bi-temporal data model, it requires no a-priori indexing, it supports shared computation, and it runs well on modern hardware (i.e., NUMA machines with large main memories).  We implemented ParTime in a parallel database system and carried out comprehensive performance experiments with a real workload from the airline industry and a synthetic benchmark, the TPC-BiH benchmark.  The results show that ParTime significantly outperforms any other available temporal database system. Furthermore,  the results show that ParTime is competitive as compared to the Timeline Index, the best known technique to process temporal queries from the research literature and which is based on pre-computation and indexing.    ","Markus Pilman*, ETH Zurich; Florian Köhl, ETH Zurich; Martin Kaufmann, ETH Zurich; Donald Kossmann, Microsoft Research; Damien Profeta, Amadeus Group",mpilman@inf.ethz.ch; krabat96@gmail.com; martin.kaufmann@inf.ethz.ch; donaldk@microsoft.com; damien.profeta@amadeus.com,"Distributed and parallel databases*; Data warehousing, OLAP, SQL Analytics; Query processing and optimization; Storage, indexing, and physical database design","Philip Bernstein, Microsoft Research(phil.bernstein@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Philip Bernstein, Microsoft Research(phil.bernstein@microsoft.com) This PC member has been a collaborator within the past two years.","paper.pdf (1,199,501 bytes)",,883,Automated Demand-driven Resource Scaling in Relational Database-as-a-Service,Industrial,"Relational Database-as-a-Service (DaaS) platforms today support the abstraction of a resource container that guarantees a fixed amount of resources. Tenants of the platform are responsible to select a container size suitable for their workloads, which they can change to leverage the cloud’s elasticity. However, automating this task is daunting for most tenants since workloads and resource requirements can vary significantly within minutes to hours, and container sizes vary by orders of magnitude both in the amount of resources as well as monetary cost. We present a solution to enable a DaaS to auto-scale container sizes on behalf of its tenants. A key problem we address is estimating resource demands for arbitrary SQL workloads in an RDBMS. Approaches to auto-scale stateless services, such as web servers, that rely on historical resource utilization as the primary signal often perform poorly in stateful database servers which are significantly more complex. We derive a set of robust signals from database engine telemetry and show how to combine these signals to significantly improve accuracy of demand estimation for database workloads. Our solution raises the abstraction for tenants by allowing them to reason about monetary budget and query latency rather than resource provisioning. We prototyped our approach in Microsoft Azure SQL Database and ran extensive experiments using workloads with realistic time-varying resource demand patterns obtained from production traces. Compared to an approach that uses only resource utilization to estimate demand, our approach results in 1.5_ to 3_ lower monetary costs while achieving comparable query latencies.","Sudipto Das*, Microsoft Research; Feng Li, Microsoft Research; Vivek Narasayya, Microsoft Research; Christian Konig, Microsoft Research",sudiptod@microsoft.com; fenl@microsoft.com; viveknar@microsoft.com; chrisko@microsoft.com,Database monitoring and tuning*; Performance and Benchmarking,"Philip Bernstein, Microsoft Research(phil.bernstein@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Chris Douglas, Microsoft(cdoug@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Jingren Zhou, Alibaba(jingren.zhou@alibaba-inc.com) This PC member has been a co-worker in the same company or university within the past two years.; Karthik Ramachandra, Microsoft Jim Gray Systems Lab(karam@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Konstantinos Karanasos, Microsoft(kokarana@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Nico Bruno, ""Google,""(nicolas@cs.columbia.edu) This PC member is a relative or close personal friend of an author.","elastic_scaling.pdf (1,058,876 bytes)",,885,Hybrid Garbage Collection for Multi-Version Concurrency Control in SAP HANA,Industrial,"While multi-version concurrency control (MVCC) supports fast and robust performance in in-memory, relational databases, it has the potential problem of a growing number of versions over time due to obsolete versions. Although a few TB of main memory is available for enterprise machines, the memory resource should be used carefully for economic and practical reasons. Thus, in order to maintain the necessary number of versions in MVCC, versions which will no longer be used need to be deleted. This process is called garbage collection. MVCC uses the concept of \emph{visibility} to define garbage. A set of versions for each record is first identified as candidate if their version timestamps are lower than the minimum value of snapshot timestamps of active snapshots in the system. All such candidates, except the one which has the maximum version timestamp, are safely reclaimed as garbage versions. In mixed  OLTP and OLAP workloads, the typical garbage collector may not effectively reclaim record versions. In these workloads, OLTP applications generate a high volume of new versions, while long-lived queries or transactions in OLAP applications often block garbage collection, since we need to compare the version timestamp of each record version with the snapshot timestamp of the oldest, long-lived snapshot. Thus, these workloads typically cause the in-memory version space to grow. Additionally,  the increasing version chains of records over time may also increase the traversal cost for them.  In this paper, we present an efficient and effective garbage collector called {\HybridGC} in SAP HANA.  HybridGC integrates three novel concepts of garbage collection: timestamp-based group garbage collection, table garbage collection, and interval garbage collection.  Through experiments using mixed OLTP and OLAP workloads, we show that {\HybridGC} effectively and efficiently collects garbage versions with negligible overhead.","Juchang Lee, SAP Laps, Korea; Hyungyu Shin, ; Chang Gyoo Park, SAP Laps, Korea; Seongyun Ko, POSTECH; Yongjae Chuh, ; Jaeyun Noh, ; Wolfgang Stephan, SAP AG, Germany; Wook-Shin Han*, POSTECH",juc.lee@sap.com; hgshin@dblab.postech.ac.kr; chang.gyoo.park@sap.com; syko@dblab.postech.ac.kr; yongjae.chuh@sap.com; jaeyun.noh@sap.com; wolfgang.stephan@sap.com; wshan.postech@gmail.com,Transaction processing*,"Shel Finkelstein, UCSC(shelfin68@yahoo.com) This PC member has been a co-worker in the same company or university within the past two years.; Guy Lohman, IBM Research - Almaden(lohmang@us.ibm.com) This PC member is a relative or close personal friend of an author.","hybrid-gc.pdf (6,210,209 bytes)",,887,Towards a Hybrid Design for Fast Query Processing in DB2 with BLU Acceleration Using Graphical Processing Units: A Technology Demonstration,Industrial,"In this paper, we show how we use Nvidia GPUs and host CPU cores for faster query processing in a DB2 database using BLU Acceleration (DB2's column store technology). Moreover, we show the benefits and problems of using hardware accelerators (more specifically GPUs) in a real commercial Relational Database Management System (RDBMS). We investigate the effect of off-loading specific database operations to a GPU, and show how doing so results in a significant performance improvement. We then demonstrate that for some queries, using just CPU to perform the entire operation is more beneficial. While we use some of Nvidia's fast kernels for operations like sort, we have also developed our own high performance kernels for operations such as group by and aggregation. Finally, we show how we use a dynamic design that can make use of optimizer metadata to intelligently choose a GPU kernel to run. For the first time in the literature, we use benchmarks representative of customer environments to gauge the performance of our prototype, the results of which show that we can get a speed increase upwards of 2x, using a realistic set of queries. ","Sina Meraji*, IBM; Berni Schiefer, ; Lan Pham, ; Lee Chu, ; Peter Kokosielis, ; Adam Storm, ; Wayne Young, ; Chang Ge, ; Geoffrey Ng, ; Kajan Kanagaratnam, ",sinamera@ca.ibm.com; schiefer@ca.ibm.com; lpham@ca.ibm.com; leechu@ca.ibm.com; pkolosie@ca.ibm.com; ajstorm@ca.ibm.com; wjyoung@ca.ibm.com; changge@ca.ibm.com; Geoffrey@ca.ibm.com; kajanak@ca.ibm,Query processing and optimization*; Distributed and parallel databases; Performance and Benchmarking; Transaction processing,"Guy Lohman, IBM Research - Almaden(lohmang@us.ibm.com) This PC member has been a co-worker in the same company or university within the past two years.","sigmod.pdf (693,531 bytes)",,889,Potential and Pitfalls of Domain-Specific Information Extraction at Web Scale ,Industrial,"In many domains, a plethora of textual information is available on the web as news reports, blog posts, community portals, etc. Information extraction (IE) is the default technique to turn unstructured text into structured fact databases, but systematically applying IE techniques to web input requires highly complex systems, starting from focused crawlers over quality assurance methods to cope with the HTML input to long pipelines of natural language processing and IE algorithms. Although a number of tools for each of these steps exists, their seamless, flexible, and scalable combination into a web scale end-to-end text analytics system still is a true challenge.  In this paper, we report our experiences from building such a system for comparing the ""web view"" on health related topics with that derived from a controlled scientific corpus, i.e., Medline. The system combines a focused crawler, applying shallow text analysis and classification to maintain focus, with a sophisticated text analytic engine inside the Big Data processing system Stratosphere. We describe a practical approach to seed generation which led us crawl a corpus of ~1 TB web pages highly enriched for the biomedical domain. Pages were run through a complex pipeline of best-of-breed tools for a multitude of necessary tasks, such as HTML repair, boilerplate detection, sentence detection, linguistic annotation, parsing, and eventually named entity recognition for several types of entities. Results are compared with those from running the same pipeline (without the web-related tasks) on a corpus of 24 million scientific abstracts and a third corpus made of ~250K scientific full texts. We evaluate scalability, quality, and robustness of the employed methods and tools. The focus of this paper is to provide a large, real-life use case to inspire future research into robust, easy-to-use, and scalable methods for domain-specific IE at web scale. ","Astrid Rheinländer*, Humboldt-Universität zu Berlin; Mario Lehmann, Humboldt-Universität zu Berlin; Anja Kunkel, Humboldt-Universität zu Berlin; Jörg Meier, Humboldt-Universität zu Berlin; Ulf Leser, Humboldt Universität zu Berlin",rheinlae@informatik.hu-berlin.de; lehmann@informatik.hu-berlin.de; kunkel@informatik.hu-berlin.de; meier@informatik.hu-berlin.de; leser@informatik.hu-berlin.de,Information retrieval and text mining*; Distributed and parallel databases; Information extraction,,"document.pdf (1,318,485 bytes)",,892,Ambry: LinkedIn's Scalable Geo-Distributed Object Store,Industrial,"The infrastructure beneath a worldwide social network has to serve billions of variable-sized media objects such as photos, videos, and audios, continually.  These objects must be stored and served with low latency and high throughput by a system that is geo-distributed, highly scalable, and load-balanced. Existing file systems and object stores face several challenges when serving such large objects. We present Ambry, a production-quality system for storing large immutable data (called blobs). Ambry is designed in a decentralized way and leverages techniques such as logical blob grouping, asynchronous replication, rebalancing mechanisms, zero-cost failure detection, and OS caching. Ambry has been running in LinkedIn's production environment for the past 2 years, serving up to 10K requests per second across more than 400 million users. Our experimental evaluation reveals that Ambry offers high efficiency (utilizing up to 88\% of the network bandwidth), responsiveness with low latency (less than 50 ms for a 1 MB object), and load balancing (improving the imbalance of requests by 8x-10x).","Shadi Abdollahian Noghabi*, University of Illinois ; Sriram Subramanian, LinkedIn Corp; Priyesh Narayanan, LinkedIn Corp; Sivabalan Narayanan, LinkedIn Corp; Gopalakrishna Holla, LinkedIn Corp; Mammad Zadeh, LinkedIn Corp; Tianwei Li, LinkedIn Corp; Indranil Gupta, University of Illinois at Urbana-Champaign; Roy Campbell, University of Illinois at Urbana-Champaign",abdolla2@illinois.edu; srsubramanian@linkedin.com; pnarayanan@linkedin.com; snarayanan@linkedin.com; gholla@linkedin.com; mzadeh@linkedin.com; tili@linkedin.com; indy@illinois.edu; rhc@illinois.edu,"Distributed and parallel databases*; Graph data management, RDF, social networks; Storage, indexing, and physical database design",,"ambry.pdf (1,506,386 bytes)",,898,Datometry Hyper-Q: Bridging the Gap Between Real Time and Historical Analytics,Industrial,"Wall Street’s trading engines are complex database applications written for time series databases like kdb+ that uses the query language Q to perform real-time analysis. Extending the models to include other data sources, e.g., historic data, is critical for backtesting and compliance. However, Q applications cannot run directly on SQL databases. Therefore, financial institutions face the dilemma of either maintaining two separate application stacks, one written in Q and the other in SQL, which means increased IT cost and increased risk, or migrating all Q applications to SQL, which results in losing the inherent competitive advantage on Q real-time processing. Neither solution is desirable as both alternatives are costly, disruptive, and suboptimal.  In this paper we present Hyper-Q, a data virtualization platform that overcomes the chasm. Hyper-Q enables Q applications to run natively on PostgreSQL-compatible databases by translating queries and results on the fly. We outline the basic concepts, detail specific difficulties, and demonstrate the viability of the approach with a case study.","Lyublena Antova, Datometry Inc.; Rhonda Baldwin, Datometry Inc.; Derrick Bryant, Datometry; Tuan Cao, Datometry Inc.; Michael Duller, Datometry Inc.; John Eshleman, Datometry Inc.; Zhongxian Gu, Datometry Inc.; Entong Shen, Datometry; Mohamed Soliman*, Datometry Inc.; F. Michael Waas, Datometry Inc.",lyublena@datometry.com; rhonda@datometry.com; dbryant@datometry.com; tuancao@datometry.com; michael.duller@datometry.com; jesh@datometry.com; zgu@datometry.com; eshen@datometry.com; mohamed@datometry.com; mike@datometry.com,"Data warehousing, OLAP, SQL Analytics*; Data models, semantics, query languages; Distributed and parallel databases; Query processing and optimization","Brian Cooper, ""Google,""(brianfrankcooper@gmail.com) This PC member has been a co-worker in the same company or university within the past two years.; Nico Bruno, ""Google,""(nicolas@cs.columbia.edu) This PC member has been a co-worker in the same company or university within the past two years.; Till Westmann, Oracle(till.westmann@oracle.com) This PC member has been a co-worker in the same company or university within the past two years.; Philip Bernstein, Microsoft Research(phil.bernstein@microsoft.com) This PC member is a relative or close personal friend of an author.; Karthik Ramasamy, Twitter(kramasamy@twitter.com) This PC member is a relative or close personal friend of an author.","paper898.pdf (1,191,626 bytes)",,900,The Snowflake Elastic Data Warehouse,Industrial,"We live in the golden age of distributed computing. Public cloud platforms now offer virtually unlimited compute and storage resources on demand. At the same time, the Software-as-a-Service (SaaS) model brings enterprise-class systems to users who previously could not afford such systems due to their cost and complexity. Alas, traditional data warehousing systems are struggling to fit into this new environment. For one thing, they have been designed for fixed resources and are thus unable to leverage the cloud's elasticity. For another thing, their dependence on complex ETL pipelines and physical tuning is at odds with the flexibility and freshness requirements of the cloud's new types of semi-structured data and rapidly evolving workloads.  We decided a fundamental redesign was in order. Our mission was to build an enterprise-ready data warehousing solution for the cloud. The result is the Snowflake Elastic Data Warehouse, or ""Snowflake"" for short. Snowflake is a multi-tenant, transactional, secure, highly scalable and elastic system with full SQL support and built-in extensions for semi-structured and schema-less data. The system is offered as a pay-as-you-go service in the Amazon cloud. Users upload their data to the cloud and can immediately manage and query it using familiar tools and interfaces. Implementation began in late 2012 and Snowflake has been generally available since June 2015. Today, Snowflake is used in production by a growing number of small and large organizations alike. The system runs several million queries per day over multiple petabytes of data.  In this paper, we describe the design of Snowflake and its novel multi-cluster, shared-data architecture. The paper highlights some of the key features of Snowflake: extreme elasticity and availability, semi-structured and schema-less data, time travel, and end-to-end security. It concludes with lessons learned and an outlook on ongoing work.","Benoit Dageville, Snowflake Computing; Thierry Cruanes, Snowflake Computing; Marcin Zukowski, Snowflake Computing; Vadim Antonov, Snowflake Computing; Artin Avanes, Snowflake Computing; Jon Bock, Snowflake Computing; Jonathan Claybaugh, Snowflake Computing; Daniel Engovatov, Snowflake Computing; Martin Hentschel, Snowflake Computing; Jiansheng Huang, Snowflake Computing; Allison Lee, Snowflake Computing; Ashish Motivala, Snowflake Computing; Abdul Munir, Snowflake Computing; Steven Pelley, Snowflake Computing; Peter Povinec, Snowflake Computing; Greg Rahn, Snowflake Computing; Spyridon Triantafyllis, Snowflake Computing; Philipp Unterbrunner*, Snowflake Computing",benoit.dageville@snowflake.net; thierry.cruanes@snowflake.net; marcin@snowflake.net; vadim.antonov@snowflake.net; artin.avanes@snowflake.net; jonb@snowflake.net; jclaybaugh@snowflake.net; daniel.engovatov@snowflake.net; martin.hentschel@snowflake.net; jiansheng.huang@snowflake.net; allison.lee@snowflake.net; ashish.motivala@snowflake.net; abdul.munir@snowflake.net; steven.pelley@snowflake.net; peter.povinec@snowflake.net; greg.rahn@snowflake.net; spyros@snowflake.net; philippu@snowflake.net,"Data warehousing, OLAP, SQL Analytics*; Distributed and parallel databases","Karthik Ramachandra, Microsoft Jim Gray Systems Lab(karam@microsoft.com) This PC member has been a co-worker in the same company or university within the past two years.; Till Westmann, Oracle(till.westmann@oracle.com) This PC member is a relative or close personal friend of an author.","snowflake.pdf (349,366 bytes)",,901,VectorH: taking SQL-on-Hadoop to the next level,Industrial,"In this paper we describe VectorH: a new SQL-on-Hadoop system built on top of the fast Vectorwise analytical database system. VectorH achieves fault tolerance and scalable data storage by relying on HDFS, extending the state-of-the-art in SQL-on-Hadoop systems by instrumenting the HDFS block replication policy to ensure local reads under most circumstances. VectorH integrates with YARN for workload management, achieving a high degree of elasticity . Even though HDFS is an append-only filesystem, and it supports ordered table storage, VectorH can accommodate trickle updates through Positional Delta Trees (PDTs), a differential update structure that can be queried efficiently. We describe the main technical extensions to single-server Vectorwise that turned it into a Hadoop-based  MPP system, in terms of workload management, parallel query optimization and execution, HDFS storage, transaction processing and Spark integration. In the evaluation section we compare VectorH with HAWQ, Impala, SparkSQL and Hive, showing orders of magnitude better  performance than these competitors. ","Andrei Costea, Actian Corp.; Adrian Ionescu, Actian Corp.; Bogdan Raducanu, Actian Corp.; Michal Switakowski, Actian Corp.; Cristian Barca, Actian Corp.; Juliusz Sompolski, Actian Corp.; Alicja Luszczak, Actian Corp.; Michal Szafranski, Actian Corp.; Giel De Nijs, Actian Corp.; Peter Boncz*, CWI Amsterdam",Andrei.Costea@actian.com; adrian.ionescu@actian.com; bogdan.rdc@gmail.com; michal.switakowski@actian.com; cristian.barca@actian.com; juliusz.sompolski@actian.com; alicja.luszczak@actian.com; michal.szafranski@actian.com; giel.denijs@actian.com; p.boncz@cwi.nl,"Distributed and parallel databases*; Benchmarking and performance evaluation; Data warehousing, OLAP, SQL Analytics; Performance and Benchmarking; Storage, indexing, and physical database design; Transaction processing",,"vortex.pdf (630,721 bytes)",,916,Real-time Video Recommendation Exploration,Industrial,"Video recommendation has attracted growing attention in recent years. However, conventional techniques have limitations in real-time processing, accuracy or scalability for the large-scale video data. To address the deficiencies of current recommendation systems, we introduce some new techniques to provide real-time and accurate recommendations to users in the video recommendation system of Tencent Inc.  We develop a scalable online collaborative filtering algorithm based upon matrix factorization, with an adjustable updating strategy considering implicit feedback solution of different user actions. To select high-quality candidate videos for real-time top-N recommendation generation, we utilize additional factors like video type and time factor to compute similar videos. In addition, we propose the scalable implementation of our algorithm together with some optimizations to make the recommendations more efficient and accurate, including the demographic filtering and demographic training. To demonstrate the effectiveness and efficiency of our model, we conduct comprehensive experiments by collecting real data from Tencent Video. Furthermore, our video recommendation system is in production to provide recommendation services in Tencent Video, one of the largest video sites in China, and verifies its superiority in performance.","Yanxiang Huang*, Peking University; Bin Cui, Peking U.; Jie Jiang, Tencent; Kunqiang Hong, Tencent Inc.; Wenyu Zhang, Tencent Inc.; Yiran Xie, Peking University",yx.huang@pku.edu.cn; bin.cui@pku.edu.cn; zeus@tencent.com; kontenhong@tencent.com; gabyzhang@tencent.com; xie.yiran@pku.edu.cn,None of the above*,,"video.pdf (2,728,042 bytes)",,919,Building the Enterprise Fabric for Big Data with Vertica and Spark Integration,Industrial,"Enterprise customers are increasingly requiring greater flexibility in the way they access and process their Big Data while at the same time they continue to request advanced analytics and access to diverse data sources. Yet they still require the robustness of enterprise class analytics for their mission-critical data.  In this abstract, we present our initial efforts toward a solution that satisfies the above requirements by integrating the HPE Vertica enterprise database with Apache Spark's open source big data computation engine. In particular, it enables transferring data between Vertica and Spark and deploying ML models created by Spark into Vertica for predictive analytics on Vertica data. This integration provides a fabric on which our customers get the best of both worlds: it extends Vertica's extensive SQL analytics capabilities with Spark's machine learning library (MLlib), giving Vertica users access to a wide range of ML functions; it also enables customers to leverage Spark as an advanced ETL engine for all data stored in their Spark cluster that require the guarantees offered by Vertica. ","Jeff LeFevre*, HPE; Rui Liu, HPE; Edward Ma, Hewlett Packard Enterprise; Malu Castellanos, HPE; Cornelio Inigo, HPE; Lupita Paz, HPE; Meichun Hsu, HPE",jeff.lefevre@hpe.com; r.liu@hpe.com; ema@hpe.com; malu.castellanos@hpe.com; cornelio.inigo@hpe.com; lupita.paz@hpe.com; meichun.hsu@hpe.com,"Distributed and parallel databases*; Data warehousing, OLAP, SQL Analytics",,"paper-j.pdf (353,051 bytes)",,890,Multi-Source Uncertain Entity Resolution at Yad Vashem: Transforming Holocaust Victim Reports into People,Industrial,"In this work we describe an entity resolution project performed at Yad Vashem, the central repository of holocaust-era information. The Yad Vashem dataset is unique with respect to classic entity resolution, by virtue of being both massively multi-source and by requiring multi-level entity resolution. With the abundance of information sources today, this project sets an example for future tasks requiring multi-source resolution on a big-data scale. We explore the benefits and shortcomings of using the MFIBlocks entity resolution algorithm in achieving the goals of the application. In particular, we provide a machine learning approach, based upon decision trees to transform soft clusters into probabilistic clustering of records into entities.    An extensive empirical evaluation demonstrates the unique properties of this dataset, highlighting the shortcomings of current methods and proposing avenues for future research in this realm.  ","Tomer Sagi*, HP Labs Israel; Avigdor Gal, Technion; Omer Barkol, Hewlett Packard Enterprise; Ruth Bergman, Hewlett Packard Labs; Alexander Avram, Yad Vashem",tomer.sagi@hpe.com; avigal@ie.technion.ac.il; omer.barkol@hpe.com; ruth.bergman@hpe.com; alexander.avram@yadvashem.org.il,"Schema matching, data integration, and data cleaning*; Uncertain, probabilistic, and approximate databases",,"yv_industrial_sigmod.pdf (1,636,569 bytes)",,899,SparkR: Scaling R Programs with Spark,Industrial,"R is a popular statistical programming language with a number of extensions that support data processing and machine learning tasks.  However, interactive data analysis in R is usually limited as the R runtime is single threaded and can only process data sets that fit in a single machine’s memory. We present SparkR, an R package that provides a frontend to Apache Spark and uses Spark’s distributed computation engine to enable large scale data analysis from the R shell. We describe the main design goals of SparkR, discuss how the high-level DataFrame API enables scalable computation and present some of the key details of our implementation.","Shivaram Venkataraman*, UC Berkeley; Zongheng Yang, ; Davies Liu, Data bricks; Eric Liang, ; Hossein Falaki, Databricks; Xiangrui Meng, ; Reynold Xin, Databricks; Ali Ghodsi, ; Michael Franklin, UC Berkeley; Ion Stoica, UC Berkeley; Matei Zaharia, MIT",shivaram@cs.berkeley.edu; zhyang@berkeley.edu; davies@databricks.com; ekl@databricks.com; hossein@databricks.com; meng@databricks.com; rxin@databricks.com; ali@databricks.com; franklin@berkeley.edu; istoica@cs.berkeley.edu; matei@mit.edu,"Distributed and parallel databases*; Data models, semantics, query languages","Neil Conway, Mesosphere(neil.conway@gmail.com) This PC member has been a co-worker in the same company or university within the past two years.","sparkr_paper.pdf (180,396 bytes)",,923,Remember Where You Came From: On The Second-Order Random Walk Based Proximity Measures,"Research, November 2015 Revision","Measuring the proximity between different nodes is a fundamental problem in graph analysis. Random walk based proximity measures have been shown to be effective and widely used. Most existing random walk measures are based on the first-order Markov model, i.e., they assume that the next step of the random surfer only depends on the current node. However, this assumption neither holds in many real-life applications nor captures the clustering structure in the graph. To address the limitation of the existing first-order measures, in this paper, we study the second-order random walk measures, which take the previously visited node into consideration. While the existing first-order measures are built on node-to-node transition probabilities, in the second-order random walk, we need to consider the edge-to-edge transition probabilities. Using incidence matrices, we develop simple and elegant matrix representations for the second-order proximity measures. A desirable property of the developed measures is that they degenerate to their original first-order forms when the effect of the previous step is zero. We further develop Monte Carlo methods to efficiently compute the second-order measures and provide theoretical performance guarantees. Experimental results show that in a variety of applications, the second-order measures can dramatically improve the performance compared to their first-order counterparts.","Yubao Wu*, Case Western Reserve Universit; Yuchen Bian, Case Western Reserve University; Xiang Zhang, Case Western Reserve University",yubao.wu@case.edu; yuchen.bian@case.edu; xiang.zhang@case.edu,"Knowledge discovery, clustering, data mining*","Ruoming Jin, Kent State University(jin@cs.kent.edu) This PC member has been a collaborator within the past two years.","923.pdf (1,283,972 bytes)",,I Agree951,Scalable Approximate Query Tracking over Highly Distributed Data Streams,"Research, November 2015 Revision","The recently-proposed Geometric Monitoring (GM) method has provided a general tool for the distributed monitoring of arbitrary non-linear queries over streaming data observed by a collection of remote sites, with numerous practical applications.  Still, as we demonstrate in this paper, GM-based techniques can suffer from serious scalability issues with increasing numbers of remote sites. In this paper, we propose novel techniques that effectively tackle the aforementioned scalability problems by exploiting a carefully designed sample of the remote sites for efficient approximate query tracking. Our novel sampling-based scheme utilizes a sample of cardinality proportional to $\sqrt{N}$ (compared to $N$ for the original GM), where $N$ is the number of sites in the network, to perform the monitoring process. By means of the aforementioned sample, our experimental evaluation over a variety of real-life data streams demonstrates that our techniques can significantly reduce the communication cost during distributed monitoring with controllable, predefined accuracy guarantees.","Nikos Giatrakos*, Technical University of Crete; Antonios Deligiannakis, Technical University of Crete; Minos Garofalakis, ",ngiatrakos@softnet.tuc.gr; adeli@softnet.tuc.gr; minos@softnet.tuc.gr,"Streams, sensor networks, complex event processing*","Antonios Deligiannakis, Technical University of Crete(adeli@softnet.tuc.gr) This PC member is a co-author of the paper.; Stavros Papadopoulos, Intel Labs and MIT(stavrosp@csail.mit.edu) This PC member has been a collaborator within the past two years.","paperR1.pdf (5,740,313 bytes)",,I Agree958,Big Data Analytics with Datalog Queries on Spark,"Research, November 2015 Revision","There is great interest in exploiting the opportunity provided by cloud computing platforms for large-scale analytics.  Among these platforms, Apache Spark is growing in popularity for machine learning and graph analytics. Developing efficient complex analytics in Spark requires deep understanding of both the algorithm at hand and the Spark API or subsystem APIs (e.g., SparkSQL, GraphX).  Our BigDatalog system addresses the problem by providing concise declarative specification of complex queries amenable to efficient evaluation.  Towards this goal, we propose compilation and optimization techniques that tackle the important problem of efficiently supporting recursion in Spark.  We perform an experimental comparison with other state-of-the-art large-scale Datalog systems and verify the efficacy of our techniques and effectiveness of Spark in supporting Datalog-based analytics.","Alexander Shkapsky*, UCLA; Mohan Yang, UCLA; Matteo Interlandi, UCLA; Hsuan Chiu, ; Tyson Condie, UCLA; Carlo Zaniolo, UCLA",shkapsky@cs.ucla.edu; yang@cs.ucla.edu; minterlandi@cs.ucla.edu; cherylautumn@cs.ucla.edu; tcondie@cs.ucla.edu; zaniolo@cs.ucla.edu,Distributed and parallel databases*; Query processing and optimization,"Tyson Condie, UCLA(tcondie@cs.ucla.edu) This PC member is a co-author of the paper.; Peter Bailis, Stanford University(pbailis@cs.berkeley.edu) This PC member has been a co-worker in the same company or university within the past two years.; Barzan Mozafari, University of Michigan(mozafari@umich.edu) This PC member is or was an author's primary thesis advisor, no matter how long ago.; Kai Zeng, Microsoft(kaizeng@microsoft.com) An author is or was the PC member's primary thesis advisor, no matter how long ago.","640_revision.pdf (1,109,948 bytes)",,I Agree983,Publishing Graph Degree Distribution with Node Differential Privacy,"Research, November 2015 Revision","Graph data publishing under node-differential privacy (node-DP) is challenging due to the huge sensitivity of queries.  However, since a node in graph data oftentimes represents a person, node-DP is necessary to achieve personal data protection.  In this paper, we investigate the problem of publishing the degree distribution of a graph under node-DP by exploring the projection approach to reduce the sensitivity.  We propose two approaches based on aggregation and cumulative histogram to publish the degree distribution.  The experiments demonstrate that our approaches greatly reduce the error of approximating the true degree distribution and have significant improvement over existing works.  We also present the introspective analysis for understanding the factors of publishing degree distribution with node-DP.","Wei-Yen Day*, Purdue University; Ninghui Li, Purdue University; Min Lyu, ",wday@purdue.edu; ninghui@cs.purdue.edu; lvmin05@ustc.edu.cn,"Database security, privacy, access control*; Graph data management, RDF, social networks",,"sigmod2016_with_response.pdf (4,118,907 bytes)",,I Agree414,Augmented Sketch: Faster and More Accurate Stream Processing,Research Subtrack 7 Revision,"Approximated algorithms are often used to estimate the frequency of items on high volume, fast data streams. The most common ones are variations of Count-Min sketch, which use sub-linear space for the count, but can produce errors in the counts of the most frequent items and can misclassify low-frequency items. In this paper, we improve the accuracy of sketch-based algorithms by increasing the frequency estimation accuracy of the most frequent items and reducing the possible misclassification of low-frequency items, while also improving the overall throughput.  Our solution, called Augmented Sketch (ASketch), is based on a pre-filtering stage that dynamically identifies and aggregates the most frequent items. Items overflowing the pre-filtering stage are processed using a conventional sketch algorithm, thereby making the solution general and applicable in a wide range of contexts. The pre-filtering stage can be efficiently implemented with SIMD instructions on multi-core machines and can be further parallelized through pipeline parallelism where the filtering stage runs in one core and the sketch algorithm runs in another core.   ASketch outperforms existing stream processing techniques in both throughput and accuracy. We also demonstrate the linear scalability of ASketch with SPMD (i.e., single program, multiple data) parallelism.","Pratanu Roy, ETH Zurich; Arijit Khan*, Nanyang Technological University; Gustavo Alonso, ETH Zurich",pratanu@inf.ethz.ch; arijit.khan@ntu.edu.sg; alonso@inf.ethz.ch,"Streams, sensor networks, complex event processing*","Francesco Bonchi, Yahoo Labs, Barcelona(bonchi@gmail.com) This PC member has been a collaborator within the past two years.; Cong Yu, Google Research(congyu@google.com) This PC member has been a collaborator within the past two years.; Chengkai Li, UT Arlington(cli@uta.edu) This PC member has been a collaborator within the past two years.","asketch-id-414.pdf (560,080 bytes)",,I Agree416,Accelerating Relational Databases by Leveraging Remote Memory and RDMA,Research Subtrack 7 Revision,"Memory is a crucial resource in relational databases (RDBMSs). When there is insufficient memory, RDBMSs are forced to use slower media such as SSDs or HDDs, which can significantly degrade workload performance. Cloud database services are deployed in data centers where network adapters supporting remote direct memory access (RDMA) at low latency and high bandwidth are becoming prevalent. We study the novel problem of how a single server RDBMS, whose memory demands exceed locally-available memory, can leverage available remote memory in the cluster accessed via RDMA to improve query performance. We expose available memory on a remote server using a lightweight file API that allows a single server RDBMS to leverage the benefits of remote memory with modest changes. We identify and implement several novel scenarios to demonstrate these benefits, and address design challenges that are crucial for efficient implementation. We implemented the scenarios in a commercial RDBMS engine and present the first end-to-end study to demonstrate benefits of remote memory for a variety of micro-benchmarks and industry-standard benchmarks. Compared to using disks when memory is insufficient, we improve the throughput and latency of queries with short reads and writes by 3X to 10X, while improving the latency of multiple TPC-H and TPC-DS queries by 2X to 100X.","Feng Li, Microsoft Research; Sudipto Das*, Microsoft Research; Manoj Syamala, Microsoft Research; Vivek Narasayya, Microsoft Research",fenl@microsoft.com; sudiptod@microsoft.com; manojsy@microsoft.com; viveknar@microsoft.com,Databases for emerging hardware*; Benchmarking and performance evaluation,"Anthony Tung, National University of Singapore(anthony@comp.nus.edu.sg) This PC member has been a co-worker in the same company or university within the past two years.; Andy Pavlo, Carnegie Mellon University(pavlo@cs.cmu.edu) This PC member has been a collaborator within the past two years.; Aaron Elmore, University of Chicago(aelmore@cs.uchicago.edu) This PC member is a relative or close personal friend of an author.","rdma_revision.pdf (1,288,511 bytes)",,I Agree418,Tornado: A System for Real-Time Iterative Analysis over Evolving Data,Research Subtrack 7 Revision,"There is an increasing demand for real-time iterative analysis over evolving data. In this paper, we propose a novel execution model to allow users to obtain timely results at given instants. We notice that a loop starting from a good initial guess usually converges fast. Hence we organize the execution of iterative methods over evolving data into a main loop and several branch loops. The main loop is responsible for the gathering of inputs and maintains the approximation to the timely results. When the precise results are requested by the user, a branch loop is forked from the main loop and iterates until convergence to obtain the precise results. Using the approximation of the main loop, the branch loop can start from a place near to the fixed-point and converges quickly to return accurate results.  As loops must gather inputs and approximate results simultaneously, we develop a novel partially asynchronous iteration model to enhance the timeliness. The partially asynchronous iteration model can achieve fine-grained updates while ensuring correctness for most iterative methods. By tuning the degree of asynchronism, users can also easily weigh the performance and fault tolerance.  Based on the proposed execution model of loops and iterations, we design and implement a prototype system named Tornado on top of Storm. Tornado provides a graph-parallel programming model which eases the programming of most real-time iterative analysis tasks. The reliability is also enhanced by provisioning efficient fault tolerance mechanisms. Empirical evaluation conducted on Tornado validates that various real-time iterative analysis tasks can improve their performance and efficiently tolerate failures with our execution model.","XIAOGANG SHI*, Peking University; Bin Cui, Peking U.; Yingxia Shao, Peking University; Yunhai Tong, Peking University",shixiaogangg@gmail.com; bin.cui@pku.edu.cn; simon0227@pku.edu.cn; yhtong@pku.edu.cn,"Streams, sensor networks, complex event processing*; Distributed and parallel databases",,"SIGMOD2016_418.pdf (1,916,044 bytes)",,I Agree419,TARDiS: Making Sense of Weak Consistency,Research Subtrack 7 Revision,"This paper presents the design and implementation of TARDiS, (Transactional Asynchronously Replicated Divergent Store), a transaction key-value store explicitly designed for weakly consistent systems. Reasoning about these systems is hard, as neither causal consistency nor per-object eventual convergence can satisfactorily allow applications to deal with the incidence of write-write conflicts. TARDiS instead exposes as its fundamental abstraction the set of conflicting branches that arise in weakly consistent systems. It achieves this through a new concurrency control mechanism: branch on conflict. On the one hand, TARDiS presents applications with the comfortable abstraction of sequential storage, keeping application logic simple. On the other, TARDiS provides them, when needed, with the tools and context necessary to merge branches atomically, when and how applications want, giving them flexibility. Since branch-on-conflict in TARDiS is fast, weakly consistent applications can benefit from adopting it not only for operations issued by different sites, but also, when appropriate, for conflicting local operations. Our results show that TARDiS successfully reduces coding complexity for weakly consistent applications while judiciously branching on conflict can improve throughput of the local site by two to six times.","Natacha Crooks*, UT Austin; Youer Pu, UT Austin; Nancy Estrada, ; Trinabh Gupta, ; Lorenzo Alvisi, UT Austin; Allen Clement, ",ncrooks@cs.utexas.edu; puyouer@cs.utexas.edu; nestrada@mpi-sws.org; trinabh@cs.utexas.edu; lorenzo@cs.utexas.edu; aclement@gmail.com,Transaction processing*; Distributed and parallel databases,"Christopher Olston, Google(olston@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Luna Dong, Google(lunadong@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Peter Pietzuch, Imperial College London(prp@doc.ic.ac.uk) This PC member has been a collaborator within the past two years.","paper.pdf (527,628 bytes)",,I Agree420,To Join or Not to Join? Thinking Twice about Joins before Feature Selection,Research Subtrack 7 Revision,"Closer integration of machine learning (ML) with data processing is a booming area in both the data management industry and academia. Almost all ML toolkits assume that the input is a single table, but many datasets are not stored as single tables due to normalization. Thus, analysts often perform key-foreign key joins to obtain features from all base tables and apply a feature selection method, either explicitly or implicitly, with the aim of improving accuracy. In this work, we show theoretically that the features brought in by such joins can usually be ignored without affecting ML accuracy significantly, i.e., we can ""avoid joins safely"". However, we then identify a subtle technical issue caused by avoiding joins that might decrease accuracy significantly in some cases, and analyze this issue theoretically. Using simulations, we validate our analysis and measure the effects of various properties of normalized data on accuracy. We apply our analysis to design easy to understand decision rules to predict when it is safe to avoid joins in order to help analysts exploit this runtime-accuracy tradeoff. Experiments with multiple real normalized datasets show that our rules are able to accurately predict when joins can be avoided safely, and in some cases, this yielded significant runtime improvements for some popular feature selection methods.","Arun Kumar*, Univ. of Wisconsin-Madison; Jeffrey Naughton, University of Wisconsin-Madison; Jignesh Patel, University of Wisconsin; Xiaojin Zhu, University of Wisconsin-Madison",arun@cs.wisc.edu; naughton@cs.wisc.edu; jignesh@cs.wisc.edu; jerryzhu@cs.wisc.edu,"Knowledge discovery, clustering, data mining*","Aaron Elmore, University of Chicago(aelmore@cs.uchicago.edu) This PC member has been a collaborator within the past two years.","SIGMOD16_Revision.pdf (4,505,624 bytes)",,I Agree421,Towards Globally Optimal Crowdsourcing Quality Management,Research Subtrack 7 Revision,"We study crowdsourcing quality management, that is, given worker responses to a set of tasks, our goal is to jointly estimate the true answers for the tasks, as well as the quality of the workers. Prior work on this problem relies primarily on applying Expectation-Maximization (EM) on the underlying maximum likelihood problem to estimate true answers as well as worker quality. Unfortunately, EM only provides a locally optimal solution rather than a globally optimal one. Other solutions to the problem (that do not leverage EM) fail to provide global optimality guarantees as well. In this paper, we focus on filtering, where tasks require the evaluation of a yes/no predicate, and rating, where tasks elicit integer scores from a finite domain. We design algorithms for finding the global optimal estimates of correct task answers and worker quality for the underlying maximum likelihood problem, and characterize the complexity of these algorithms. Our algorithms conceptually consider all mappings from tasks to true answers (typically a very large number), leveraging two key ideas to reduce, by several orders of magnitude, the number of mappings under consideration, while preserving optimality. We also demonstrate that these algorithms often find more accurate estimates than EM-based algorithms. This paper makes an important contribution towards under- standing the inherent complexity of globally optimal crowdsourcing quality management.","Akash Das Sarma*, Stanford University; Aditya Parameswaran, UIUC; Jennifer Widom, Stanford University",akashds.iitk@gmail.com; adityagp@illinois.edu; widom@stanford.edu,Crowd sourcing*,"Samuel Madden, MIT(madden@csail.mit.edu) This PC member has been a co-worker in the same company or university within the past two years.; Aaron Elmore, University of Chicago(aelmore@cs.uchicago.edu) This PC member has been a co-worker in the same company or university within the past two years.; Barzan Mozafari, University of Michigan(mozafari@umich.edu) This PC member has been a co-worker in the same company or university within the past two years.; Christopher Olston, Google(olston@google.com) This PC member has been a co-worker in the same company or university within the past two years.; Nesime Tatbul, Intel Labs and MIT(tatbul@csail.mit.edu) This PC member has been a co-worker in the same company or university within the past two years.","ReviewResponse_RevisedPaper_Merged.pdf (1,581,430 bytes)",,I Agree422,Ontological Pathfinding: Mining First-Order Knowledge from Large Knowledge Bases,Research Subtrack 7 Revision,"Recent years have seen a drastic rise in the construction of web-scale knowledge bases (e.g., Freebase, YAGO, DBPedia). These knowledge bases store structured information about real-world people, places, organizations, etc. However, due to the limitations of human knowledge and extraction algorithms, current knowledge bases are still far from complete. In this paper, we study the problem of mining first-order inference rules to facilitate knowledge expansion. We propose the Ontological Pathfinding algorithm (OP) that scales to web-scale knowledge bases via a series of parallelization and optimization techniques, including a new parallel rule mining algorithm that parallelizes multi-way join queries, a novel partitioning algorithm to break the learning tasks into smaller independent sub-tasks, and a pruning strategy to eliminate unsound and resource-consuming rules before applying them. Combining these techniques, we are able to develop a first rule learning system that scales to Freebase--the largest public knowledge base with 112 million entities and 388 million facts. We mine 36,625 inference rules in 34 hours; no existing system achieves this scale. ","Yang Chen*, University of Florida; Sean Goldberg, University of Florida; Daisy Zhe Wang, University of Florida; Soumitra Siddharth Johri, University of Florida",yang@cise.ufl.edu; sean@cise.ufl.edu; daisyw@cise.ufl.edu; Soumitra.Johri@ufl.edu,"Knowledge discovery, clustering, data mining*; Benchmarking and performance evaluation; Graph data management, RDF, social networks; Query processing and optimization",,"sigmod16.pdf (886,810 bytes)",response.pdf (124895 bytes),I Agree423,Constraint-Variance Tolerant Data Repairing,Research Subtrack 7 Revision,"Integrity constraints, guiding the cleaning of dirty data, are often found to be imprecise as well. Existing studies con- sider the inaccurate constraints that are oversimplified, and thus refine the constraints via inserting more predicates (at- tributes). We note that imprecise constraints could not only be oversimplified so that correct data are erroneously iden- tified as violations, but also could be overrefined that the constraints overfit the data and fail to identify true viola- tions. In the latter case, deleting excessive predicates may apply. To address the aforesaid oversimplified and overrefined constraint inaccuracies, in this paper, we propose to repair data by allowing a small variation (with both predicate in- sertion and deletion) on the constraints. A novel _-tolerant repair model is introduced, which returns a (minimum) data repair that satisfies at least one variant of the constraints (with constraint variation no greater than _ compared to the given constraints). To efficiently repair data among var- ious constraint variants, we propose a single round, sharing enabled approach. Results on real data sets demonstrate that our proposal can capture more accurate data repairs compared to the existing methods with/without constraint repairs.","Shaoxu Song*, Tsinghua University; Han Zhu, Tsinghua University; Jianmin Wang, Tsinghua University",sxsong@tsinghua.edu.cn; zhuhan10@tsinghua.edu.cn; jimwang@tsinghua.edu.cn,"Schema matching, data integration, and data cleaning*","Jiannan Wang, UC Berkeley(jnwang@berkeley.edu) This PC member has been a co-worker in the same company or university within the past two years.","423-239.pdf (691,372 bytes)",,I Agree424,Privacy Preserving Subgraph Matching on Large Graphs in Cloud,Research Subtrack 7 Revision,"The wide presence of large graph data and the increasing popularity of storing data in the cloud drive the needs for graph query processing on a remote cloud. But a fundamental challenge is to process user queries without compromising sensitive information. This work focuses on privacy preserving subgraph matching in a cloud server. The goal is to minimize the overhead on both cloud and client sides for subgraph matching, without compromising users’ sensitive information. To that end, we transform an original graph G into a privacy preserving graph Gk, which meets the requirement of an existing privacy model known as k-automorphism. By making use of the symmetry in a k-automorphic graph, a subgraph matching query can be efficiently answered using a graph Go, a small subset of Gk. This approach saves both space and query cost in the cloud server. In addition, we anonymize the original query graphs to protect their label information using label generalization technique. To reduce the search space for a subgraph matching query, we propose a cost model to select the more effective label combinations. The effectiveness and efficiency of our method are demonstrated through extensive experimental results on real datasets.","Zhao Chang, ; Lei Zou*, ; Feifei Li, University of Utah",pkuchangzhao@gmail.com; zoulei@pku.edu.cn; lifeifei@cs.utah.edu,"Graph data management, RDF, social networks*; Database security, privacy, access control",,"paper.pdf (424,304 bytes)",,I Agree426,TableShare: Results from a Multi-Year SQL-as-a-Service Experiment,Research Subtrack 7 Revision,"We analyze the workload from a multi-year deployment of a database-as-a-service platform targeting scientists and data scientists with minimal database experience.  Our hypothesis was that relatively minor changes to the way databases are delivered can increase their use in ad hoc analysis environments. The web-based TableShare system emphasizes easy dataset-at-a-time ingest, relaxed schemas and automatic schema inference, easy view creation and sharing, and full SQL support. We find that these features helped attract workloads typically associated with scripts and files rather than relational databases: complex analytics, routine processing pipelines, data publishing, and collaborative analysis. Quantitatively, these workloads are characterized by shorter dataset “lifetimes”, higher query complexity, and higher data complexity. We find evidence of research labs using SQL to replace scripts in one-off data analysis, pay-as-you-go schematization, and ad hoc data sharing. We find that relational models and languages are sufficient to support ad hoc analytics in science, but that the “delivery vector” for the technology strongly influences use. We conclude that a new class of relational systems emphasizing short-term, ad hoc analytics over permanent schemas can improve uptake of relational databases in attention-intensive data science contexts. Our contributions include a system design for delivering databases into these contexts, a description of a public research query workload dataset released to advance research in analytic data systems, and an initial analysis of the workload that provides evidence of new use cases undersupported in existing systems.","Shrainik Jain*, University of Washington; Bill Howe, University of Washington; Dominik Moritz, ; Ed Lazowska, University of Washington; Dan Halperin, University of Washington",shrainik@cs.washington.edu; billhowe@cs.washington.edu; domoritz@cs.washington.edu; lazowska@cs.washington.edu; dhalperi@cs.washington.edu,Database usability*; Data provenance; Scientific databases,"Aaron Elmore, University of Chicago(aelmore@cs.uchicago.edu) This PC member has been a collaborator within the past two years.","426.pdf (2,919,258 bytes)",SIGMOD 2016 Paper Revision.docx (202385 bytes),I Agree428,Scaling Multi-core OLTP Under High Contention,Research Subtrack 7 Revision,"Although significant recent progress has been made in improving the multi-core scalability of high throughput transactional database systems, modern systems still fail to achieve scalable throughput for workloads involving frequent access to highly contended data. Most of this inability to achieve high throughput is explained by the fundamental constraints involved in guaranteeing ACID --- the addition of cores results in more concurrent transactions accessing the same contended data for which access must be serialized in order to guarantee isolation. Thus, linear scalability for contended workloads is impossible. However, there exist flaws in many modern architectures that exacerbate their poor scalability, and result in throughput that is much worse than fundamentally required by the workload.  In this paper we identify two prevalent design principles that are present in many (but not all) transactional database systems that limit their multi-core scalability on contended workloads: the multi-purpose nature of the threads of execution in these systems, and the lack of advanced planning of data access. We demonstrate the deleterious results of these design principles on multi-core scalability by implementing a prototype system, Orthrus, that is motivated by the principles of separation of database component functionality and advanced planning of transactions. We find that these two principles alone result in significantly improved scalability on high-contention workloads, and an order of magnitude increase in throughput for a non-trivial subset of these contended workloads. ","Kun Ren, Yale University; Jose Faleiro*, Yale University; Daniel Abadi, Yale University",kun.ren@yale.edu; jose.faleiro@yale.edu; dna@cs.yale.edu,Transaction processing*; Databases for emerging hardware; Distributed and parallel databases,"Samuel Madden, MIT(madden@csail.mit.edu) This PC member is or was an author's primary thesis advisor, no matter how long ago.","paper.pdf (460,399 bytes)",,I Agree